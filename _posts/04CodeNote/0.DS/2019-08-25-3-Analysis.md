---
title: DS - pythonds3 - 3. Analysis
# author: Grace JyL
date: 2019-08-25 11:11:11 -0400
description:
excerpt_separator:
categories: [04CodeNote, DS]
tags:
math: true
# pin: true
toc: true
# image: /assets/img/sample/devices-mockup.png
---

- [3. Analysis](#3-analysis)
  - [3.1. Objectives](#31-objectives)
  - [3.3. Big-O Notation](#33-big-o-notation)
    - [common **order of magnitude functions**](#common-order-of-magnitude-functions)
    - [O(n) çº¿æ€§ç®—æ³•](#on-çº¿æ€§ç®—æ³•)
    - [O(ã’N) äºŒåˆ†æŸ¥æ‰¾ç®—æ³•](#on-äºŒåˆ†æŸ¥æ‰¾ç®—æ³•)
    - [`O(n ã’N`) äºŒåˆ†æŸ¥æ‰¾ç®—æ³•](#on-n-äºŒåˆ†æŸ¥æ‰¾ç®—æ³•)
    - [O($n^2$) å¯¹æ•°ç»„è¿›è¡Œæ’åºçš„ç®€å•ç®—æ³•ï¼Œä¾‹å¦‚ç›´æ¥æ’å…¥æ’åºçš„ç®—æ³•ã€‚](#on2-å¯¹æ•°ç»„è¿›è¡Œæ’åºçš„ç®€å•ç®—æ³•ä¾‹å¦‚ç›´æ¥æ’å…¥æ’åºçš„ç®—æ³•)
    - [O($n^3$) åšä¸¤ä¸ªné˜¶çŸ©é˜µçš„ä¹˜æ³•è¿ç®—](#on3-åšä¸¤ä¸ªné˜¶çŸ©é˜µçš„ä¹˜æ³•è¿ç®—)
    - [O($2^n$): æ±‚å…·æœ‰nä¸ªå…ƒç´ é›†åˆçš„æ‰€æœ‰å­é›†çš„ç®—æ³•](#o2n-æ±‚å…·æœ‰nä¸ªå…ƒç´ é›†åˆçš„æ‰€æœ‰å­é›†çš„ç®—æ³•)
    - [O(n!): æ±‚å…·æœ‰Nä¸ªå…ƒç´ çš„å…¨æ’åˆ—çš„ç®—æ³•](#on-æ±‚å…·æœ‰nä¸ªå…ƒç´ çš„å…¨æ’åˆ—çš„ç®—æ³•)
  - [3.5. Performance of Python Data Structures](#35-performance-of-python-data-structures)
  - [3.6. Lists](#36-lists)
  - [3.7. Dictionaries](#37-dictionaries)

---

# 3. Analysis

Problem Solving with Algorithms and Data Structures using Python 3

---

## 3.1. Objectives


what we really mean by computing resources.two different ways to look at this.
1. `the amount of space or memory` an algorithm requires to solve the problem.
   1. The amount of space required by a problem solution is typically dictated by the problem instance itself.
   2. Every so often, however, there are algorithms that have very specific space requirements, and in those cases we will be very careful to explain the variations.
2. `the amount of time` require to execute.
   1. â€œexecution time, running time of the algorithm.
   2. One way to measure is to do a benchmark analysis. track the actual time required for the program to compute its result.
      1. In Python, we can benchmark a function by noting the starting time and ending time with respect to the system we are using.
      2. In the time module, function called time, will return the current system clock time in seconds since some arbitrary starting point.
      3. By calling this function twice, at the beginning and at the end, and then computing the difference, we can get an exact number of seconds (fractions in most cases) for execution.


```py
# =============== 1 =================

def sum_of_n_2(n):
    start = time.time()
    the_sum = 0
    for i in range(1, n + 1):
        the_sum = the_sum + i
    end = time.time()
    return the_sum, end - start

print("Sum is %d required %10.7f seconds" % sum_of_n_2(10000))
print("Sum is %d required %10.7f seconds" % sum_of_n_2(100000))
print("Sum is %d required %10.7f seconds" % sum_of_n_2(1000000))
# Sum is 50005000     required  0.0018950 seconds
# Sum is 5000050000   required  0.0199420 seconds
# Sum is 500000500000 required  0.1948988 seconds


# =============== 2 =================


def sum_of_n_3(n):
    return (n * (n + 1)) / 2
print(sum_of_n_3(10))
# Sum is 50005000         required 0.00000095 seconds
# Sum is 5000050000       required 0.00000191 seconds
# Sum is 500000500000     required 0.00000095 seconds
# Sum is 50000005000000   required 0.00000095 seconds
# Sum is 5000000050000000 required 0.00000119 seconds
```

> First, the times recorded above are shorter than any of the previous examples.
> Second, they are very consistent no matter what the value of n.
> It appears that sum_of_n_3 is hardly impacted by the number of integers being added.

> the iterative solutions is doing more work since some program steps are being repeated.


---

## 3.3. Big-O Notation

<kbd>T(n)</kbd> is **the time it takes to solve a problem of size n**

> the time required to solve the larger case would be greater than for the smaller case.
> Our goal then is to show how `the algorithmâ€™s execution time changes` with respect to `the size of the problem`.

It turns out that the exact number of operations is not as important as determining the most dominant part of the `ğ‘‡(ğ‘›)` function.
- as the problem gets larger, some portion of the `ğ‘‡(ğ‘›)` function tends to overpower the rest.
- This dominant term is what, in the end, is used for comparison.


variable name define info.
- å­˜å‚¨ç©ºé—´
- æ‰§è¡Œæ—¶é—´: `import time: time.time()=8888888`


The **order of magnitude** function
- describes the part of `ğ‘‡(ğ‘›)` that increases the fastest as the value of `n` increases.
- **Order of magnitudeé‡è¦æ€§** is often called **Big-O notation** (for â€œorderâ€) and written as <kbd>ğ‘‚(ğ‘“(ğ‘›))</kbd>.

It provides a useful approximation to the actual number of steps in the computation.
The function `ğ‘“(ğ‘›)` provides a simple representation of the dominant part of the original `ğ‘‡(ğ‘›)`.

> example 1

`ğ‘‡(ğ‘›)=1+ğ‘›`
- The parameter n is often referred to as the â€œsize of the problem,â€
- read this as â€œT(n) is the time it takes to solve a problem of size n, namely 1 + n steps.â€
- As `n` gets large, the constant 1 will become less and less significant to the final result.
- If looking for an approximation for `ğ‘‡(ğ‘›)`, then can drop the 1 and simply say that the running time is <kbd>ğ‘‚(ğ‘›)</kbd>.
- It is important to note that the **1 is certainly significant for `ğ‘‡(ğ‘›)`**.
- However, as `n` gets large, our approximation will be just as accurate without it.


> example 2

`ğ‘‡(ğ‘›)=5ğ‘›^2+27ğ‘›+1005`.
- When `n` is small, the constant 1005 seems to be the dominant part of the function.
- However, as n gets larger
  - the `ğ‘›^2` term becomes the most important.
  - the other two terms become insignificant in the role that they play in determining the final result.
  - can ignore the other terms and focus on `5ğ‘›^2`.
  - the `coefficient 5` also becomes insignificant as `n` gets large.
- the function `ğ‘‡(ğ‘›)` has an **order of magnitude** ğ‘“(ğ‘›)=ğ‘›^2, or simply that it is <kbd>ğ‘‚(ğ‘›^2)</kbd>.

---

Sometimes the performance of an algorithm depends on the `exact values of the data` rather than simply `the size of the problem`.
- For these kinds of algorithms, characterize performance in terms of `best case`, `worst case`, or `average case` performance.
- The `worst case performance` refers to a particular data set where the algorithm performs especially poorly. Whereas a different data set for the exact same algorithm might have extraordinarily good performance. However, in most cases the algorithm performs somewhere in between these two extremes (average case).


### common **order of magnitude functions**
- decide which of these functions is the dominant part of any `ğ‘‡(ğ‘›)` function
- see how they compare with one another as n gets large.


f(n) | Name
---|---
`1`      | <kbd>Constant</kbd>    è¡¨ç¤ºç®—æ³•çš„è¿è¡Œæ—¶é—´ä¸ºå¸¸é‡
`logğ‘›`   | <kbd>Logarithmic</kbd> äºŒåˆ†æŸ¥æ‰¾ç®—æ³•
`ğ‘›`      | <kbd>Linear</kbd>
`ğ‘›logğ‘›`  | <kbd>Log Linear</kbd>
$ğ‘›^2$    | <kbd>Quadratic</kbd>   å¯¹æ•°ç»„è¿›è¡Œæ’åºçš„å„ç§ç®€å•ç®—æ³•ï¼Œä¾‹å¦‚ç›´æ¥æ’å…¥æ’åºçš„ç®—æ³•ã€‚
$ğ‘›^3$    | <kbd>Cubic</kbd>       åšä¸¤ä¸ªné˜¶çŸ©é˜µçš„ä¹˜æ³•è¿ç®—
$2^ğ‘›$    | <kbd>Exponential</kbd> æ±‚å…·æœ‰nä¸ªå…ƒç´ é›†åˆçš„æ‰€æœ‰å­é›†çš„ç®—æ³•
O(n!)    | æ±‚å…·æœ‰Nä¸ªå…ƒç´ çš„å…¨æ’åˆ—çš„ç®—æ³•

![newplot](https://i.imgur.com/wDUItRW.png)

![newplot2](https://i.imgur.com/ZE0qJ9I.png)

> when n is small, the functions are not very well defined with respect to one another.
> as n grows, there is a definite relationship and it is easy to see how they compare with one another.

ä¼˜<---------------------------<åŠ£

`O(1)<O(ã’n)<O(n)<O(n ã’n)<O(n^2)<O(2^n)<O(n!)`

å¸¸æ•°é˜¶O(1)ã€å¯¹æ•°é˜¶O(log2n)ã€çº¿æ€§é˜¶O(n)ã€çº¿æ€§å¯¹æ•°é˜¶O(nlog2n)ã€å¹³æ–¹é˜¶O(n2)ã€ç«‹æ–¹é˜¶O(n3)ã€â€¦â€¦kæ¬¡æ–¹é˜¶O(nk)ã€æŒ‡æ•°é˜¶O(2n)ã€‚

### O(n) çº¿æ€§ç®—æ³•

```java
sum=1;
// T(n)=1
// O(1): è¡¨ç¤ºç®—æ³•çš„è¿è¡Œæ—¶é—´ä¸ºå¸¸é‡


a=0;
b=1;               â‘  1+1
for (i=1;i<=n;i++) â‘¡ n
{
    s=a+b;ã€€ã€€ã€€ã€€â‘¢ n-1
    b=a;ã€€ã€€ã€€ã€€ã€€â‘£ n-1
    a=s;ã€€ã€€ã€€ã€€ã€€â‘¤ n-1
}
// è§£: T(n)=2+n+3(n-1)=4n-1= O(n).


int aFunc(int n) {
    for(int i = 0; i<n; i++) {         // 0,1,...,n-1,n: éœ€è¦æ‰§è¡Œ (n + 1) æ¬¡
        printf("Hello, World!\n");     // 0,1,...,n-1:   éœ€è¦æ‰§è¡Œ n æ¬¡
    }
    return 0;       // éœ€è¦æ‰§è¡Œ 1 æ¬¡
}
// è§£: T(n)= (n + 1 + n + 1) = 2n + 2 = O(n).
```


### O(ã’N) äºŒåˆ†æŸ¥æ‰¾ç®—æ³•

$2^t$ < n

t < log2(n)

```java
int i=1;                   // 1
while (i<=n) {             //2,3,....n
    i=i*2;                 // è®¾è¯­å¥2çš„é¢‘åº¦æ˜¯t
}
// 2^t <= n
// t <= log2(n)   
// å–æœ€å¤§å€¼t = log2(n),
// T(n) = O(log2n)


aFunc(int n) {
    for (int i = 2; i < n; i++) {  //2,3,....n:  n-1
        i *= 2;                    //å‡è®¾å¾ªç¯æ¬¡æ•°ä¸º tï¼Œåˆ™å¾ªç¯æ¡ä»¶æ»¡è¶³ 2^t < nã€‚
    }
}
// 2^t < n
// t < log2(n)
```


### `O(n ã’N`) äºŒåˆ†æŸ¥æ‰¾ç®—æ³•

```java
for(int i = 2; i < n; i++) {   // n
    int i=1;                   // 1
    while (i<=n) {
        i=i*2;                 // è®¾è¯­å¥2çš„é¢‘åº¦æ˜¯t
    }
}    
// T(n)=O(n * log2(N))
```


### O($n^2$) å¯¹æ•°ç»„è¿›è¡Œæ’åºçš„ç®€å•ç®—æ³•ï¼Œä¾‹å¦‚ç›´æ¥æ’å…¥æ’åºçš„ç®—æ³•ã€‚

```java
for (i=1;i<n;i++){            // n
    for (j=0;j<=n;j++) {      // n
        x++;   
    }
}
// T(n) = O(n^2)    
```


### O($n^3$) åšä¸¤ä¸ªné˜¶çŸ©é˜µçš„ä¹˜æ³•è¿ç®—

n^3 å¢é•¿é€Ÿåº¦è¿œè¶… n^2ï¼Œn^2 å¢é•¿é€Ÿåº¦è¿œè¶… n

```java
for(i=0;i<n;i++){           // i=m
    for(j=0;j<i;j++){       // j=(m-1)*m
        for(k=0;k<j;k++)    // k=(m-1)m-1
            x=x+2;  
    }
}
```

### O($2^n$): æ±‚å…·æœ‰nä¸ªå…ƒç´ é›†åˆçš„æ‰€æœ‰å­é›†çš„ç®—æ³•

### O(n!): æ±‚å…·æœ‰Nä¸ªå…ƒç´ çš„å…¨æ’åˆ—çš„ç®—æ³•

---


## 3.5. Performance of Python Data Structures

![Screen Shot 2020-05-26 at 00.32.28](https://i.imgur.com/3Fma2jK.png)

![Screen Shot 2020-05-26 at 00.43.30](https://i.imgur.com/1FedT9U.png)

---

## 3.6. Lists

Common programming task is to grow a list.

```py
def test1():
    l = []
    for i in range(1000):
        l = l + [i]


def test2():
    l = []
    for i in range(1000):
        l.append(i)


def test3():
    l = [i for i in range(1000)]


def test4():
    l = list(range(1000))
```

use Pythonâ€™s `timeit` module: make cross-platform timing measurements by running functions in a consistent environment and using timing mechanisms that are as similar as possible across operating systems.
- create a `Timer` object
- parameters are two Python statements.
  - The first parameter is a `Python statement that want to time`;
  - the second parameter is a `statement that will run once to set up the test`.
- The timeit module will then time how long it takes to execute the statement some number of times.
- By default timeit will try to run the statement one million times.
- When its done it returns the time as a floating point value representing the total number of seconds.
- However, since it executes the statement a million times you can read the result as the number of microseconds to execute the test one time. You can also pass timeit a named parameter called number that allows you to specify how many times the test statement is executed.
- The following session shows how long it takes to run each of our test functions 1000 times.

```py
from timeit import Timer

t1 = Timer("test1()", "from __main__ import test1")
print(f"concatenation: {t1.timeit(number=1000):15.2f} milliseconds")

t2 = Timer("test2()", "from __main__ import test2")
print(f"appending: {t2.timeit(number=1000):19.2f} milliseconds")

t3 = Timer("test3()", "from __main__ import test3")
print(f"list comprehension: {t3.timeit(number=1000):10.2f} milliseconds")

t4 = Timer("test4()", "from __main__ import test4")
print(f"list range: {t4.timeit(number=1000):18.2f} milliseconds")

# concatenation:           6.54 milliseconds
# appending:               0.31 milliseconds
# list comprehension:      0.15 milliseconds
# list range:              0.07 milliseconds
```

> In this case the statement from __main__ import test1 imports the function test1 from the __main__ namespace into the namespace that timeit sets up for the timing experiment.
> The timeit module does this because it wants to run the timing tests in an environment that is uncluttered by any stray variables you may have created, that may interfere with your functionâ€™s performance in some unforeseen way.

all of the times include some overhead for actually calling the test function,
- but we can assume that the function call overhead is identical in all four cases so we still get a meaningful comparison of the operations.
- So it would not be accurate to say that the concatenation operation takes 6.54 milliseconds but rather `the concatenation test function takes 6.54 milliseconds`.


Big-O Efficiency of Python List Operators

Operation | Big-O Efficiency
---|--
index []  | <kbd>O(1)</kbd>
index assignment  | <kbd>O(1)</kbd>
append  | <kbd>O(1)</kbd>
pop()  | <kbd>O(1)</kbd>
pop(i)  | <kbd>O(n)</kbd>
insert(i,item)  | <kbd>O(n)</kbd>
del operator  | <kbd>O(n)</kbd>
iteration  | <kbd>O(n)</kbd>
contains (in)  | <kbd>O(n)</kbd>
`get slice [x:y]`  | <kbd>O(k)</kbd>
del slice  | <kbd>O(n)</kbd>
set slice  | <kbd>O(n+k)</kbd>
reverse  | <kbd>O(n)</kbd>
concatenate  | <kbd>O(k)</kbd>
sort  | <kbd>O(n log n)</kbd>
multiply  | <kbd>O(nk)</kbd>



different times for pop.
- When pop is called on the end of the list it takes <kbd>ğ‘‚(1)</kbd>
- when pop is called on the first element in the list or anywhere in the middle it is <kbd>ğ‘‚(ğ‘›)</kbd>
  - The reason for this lies in how Python chooses to implement lists.
  - When an item is taken from the front of the list, in Pythonâ€™s implementation, all the other elements in the list are shifted one position closer to the beginning.


As a way of demonstrating this difference in performance letâ€™s do another experiment using the timeit module. Our goal is to be able to verify the performance of the pop operation on a list of a known size when the program pops from the end of the list, and again when the program pops from the beginning of the list. We will also want to measure this time for lists of different sizes. What we would expect to see is that the time required to pop from the end of the list will stay constant even as the list grows in size, while the time to pop from the beginning of the list will continue to increase as the list grows.

Listing 4 shows one attempt to measure the difference between the two uses of pop. As you can see from this first example, popping from the end takes 0.0003 milliseconds, whereas popping from the beginning takes 4.82 milliseconds. For a list of two million elements this is a factor of 16,000.

> the statement from __main__ import x. Although we did not define a function we do want to be able to use the list object x in our test. This approach allows us to time just the single pop statement and get the most accurate measure of the time for that single operation.
> Because the timer repeats 1000 times it is also important to point out that the list is decreasing in size by 1 each time through the loop. But since the initial list is two million elements in size we only reduce the overall size by 0.05%


to show that pop(0) is indeed slower than pop():

```py
pop_zero = Timer("x.pop(0)", "from __main__ import x")
pop_end = Timer("x.pop()", "from __main__ import x")

x = list(range(2000000))
print(f"pop(0): {pop_zero.timeit(number=1000):10.5f} milliseconds")

x = list(range(2000000))
print(f"pop(): {pop_end.timeit(number=1000):11.5f} milliseconds")

# pop(0):    2.09779 milliseconds
# pop():     0.00014 milliseconds
```


to validate the claim that pop(0) is ğ‘‚(ğ‘›) while pop() is ğ‘‚(1)
- look at the performance of both calls over a range of list sizes.

```py
pop_zero = Timer("x.pop(0)", "from __main__ import x")
pop_end = Timer("x.pop()", "from __main__ import x")
print(f"{'n':10s}{'pop(0)':>15s}{'pop()':>15s}")

for i in range(1_000_000, 100_000_001, 1_000_000):
    x = list(range(i))
    pop_zero_t = pop_zero.timeit(number=1000)
    x = list(range(i))
    pop_end_t = pop_end.timeit(number=1000)
    print(f"{i:<10d}{pop_zero_t:>15.5f}{pop_end_t:>15.5f}")
```

![poptime](https://i.imgur.com/30bdfeg.png)

- the list gets longer and longer
- the time it takes to pop(0) also increases
- while the time for pop stays very flat.
- This is exactly what we would expect to see for a ğ‘‚(ğ‘›) and ğ‘‚(1) algorithm.

> Some sources of error in our little experiment include the fact that there are other processes running on the computer as we measure that may slow down our code,
> That is why the loop runs the test one thousand times in the first place to statistically gather enough information to make the measurement reliable.

---

## 3.7. Dictionaries

- the `get item` and `set item` operations on a dictionary are ğ‘‚(1).
- Checking to see `whether a key is in the dictionary` or not is also ğ‘‚(1).

> the efficiencies we provide in the table are for average performance.
> In some rare cases the contains, get item, and set item operations can degenerate into ğ‘‚(ğ‘›) performance

Big-O Efficiency of Python Dictionary Operations

operation | Big-O Efficiency
---|---
copy | <kbd>O(n)</kbd>
get item | <kbd>O(1)</kbd>
set item | <kbd>O(1)</kbd>
delete item | <kbd>O(1)</kbd>
contains (in) | <kbd>O(1)</kbd>
iteration | <kbd>O(n)</kbd>


compare the performance of the `contains operation` between **lists** and **dictionaries**.
- make a list with a range of numbers in it.
- pick numbers at random and check to see if the numbers are in the list.
- If our performance tables are correct, the bigger the list the longer it should take to determine if any one number is contained in the list.

We will repeat the same experiment for a dictionary that contains numbers as the keys.
- determining whether or not a number is in the dictionary is not only much faster,
- but the time it takes to check should remain constant even as the dictionary grows larger.

Listing 6 implements this comparison. Notice that we are performing exactly the same operation, number in container. The difference is that on line 8 x is a list, and on line 10 x is a dictionary.

```py
import timeit
import random

print(f"{'n':10s}{'list':>10s}{'dict':>10s}")

for i in range(10_000, 1_000_001, 20_000):
    t = timeit.Timer(f"random.randrange({i}) in x", "from __main__ import random, x")

    x = list(range(i))
    lst_time = t.timeit(number=1000)

    x = {j: None for j in range(i)}
    dict_time = t.timeit(number=1000)

    print(f"{i:<10,} {lst_time:>10.3f} {dict_time:>10.3f}")

# n               list      dict
# 10,000          0.085      0.001
# 30,000          0.225      0.001
# 50,000          0.381      0.001
# 70,000          0.542      0.001
# 90,000          0.770      0.001
# 110,000         1.104      0.001
# 130,000         0.993      0.001
# 150,000         1.121      0.001
# 170,000         1.243      0.001
# 190,000         1.375      0.001
# 210,000         1.546      0.001
```

![listvdict](https://i.imgur.com/ziaxv2F.png)


---






























.
