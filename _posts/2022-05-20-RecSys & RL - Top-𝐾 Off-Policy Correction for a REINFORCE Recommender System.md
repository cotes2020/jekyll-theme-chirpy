---
title: (ì‘ì„±ì¤‘) RecSys & RL - Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System
author: Beanie
date: 2022-05-20 15:03:00 +0800
categories: [RecSys]
tags: [RecSys, RL, paper]
layout: post
current: post
class: post-template
subclass: 'post'
navigation: True
cover:  assets/img/post_images/recsys_cover1.jpg
---

ì´ì „ì— ì½ì—ˆë˜ `A Deep Reinforcement Learning Framework for News Recommendation` ë…¼ë¬¸ì€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì½”ë“œë¥¼ ì°¾ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ê·¸ë˜ì„œ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œê°€ ê°™ì´ ìˆìœ¼ë©´ì„œ ì ë‹¹íˆ challengingí•œ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì°¾ì•„ë³´ì•˜ê³  ì´ `Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System` ì´ ê°€ì¥ ê´œì°®ì•„ë³´ì—¬ ì½ì–´ë³´ì•˜ë‹¤. ì´í›„ ì—¬ëŸ¬ ìƒ˜í”Œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ public datasetìœ¼ë¡œ ì§ì ‘ êµ¬í˜„ê¹Œì§€ í•´ë³¼ ì˜ˆì •ì´ë‹¤.

## ì£¼ìš” Contribution
&nbsp;

ì´ ë…¼ë¬¸ì˜ ì£¼ìš”í•œ ê¸°ì—¬ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
* **REINFORCE Recommender**
    * Large, non-stationary state and action spaces
* **Off-Policy Candidate Generation**

    RLì—ì„œ ì¶”ì²œì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ë°ì´í„°ì˜ ë¶€ì¡±ì´ íŠ¹íˆ ë¬¸ì œê°€ ëœë‹¤. ê³ ì „ì ì¸ RLì—ì„œëŠ” ì´ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë§ì€ ì–‘ì˜ training dataë¥¼ self-replayë‚˜ simulationì„ í†µí•´ ìˆ˜ì§‘í•˜ì˜€ë‹¤.
    í•˜ì§€ë§Œ ì¶”ì²œì‹œìŠ¤í…œ ë¬¸ì œì˜ ê²½ìš°, ë³µì¡í•˜ê²Œ ì–½í˜€ìˆëŠ” ì¶”ì²œì‹œìŠ¤í…œ í™˜ê²½ ë•Œë¬¸ì— simulationì„ í†µí•˜ì—¬ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ rewardë¥¼ ê´€ì°°í•˜ëŠ” ê²ƒ ìì²´ê°€ ì‹¤ì œ ìœ ì €ì—ê²Œ ì‹¤ì œ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— íƒìƒ‰ë˜ì§€ ì•Šì€ ê³µê°„ì˜ state, action spaceì— ëŒ€í•˜ì—¬ rewardê°’ì„ ì‰½ê²Œ ì•Œì•„ë‚´ê¸° ì–´ë µë‹¤.

    \
    &nbsp;

    ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, **ë‹¤ë¥¸ ì¶”ì²œì‹œìŠ¤í…œì´ë‚˜ ê³¼ê±°ì˜ policyì—ì„œ ì–»ì€ ìœ ì € feedbackì˜ ë¡œê·¸ë“¤ì„ í™œìš©í•˜ì—¬ Off-policy learningì„ ìˆ˜í–‰** í•œë‹¤. ì´ ë•Œ, ë‹¤ë¥¸ policyì—ì„œ ìˆ˜ì§‘í•˜ì˜€ê¸° ë•Œë¬¸ì— í•„ì—°ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” biasë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
* **Top-K Off-Policy Correction**
    * ì‹¤ì œ ì¶”ì²œì‹œìŠ¤í…œ í™˜ê²½ì—ì„œëŠ” 1ê°œê°€ ì•„ë‹Œ ì—¬ëŸ¬ê°œì˜ ì¶”ì²œì„ ë™ì‹œì— ì œê³µí•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” top-K recommender systemì„ ìœ„í•œ ìƒˆë¡œìš´ top-K off-policy correctionì„ ì •ì˜í•œë‹¤.
* **Benefits in Live Experiments**
    * ì•„ì´í…œì— ëŒ€í•œ ìœ ì € ì„ í˜¸ë„ëŠ” ê³„ì† ë³€í•¨ -> ìœ ì € state ê°’ì´ ì§€ì†ì ìœ¼ë¡œ ë°”ë€ë‹¤.

&nbsp;
## Reinforce Recommender
&nbsp;
### MDP Modeling
ì¶”ì²œì‹œìŠ¤í…œì„ ê°•í™”í•™ìŠµ ì„¸íŒ…ì— ì í•©í•˜ê²Œ ë§ì¶°ë³´ì. ê°•í™”í•™ìŠµì„ ìœ„í•˜ì—¬ í™˜ê²½ì„ Markov Decision Process(MDP)ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
* $S$ : ìœ ì € stateë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—°ì†ì ì¸ state space
* $A$ : ì¶”ì²œ ì•„ì´í…œ í›„ë³´êµ°ì„ í¬í•¨í•˜ê³  ìˆëŠ” discreteí•œ action space
* $P$ : state transition probability ( $S \times A \times S \to \mathbb{R}$ )
* $R$ : ë³´ìƒ í•¨ìˆ˜ ( $S \times A \to \mathbb{R}$ ), ì´ ë•Œ, $ r(s, a) $ ëŠ” ìœ ì € state sì—ì„œ action aë¥¼ í•  ë•Œ ë°”ë¡œ ì–»ì–´ì§€ëŠ” rewardë¥¼ ì˜ë¯¸í•œë‹¤.
* $ \rho_{0} $ : ì´ˆê¸° ìƒíƒœ ë¶„í¬
* $ \gamma $ future rewardì— ëŒ€í•œ discount factor

### Policy Gradient

&nbsp;
## Model Architecture
&nbsp;

&nbsp;
## Off-Policy Correction
&nbsp;

&nbsp;
## Top-K Recommendation
&nbsp;

&nbsp;
## Exploration
&nbsp;

&nbsp;
## Experiment Results
&nbsp;