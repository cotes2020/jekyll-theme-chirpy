---
title: AI - AI
date: 2021-08-11 11:11:11 -0400
description:
categories: [51AI]
# img: /assets/img/sample/rabbit.png
tags: [AI, ML]
---

# AIML - AI

- [AIML - AI](#aiml---ai)
  - [Overall](#overall)
  - [AI](#ai)
    - [Divisions of AI](#divisions-of-ai)
      - [Artificial Narrow Intelligence (ANI)](#artificial-narrow-intelligence-ani)
      - [Artificial General Intelligence (AGI)](#artificial-general-intelligence-agi)
  - [Traditional AIML vs GenAI](#traditional-aiml-vs-genai)
    - [RNN - Recurrent neural networks](#rnn---recurrent-neural-networks)
    - [Attention is all you need](#attention-is-all-you-need)
      - [High alignment](#high-alignment)
      - [Multi-Headed Attention](#multi-headed-attention)
    - [åè¯è§£é‡Š](#åè¯è§£é‡Š)
  - [GenAI](#genai)
    - [Large Language Model](#large-language-model)
    - [Features of LLMs](#features-of-llms)
      - [Translation](#translation)
      - [Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡](#automating-mundane-tasks-è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡)
      - [Emergent Abilities æ–°å…´èƒ½åŠ›](#emergent-abilities-æ–°å…´èƒ½åŠ›)
    - [Drawbacks of LLMs](#drawbacks-of-llms)
      - [Hallucination](#hallucination)
      - [Bias](#bias)
      - [Glitch tokens](#glitch-tokens)
      - [LLM Generation Inefficient](#llm-generation-inefficient)
        - [Speculative æ¨æµ‹çš„ Decoding](#speculative-æ¨æµ‹çš„-decoding)
    - [LLM Subject](#llm-subject)
    - [Generative configuration](#generative-configuration)
    - [Encoder \& Decoder](#encoder--decoder)
      - [Encoder vs Decoder](#encoder-vs-decoder)
      - [How the model works](#how-the-model-works)
      - [Overall prediction process](#overall-prediction-process)
        - [Data preprocessing / embedding](#data-preprocessing--embedding)
        - [Prompt construction / retrieval](#prompt-construction--retrieval)
        - [Prompt execution / inference](#prompt-execution--inference)
      - [AI agents frameworks](#ai-agents-frameworks)
    - [LLM Tools](#llm-tools)
      - [Medusa](#medusa)
        - [Medusa heads](#medusa-heads)
        - [Tree attention](#tree-attention)
        - [Typical acceptance](#typical-acceptance)
        - [accelerate models](#accelerate-models)
        - [Ablation Study æ¶ˆèç ”ç©¶](#ablation-study-æ¶ˆèç ”ç©¶)
    - [LLM Data Training](#llm-data-training)
    - [Confidence score for ML model](#confidence-score-for-ml-model)
    - [Transparency](#transparency)
      - [The Foundation Model Transparency Index](#the-foundation-model-transparency-index)
    - [Generative AI project lifecycle](#generative-ai-project-lifecycle)
    - [Pre-training large language models](#pre-training-large-language-models)
      - [Model Architecture and Pre-training objective](#model-architecture-and-pre-training-objective)
      - [Computational challenges of training LLMs](#computational-challenges-of-training-llms)
        - [quantization](#quantization)
      - [Efficient multi-GPU compute strategies](#efficient-multi-gpu-compute-strategies)
      - [Scaling laws and compute-optimal models](#scaling-laws-and-compute-optimal-models)
      - [Pre-training for domain adaptation](#pre-training-for-domain-adaptation)
      - [Domain-specific training: BloombergGPT](#domain-specific-training-bloomberggpt)
  - [GenAI for Code](#genai-for-code)
    - [Fine-tuning](#fine-tuning)
    - [additional pretraining](#additional-pretraining)
    - [EXPERIMENTAL](#experimental)
  - [usage](#usage)
    - [Juptyper](#juptyper)
  - [Hugging Face](#hugging-face)
    - [Generative AI Time Series Forecasting](#generative-ai-time-series-forecasting)
    - [Multivariate Time Series Forecasting](#multivariate-time-series-forecasting)
    - [Generative AI Transformers for Time Series Forecasting](#generative-ai-transformers-for-time-series-forecasting)
    - [Falcon 40b](#falcon-40b)
    - [CodeParrot](#codeparrot)
    - [TAPEX](#tapex)
  - [common LLM](#common-llm)
    - [1 GPT ç³»åˆ—(OpenAI)](#1-gpt-ç³»åˆ—openai)
      - [è¯­è¨€æ¨¡å‹](#è¯­è¨€æ¨¡å‹)
      - [1.1 GPT-1, GPT-2, GPT-3](#11-gpt-1-gpt-2-gpt-3)
        - [GPT-1](#gpt-1)
        - [GPT-2](#gpt-2)
        - [GPT-3](#gpt-3)
      - [1.2 InstructGPT](#12-instructgpt)
      - [1.3 ChatGPT](#13-chatgpt)
      - [1.4 GPT-4](#14-gpt-4)
    - [2 å…¶ä»–å¤§æ¨¡å‹](#2-å…¶ä»–å¤§æ¨¡å‹)
      - [2.1 ChatGLM](#21-chatglm)
      - [2.2 LLaMA](#22-llama)

ref:

- [OWAPS Top10 for LLM v1](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0.pdf)
- https://www.freecodecamp.org/news/large-language-models-and-cybersecurity/
- https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html
- https://docs.whylabs.ai/docs/integrations-llm-whylogs-container
- https://hackernoon.com/security-threats-to-high-impact-open-source-large-language-models
- https://a16z.com/emerging-architectures-for-llm-applications/
- [Examining Zero-Shot Vulnerability Repair with Large Language Models](https://www.connectedpapers.com/main/a5731122200fbb8b37f048010a1e1ca4474aa606/Examining-Zero%20Shot-Vulnerability-Repair-with-Large-Language-Models/graph)
- [medusa](https://together.ai/blog/medusa)
- [awesome-generative-ai](https://github.com/steven2358/awesome-generative-ai)
- [Googleâ€™s Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)
- [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/fmti.pdf)

Link:

- [Browse State-of-the-Art](https://paperswithcode.com/sota)

---

## Overall

> Research in artificial intelligence is increasing at an exponential rate. Itâ€™s difficult for AI experts to keep up with everything new being published, and even harder for beginners to know where to start.

> - Transformers Neural Network

> - After the big success of `Transformers Neural Network`, it has been adapted to many `Natural Language Processing (NLP)` tasks (such as question answering, text translation, automatic summarization)

â€œAI Canonâ€

- a curated list of resources weâ€™ve relied on to get smarter about modern AI
- because these papers, blog posts, courses, and guides have had an outsized impact on the field over the past several years.

**Data pipelines**

- Databricks
- Airflow
- Unstructured

**Embedding model**

- OpenAI
- Cohere
- Hugging Face

**Vector database**

- Pinecone
- Weaviate
- ChromaDB
- pgvector

**Playground**

- OpenAI
- nat.dev
- Humanloop

**Orchestration**

- Langchain
- LlamaIndex
- ChatGPT

**APIs/plugins**

- Serp
- Wolfram
- Zapier

**LLM cache**

- Redis
- SQLite
- GPTCache

**Logging / LLMops**

- Weights & Biases
- MLflow
- PromptLayer
- Helicone

**Validation**

- Guardrails
- Rebuff
- Microsoft Guidance
- LMQL

**App hosting**

- Vercel
- Steamship
- Streamlit
- Modal

**LLM APIs (proprietary)**

- OpenAI
- Anthropic

**LLM APIs (open)**

- Hugging Face
- Replicate

**Cloud providers**

- AWS
- GCP
- Azure
- CoreWeave

**Opinionated clouds**

- Databricks
- Anyscale
- Mosaic
- Modal
- RunPod

**OpenSource**

- hugging face
- OpenAI
- Generative AI (answers for everything)

**Programming**

- python
- panda

**AI modeling**

- pyTorch
- Tensor flow (Google)

**ML platforms**

- Jupyter Notebooks

**Time series**

- Forecasting and predictive Analytics

**Use case**

- Supply Chain Management with GenAI

OpenSource -> fine tuning -> custom result

---

## AI

- Artificial Intelligence refers to `the ability of computers to perform tasks that typically require human-level intellect`.
- AI is useful in many contexts, from automation to problem solving and merely trying to understand how humans think.

- But it is important to note that AI is only concerned with human intelligence for now â€“ it could possibly go beyond that.

  - Many people correlate the word â€˜Intelligenceâ€™ with only â€˜Human Intelligenceâ€™. Just because a chicken may not be able to solve a mathematical equation doesnâ€™t mean it wonâ€™t run when you chase it. It is â€˜Intelligentâ€™ enough to know it doesnâ€™t want you to catch it ğŸ”ğŸ—.

  - Intelligence spans a much wider spectrum, and practically expands to any living thing that can make decisions or carry out actions autonomously, even plants.

### Divisions of AI

- Artificial Intelligence is `centered around computers and their ability to mimic human actions and thought processes`.

- Programming and experiments have allowed humans to produce ANI systems. These can do things like classifying items, sorting large amounts of data, looking for trends in charts and graphs, code debugging, and knowledge representation and expression. But computers donâ€™t think like humans, they merely mimic humans.

- This is evident in voice assistants such as `Googleâ€™s Assistant, Appleâ€™s Siri, Amazonâ€™s Alexa, and Microsoftâ€™s Cortana`. They are basic ANI programs that add â€˜the human touchâ€™. In fact, people are known to be polite to these systems simply because they combine computerized abilities with a human feel.

- These assistants have gotten better over the years but fail to reach high levels of sophistication when compared to their AGI counterparts.

There are two major divisions of AI:

#### Artificial Narrow Intelligence (ANI)

- focused on a small array of similar tasks or a small task that is programmed only for one thing.
- ANI is not great in dynamic and complex environments and is used in only areas specific to it.
- Examples include self-driving cars, as well as facial and speech recognition systems.

#### Artificial General Intelligence (AGI)

- focused on a wide array of tasks and human activities.
- AGI is currently theoretical and is proposed to adapt and carry out most tasks in many dynamic and complex environments.
- Examples include J.A.R.V.I.S from Marvelâ€™s _Iron Man_ and Ava from _Ex-Machina_.

---

## Traditional AIML vs GenAI

**Traditional AIML**

- good at **identify pattern**
- learning from the pattern
- limit success with close supervised learning of very large amount of data
- must have human involved

**GenAI**

- produces 'content' (text, image, music, art, forecasts, etc...)
- use 'transformers' (Encoders/Decoders) based on pre-trained data using small amount of fine tuning data

  - encode and decode at the same time
    - less data and faster
    - GenAI use small data and uses encoders and decoders and Transformers to take that smaller data and be able to use it for other types of models. (Pre training)
    - then add on top of it small amounts of fine tuning data
    - and then get a a training model.
  - As perceptions, not neurons.

- Generative AI is a subset of traditional machine learning.
  - And the generative AI machine learning models have learned these abilities by `finding statistical patterns in massive datasets of content` that was originally generated by humans.

choosing between LLMs or layout-based Traditional AIML[^Choosing_an_extraction_approach]

- recommends using LLM prompts for free-form, highly variable documents
- layout-based or "rule-based" queries for structured, less-variable documents.

![Screenshot 2023-11-13 at 16.21.05](/assets/img/Screenshot%202023-11-13%20at%2016.21.05.png)

[^Choosing_an_extraction_approach]: Choosing an extraction approach, https://docs.sensible.so/docs/author

---

### RNN - Recurrent neural networks

> generative algorithms are not new.

recurrent neural networks - RNNs

- Previous generations of language models made use of an architecture called RNNs.
- RNNs were limited by the amount of `compute and memory` needed to perform well at generative tasks.

- With just one previous words seen by the model, the prediction can't be very good.
  - scale the RNN implementation to be able to see more of the preceding words in the text,
  - significantly scale the resources that the model uses.
  - As for the prediction, Even though scale the model, it still hasn't seen enough of the input to make a good prediction.

![Screenshot 2023-10-10 at 00.31.57](/assets/img/post/Screenshot%202023-10-10%20at%2000.31.57.png)

- To successfully predict the next word,
  - models need to see more than just the previous few words.
  - Models needs to have an understanding of the whole sentence or even the whole document.

> How can an algorithm make sense of human language if sometimes we can't?
> in 2017, after the publication of this paper, `Attention is All You Need`, from Google and the University of Toronto, everything changed.
> The transformer architecture had arrived.
>
> - It can be scaled efficiently to use multi-core GPUs,
> - parallel process input data, making use of much larger training datasets, and crucially,
> - it's able to learn to pay attention to the meaning of the words it's processing.

---

### Attention is all you need

![Screenshot 2023-10-10 at 00.33.00](/assets/img/post/Screenshot%202023-10-10%20at%2000.33.00.png)

---

#### High alignment

![Screenshot 2023-09-06 at 22.55.58](/assets/img/post/Screenshot%202023-09-06%20at%2022.55.58.png)

![Screenshot 2023-09-06 at 22.56.59](/assets/img/post/Screenshot%202023-09-06%20at%2022.56.59.png)

---

#### Multi-Headed Attention

![Screenshot 2023-09-06 at 23.10.28](/assets/img/post/Screenshot%202023-09-06%20at%2023.10.28.png)

---

### åè¯è§£é‡Š

- **LLM**: Large Language Modelï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- **PLM**: Pretrain Language Modelï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
- **RL**: Reinforcement Learningï¼Œå¼ºåŒ–å­¦ä¹ ã€‚
- **SFT**: Supervised Fine-Tuningï¼Œæœ‰ç›‘ç£å¾®è°ƒã€‚
- **ICL**: In-Context Learningï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

- **å¯¹æ¯”å­¦ä¹ **:

  - è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)å¯ä»¥é¿å…å¯¹æ•°æ®é›†è¿›è¡Œå¤§é‡çš„æ ‡ç­¾æ ‡æ³¨ã€‚
  - æŠŠè‡ªå·±å®šä¹‰çš„ä¼ªæ ‡ç­¾å½“ä½œè®­ç»ƒçš„ä¿¡å·ï¼Œç„¶åæŠŠå­¦ä¹ åˆ°çš„è¡¨ç¤º(representation)ç”¨ä½œä¸‹æ¸¸ä»»åŠ¡é‡Œã€‚
  - æœ€è¿‘ï¼Œå¯¹æ¯”å­¦ä¹ è¢«å½“ä½œè‡ªç›‘ç£å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œè¢«å¹¿æ³›è¿ç”¨åœ¨è®¡ç®—æœºè§†è§‰, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚
  - å®ƒçš„ç›®æ ‡æ˜¯: å°†ä¸€ä¸ªæ ·æœ¬çš„ä¸åŒçš„, å¢å¼ºè¿‡çš„æ–°æ ·æœ¬ä»¬åœ¨åµŒå…¥ç©ºé—´ä¸­å°½å¯èƒ½åœ°è¿‘ï¼Œç„¶åè®©ä¸åŒçš„æ ·æœ¬ä¹‹é—´å°½å¯èƒ½åœ°è¿œã€‚
  - SimCSE[ã€ŠSimCSE: Simple Contrastive Learning of Sentence Embeddingsã€‹](https://aclanthology.org/2021.emnlp-main.552.pdf)æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå³é‡‡ç”¨å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œè·å–æ›´å¥½çš„æ–‡æœ¬è¡¨å¾ã€‚
  - SimCSE ç»†èŠ‚å¯å‚è€ƒ[è®ºæ–‡ç²¾è¯»-SimCSE](https://www.bilibili.com/video/BV12D4y1p788/?spm_id_from=333.337.search-card.all.click&vd_source=25d0b87065d3da39fe110c6e0b4906e1)ã€‚

- **Fine-Tuning** : å¾®è°ƒã€‚
- **Prompt-Tuning**: æç¤ºå¾®è°ƒã€‚
- **Instruction-Tuning**: æŒ‡ç¤º/æŒ‡ä»¤å¾®è°ƒã€‚

- **NLU**: Natural Language Understandingï¼Œè‡ªç„¶è¯­è¨€ç†è§£ã€‚
- **NLG**: Natural Language Generationï¼Œè‡ªç„¶è¯­è¨€ç”Ÿæˆã€‚
- **CoT**: Chain-of-Thoughtï¼Œæ€ç»´é“¾ã€‚
- **OOV**:

  - OOV é—®é¢˜æ˜¯ NLP ä¸­å¸¸è§çš„ä¸€ä¸ªé—®é¢˜ï¼Œå…¶å…¨ç§°æ˜¯ Out Of Vocabulary è¶…å‡ºè¯è¡¨å¤–çš„è¯ã€‚
  - **å®šä¹‰**:
    - åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸€ä¸ªå­—è¯åº“(vocabulary)ã€‚
    - è¿™ä¸ª vocabulary æˆ–è€…æ˜¯æå‰åŠ è½½çš„ï¼Œæˆ–è€…æ˜¯è‡ªå·±å®šä¹‰çš„ï¼Œæˆ–è€…æ˜¯ä»å½“å‰æ•°æ®é›†æå–çš„ã€‚
    - å‡è®¾é€šè¿‡ä¸Šè¿°æ–¹æ³•å·²ç»è·å–åˆ°ä¸€ä¸ª vocabularyï¼Œä½†åœ¨å¤„ç†å…¶ä»–æ•°æ®é›†æ—¶ï¼Œå‘ç°è¿™ä¸ªæ•°æ®é›†ä¸­æœ‰ä¸€äº›è¯å¹¶ä¸åœ¨ç°æœ‰çš„ vocabulary ä¸­ï¼Œè¿™æ—¶ç§°è¿™äº›è¯æ˜¯ Out-Of-Vocabularyï¼Œå³ OOVï¼›
  - **è§£å†³æ–¹æ³•**:
    - Bert ä¸­è§£å†³ OOV é—®é¢˜ã€‚å¦‚æœä¸€ä¸ªå•è¯ä¸åœ¨è¯è¡¨ä¸­ï¼Œåˆ™æŒ‰ç…§ subword çš„æ–¹å¼é€ä¸ªæ‹†åˆ† tokenï¼Œå¦‚æœè¿é€ä¸ª token éƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™ç›´æ¥åˆ†é…ä¸º[unknown]ã€‚

- **shifted right**: æŒ‡çš„æ˜¯ Transformer Decoder ç»“æ„ä¸­ï¼Œdecoder åœ¨ä¹‹å‰æ—¶åˆ»çš„ä¸€äº›è¾“å‡ºï¼Œä½œä¸ºæ­¤æ—¶çš„è¾“å…¥ï¼Œä¸€ä¸ªä¸€ä¸ªå¾€å³ç§»ã€‚
- **é‡å‚æ•°åŒ–**: å¸¸è§„æ€æƒ³: å¯¹äºç½‘ç»œå±‚éœ€è¦çš„å‚æ•°æ˜¯ $\Phi$ï¼Œè®­ç»ƒå‡ºæ¥çš„å‚æ•°å°±æ˜¯ $\Phi$ã€‚é‡å‚æ•°åŒ–æ–¹æ³•: è®­ç»ƒæ—¶ç”¨çš„æ˜¯å¦ä¸€å¥—ä¸åŒäº $\Phi$çš„å‚æ•°ï¼Œè®­ç»ƒå®Œåç­‰ä»·è½¬æ¢ä¸º $\Phi$ç”¨äºæ¨ç†ã€‚
- **PPL**: å›°æƒ‘åº¦(Perplexity)ï¼Œç”¨äºè¯„ä»·è¯­è¨€æ¨¡å‹çš„å¥½åã€‚
- **FCNN**: Fully connected neural networkï¼Œå…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚
- **FNN**: Feedforward neural networkï¼Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚
- **DNN**: Deep neural networkï¼Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚
- **MLP**: Multi-layer perceptron neural networksï¼Œå¤šå±‚æ„ŸçŸ¥æœºã€‚
- **RM**: Reward Modelï¼Œå¥–åŠ±æ¨¡å‹ã€‚
- **PPO**ï¼ŒProximal Policy Optimizationï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¯¹ç›®æ ‡å‡½æ•°é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ã€‚
- **Emergent Ability**: å¾ˆå¤šèƒ½åŠ›å°æ¨¡å‹æ²¡æœ‰ï¼Œåªæœ‰å½“æ¨¡å‹å¤§åˆ°ä¸€å®šçš„é‡çº§ä¹‹åæ‰ä¼šå‡ºç°ã€‚è¿™æ ·çš„èƒ½åŠ›ç§°ä¸ºæ¶Œç°èƒ½åŠ›ã€‚
- **AutoRegression Language Model**: è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚
- **Autoencoder Language Model**: è‡ªç¼–ç è¯­è¨€æ¨¡å‹ã€‚
- **CLM**: Causal language modelingï¼Œå› æœè¯­è¨€å»ºæ¨¡ï¼Œç­‰ä»·äº AutoRegression Language Modelã€‚
- **AIGC**: Artificial Intelligence Generated Contentï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚
- **AGI**: Artificial General Intelligenceï¼Œé€šç”¨äººå·¥æ™ºèƒ½ã€‚

- **Bert**ï¼›è‡ªç¼–ç æ¨¡å‹ï¼Œé€‚ç”¨äº NLU(é¢„è®­ç»ƒä»»åŠ¡ä¸»è¦æŒ–æ˜å¥å­ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³») [^â€”æ­¥æ­¥èµ°è¿›Bert]

[^â€”æ­¥æ­¥èµ°è¿›Bert]: â€”æ­¥æ­¥èµ°è¿› Bert, https://zhuanlan.zhihu.com/p/519432336

- **GPT**ï¼›è‡ªå›å½’æ¨¡å‹ï¼Œé€‚ç”¨äº NLG(é¢„è®­ç»ƒä»»åŠ¡ä¸»è¦ç”¨äºç”Ÿæˆä¸‹æ–‡)ï¼Œæœ‰å…³ GPT ç³»åˆ—çš„ç†è®ºçŸ¥è¯†ï¼Œå¯å‚è€ƒæœ¬æ–‡çš„ GPT ç³»åˆ—ç« èŠ‚ï¼›

---

## GenAI

With the rise in popularity of Foundation Models, new models and tools are released almost every week and yet

![Screenshot 2023-11-13 at 22.11.36](/assets/img/Screenshot%202023-11-13%20at%2022.11.36.png)

- å°±ç›®å‰çš„å‘å±•è¶‹åŠ¿è€Œè¨€ï¼Œå¤§æ¨¡å‹çš„è®­ç»ƒåŸºæœ¬å°±æ˜¯æŒ‰ç…§**Pretrain**, **Instruction-Tuning**, **RLHF**ä¸‰æ­¥èµ°æ¨¡å¼è¿›è¡Œï¼Œå› æ­¤æŠ€æœ¯ä¸ŠåŸºæœ¬æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä¸»è¦ç“¶é¢ˆå­˜åœ¨äºç®—åŠ›
- å¯¹äºä¸­æ–‡å¤§æ¨¡å‹æ¥è¯´ï¼Œè·å–é«˜è´¨é‡ä¸­æ–‡æ•°æ®ä¹Ÿæ˜¯ä¸ªé—®é¢˜ã€‚å›½å†…èƒ½è€—è´¹å¤§è§„æ¨¡æˆæœ¬è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒçš„å‚å•†å±ˆæŒ‡å¯æ•°

- æœªæ¥çš„å‘å±•è¶‹åŠ¿ï¼Œå¯èƒ½æœ‰ä»¥ä¸‹å››ä¸ªæ–¹å‘:
  - **ç»Ÿä¸€å¤§æ¨¡å‹**: å¤´éƒ¨ä¼ä¸šé€æ­¥è¿­ä»£å‡ºå¯ç”¨æ€§å¾ˆå¼ºçš„å¤§è¯­è¨€æ¨¡å‹(åƒäº¿çº§åˆ«)ï¼Œå¼€æ”¾ API æˆ–åœ¨å…¬æœ‰äº‘ä¸Šä¾›å¤§å®¶ä½¿ç”¨ï¼›
  - **å‚ç›´é¢†åŸŸæ¨¡å‹**: éƒ¨åˆ†ä¼ä¸šé€æ­¥è¿­ä»£å‡ºåœ¨ç›¸å…³å‚ç›´é¢†åŸŸå¯ç”¨æ€§å¾ˆå¼ºçš„å¤§è¯­è¨€æ¨¡å‹(ç™¾äº¿çº§åˆ«)ï¼Œè‡ªç”¨æˆ–ç§æœ‰åŒ–æä¾›å®¢æˆ·ä½¿ç”¨ï¼›
  - **å¹¶è¡Œè®­ç»ƒæŠ€æœ¯**: å…¨æ–°æˆ–æ›´å…·å¯ç”¨æ€§, ç”Ÿæ€æ›´å®Œæ•´çš„å¹¶è¡Œè®­ç»ƒæŠ€æœ¯/æ¡†æ¶å¼€æºï¼Œæ»¡è¶³å¤§éƒ¨åˆ†æœ‰è®­ç»ƒéœ€æ±‚çš„ä¼ä¸š/ä¸ªäººä½¿ç”¨ï¼Œé€æ­¥å®ç°äººäººéƒ½èƒ½è®­ç»ƒå¤§æ¨¡å‹ï¼›
  - **é¢ è¦†**: æˆ–è®¸æŸä¸€å¤©ï¼Œæ¨ªç©ºå‡ºä¸–çš„è®ºæ–‡æ¨ç¿»äº†ç›®å‰çš„å¤§æ¨¡å‹å‘å±•è·¯çº¿ï¼Œè½¬è€Œè¯æ˜äº†äººä»¬æ›´éœ€è¦çš„æ˜¯å¦ä¸€ç§å¤§æ¨¡å‹æŠ€æœ¯ï¼Œäº‹æƒ…å°±ä¼šå˜å¾—æœ‰æ„æ€äº†ã€‚

---

### Large Language Model

> "... a language model is a Turing-complete weird machine running programs written in natural language; when you do retrieval, you are not 'plugging updated facts into the AI', you are actually downloading random new unsigned blobs of code from the Internet (many written by adversaries) and casually executing them on the LM with full privileges. This does not end well." - [Gwern Branwen on LessWrong](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K)

- a deep learning model which consists of a `neural network` with billions of parameters, trained on distinctively large amounts of unlabelled data using self-supervised learning.

- At the core of all AI are algorithms. Algorithms are procedures or steps to carry out a specific task. The more complex the algorithm, the more tasks can be carried out and the more widely it can be applied. The aim of AI developers is to find the most complex algorithms that can solve and perform a wide array of tasks.

- The procedure to create a basic fruit recognition model using an simple analogy:

  1. There are two people: A teacher and a bot creator
  2. The bot creator creates random bots, and the teacher teaches and tests them on identifying some fruits
  3. The bot with the highest test score is then sent back to the creator as a base to make new upgraded bots
  4. These new upgraded bots are sent back to the teacher for teaching and testing, and the one with the highest test score is sent back to the bot creator to make new better bots.

  - This is an oversimplification of the process, but nevertheless it relays the concept. The Model/Algorithm/Bot is continuously trained, tested, and modified until it is found to be satisfactory. More data and higher complexity means more training time required and more possible modifications.

- the developer of the model can tweak a few things about the model but may not know how those tweaks might affect the results.
- A common example of this are neural networks, which have hidden layers whose deepest layers and workings even the creator may not fully understand.

- Self-supervised learning means that rather than the teacher and the bot creator being two separate people, it is one highly skilled person that can both create bots and teach them.

  - This makes the process much faster and practically autonomous.
  - The result is a bot or set of bots that are both sophisticated and complex enough to recognise fruit in dynamic and different environments.

- In the case of LLMs, the data here are human text, and possibly in various languages. The reason why the data are large is because the LLMs take in huge amounts of text data with the aim of finding connections and patterns between words to derive context, meaning, probable replies, and actions to these text.

- The results are models that seem to understand language and carry out tasks based on prompts they're given.

- Tuning æŠ€æœ¯ä¾èµ–äº LLM çš„å‘å±•ï¼ŒåŒæ—¶ä¹Ÿåœ¨æ¨åŠ¨ç€ LLM çš„å‘å±•ã€‚

- é€šå¸¸ï¼ŒLLM æŒ‡çš„æ˜¯åŒ…å«æ•°ç™¾äº¿(æˆ–æ›´å¤š)å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒã€‚

---

### Features of LLMs

#### Translation

- LLMs that are trained on an array of languages rather than just one can be used for translation from one language to another.
- It's even theorised that large enough LLMs can find patterns and connections in other languages to derive meaning from unknown and lost languages, despite not knowing what each individual word may mean.

#### Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡

- Task automation has always been a major aim of AI development. Language models have always been able to carry out syntax analysis, finding patterns in text and responding appropriately.

- Large language models have an advantage with semantic analysis è¯­ä¹‰åˆ†æ, enabling the model to understand the underlying meaning and context, giving it a higher level of accuracy.

- This can be applied to a number of basic tasks like `text summarising, text rephrasing, and text generation`.

#### Emergent Abilities æ–°å…´èƒ½åŠ›

- Emergent Abilities are `unexpected but impressive` abilities LLMs have due to the high amount of data they are trained on.

- These behaviours are usually discovered when the model is used rather than when it is programmed.

- Examples include multi-step arithmetic, taking college-level exams, and chain-of-thought prompting. æ€ç»´é“¾æç¤º

---

### Drawbacks of LLMs

#### Hallucination

- An infamous outcome of Microsoftâ€™s Sydney were instances when the AI gave responses that were either bizarre å¼‚ä¹å¯»å¸¸, untrue, or seemed sentient æœ‰æ„Ÿæƒ….
- These instances are termed Hallucination, where the model gives answers or makes claims that are not based on its training data.

#### Bias

- Sometimes, the data could be the source of the problem. If a model is trained on data that is discriminatory to a person, group, race, or class, the results would also tend to be discriminatory.

- Sometimes, as the model is being used, the bias could change to fit what users tend to input. Microsoftâ€™s Tay in 2016 was a great example of how bias could go wrong.

#### Glitch tokens

- Also known as adversarial examples å¯¹æŠ—æ€§ç¤ºä¾‹, glitch tokens are inputs given to a model to intentionally make it malfunction and be inaccurate when delivering answers.

#### LLM Generation Inefficient

> From a systems perspective, LLM generation follows a memory-bound computational pattern with the main latency bottleneck arising from memory reads/writes rather than arithmetic computations. This issue is rooted in the inherently sequential nature of the auto-regressive decoding process. Each forward pass necessitates the transfer of the entire model's parameters from High-Bandwidth Memory (HBM) to the accelerator's compute units. This operation, while only producing a single token for each sample, fails to fully utilize the arithmetic computation capabilities of modern accelerators, resulting in inefficiency.

Before the rise of LLMs, a common mitigation for this inefficiency was to `simply increase the batch size, enabling the parallel production of more tokens`.

But the **situation becomes far more complicated with LLMs**.

- Increasing the batch size in this context not only introduces higher latency but also substantially `inflates è†¨èƒ€ the memory requirements` for the Transformer model's key-value cache.

  - This trade-off makes the use of large batches impractical for many applications where low latency is a critical requirement.

- also, for cost structures, as of September 2023, generation costs approximately 2x higher for GPT-4 and roughly 3x for Claude 2, compared to merely processing prompts.

![Screenshot 2023-09-20 at 17.51.23](/assets/img/post/Screenshot%202023-09-20%20at%2017.51.23.png)

##### Speculative æ¨æµ‹çš„ Decoding

> Given the challenges outlined, one appealing strategy to accelerate **text generation** is `more efficient computational utilizationâ€”specifically`, by `processing more tokens in parallel`.

speculative decoding

- The methodology employs a streamlined "draft" model to generate a batch of token candidates at each step quickly. These candidates are then validated by the original, full-scale language model to identify the most reasonable text continuations.

- The underlying logic hinges on an intriguing å¼•èµ·å…´è¶£çš„ assumption:

  - the draft model, although smaller, should be proficient enough to churn out sequences that the original model will find acceptable.

  - the draft model can rapidly produce token sequences while the original model efficiently vets å®¡æŸ¥ multiple tokens in parallel, which maximizing computational throughput.

  - Recent research indicates that with a well-tuned draft model, speculative decoding can cut latency by an impressive factor of up to 2.5x.

- However, the approach is not without its challenges:

  - Finding the Ideal Draft Model: Identifying a "small yet mighty" draft model that aligns well with the original model is easier said than done.

  - System Complexity: Hosting two distinct models in one system introduces layers of complexity, both computational and operational, especially in distributed settings.

  - Sampling Inefficiency: When doing sampling with speculative decoding, an importance sampling scheme needs to be used. This introduces additional overhead on generation, especially at higher sampling temperatures.

- These complexities and trade-offs have limited the broader adoption of speculative decoding techniques. So speculative decoding isn't widely adopted.

- Remark: We use speculative decoding to refer to those methods that require an independent draft model here. In a broader sense, our method can also be viewed as speculative decoding, while the draft model is entangled with the original model.

---

### LLM Subject

**Large language models**

- Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power.
- These foundation models with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve.

**foundation models (base models)**

- their relative size in terms of their parameters.
- ![Screenshot 2023-10-09 at 15.57.07](/assets/img/post/Screenshot%202023-10-09%20at%2015.57.07.png)
- **parameters**:
  - the model's `memory`.
  - the more parameters a model has, the more memory, the more `sophisticated` the tasks it can perform.
- By either using these models as they are or by applying fine tuning techniques to adapt them to the specific use case, you can rapidly `build customized solutions without the need to train a new model` from scratch.

**Augmenting LLMs**

- connecting LLM to external data sources or using them to invoke external APIs.
- use this ability to provide the model with information it doesn't know from its pre-training and to enable the model to power interactions with the real-world.

**Interact**

- `other machine learning and programming paradigms`: write computer code with formalized syntax to interact with `libraries and APIs`.
- `large language models`: able to take natural language or human written instructions and perform tasks much as a human would.

**prompt**

- The text that you pass to an LLM
- The space or memory that is available to the prompt is called the `context window`, and this is typically large enough for a few thousand words, but differs from model to model.

- example
  - ask the model to determine where Ganymede is located in the solar system.
  - The prompt is passed to the model, the model then predicts the next words, and because the prompt contained a question, this model generates an answer.
- The output of the model is called a `completion`, and the act of using the model to generate text is known as `inference`.
- The completion is comprised of the text contained in the original prompt, followed by the generated text.

![Screenshot 2023-10-10 at 00.25.13](/assets/img/post/Screenshot%202023-10-10%20at%2000.25.13.png)

**GPU**

- cloud class instance (from NVIDIA)
  - google colab
  - kaggle
  - amazon sagemaker
  - gradient
  - microsoft azure

> Tesla is graphics cards from NVIDIA for AI

**pyTorch**

- it does these heavy mathematical computation very easily with libraries
- it got a whole set of APIs and utilities that let you manipulate all of these different tensors

**tensors**

- a tensor is a computer data object (data structure) that represents numeric data,
- it could be floating point data values or data objects within data objects.
- 1d tensors (column)
- 2d tensors (xy)
- 3d tensors (xyz)
- 4d tensors (**cube**)
- 5d tensors
- 6d tensors

---

### Generative configuration

- Each model exposes a set of configuration parameters that can influence the model's output during inference.
  - training parameters: learned during training time.
  - configuration parameters: invoked at inference time and give control over things like the maximum number of tokens in the completion, and how creative the output is.

![Screenshot 2023-10-21 at 11.40.09](/assets/img/Screenshot%202023-10-21%20at%2011.40.09.png)

- **Max new tokens**: limit the number of tokens that the model will generate.

  - putting a cap on the number of times the model will go through the selection process.
  - the length of the completion is shorter

    - because another stop condition was reached, such as the model predicting and end of sequence token.
    - it's max new tokens, not a hard number of new tokens generated.
    - ![Screenshot 2023-10-21 at 11.46.18](/assets/img/Screenshot%202023-10-21%20at%2011.46.18.png)

  - The output from the `transformer's softmax layer` is a `probability distribution` across the entire dictionary of words that the model uses.
    - Here you can see a selection of words and their probability score next to them.
    - this is a list that carries on to the complete dictionary.

- **controls**

  - to generate text that's more natural, more creative and avoids repeating words, you need to use some other controls.

    - in some implementations, you may need to disable greedy and enable random sampling explicitly.
    - For example, the Hugging Face transformers implementation that we use in the lab requires that we set do sample to equal true.

  - `greedy decoding`

    - Most large language models by default will operate with `greedy decoding`.
    - the simplest form of next-word prediction
    - the model will always choose the word with the highest probability.
    - This method can work very well for short generation but is susceptible to repeated words or repeated sequences of words.

  - `Random sampling`

    - the easiest way to introduce some variability.
    - Instead of selecting the most probable word every time with random sampling, the model `chooses an output word at random using the probability distribution to weight the selection`.
    - depending on the setting, there is a possibility that the output may be too creative, producing words that cause the generation to wander off into topics or words that just don't make sense.

    - For example, in the illustration, the word banana has a probability score of 0.02. With random sampling, this equates to a 2% chance that this word will be selected. By using this sampling technique, we reduce the likelihood that words will be repeated.

    - ![Screenshot 2023-10-21 at 11.52.25](/assets/img/Screenshot%202023-10-21%20at%2011.52.25.png)

- **top k, top p sampling techniques**

  - help `limit the random sampling` and `increase the chance that the output will be sensible`.

    - With `top k`, you specify the number of tokens to randomly choose from
    - with `top p`, you specify the total probability that you want the model to choose from.

  - `top k` value:

    - limit the options while still allowing some variability
    - instructs the model to choose from only the k tokens with the highest probability.
    - In this example here, k is set to three, so you're restricting the model to choose from these three options. The model then selects from these options using the probability weighting and in this case, it chooses donut as the next word.
    - This method can help the model have some randomness while preventing the selection of highly improbable completion words.
    - This in turn makes the text generation more likely to sound reasonable and to make sense.
    - ![Screenshot 2023-10-21 at 12.04.16](/assets/img/Screenshot%202023-10-21%20at%2012.04.16.png)

  - `top p` setting:
    - limit the random sampling to the predictions whose combined probabilities do not exceed p.
    - For example, if you set p to equal 0.3, the options are cake and donut since their probabilities of 0.2 and 0.1 add up to 0.3. The model then uses the random probability weighting method to choose from these tokens.
    - ![Screenshot 2023-10-21 at 12.04.53](/assets/img/Screenshot%202023-10-21%20at%2012.04.53.png)

- **temperature**

  - control the randomness of the model output
  - can be adjusted to either increase or decrease randomness within the model output layer (softmax layer)

  - influences `the shape of the probability distribution` that the model calculates for the next token.

    - The temperature value is a `scaling factor that's applied within the final softmax layer of the model` that impacts `the shape of the probability distribution of the next token`.
    - In contrast to the `top k & p` parameters, changing the temperature actually alters the predictions that the model will make.

  - Broadly speaking, the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness.
  - ![Screenshot 2023-10-21 at 12.07.00](/assets/img/Screenshot%202023-10-21%20at%2012.07.00.png)

  - low value of temperature, say less than one,

    - the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words.
    - blue bars in the table show a probability bar chart turned on its side.
    - Most of the probability here is concentrated on the word cake. The model will select from this distribution using random sampling and the resulting text will be less random and will more closely follow the most likely word sequences that the model learned during training.

  - if set the temperature to a higher value, say, greater than one, then the model will calculate a broader flatter probability distribution for the next token.

    - Notice that in contrast to the blue bars, the probability is more evenly spread across the tokens.
    - This leads the model to generate text with a `higher degree of randomness and more variability` in the output compared to a `cool temperature setting`.
    - This can help you generate text that sounds more creative.

  - If leave the temperature value equal to one, this will leave the softmax function as default and the unaltered probability distribution will be used.

```py
generation_config = GenerationConfig(max_new_tokens=50)
# generation_config = GenerationConfig(max_new_tokens=10)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)

inputs = tokenizer(few_shot_prompt, return_tensors='pt')
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        generation_config=generation_config,
    )[0],
    skip_special_tokens=True
)

print(dash_line)
print(f'MODEL GENERATION - FEW SHOT:\n{output}')
print(dash_line)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
```

---

### Encoder & Decoder

- The transformer architecture is split into two distinct parts

  - the **encoder** and the **decoder**.
  - These components work in conjunction with each other and they share a number of similarities.

- The encoder `encodes input sequences into a deep representation of the structure and meaning of the input`.

- The decoder, working from input token triggers, `uses the encoder's contextual understanding to generate new tokens`.

- It does this in a loop until some stop condition has been reached.

- the inputs to the model are at the bottom and the outputs are at the top

![Screenshot 2023-10-15 at 19.52.36](/assets/img/Screenshot%202023-10-15%20at%2019.52.36.png)

#### Encoder vs Decoder

- **Encoder-only models**

  - also work as sequence-to-sequence models
  - without further modification, the input sequence and the output sequence or the same length, less common these days,
  - by adding additional layers to the architecture, you can train encoder-only models to perform `classification tasks` such as sentiment analysis,
  - encoder-only model example: BERT

- **Encoder-decoder models**

  - perform well on sequence-to-sequence tasks such as translation
  - the input sequence and the output sequence can be different lengths.
  - You can also scale and train this type of model to perform `general text generation tasks`.
  - encoder-decoder models examples: BART, T5

- **decoder-only models**
  - the most commonly used today.
  - as they have scaled, their capabilities have grown. These models can now generalize to most tasks.
  - Popular decoder-only models: GPT family of models, BLOOM, Jurassic, LLaMA, and many more.

![Screenshot 2023-10-15 at 20.45.16](/assets/img/Screenshot%202023-10-15%20at%2020.45.16.png)

LLM ä¸ºä»€ä¹ˆéƒ½ç”¨ Decoder only æ¶æ„ï¼Ÿ[^LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoderonlyæ¶æ„]

- è®­ç»ƒæ•ˆç‡å’Œå·¥ç¨‹å®ç°ä¸Šçš„ä¼˜åŠ¿
- åœ¨ç†è®ºä¸Šæ˜¯å› ä¸º Encoder çš„åŒå‘æ³¨æ„åŠ›ä¼šå­˜åœ¨ä½ç§©é—®é¢˜ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼›
- å¦ä¸€æ–¹é¢ï¼Œå°±ç”Ÿæˆä»»åŠ¡è€Œè¨€ï¼Œå¼•å…¥åŒå‘æ³¨æ„åŠ›å¹¶æ— å®è´¨å¥½å¤„ã€‚
- è€Œ Encoder-Decoder æ¶æ„ä¹‹æ‰€ä»¥èƒ½å¤Ÿåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ï¼Œå¤§æ¦‚åªæ˜¯å› ä¸ºå®ƒå¤šäº†ä¸€å€å‚æ•°ã€‚
- æ‰€ä»¥ï¼Œåœ¨åŒç­‰å‚æ•°é‡, åŒç­‰æ¨ç†æˆæœ¬ä¸‹ï¼ŒDecoder-only æ¶æ„å°±æ˜¯æœ€ä¼˜é€‰æ‹©äº†ã€‚

[^LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoderonlyæ¶æ„]: LLM ä¸ºä»€ä¹ˆéƒ½ç”¨ Decoder only æ¶æ„, https://www.zhihu.com/question/588325646?utm_division=hot_list_page

- **è®­ç»ƒæ•ˆç‡**: Decoder-only æ¶æ„åªéœ€è¦è¿›è¡Œå•å‘çš„è‡ªå›å½’é¢„æµ‹ï¼Œè€Œ Encoder-Decoder æ¶æ„éœ€è¦è¿›è¡ŒåŒå‘çš„è‡ªç¼–ç é¢„æµ‹å’Œå•å‘çš„è‡ªå›å½’é¢„æµ‹ï¼Œè®¡ç®—é‡æ›´å¤§ï¼›
- **å·¥ç¨‹å®ç°**: Decoder-only æ¶æ„åªéœ€è¦ä¸€ä¸ªæ¨¡å—ï¼Œè€Œ Encoder-Decoder æ¶æ„éœ€è¦ä¸¤ä¸ªæ¨¡å—ï¼Œå¹¶ä¸”éœ€è¦å¤„ç†ä¸¤è€…ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’å’Œå¯¹é½ï¼Œå®ç°èµ·æ¥æ›´å¤æ‚ï¼›
- **ç†è®ºåˆ†æ**: Encoder çš„åŒå‘æ³¨æ„åŠ›ä¼šå­˜åœ¨ä½ç§©é—®é¢˜ï¼Œå³æ³¨æ„åŠ›çŸ©é˜µçš„ç§©éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ è€Œé™ä½[^Attention]ï¼Œç»“è®ºæ˜¯å¦‚æœæ²¡æœ‰æ®‹å·®è¿æ¥å’Œ MLP å…œç€ï¼Œæ³¨æ„åŠ›çŸ©é˜µä¼šæœç§©ä¸º 1 çš„çŸ©é˜µæ”¶æ•›ï¼Œæœ€åæ¯ä¸ª token çš„è¡¨ç¤ºéƒ½ä¸€æ ·äº†ï¼Œç½‘ç»œå°±åºŸäº†ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è€Œ Decoder çš„å•å‘æ³¨æ„åŠ›åˆ™ä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜ï¼›
- **ç”Ÿæˆä»»åŠ¡**: å¯¹äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒEncoder çš„åŒå‘æ³¨æ„åŠ›å¹¶æ— å®è´¨å¥½å¤„ï¼Œå› ä¸ºå®ƒä¼šå¼•å…¥å³ä¾§çš„ä¿¡æ¯ï¼Œç ´åäº†è‡ªå›å½’çš„å‡è®¾ã€‚è€Œ Decoder çš„å•å‘æ³¨æ„åŠ›åˆ™å¯ä»¥ä¿æŒè‡ªå›å½’çš„ä¸€è‡´æ€§ã€‚

[^Attention]: Attention is not all you need: pure attention loses rank doubly exponentially with depth, https://arxiv.org/pdf/2103.03404.pdf

---

#### How the model works

![Screenshot 2023-10-15 at 20.46.53](/assets/img/Screenshot%202023-10-15%20at%2020.46.53.png)

- machine-learning models are just big statistical calculators and they work with numbers, not words.

  - **Tokenize**:

    - Before passing texts into the model to process, must first tokenize the words.
    - converts the words into numbers
    - each number representing a position in a dictionary of all the possible words that the model can work with.

    - You can choose from multiple tokenization methods. For example,

      - token IDs matching two complete words,
      - ![Screenshot 2023-10-15 at 19.53.08](/assets/img/Screenshot%202023-10-15%20at%2019.53.08.png)

      - or using token IDs to represent parts of words.
      - ![Screenshot 2023-10-15 at 19.53.34](/assets/img/Screenshot%202023-10-15%20at%2019.53.34.png)

    - once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text.

  - **Token Embedding**:

    - Now that the input is represented as numbers, pass it to the embedding layer.
    - This layer is a `trainable vector embedding space`,
    - high-dimensional space where each token is represented as a vector and occupies a unique location within that space.
    - Each token ID in the vocabulary is matched to a `multi-dimensional vector`, and the intuition is that these vectors learn to `encode the meaning and context of individual tokens in the input sequence`.

    - Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.

    - each word has been matched to a token ID, and each token is mapped into a vector.

      - In the original transformer paper, the **vector size** was actually 512
      - ![Screenshot 2023-10-15 at 20.02.13](/assets/img/Screenshot%202023-10-15%20at%2020.02.13.png)

      - For simplicity
        - imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.
        - relate words that are located close to each other in the embedding space, and calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language.
        - ![Screenshot 2023-10-15 at 20.02.47](/assets/img/Screenshot%202023-10-15%20at%2020.02.47.png)

  - **Positional Encoding**:

    - Added the token vectors into `the base of the encoder or the decoder`, also add positional encoding.
    - The model processes each of the input tokens in parallel.
    - it preserve the information about the word order and don't lose the relevance of the position of the word in the sentence.
    - ![Screenshot 2023-10-15 at 20.05.45](/assets/img/Screenshot%202023-10-15%20at%2020.05.45.png)

  - **Self-attention**

    - sum the `input tokens` and the `positional encodings`, pass the resulting vectors to the self-attention layer.
    - the model analyzes the relationships between the tokens in the input sequence, it allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.
    - The `self-attention weights` that are learned during training and stored in these layers reflect `the importance of each word in that input sequence to all other words in the sequence`.

    - But this does not happen just once, the transformer architecture actually has `multi-headed self-attention`.
    - ![Screenshot 2023-10-15 at 20.09.13](/assets/img/Screenshot%202023-10-15%20at%2020.09.13.png)

    - This means that `multiple sets of self-attention weights or heads` are learned in parallel independently of each other.

    - The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.

    - each self-attention head will learn a different aspect of language. For example,

      - one head may see the relationship between the people entities in our sentence.
      - another head may focus on the activity of the sentence.
      - another head may focus on some other properties such as if the words rhyme.
      - ![Screenshot 2023-10-15 at 20.12.47](/assets/img/Screenshot%202023-10-15%20at%2020.12.47.png)

    - don't dictate ahead of time what aspects of language the attention heads will learn.

      - The weights of each head are randomly initialized and given sufficient training data and time,
      - each will learn different aspects of language.
      - While some attention maps are easy to interpret, like the examples discussed here, others may not be.
      - Now that all of the attention weights have been applied to the input data, the output is processed through a `fully-connected feed-forward network`.
      - ![Screenshot 2023-10-15 at 20.13.33](/assets/img/Screenshot%202023-10-15%20at%2020.13.33.png)

    - The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary

  - **Softmax**

    - then pass these logits to a final softmax layer, where they are normalized into a `probability score for each word`.
    - This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here.
    - One single token will have a score higher than the rest.
    - This is the most likely predicted token.
    - there are a number of methods that you can use to vary the final selection from this vector of probabilities.

    - ![Screenshot 2023-10-15 at 20.22.45](/assets/img/Screenshot%202023-10-15%20at%2020.22.45.png)

---

#### Overall prediction process

At a very high level, the workflow can be divided into three stages:

- **Data preprocessing / embedding**:

  - This stage involves storing private data to be retrieved later.
  - Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database.

- **Prompt construction / retrieval**:

  - When a user submits a query, the application constructs a series of prompts to submit to the language model.
  - A compiled prompt typically combines
    - a prompt template hard-coded by the developer;
    - examples of valid outputs called few-shot examples;
    - any necessary information retrieved from external APIs;
    - and a set of relevant documents retrieved from the vector database.

- **Prompt execution / inference**:
  - Once the prompts have been compiled, they are submitted to a pre-trained LLM for inferenceâ€”including both proprietary model APIs and open-source or self-trained models.
  - Some developers also add operational systems like logging, caching, and validation at this stage.

example: Generating text with transformers

- translation task
  - a sequence-to-sequence task: the original objective of the transformer architecture designers.
  - use a transformer model to translate the French phrase `[FOREIGN]` into English.

Encoded side:

- First, `tokenize the input words` using this same tokenizer that was used to train the network.
- These tokens are then added into the input on the encoder side of the network, passed through the embedding layer, and then fed into the `multi-headed attention layers`.
- The outputs of the multi-headed attention layers are fed through a `feed-forward network` to the output of the encoder.

- At this point, the data that leaves the encoder is a **deep representation of the structure and meaning of the input sequence**.

Decoded side:

- This representation is inserted into the middle of the decoder to influence the `decoder's self-attention mechanisms`.
- Next, a `start of sequence token` is added to the input of the decoder.
- This `triggers the decoder to predict the next token`, based on the contextual understanding that it's being provided from the encoder.
- The output of the decoder's self-attention layers gets passed through the `decoder feed-forward network` and through a final `softmax output layer`.

- At this point, we have our first token.

- You'll continue this loop, passing the output token back to the input to trigger the generation of the next token, until the model predicts an end-of-sequence token.

- At this point, the final sequence of tokens can be detokenized into words, and you have the output.

- There are multiple ways in which you can use the output from the softmax layer to predict the next token. These can influence how creative you are generated text is.

![Screenshot 2023-10-15 at 20.43.16](/assets/img/Screenshot%202023-10-15%20at%2020.43.16.png)

---

##### Data preprocessing / embedding

**Contextual data input**

- Contextual data for LLM apps includes text documents, PDFs, and even structured formats like CSV or SQL tables.
- Data-loading and transformation solutions for this data vary widely across developers.
  - Most use traditional ETL tools like `Databricks` or `Airflow`.
  - Some also use `document loaders` built into orchestration frameworks like `LangChain` (powered by Unstructured) and `LlamaIndex` (powered by Llama Hub).

**embeddings**,

- most developers use the `OpenAI API`, specifically with the text-embedding-ada-002 model. Itâ€™s easy to use (especially if youâ€™re already already using other OpenAI APIs), gives reasonably good results, and is becoming increasingly cheap.
- Some larger enterprises are also exploring `Cohere`, which focuses their product efforts more narrowly on embeddings and has better performance in certain scenarios.
- For developers who prefer open-source, the `Sentence Transformers library` from `Hugging Face` is a standard.

**vector database**

- The most important piece of the preprocessing pipeline, from a systems standpoint
- Itâ€™s responsible for efficiently storing, comparing, and retrieving up to billions of embeddings (i.e., vectors).

  - The most common choice is `Pinecone`. Itâ€™s the default because itâ€™s fully cloud-hosted, easy to get started with, and has many of the features larger enterprises need in production (e.g., good performance at scale, SSO, and uptime SLAs).

  - `Open source systems` like Weaviate, Vespa, and Qdrant: They generally give excellent single-node performance and can be tailored for specific applications, so they are popular with experienced AI teams who prefer to build bespoke platforms.

  - `Local vector management libraries` like Chroma and Faiss: They have great developer experience and are easy to spin up for small apps and dev experiments. They donâ€™t necessarily substitute for a full database at scale.

  - `OLTP extensions` like pgvector: good solution for devs who see every database-shaped hole and try to insert Postgres, or enterprises who buy most of their data infrastructure from a single cloud provider. Itâ€™s not clear, in the long run, if it makes sense to tightly couple vector and scalar workloads.

- Looking ahead, most of the open source vector database companies are developing cloud offerings. Our research suggests achieving strong performance in the cloud, across a broad design space of possible use cases, is a very hard problem. Therefore, the option set may not change massively in the near term, but it likely will change in the long term. The key question is whether vector databases will resemble their OLTP and OLAP counterparts, consolidating around one or two popular systems.

- the embedding pipeline may become more important over time
  - how embeddings and vector databases will evolve as the usable context window grows for most models.
  - Itâ€™s tempting to say embeddings will become less relevant, because contextual data can just be dropped into the prompt directly.
  - However, feedback from experts on this topic suggests the opposite, that the embedding pipeline may become more important over time. Large context windows are a powerful tool, but they also entail significant computational cost. So making efficient use of them becomes a priority.
  - We may start to see different types of embedding models become popular, trained directly for model relevancy, and vector databases designed to enable and take advantage of this.

##### Prompt construction / retrieval

- Strategies for prompting LLMs and incorporating contextual data are becoming increasingly complexâ€”and increasingly important as a source of product differentiation.

- Most developers start new projects by experimenting with simple prompts, consisting of direct instructions (`zero-shot prompting`) or some example outputs (`few-shot prompting`).

  - These prompts often give good results but fall short of accuracy levels required for production deployments.

- The next level of prompting `jiu jitsu` is designed to ground model responses in some source of truth and provide external context the model wasnâ€™t trained on.

**advanced prompting strategies**

- The Prompt Engineering Guide catalogs no fewer than 12 more advanced prompting strategies, including:
  - chain-of-thought, self-consistency, generated knowledge, tree of thoughts, directional stimulus, and many others.
- These strategies can also be used in conjunction to support different LLM use cases like document question answering, chatbots, etc.

**Orchestration frameworks**

- `LangChain` and `LlamaIndex` shine.
- workflow:

  - They abstract away many of the details of prompt chaining;
  - interfacing with external APIs (including determining when an API call is needed);
  - retrieving contextual data from vector databases;
  - and maintaining memory across multiple LLM calls.
  - They also provide templates for many of the common applications mentioned above.

- Their output is a prompt, or series of prompts, to submit to a language model. These frameworks are widely used among hobbyists and startups looking to get an app off the ground .

- LangChain is still a relatively new project (currently on version 0.0.201), but weâ€™re already starting to see apps built with it moving into production.
- Some developers, especially early adopters of LLMs, prefer to switch to raw Python in production to eliminate an added dependency. But we expect this DIY approach to decline over time for most use cases, in a similar way to the traditional web app stack.

- ChatGPT.
  - In its normal incarnation, ChatGPT is an app, not a developer tool. But it can also be accessed as an API.
  - it performs some of the same functions as other orchestration frameworks, such as: abstracting away the need for bespoke prompts; maintaining state; and retrieving contextual data via plugins, APIs, or other sources.
  - While not a direct competitor to the other tools listed here, ChatGPT can be considered a substitute solution, and it may eventually become a viable, simple alternative to prompt construction.

##### Prompt execution / inference

- Prompt execution / inference

  - `OpenAI`
    - Today, OpenAI is the leader among language models. Nearly every developer starts new LLM apps using the OpenAI API with the gpt-4 or gpt-4-32k model.
    - This gives a best-case scenario for app performance and is easy to use, in that it operates on a wide range of input domains and usually requires no fine-tuning or self-hosting.

- When projects go into production and start to scale, a broader set of options come:

  - `Switching to gpt-3.5-turbo`: Itâ€™s ~50x cheaper and significantly faster than GPT-4. Many apps donâ€™t need GPT-4-level accuracy, but do require low latency inference and cost effective support for free users.

  - `Other proprietary vendors (like Anthropicâ€™s Claude models)`: Claude offers fast inference, GPT-3.5-level accuracy, more customization options for large customers, and up to a 100k context window (though weâ€™ve found accuracy degrades with the length of input).

  - `Triaging requests to open source models`: This can be especially effective in high-volume B2C use cases like search or chat, where thereâ€™s wide variance in query complexity and a need to serve free users cheaply.

    - conjunction with fine-tuning open source base models, platforms like Databricks, Anyscale, Mosaic, Modal, and RunPod are used by a growing number of engineering teams.

    - A variety of inference options are available for open source models, including simple API interfaces from Hugging Face and Replicate; raw compute resources from the major cloud providers; and more opinionated cloud offerings like those listed above.

- `Open-source models` trail `proprietary offerings`, but the gap is starting to close.

  - The LLaMa models from Meta

    - set a new bar for open source accuracy and kicked off a flurry of variants.
    - Since LLaMa was licensed for research use only, a number of new providers have stepped in to train alternative base models (e.g., Together, Mosaic, Falcon, Mistral).
    - Meta is also debating a truly open source release of LLaMa2.

    - When open source LLMs reach accuracy levels comparable to GPT-3.5, we expect to see a Stable Diffusion-like moment for textâ€”including massive experimentation, sharing, and productionizing of fine-tuned models.

  - Hosting companies like Replicate are already adding tooling to make these models easier for software developers to consume. Thereâ€™s a growing belief among developers that smaller, fine-tuned models can reach state-of-the-art accuracy in narrow use cases.

- Most developers havenâ€™t gone deep on operational tooling for LLMs yet.

  - Caching is relatively commonâ€”usually based on Redisâ€”because it improves application response times and cost.
  - Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used. They can log, track, and evaluate LLM outputs, usually for the purpose of improving prompt construction, tuning pipelines, or selecting models.
  - There are also a number of new tools being developed to validate LLM outputs (e.g., Guardrails) or detect prompt injection attacks (e.g., Rebuff). Most of these operational tools encourage use of their own Python clients to make LLM calls, so it will be interesting to see how these solutions coexist over time.

- the static portions of LLM apps (i.e. everything other than the model) also need to be hosted somewhere.
  - The most common solutions weâ€™ve seen so far are standard options like Vercel or the major cloud providers.
  - Startups like Steamship provide end-to-end hosting for LLM apps, including orchestration (LangChain), multi-tenant data contexts, async tasks, vector storage, and key management.
  - And companies like Anyscale and Modal allow developers to host models and Python code in one place.

#### AI agents frameworks

- `AutoGPT`, described as â€œan experimental open-source attempt to make GPT-4 fully autonomous,â€

- The in-context learning pattern is effective at solving hallucination and data-freshness problems, in order to better support content-generation tasks.

- Agents, on the other hand, give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment.

  - They do this through a combination of `advanced reasoning/planning, tool usage, and memory / recursion / self-reflection`.

- agents have the potential to become a central piece of the LLM app architecture

- And existing frameworks like LangChain have incorporated some agent concepts already. Thereâ€™s only one problem: agents donâ€™t really work yet. Most agent frameworks today are in the proof-of-concept phaseâ€”capable of incredible demos but not yet reliable, reproducible task-completion.

---

### LLM Tools

#### Medusa

> Our approach revisits an underrated gem from the paper "Blockwise Parallel Decoding for Deep Autoregressive Models" [Stern et al. 2018] back to the invention of the Transformer model.
> rather than pulling in an entirely new draft model to predict subsequent tokens, why not simply extend the original model itself? This is where the "Medusa heads" come in.

- a simpler, user-friendly framework for accelerating LLM generation.

- Instead of using an additional draft model like `speculative decoding`, Medusa merely introduces a few additional decoding heads, following the idea of [Stern et al. 2018] with some other ingredients.

- Despite its simple design, Medusa can improve the generation efficiency of LLMs by about 2x.

- These additional decoding heads seamlessly integrate with the original model, producing blocks of tokens at each generative juncture.

benefit:

- Unlike the **draft model**, Medusa heads can be trained in conjunction with the **original model** (which remains frozen during training). This method allows for `fine-tuning large models on a single GPU`, taking advantage of the powerful base model's learned representations.

- also, since the new heads consist of just a single layer akin ç±»ä¼¼çš„ to the **original language model head**, Medusa does not add complexity to the serving system design and is friendly to distributed settings.

- On its own, Medusa heads don't quite hit the mark of doubling processing speeds. But here's the twist:

  - When we pair this with a tree-based attention mechanism, we can verify several candidates generated by Medusa heads in parallel. This way, the Medusa heads' predictive prowess truly shone through, offering a 2x to 3x boost in speed.

  - Eschewing the traditional importance sampling scheme, we created an efficient and high-quality alternative crafted specifically for the generation with Medusa heads. This new approach entirely sidesteps the **sampling** overhead, even adding an extra pep to Medusa's already accelerated step.

- In a nutshell, we solve the challenges of speculative decoding with a simple system:

  - No separate model: Instead of introducing a new draft model, train multiple decoding heads on the same model.

  - Simple integration to existing systems: The training is parameter-efficient so that even GPU poor can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.

  - Treat sampling as a relaxation æ”¾æ¾: Relaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.

- The figure below offers a visual breakdown of the Medusa pipeline for those curious about the nuts and bolts.

![Screenshot 2023-09-21 at 14.49.26](/assets/img/post/Screenshot%202023-09-21%20at%2014.49.26.png)

Overview of Medusa

- Medusa introduces `multiple heads` on top of the last hidden states of the LLM, enabling the prediction of several subsequent tokens in parallel.

- When augmenting a model with Medusa heads, the original model is frozen during training, and only the Medusa heads undergo fine-tuning. This approach makes it feasible to `fine-tune large models on a single GPU`.

- During inference, each head generates multiple top predictions for its designated position. These predictions are assembled into candidates and processed in parallel using a `tree-based attention mechanism`.

- The final step involves utilizing a **typical acceptance scheme** to select reasonable continuations, and the longest accepted candidate prefix will be used for the next decoding phase.

- The efficiency of the decoding process is enhanced by accepting more tokens simultaneously, thus reducing the number of required decoding steps.

Let's dive into the three components of Medusa: Medusa heads, tree attention, and typical acceptance scheme.

##### Medusa heads

- akin to the language model head in the original architecture (the last layer of a causal Transformer model), but with a twist:

  - they predict multiple forthcoming tokens, not just the immediate next one. Drawing inspiration from the Blockwise Parallel Decoding approach, we implement each Medusa head as a single layer of feed-forward network, augmented with a residual connection.

- Training these heads is remarkably straightforward. either use the same corpus æœ¬ä½“ that trained the original model or generate a new corpus using the model itself.

- Importantly, during this training phase, the original model remains static; only the Medusa heads are fine-tuned.

- This targeted training results in a highly parameter-efficient process that reaches convergence è¶‹åŒ swiftly è¿…é€Ÿåœ°, especially when compared to the computational heaviness æ²‰é‡ of **training a separate draft model in speculative decoding methods**.

- The efficacy of Medusa heads is quite impressive. Medusa heads achieve a top-1 accuracy rate of approximately 60% for predicting the 'next-next' token.

##### Tree attention

- During our tests, we uncovered some striking metrics: although the top-1 accuracy for predicting the 'next-next' token hovers around 60%, the top-5 accuracy soars to over 80%.

- This substantial increase indicates that if we can strategically leverage the multiple top-ranked predictions made by the Medusa heads, we can significantly amplify the number of tokens generated per decoding step.

  - With this goal, we first craft a set of candidates by taking the Cartesian product of the top predictions from each Medusa head.

  - We then encode the dependency graph into the attention following the idea from graph neural networks so that we can process multiple candidates in parallel.

![Screenshot 2023-09-21 at 15.05.03](/assets/img/post/Screenshot%202023-09-21%20at%2015.05.03.png)

Tree Attention. This visualization demonstrates the use of tree attention to process multiple candidates concurrently.

- As exemplified, the top-2 predictions from the first Medusa head and the top-3 from the second result in 2\*3=6 candidates. Each of these candidates corresponds to a distinct branch within the tree structure.

- To guarantee that each token only accesses its predecessors, we devise an **attention mask** that exclusively permits attention flow from the current token back to its antecedent tokens. The positional indices for positional encoding are adjusted in line with this structure.

- For example, let's consider a scenario where we use top-2 predictions from the first Medusa head and top-3 predictions from the second

  - In this case, any prediction from the first head could be paired with any prediction from the second head, culminating in a multi-level tree structure.
  - Each level of this tree corresponds to predictions from one of the Medusa heads. Within this tree, we implement an attention mask that restricts attention only to a token's predecessors, preserving the concept of historical context.
  - By doing so and by setting positional indices for positional encoding accordingly, we can process a wide array of candidates simultaneously without needing to inflate the batch size.

- We would also remark that a few independent works also adopt very similar ideas of tree attention [1, 2]. Compared with them, our methodology leans towards a simpler form of tree attention where the tree pattern is regular and fixed during inference, which enables a preprocessing of tree attention mask that further improves the efficiency.

##### Typical acceptance

> - In earlier research on speculative decoding, the technique of **importance sampling** was used to generate diverse outputs closely aligned with the original model's predictions.

> - However, later studies showed that this method tends to become less efficient as you turn up the "creativity dial," known as the **sampling temperature**.

- In simpler terms, if the draft model is just as good as the original model, you should ideally accept all its outputs, making the process super efficient. However, importance sampling will likely reject this solution in the middle.

- In the real world, we often tweak the sampling temperature just to control the model's creativity, not necessarily to match the original model's distribution. So why not focus on just accepting plausible è²Œä¼¼åˆç†çš„ candidates?

- We then introduce the **typical acceptance scheme**.

  - Drawing inspiration from existing work on truncation ç¼©çŸ­ sampling, we aim to pick candidates that are likely enough according to the original model. We set a threshold based on the original model's prediction probabilities, and if a candidate exceeds this, it's accepted.

  - In technical jargon, we take the minimum of a hard threshold and an entropy ç†µ, dependent threshold to decide whether to accept a candidate as in truncation sampling.
  - This ensures that meaningful tokens and reasonable continuations are chosen during decoding.
  - We always accept the first token using greedy decoding, ensuring that at least one token is generated in each step.
  - The final output is then the longest sequence that passes our acceptance test.

- What's great about this approach is its adaptability.

  - If set the sampling temperature to zero, it simply reverts to the most efficient form, greedy decoding.

  - When you increase the temperature, our method becomes even more efficient, allowing for longer accepted sequences, a claim we've confirmed through rigorous testing.

- in essence, our typical acceptance scheme offers a more efficient way to generate the creative output of LLMs.

##### accelerate models

- We tested Medusa with Vicuna models (specialized Llama models fine-tuned specifically for chat applications).

  - These models vary in size, with parameter counts of 7B, 13B, and 33B.
  - Our goal was to measure how Medusa could accelerate åŠ é€Ÿ these models in a real-world chatbot environment.

- When it comes to training Medusa heads, we opted for a simple approach. We utilized the **publicly available ShareGPT dataset**, a subset of the **training data originally used for Vicuna models** and only trained for a single epoch.

- this entire training process could be completed in just a few hours to a day, depending on the model size, all on a single A100-80G GPU.

- Notably, Medusa can be easily combined with a **quantized base model** to reduce the memory requirement. We take this advantage and use an 8-bit quantization é‡åŒ– when training the 33B model.

- To simulate a real-world setting, we use the MT bench for evaluation. The results were encouraging: With its simple design, Medusa consistently achieved approximately a 2x speedup in wall time across a broad spectrum of use cases.

- Remarkably, with Medusa's optimization, a 33B parameter Vicuna model could operate as swiftly as a 13B model.

![Screenshot 2023-09-21 at 15.23.56](/assets/img/post/Screenshot%202023-09-21%20at%2015.23.56.png)

![Screenshot 2023-09-21 at 15.24.02](/assets/img/post/Screenshot%202023-09-21%20at%2015.24.02.png)

##### Ablation Study æ¶ˆèç ”ç©¶

- When harnessing the predictive abilities of Medusa heads, we enjoy the flexibility to select how many top candidates each head should consider.

  - For instance, we might opt for the top-3 predictions from the first head and the top-2 from the second. When we take the Cartesian product of these top candidates, we generate a set of six continuations for the model to evaluate.

- This level of configurability comes with its trade-offs.
- On the one hand, selecting more top predictions increases the likelihood of the model accepting generated tokens.
- On the other, it also raises the computational overhead at each decoding step. To find the optimal balance, we experimented with various configurations and identified the most effective setup, as illustrated in the accompanying figure.

- In the typical acceptance scheme, a critical hyperparameterâ€”referred to as the
- 'threshold': whether the tokens generated are plausible based on the model's own predictions. The higher this threshold, the more stringent the criteria for acceptance, which in turn impacts the overall speedup gained through this approach.

- We explore this trade-off between quality and speedup through experiments on two creativity-oriented tasks from the MT bench. The results, depicted in the figure, reveal that the typical acceptance offers a 10% speedup compared to greedy decoding methods. This speedup is notably better than when employing speculative decoding with random sampling, which actually slowed down the process compared to greedy decoding.

![Screenshot 2023-09-21 at 15.28.12](/assets/img/post/Screenshot%202023-09-21%20at%2015.28.12.png)

---

### LLM Data Training

[./2023-04-24-LLM_DataTraining.md]

---

### Confidence score for ML model

[./2023-04-24-AI_ConfidenceScore.md]

---

### Transparency

Transparency in the context of AI models refers to the degree to which the inner workings of the model are understandable, interpretable, and explainable to humans. It encompasses several aspects:

- Explainability:
  - This refers to the ability to understand and interpret the model's decisions.
  - An interpretable model `provides clear and understandable reasons` for its predictions or actions.
  - This is crucial, especially in high-stakes applications like healthcare or finance, where accountability and trust are essential.
- Visibility:
  - Transparency also involves making the model architecture, parameters, and training data visible to those who are affected by its decisions.
  - This allows external parties to scrutinize the model for biases, ethical concerns, or potential risks.
- Audibility:
  - The ability to audit an AI model involves examining its processes, inputs, and outputs to ensure it aligns with ethical and legal standards.
  - Auditing enhances accountability and helps identify and rectify issues or biases.
- Comprehensibility:
  - A transparent AI model should be comprehensible to various stakeholders, including domain experts, policymakers, and the general public. This involves presenting complex technical concepts in a way that is accessible to non-experts.
- Fairness and Bias: Transparency also relates to addressing biases in AI models. Understanding how the model makes decisions can help identify and rectify biased behavior, ensuring fair treatment across diverse demographic groups.
- Transparency is crucial for building trust in AI systems, especially as they are increasingly integrated into various aspects of society. It helps users, regulators, and the general public understand how AI systems function, assess their reliability, and hold developers and organizations accountable for their impact. Various techniques and tools are being developed to enhance the transparency of AI models, but it remains an ongoing area of research and development.

#### The Foundation Model Transparency Index

The Foundation Model Transparency Index [^The_Foundation_Model_Transparency_Index].

[^The_Foundation_Model_Transparency_Index]: The Foundation Model Transparency Index, https://crfm.stanford.edu/fmti/fmti.pdf

Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts.

While the societal impact of foundation models is growing, `transparency is on the decline`, mirroring the opacity that has plagued past digital technologies (e.g. social media).

Reversing this trend is essential:

- transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the founda- tion model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The 2023 Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g. data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.

![Screenshot 2023-11-13 at 12.47.10](/assets/img/Screenshot%202023-11-13%20at%2012.47.10.png)

---

### Generative AI project lifecycle

![Screenshot 2023-10-21 at 12.31.56](/assets/img/Screenshot%202023-10-21%20at%2012.31.56.png)

![Screenshot 2023-10-21 at 12.32.35](/assets/img/Screenshot%202023-10-21%20at%2012.32.35.png)

---

### Pre-training large language models

Source:

- [coursera:generative-ai-with-llms](https://www.coursera.org/learn/generative-ai-with-llms/lecture/2T3Au/pre-training-large-language-models)

> Once scoped out the use case, and determined how you'll need the LLM to work within the application, the next step is to select a model to work with.
> the first choice will be to either work with an existing model, or train the own from scratch.
> There are specific circumstances where training the own model from scratch might be advantageous, and you'll learn about those later in this lesson. In general, however, you'll begin the process of developing the application using an existing foundation model.

- The developers of some of the major frameworks for building generative AI applications like `Hugging Face` and `PyTorch`, have curated hubs where you can browse these models.

- Variance of the transformer model architecture are suited to different language tasks, largely because of differences in how the models are trained.

![Screenshot 2024-05-01 at 11.23.17](/assets/img/Screenshot%202024-05-01%20at%2011.23.17.png)

#### Model Architecture and Pre-training objective

- High-level look at the **initial training process** for LLMs.

- This phase is often referred to as **pre-training phase**.

- self-supervised learning step

  1. LLMs encode a `deep statistical representation` of language

  2. the model <font color=OrangeRed> learns from vast amounts of unstructured textual data </font>.
     1. This can be gigabytes, terabytes, and even petabytes of text.
     2. This data is pulled from many sources and assembled specifically for training language models.

  3. the model <font color=OrangeRed> internalizes the patterns and structures </font> present in the language.
     1. These patterns then enable the model to complete its training objective, depends on the architecture of the model

  4. During pre-training, the model <font color=OrangeRed> weights get updated to minimize the loss of the training objective </font>.

  5. The **encoder** generates an `embedding or vector representation` for each **token**.

![Screenshot 2024-05-01 at 11.26.33](/assets/img/Screenshot%202024-05-01%20at%2011.26.33.png)

- Pre-training also requires a large amount of compute and the use of GPUs.

- data quality curation
  - when scrape training data from public sites such as the Internet, often need to process the data to increase quality, address bias, and remove other harmful content.
  - As a result of this data quality curation, often `only 1-3% of tokens` are used for pre-training.
  - You should consider this when you estimate how much data you need to collect to pre-train the own model.

![Screenshot 2024-05-01 at 11.28.54](/assets/img/Screenshot%202024-05-01%20at%2011.28.54.png)

- 3 variance of the transformer model;
  - encoder-only, encoder-decoder models, and decode-only.
    - Each of these is trained on a different objective, and so learns how to carry out different tasks.
    - **Autoencoding models**: pre-trained using `masked language modeling`, correspond to the encoder part of the original transformer architecture, and are often used with `sentence classification or token classification`.
    - **Autoregressive models**: pre-trained using `causal language modeling`, make use of the decoder component of the original transformer architecture, and often used for `text generation`.
    - **Sequence-to-sequence models**: use both the encoder and decoder part off the original transformer architecture. The exact details of the pre-training objective vary from model to model. often used for `translation, summarization, and question-answering`.

![Screenshot 2024-05-01 at 11.29.07](/assets/img/Screenshot%202024-05-01%20at%2011.29.07.png)

![Screenshot 2024-05-01 at 11.34.13](/assets/img/Screenshot%202024-05-01%20at%2011.34.13.png)

- <font color=LightSlateBlue> Encoder-only models </font>

  - also known as `Autoencoding models`
  - pre-trained using `masked language modeling`.
  - This is also called a `denoising objective`.
  - tokens in the input sequence are randomly mask, and the training objective is to <font color=LightSlateBlue> predict the mask tokens in order to reconstruct the original sentence. </font>.
  - Autoencoding models spilled bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before.

  - Encoder-only models are ideally suited to task that benefit from this <Font color=OrangeRed> bi-directional </font> contexts.
    - carry out sentence classification tasks, for example, sentiment analysis or token-level tasks like <font color=OrangeRed> named entity recognition or word classification. </font>
    - autoencoder model: BERT and RoBERTa.

![Screenshot 2024-05-01 at 11.29.41](/assets/img/Screenshot%202024-05-01%20at%2011.29.41.png)

![Screenshot 2024-05-01 at 11.30.18](/assets/img/Screenshot%202024-05-01%20at%2011.30.18.png)

- <font color=LightSlateBlue> decoder-only </font>

  - `autoregressive models`,
  - pre-trained using `causal language modeling`.
  - Here, the training objective is to <font color=LightSlateBlue> predict the next token based on the previous sequence of tokens </font>.
  - `full language modeling`
    - Decoder-based autoregressive models, mask the input sequence and can only see the input tokens leading up to the token in question.
    - The model has no knowledge of the end of the sentence. The model then iterates over the input sequence one by one to predict the following token.
    - In contrast to the encoder architecture, this means that the context is <Font color=OrangeRed> unidirectional </font>.
  - By learning to predict the next token from a vast number of examples, the model `builds up a statistical representation of language`.

  - Models of this type make use of the decoder component off the original architecture without the encoder.
    - Decoder-only models are often used for <font color=OrangeRed> text generation </font>, although larger decoder-only models show strong zero-shot inference abilities, and can often perform a range of tasks well.
    - decoder-based autoregressive models: GBT and BLOOM.

![Screenshot 2024-05-01 at 11.31.14](/assets/img/Screenshot%202024-05-01%20at%2011.31.14.png)

![Screenshot 2024-05-01 at 11.31.42](/assets/img/Screenshot%202024-05-01%20at%2011.31.42.png)

- <font color=LightSlateBlue> encoder-decoder models </font>
  - sequence-to-sequence model
  - uses both the encoder and decoder parts off the original transformer architecture.
  - The exact details of the pre-training objective vary from model to model.
  - A popular sequence-to-sequence model T5,
    - pre-trains the encoder using span corruption, which <font color=LightSlateBlue> masks random sequences of input tokens. Those mass sequences are then replaced with a unique Sentinel token (x). </font>.
      - Sentinel tokens are special tokens added to the vocabulary, but do not correspond to any actual word from the input text
    - The decoder is then tasked with <font color=LightSlateBlue> reconstructing the mask token sequences auto-regressively. The output is the Sentinel token followed by the predicted tokens </font>.
  - use sequence-to-sequence models for <font color=OrangeRed> translation, summarization, and question-answering </font>. cases where you have a body of texts as both input and output.
  - well-known encoder-decoder model: T5. BART

![Screenshot 2024-05-01 at 11.33.09](/assets/img/Screenshot%202024-05-01%20at%2011.33.09.png)

![Screenshot 2024-05-01 at 11.33.02](/assets/img/Screenshot%202024-05-01%20at%2011.33.02.png)

- the larger a model, the more likely it is to work as you needed to without additional in-context learning or further training.
- training these enormous models is difficult and very expensive, so that it may be infeasible to continuously train larger and larger models.

![Screenshot 2024-05-01 at 11.36.11](/assets/img/Screenshot%202024-05-01%20at%2011.36.11.png)

---

#### Computational challenges of training LLMs

One of the most common issues you still counter when you try to train large language models is running out of memory.

![Screenshot 2024-05-01 at 11.52.37](/assets/img/Screenshot%202024-05-01%20at%2011.52.37.png)

- on Nvidia GPUs, CUDA, short for `Compute Unified Device Architecture`, is a collection of libraries and tools developed for Nvidia GPUs.
- Libraries such as PyTorch and TensorFlow use CUDA to boost performance on metrics multiplication and other operations common to deep learning.
- You'll encounter these out-of-memory issues because most LLMs are huge, and require a ton of memory to store and train all of their parameters.

![Screenshot 2024-05-01 at 11.58.17](/assets/img/Screenshot%202024-05-01%20at%2011.58.17.png)

- A single parameter is typically represented by a `32-bit float`, which is a way computers represent real numbers. A 32-bit float takes up four bytes of memory.
- So to store `1B parameter`s -> four gigabyte of GPU RAM at 32-bit full precision.

to train the model, you'll have to plan for `the memory to store the model weights` + `additional components that use GPU memory during training`.

- These include two Adam optimizer states, gradients, activations, and temporary variables needed by the functions. This can easily lead to 20 extra bytes of memory per model parameter.

- to account for all of these overhead during training, you'll actually require approximately 6 times the amount of GPU RAM that the model weights alone take up.
- To train a `1b parameter` model at 32-bit full precision -> approximately 24 gigabyte of GPU RAM.

![Screenshot 2024-05-01 at 12.00.53](/assets/img/Screenshot%202024-05-01%20at%2012.00.53.png)

![Screenshot 2024-05-01 at 12.02.35](/assets/img/Screenshot%202024-05-01%20at%2012.02.35.png)

To reduce the memory required for training

##### quantization

![Screenshot 2024-05-01 at 12.03.52](/assets/img/Screenshot%202024-05-01%20at%2012.03.52.png)

- reduce the memory required to store the weights of the model by reducing their precision from `32-bit floating point numbers` to `16-bit floating point numbers`, or `8-bit integer numbers`.

- Quantization statistically projects the original `32-bit floating point numbers` into a lower precision space, using scaling factors calculated based on the range of the original `32-bit floating point numbers`.

- Let's look at an example.
- ![Screenshot 2024-05-01 at 12.05.48](/assets/img/Screenshot%202024-05-01%20at%2012.05.48.png)
- ![Screenshot 2024-05-01 at 12.06.14](/assets/img/Screenshot%202024-05-01%20at%2012.06.14.png)
- ![Screenshot 2024-05-01 at 12.06.14](/assets/img/Screenshot%202024-05-01%20at%2012.06.14_774fet8i4.png)

![Screenshot 2024-05-01 at 12.06.55](/assets/img/Screenshot%202024-05-01%20at%2012.06.55.png)

- Suppose you want to store a PI to six decimal places in different positions.

  - Floating point numbers are stored as a series of bits zeros and ones.
  - The **32 bits** to store numbers in full precision with FP32 consist of
    - 1 bit for the sign where zero indicates a positive number, and one a negative number.
    - 8 bits for the exponent of the number,
    - 23 bits representing the fraction of the number. The fraction is also referred to as the mantissa, or significant. It represents the precision bits off the number.
  - If convert the 32-bit floating point value back to a decimal value, you notice the slight loss in precision.

  - if project this FP32 representation of Pi into the **FP16**, 16-bit lower precision space. The 16 bits consists of one bit for the sign, as you saw for FP32, but now FP16 only assigns five bits to represent the exponent and 10 bits to represent the fraction. Therefore, the range of numbers you can represent with FP16 is vastly smaller from negative 65,504 to positive 65,504. The original FP32 value gets projected to 3.140625 in the 16-bit space. Notice that you lose some precision with this projection. There are only six places after the decimal point now. You'll find that `this loss in precision is acceptable in most cases because you're trying to optimize for memory footprint`. Storing a value in FP32 requires four bytes of memory. In contrast, storing a value on FP16 requires only two bytes of memory, so with quantization you have reduced the memory requirement by half.

  - The AI research community has explored ways to optimize16-bit quantization. One datatype in particular **BFLOAT16**, has recently become a popular alternative to FP16. BFLOAT16, short for Brain Floating Point Format developed at Google Brain has become a popular choice in deep learning. Many LLMs, including FLAN-T5, have been pre-trained with BFLOAT16. BFLOAT16 or BF16 is a hybrid between half precision FP16 and full precision FP32. BF16 significantly helps with training stability and is supported by newer GPU's such as NVIDIA's A100. BFLOAT16 is often described as a truncated 32-bit float, as it captures the full dynamic range of the full 32-bit float, that uses only 16-bits. BFLOAT16 uses the full eight bits to represent the exponent, but truncates the fraction to just seven bits. `This not only saves memory, but also increases model performance by speeding up calculations`. The downside is that BF16 is not well suited for integer calculations, but these are relatively rare in deep learning.

  - if you quantize Pi from the 32-bit into **INT8** eight bit space. If you use one bit for the sign INT8 values are represented by the remaining seven bits. This gives you a range to represent numbers from negative 128 to positive 127 and unsurprisingly Pi gets projected two or three in the 8-bit lower precision space. This brings new memory requirement down from originally four bytes to just one byte, but obviously results in a pretty dramatic loss of precision.

  - By applying quantization, you can
    - reduce the `memory consumption` required to store the model parameters down to only two gigabyte using 16-bit half precision of 50% saving
    - further reduce the `memory footprint` by another 50% by representing the model parameters as eight bit integers, which requires only one gigabyte of GPU RAM.

![Screenshot 2024-05-01 at 12.10.36](/assets/img/Screenshot%202024-05-01%20at%2012.10.36.png)

- the goal of quantization is to <font color=OrangeRed> reduce the memory required to store and train models by reducing the precision off the model weights </font>.
- Quantization statistically projects the original `32-bit floating point numbers` into lower precision spaces using scaling factors calculated based on the range of the original 32-bit floats.

- Modern deep learning frameworks and libraries support quantization-aware training, which learns the quantization scaling factors during the training process.

- **BFLOAT16** has become a popular choice of precision in deep learning as `it maintains the dynamic range` of **FP32**, but reduces the memory footprint by half.
- in all these cases you still have a model with `1B parameters`.

- Quantization will give you the same degree of savings when it comes to **training**. However, many models now have sizes in excess of 50B or even 100B parameters. Meaning you'd need up to 500 times more memory capacity to train them, tens of thousands of gigabytes. These enormous models dwarf the `1B parameter` model we've been considering, shown here to scale on the left.

- As modal scale beyond a few billion parameters, it becomes impossible to train them on a single GPU. you'll need to turn to **distributed computing techniques** while you `train the model across multiple GPUs`. This could require access to hundreds of GPUs, which is very expensive.
- Another reason why you won't pre-train the own model from scratch most of the time. However, an additional training process called fine-tuning, also require storing all training parameters in memory and it's very likely you'll want to fine tune a model at some point.

---

#### Efficient multi-GPU compute strategies

`multi GPU compute strategies`: distribute compute across GPUs, scale the model training efforts beyond a single GPU.
- when the model becomes too big to fit in a single GPU.
- or speed up the training even if the model does fit onto a single GPU.

![Screenshot 2024-06-10 at 17.52.20](/assets/img/Screenshot%202024-06-10%20at%2017.52.20.png)

**scaling model training**

the model is still fits on a single GPU
- distribute large data-sets across multiple GPUs and process these batches of data in parallel.
- DDP distributed data-parallel
  - A popular implementation of this model replication technique
  - copy the model onto each GPU and sends batches of data to each of the GPUs in parallel.
  - Each data-set is processed in parallel and then a synchronization step combines the results of each GPU, which in turn updates the model on each GPU, which is always identical across chips.
  - This implementation <font color=OrangeRed> allows parallel computations across all GPUs that results in faster training </font>.
  - Note that DDP requires that the model weights and all of the additional parameters, gradients, and optimizer states that are needed for training, fit onto a single GPU. If the model is too big for this, you should look into another technique called modal sharding. A popular implementation of modal sharding is Pi Torch is fully sharded data parallel, or FSDP for short.


FSDP is motivated by a paper published by researchers at Microsoft in 2019 that proposed a technique called ZeRO. ZeRO stands for zero redundancy optimizer and the goal of ZeRO is to optimize memory by distributing or sharding model states across GPUs with ZeRO data overlap.
- This allows you to scale model training across GPUs when the model doesn't fit in the memory of a single chip.
- how ZeRO works before coming back to FSDP.
- Earlier this week, you looked at all of the memory components required for training LLMs, the largest memory requirement was for the optimizer states, which take up twice as much space as the weights, followed by weights themselves and the gradients.
- Let's represent the parameters as this blue box, the gradients and yellow and the optimizer states in green.
- One limitation off the model replication strategy that I showed before is that you need to keep a full model copy on each GPU, which leads to redundant memory consumption. You are storing the same numbers on every GPU. ZeRO, on the other hand, eliminates this redundancy by distributing also referred to as sharding the model parameters, gradients, and optimizer states across GPUs instead of replicating them. At the same time, the communication overhead for a sinking model states stays close to that of the previously discussed ADP. ZeRO offers three optimization stages. ZeRO Stage 1, shots only optimizer states across GPUs, this can reduce the memory footprint by up to a factor of four. ZeRO Stage 2 also shots the gradients across chips. When applied together with Stage 1, this can reduce the memory footprint by up to eight times. Finally, ZeRO Stage 3 shots all components including the model parameters across GPUs. When applied together with Stages 1 and 2, memory reduction is linear with a number of GPUs. For example, sharding across 64 GPUs could reduce the memory by a factor of 64. Let's apply this concept to the visualization of GDP and replace the LLM by the memory representation of model parameters, gradients, and optimizer states. When you use FSDP, you distribute the data across multiple GPUs as you saw happening in GDP. But with FSDP, you also distributed or shard the model parameters, gradients, and optimize the states across the GPU nodes using one of the strategies specified in the ZeRO paper. With this strategy, you can now work with models that are too big to fit on a single chip. In contrast to GDP, where each GPU has all of the model states required for processing each batch of data available locally, FSDP requires you to collect this data from all of the GPUs before the forward and backward pass. Each CPU requests data from the other GPUs on-demand to materialize the sharded data into uncharted data for the duration of the operation. After the operation, you release the uncharted non-local data back to the other GPUs as original sharded data You can also choose to keep it for future operations during backward pass for example. Note, this requires more GPU RAM again, this is a typical performance versus memory trade-off decision. In the final step after the backward pass, FSDP is synchronizes the gradients across the GPUs in the same way they were for DDP. Model sharding S described with FSDP allows you to reduce the overall GPU memory utilization. Optionally, you can specify that FSDP offloads part of the training computation to GPUs to further reduce the GPU memory utilization. To manage the trade-off between performance and memory utilization, you can configure the level of sharding using FSDP is charting factor. A sharding factor of one basically removes the sharding and replicates the full model similar to DDP. If you set the sharding factor to the maximum number of available GPUs, you turn on full sharding. This has the most memory savings, but increases the communication volume between GPUs. Any sharding factor in-between enables hyper sharding. Let's take a look at how FSDP performs in comparison to DDP measured in teraflops per GPU. These tests were performed using a maximum of 512 NVIDIA V100 GPUs, each with 80 gigabytes of memory. Note, one teraflop corresponds to one trillion floating-point operations per second. The first figure shows FSDP performance for different size T5 models. You can see the different performance numbers for FSDP, full sharding in blue, hyper shard in orange and full replication in green. For reference, DDP performance is shown in red. For the first 25 models with 611 million parameters and 2.28 billion parameters, the performance of FSDP and DDP is similar. Now, if you choose a model size beyond 2.28 billion, such as 25 with 11.3 billion parameters, DDP runs into the out-of-memory error. FSDP on the other hand can easily handle models this size and achieve much higher teraflops when lowering the model's precision to 16-bit. The second figure shows 7% decrease in per GPU teraflops when increasing the number of GPUs from 8-512 for the 11 billion T5 model, plotted here using a batch size of 16 and orange and a batch size of eight in blue. As the model grows in size and is distributed across more and more GPUs, the increase in communication volume between chips starts to impact the performance, slowing down the computation. In summary, this shows that you can use FSDP for both small and large models and seamlessly scale the model training across multiple GPUs. I know this discussion has been highly technical and I want to emphasize that you don't need to remember all of the details here. The most important thing is to have a sense of how the data model parameters and training computations are shared across processes when training LLMs. Given the expense and technical complexity of training models across GPUs, some researchers have been exploring ways to achieve better performance with smaller models. In the next video, you'll learn more about this research into compute optimal models. Let's move on and take a look.



#### Scaling laws and compute-optimal models

#### Pre-training for domain adaptation

#### Domain-specific training: BloombergGPT

---

## GenAI for Code

- `Pretrained transformer-based models` have shown high performance in `natural language generation` task. However, a new wave of interest has surged: `automatic programming language generation`.

- This task consists of translating natural language instructions to a programming code. effort is still needed in automatic code generation

- When developing software, programmers use both `natural language (NL)` and `programming language (PL)`.

  - natural language is used to write documentation (ex: JavaDoc) to describe different classes, methods and variables.
  - Documentation is usually written by experts and aims to provide a comprehensive explanation of the source code to every person who wants to use/develop the project.

- the automation of `programming code generation` from natural language has been studied using various techniques of artificial intelligence (AI): automatically generate code for simple tasks, while allowing them to tackle only the most difficult ones.

- After the big success of Transformers Neural Network, it has been adapted to many `Natural Language Processing (NLP) tasks` (such as question answering, text translation, automatic summarization)

  - popular models are GPT, BERT, BART, and T5

- One of the main factors of success: trained on very large corpora.

- Recently, there has been an increasing interest in programming code generation. scientific community based its research on proposing systems that are based on pretrained transformers.

  - `CodeGPT` and `GPT-adapted` are based on `GPT2`
  - `PLBART` is based on `BART`,
  - `CoTexT` follows `T5`.

  - Note that these models have been pretrained on `bimodal data` (containing both PL and NL) and on `unimodal data` (containing only PL).

- Programming language generation is more challenging than standard text generation, PLs contain stricter grammar and syntactic rules.

> an example of an input sequence received by the model (in NL), the output of the model (in PL) and the target code (called gold standard or reference code).

![Screenshot 2024-02-13 at 12.59.44](/assets/img/Screenshot%202024-02-13%20at%2012.59.44.png)

study from the `state of the art`:

- **initialize the model from powerful pretrained models**, instead of performing a training from scratch.

  - Models initialized from previous pretrained weights achieve better performance than models trained from scratch

- **additional pretraining**

  - `Transformers neural network improves its performance significantly from increasing the amount of pretraining data`

  - for some specific tasks, the way to improve the modelâ€™s performance is to `pretrain it with a dataset that belongs to a specific domain`,

    - Models such as `SciBERT` and `BioBERT` have shown the benefits to pretrain a model using data related to a specific domain.

  - Increased data implies better training performance. This finding is intuitive since a large and diversified dataset helps improving the modelâ€™s representation.

  - The objective learning used during the pretraining stage gives the model some benefits when learning the downstream tasks

  - `a low number of epochs when pretraining a model leads to higher scores in generation tasks`.

- scale the input and output length during the fine-tuning of the model

  - **The input and output sequence length** used to train the model matters in the performance of the model

  - **Number of steps**, when increased the length of sequences in the model, it increased the number of fine-tuning steps. a way to improve the modelâ€™s performance is by increasing the number of steps in the training.

- carry out experiments **combining the unimodal and bimodal data in the training**

- T5 has shown the best performance in language generation tasks.

### Fine-tuning

### additional pretraining

dataset

- `CONCODE`

  - contains context of a real world Java programming environment.
  - aims to generate Java member functions that have class member variables from documentation.
  - `CONCODE` dataset:
  - ![Screenshot 2024-02-13 at 16.32.48](/assets/img/Screenshot%202024-02-13%20at%2016.32.48.png)

- `CodeSearchNet`, Corpus, and GitHub Repositories.

### EXPERIMENTAL

- **Evaluation Metrics**

  - To evaluate the models

  - `BLEU`:

    - a metric based on `n-gram precision` computed between the candidate and the reference(s).

    - `N-gram precision` penalizes the model if:

      - (1) words appear in the candidate but not in any of the references, or
      - (2) word appear more times in the candidate than in the maximum reference count.

    - However, the metric fails if the candidate does not have the appropriate length.
    - we use the `corpus-level BLEU score` in the code generation task.

  - `CodeBLEU`:

    - works via `n-gram` match, and it takes into account both the syntactic and semantic matches.
    - The syntax match is obtained by matching between the `code candidate and code reference(s) sub-trees` of `abstract syntax tree (AST)`.
    - The semantic match considers the data-flow structure.

  - `Exact Match (EM)`:
    - the ratio of the number of predictions that match exactly any of the code reference(s).

---

## usage

### Juptyper

> To run a shell command from within a notebook cell, you must put a ! in front of the command:
> !pip install hyperopt

```py
!nvidia-smi --list-gpus


!pip install --upgrade pip

!pip uninstall -y git+https://github.com/openai/CLIP.git \
  urllib3==1.25.10 \
  sentence_transformers \
  torch torchvision pytorch-lightning lightning-bolts

# install supporting puthon packages for Data Frame processing
# and for Progress Bar
!pip install numpy pandas matplotlib tqdm scikit-learn

# install only the older version of Torch
!pip install --ignore-installed \
    urllib3==1.25.10 \
    torch torchvision pytorch-lightning lightning-bolts

# install latest (Upgrade) sentence transformers for fine-tuning
!pip install --ignore-installed \
  urllib3==1.25.10 \
  pyyaml \
  sentence_transformers

# Use CLIP model from OpenAI
!pip install git+https://github.com/openai/CLIP.git

# load the python package to run Pandas in parallel for better speed
!pip install pandarallel

!pip install torchaudio

!pip uninstall -y nvidia_cublas_cu11
```

---

## Hugging Face

> like github repo

- search for an AI model

```bash
# +++++ Getting started with our git and git-lfs interface
# If you need to create a repo from the command line (skip if you created a repo from the website)
pip install huggingface_hub
# You already have it if you installed transformers or datasets
huggingface-cli login
# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}


# +++++ Clone the model or dataset locally
# Make sure you have git-lfs installed
# (https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name


# +++++ Then add, commit and push any file you want, including larges files
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push


# +++++ In most cases, if you're using one of the compatible libraries, the repo will then be accessible from code, through its identifier: username/repo_name
# For example for a transformers model, anyone can load it with:
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```

### Generative AI Time Series Forecasting

> [Generative AI Time Series Forecasting](https://huggingface.co/blog/time-series-transformers)

### Multivariate Time Series Forecasting

> [Multivariate Time Series Forecasting](https://huggingface.co/blog/informer)

### Generative AI Transformers for Time Series Forecasting

> [Generative AI Transformers for Time Series Forecasting](https://huggingface.co/blog/autoformer)

---

### Falcon 40b

- Chatgpt competitor - https://huggingface.co/tiiuae/falcon-40b

- Power of Falcon 40b chat - https://huggingface.co/spaces/HuggingFaceH4/falcon-chat

- Pre-Training - https://huggingface.co/tiiuae/falcon-40b#training-data

- or https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/studio-notebook-fine-tuning/falcon-40b-qlora-finetune-summarize.ipynb

Chat with Falcon-40B-Instruct, brainstorm ideas, discuss the holiday plans, and more!

- âœ¨ This demo is powered by Falcon-40B, finetuned on the Baize dataset, and running with Text Generation Inference. Falcon-40B is a state-of-the-art `large language model` built by the Technology Innovation Institute in Abu Dhabi. It is trained on 1 trillion tokens (including RefinedWeb) and available under the Apache 2.0 license. It currently holds the ğŸ¥‡ 1st place on the ğŸ¤— Open LLM leaderboard. This demo is made available by the HuggingFace H4 team.
- ğŸ§ª This is only a first experimental preview: the H4 team intends to provide increasingly capable versions of Falcon Chat in the future, based on improved datasets and RLHF/RLAIF.
- ğŸ‘€ Learn more about Falcon LLM: `falconllm.tii.ae`
- â¡ï¸ï¸ Intended Use: this demo is intended to showcase an early finetuning of Falcon-40B, to illustrate the impact (and limitations) of finetuning on a dataset of conversations and instructions. We encourage the community to further build upon the base model, and to create even better instruct/chat versions!
- âš ï¸ Limitations: the model can and will produce factually incorrect information, hallucinating facts and actions. As it has not undergone any advanced tuning/alignment, it can produce problematic outputs, especially if prompted to do so. Finally, this demo is limited to a session length of about 1,000 words.

---

### CodeParrot

---

### TAPEX

> [Table Pre-training via Execution](https://huggingface.co/microsoft/tapex-large)

Give a table of data and then query

- 0 shot question (answer right away)
- fine tune: https://github.com/SibilTaram/tapax_transformers/tree/add_tapex_bis/examples
- demo: https://huggingface.co/microsoft/tapex-base

---

## common LLM

å‡ ä¸ªè€³ç†Ÿèƒ½è¯¦çš„å¤§è¯­è¨€æ¨¡å‹ [^LLMSurvey] [^Open_LLM_Leaderboard] [^å¼€æºå¤§è¯­è¨€æ¨¡å‹æ±‡æ€»]

[^LLMSurvey]: LLMSurvey, https://github.com/RUCAIBox/LLMSurvey
[^Open_LLM_Leaderboard]: Open_LLM_Leaderboard, https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
[^å¼€æºå¤§è¯­è¨€æ¨¡å‹æ±‡æ€»]: å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLM)æ±‡æ€»(æŒç»­æ›´æ–°ä¸­),https://blog.csdn.net/jarodyv/article/details/129992142

**GPT**

- the greatest achievement in this field as it amassed 100 million active users in 2 months from the day of its release.
- ChatGPT
- GPT-4 by OpenAI ğŸ”¥
- programming code generation
  - `CodeGPT` and `GPT-adapted` based on GPT2
  - `CodeGPT` is trained from scratch on `CodeSearchNet` dataset
  - `CodeGPT-adapted` is initialized from GPT-2 pretrained weights.

LLaMA

- by Meta ğŸ¦™

AlexaTM

- by Amazon ğŸ«

Minerva

- by Google âœ–ï¸â•

BERT

- programming code generation: `CodeBERT`
- `SciBERT`, `BioBERT`

BART

- programming code generation
  - `PLBART` is based on `BART`,
  - `PLBART` uses the same architecture than `BARTbase`. `PLBART` uses three noising strategies: token masking, token deletion and token infilling.

T5

- programming code generation
  - `CoTexT` follows `T5`: based their pretraining on `CodeSearchNet`

JaCoText

- **model based on Transformers neural network**.
- pretrained model based on Transformers
- It aims to generate java source code from natural language text.

---

### 1 GPT ç³»åˆ—(OpenAI)

#### è¯­è¨€æ¨¡å‹

- è¯­è¨€æ¨¡å‹æ˜¯ GPT ç³»åˆ—æ¨¡å‹çš„åŸºåº§ã€‚[^é€šä¿—æ˜“æ‡‚çš„LLM(ä¸Šç¯‡)]

[^é€šä¿—æ˜“æ‡‚çš„LLM(ä¸Šç¯‡)]: é€šä¿—æ˜“æ‡‚çš„ LLM(ä¸Šç¯‡), https://blog.csdn.net/qq_39439006/article/details/130796416

- è¯­è¨€æ¨¡å‹,

  - ç®€å•æ¥è¯´ï¼Œå°±æ˜¯çœ‹ä¸€ä¸ªå¥å­æ˜¯äººè¯çš„å¯èƒ½æ€§ã€‚
  - ä¸“ä¸šä¸€ç‚¹æ¥è¯´

    - ç»™å®šä¸€ä¸ªå¥å­ï¼Œå…¶å­—ç¬¦æ˜¯ $W=(w_{1},w_{2},\cdots,w_{L})$
    - é‚£ä¹ˆï¼Œä»è¯­è¨€æ¨¡å‹æ¥çœ‹ï¼Œè¿™ä¸ªå¥å­æ˜¯äººè¯çš„å¯èƒ½æ€§å°±æ˜¯: $\begin{aligned} P(W)&=P(w_{1},w_{2},\cdots,w_{L})\\ &=P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1},w_{2})\cdots P(w_{L}|w_{1},w_{2},\cdots,w_{L-1})\\ \end{aligned}$

    - ä½†æ˜¯ï¼Œ $L$ å¤ªé•¿å°±ä¼šå¾ˆç¨€ç–ï¼Œç›´æ¥ç®—è¿™ä¸ªæ¦‚ç‡ä¸å¥½è®¡ç®—ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¿‘ä¼¼è®¡ç®—: $\begin{aligned} P(W)&=P(w_{1},w_{2},\cdots,w_{L})\\ &=P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1},w_{2})\cdots P(w_{L}|w_{1},w_{2},\cdots,w_{L-1})\\ &=P(w_{1})P(w_{2}|w_{1})\cdots P(w_{L}|w_{L-N},\cdots,w_{L-1}) \end{aligned}$

    - è¿™å°±æ˜¯å¸¸è¯´çš„ N-gram ç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼ŒN é€šå¸¸æ˜¯ 2ï¼Œ3ï¼Œ4ã€‚
    - ç‰¹åˆ«çš„ï¼Œå½“`N=1`æ—¶ï¼Œè¯­è¨€æ¨¡å‹å°±é€€åŒ–ä¸ºå„ä¸ªå­—ç¬¦å‡ºç°çš„æ¦‚ç‡ä¹‹ç§¯ã€‚
    - å½“`N=4`æ—¶è¯­è¨€æ¨¡å‹å°±æ¯”è¾ƒå¤§äº†ï¼Œå®é™…åº”ç”¨ä¸­ä¸€èˆ¬æœ€å¤§ä¹Ÿå°±æ˜¯ 4 äº†ã€‚
    - æ ¹æ®æ¡ä»¶æ¦‚ç‡ $P(w_{L}|w_{L-N},\cdots,w_{L-1})$ï¼Œæˆ‘ä»¬å°±èƒ½çŸ¥é“ç»™å®šå‰ N ä¸ªå­—ï¼Œä¸‹ä¸€ä¸ªå­—æ˜¯ä»€ä¹ˆå­—çš„æ¦‚ç‡äº†ã€‚

- è¯­è¨€æ¨¡å‹çš„è¯„ä»·æŒ‡æ ‡å¯ä»¥é‡‡ç”¨ PPL(å›°æƒ‘åº¦ï¼ŒPerplexity [^è¯­è¨€æ¨¡å‹])

[^è¯­è¨€æ¨¡å‹]: è¯­è¨€æ¨¡å‹, https://zhuanlan.zhihu.com/p/90741508

---

#### 1.1 GPT-1, GPT-2, GPT-3

- 2017 å¹´ï¼Œ`Google` æ¨å‡º `Transformer` ï¼Œåˆ©ç”¨ attention å®Œå…¨æ›¿ä»£è¿‡å¾€æ·±åº¦å­¦ä¹ ä¸­çš„ Recurrence å’Œ Convolution ç»“æ„ï¼Œç›´ç™½åœ°å±•ç°å‡ºäº†â€œå¤§ä¸€ç»Ÿæ¨¡å‹â€çš„é‡å¿ƒï¼Œ"xxx is all you need"ä¹Ÿæˆäº†ä¸€ä¸ªç©ä¸çƒ‚çš„æ¢—ã€‚

- 2018 å¹´ 6 æœˆï¼Œ`OpenAI` æ¨å‡ºåŸºäº `Transformer Decoder` æ”¹é€ çš„ç¬¬ä¸€ä»£**GPT(Generative Pre-Training)**ï¼Œæœ‰æ•ˆè¯æ˜äº†åœ¨ NLP é¢†åŸŸä¸Šä½¿ç”¨é¢„è®­ç»ƒ+å¾®è°ƒæ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚
- 2018 å¹´ 10 æœˆ,`Google` æ¨å‡ºåŸºäº `Transformer Encoder` éƒ¨åˆ†çš„**Bert**ï¼Œåœ¨åŒæ ·å‚æ•°å¤§å°çš„å‰æä¸‹ï¼Œå…¶æ•ˆæœé¢†è·‘äº GPT-1ï¼Œä¸€æ—¶æˆä¸º NLP é¢†åŸŸçš„é¢†å¤´ç¾Šã€‚

- ä¸ç”˜ç¤ºå¼±çš„ OpenAI åœ¨ 4 ä¸ªæœˆåï¼Œæ¨å‡ºæ›´å¤§çš„æ¨¡å‹**GPT-2(GPT-1: 110Mï¼ŒBert: 340Mï¼ŒGPT-2: 1.5B)**ï¼ŒåŒæ—¶ï¼ŒOpenAI ä¹ŸçŸ¥é“ï¼Œå…‰é å¢åŠ æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†æ¥è·å¾—ä¸€ä¸ªå’Œ Bert å·®ä¸å¤šæ•ˆæœçš„æ¨¡å‹ï¼Œå…¶å®æ˜¯æ²¡æœ‰æŠ€æœ¯å«é‡çš„ã€‚äºæ˜¯ï¼Œåœ¨ GPT-2 é‡Œï¼ŒOpenAI å¼•å…¥ zero-shot å¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

- æ­¤åï¼ŒOpenAI åœ¨ LLM ä¸Šä¹‰æ— åé¡¾åœ°èµ°äº†ä¸‹å»ï¼Œåœ¨ 2020 å¹´ 6 æœˆæ¨å‡ºå·¨äºº GPT-3ï¼Œå‚æ•°é‡é«˜è¾¾ 175Bï¼Œå„ç±»å®éªŒæ•ˆæœè¾¾åˆ°é¡¶å³°ï¼Œæ®è¯´ä¸€æ¬¡è®­ç»ƒè´¹ç”¨ä¸º 1200w ç¾å…ƒï¼Œâ€œè´µâ€ä¹Ÿæˆäº†æ™®é€šå·¥ä¸šç•Œè¸è¶³ GPT ç³»åˆ—çš„å£å’ä¹‹ä¸€ã€‚

---

##### GPT-1

- GPT-1 æ˜¯ OpenAI åœ¨è®ºæ–‡[ã€ŠImproving Language Understanding by Generative Pre-Trainingã€‹](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)ä¸­æå‡ºçš„ç”Ÿæˆå¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚

è¯¥æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³: é€šè¿‡äºŒæ®µå¼çš„è®­ç»ƒ

- ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒ(æ— ç›‘ç£å½¢å¼)
- ç¬¬äºŒé˜¶æ®µé€šè¿‡ Fine-Tuning çš„æ¨¡å¼è§£å†³ä¸‹æ¸¸ä»»åŠ¡(ç›‘ç£æ¨¡å¼ä¸‹
  GPT-1 å¯ä»¥å¾ˆå¥½åœ°å®Œæˆè‹¥å¹²ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±», è‡ªç„¶è¯­è¨€æ¨ç†, é—®ç­”, è¯­ä¹‰ç›¸ä¼¼åº¦ç­‰ã€‚
- åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¾®è°ƒåçš„ GPT-1 æ€§èƒ½å‡è¶…è¿‡äº†å½“æ—¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„ SOTA æ¨¡å‹ã€‚

- **è‡ªç„¶è¯­è¨€æ¨ç†(Natural Language Inference æˆ–è€… Textual Entailment)**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯åŒ…å«å…³ç³»(entailment)ï¼ŒçŸ›ç›¾å…³ç³»(contradiction)ï¼Œæˆ–è€…ä¸­ç«‹å…³ç³»(neutral)ï¼›

- **é—®ç­”å’Œå¸¸è¯†æ¨ç†(Question answering and commonsense reasoning)**: ç±»ä¼¼äºå¤šé€‰é¢˜ï¼Œè¾“å…¥ä¸€ä¸ªæ–‡ç« ï¼Œä¸€ä¸ªé—®é¢˜ä»¥åŠè‹¥å¹²ä¸ªå€™é€‰ç­”æ¡ˆï¼Œè¾“å‡ºä¸ºæ¯ä¸ªç­”æ¡ˆçš„é¢„æµ‹æ¦‚ç‡ï¼›

- **è¯­ä¹‰ç›¸ä¼¼åº¦(Semantic Similarity)**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¯­ä¹‰ä¸Šæ˜¯ç›¸å…³çš„ï¼›

- **åˆ†ç±»(Classification)**: åˆ¤æ–­è¾“å…¥æ–‡æœ¬æ˜¯æŒ‡å®šçš„å“ªä¸ªç±»åˆ«ã€‚

GPT-1 çš„æ¨¡å‹ç»“æ„åŠè®­ç»ƒæµç¨‹ã€‚

- **æ¨¡å‹ç»“æ„**:

  - GPT-1 åŸºç¡€æ¶æ„æ˜¯åŸºäº`Transformerçš„Decoder`éƒ¨åˆ†ï¼ŒåŒæ—¶åˆ é™¤äº†`Encoder-Decoder Attention`å±‚ï¼Œåªä¿ç•™äº†`Masked Multi-Head Attention`å±‚å’Œ`Feed Forward`å±‚ã€‚

  - Transformer ç»“æ„æå‡ºä¹‹å§‹ä¾¿ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡

    - æœºå™¨ç¿»è¯‘æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡,å› æ­¤ Transformer è®¾è®¡äº†
    - Encoder ç”¨äºæå–æºç«¯è¯­è¨€çš„è¯­ä¹‰ç‰¹å¾
    - Decoder æå–ç›®æ ‡ç«¯è¯­è¨€çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆç›¸å¯¹åº”çš„è¯‘æ–‡ã€‚

  - GPT-1 ç›®æ ‡æ˜¯æœåŠ¡äºå•åºåˆ—æ–‡æœ¬çš„ç”Ÿæˆå¼ä»»åŠ¡ï¼Œæ‰€ä»¥å«å¼ƒäº†å…³äº Encoder éƒ¨åˆ†ï¼ŒåŒ…æ‹¬ Decoder çš„ Encoder-Decoder Attention å±‚ã€‚

  - æ•´ä½“æ˜¯ 12 å±‚çš„ Transformer-Decoder å˜ä½“ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:

  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/7f79e1bd37914445a22c151d4d91bd12.jpeg#pic_center)

  - é™¤æ­¤ä¹‹å¤–ï¼ŒGPT-1 è¿˜å°† attention çš„ç»´æ•°æ‰©å¤§åˆ° 768(åŸæ¥ä¸º 512)ï¼Œå°† attention çš„å¤´æ•°å¢åŠ åˆ° 12 ä¸ª(åŸæ¥ä¸º 8 ä¸ª)ï¼Œå°† Feed Forward å±‚çš„éšå±‚ç»´æ•°å¢åŠ åˆ° 3072(åŸæ¥ä¸º 2048)ï¼Œæ€»å‚æ•°è¾¾åˆ° 110Mã€‚
  - GPT-1 è¿˜ä¼˜åŒ–äº†å­¦ä¹ ç‡é¢„çƒ­ç®—æ³•ï¼Œä½¿ç”¨æ›´å¤§çš„ BPE ç è¡¨(è¯è¡¨å¤§å°ä¸º 40478ï¼Œ478 ä¸ª base characters + 40000 ä¸ªç»“åˆçš„å­—ç¬¦)ï¼Œæ¿€æ´»å‡½æ•° ReLU æ”¹ä¸ºå¯¹æ¢¯åº¦æ›´æ–°æ›´å‹å¥½çš„é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒ GeLUï¼Œå°†æ­£ä½™å¼¦æ„é€ çš„ä½ç½®ç¼–ç æ”¹ä¸ºäº†å¸¦å­¦ä¹ çš„ä½ç½®ç¼–ç ã€‚

- **æ¨¡å‹è®­ç»ƒ**:

  - GPT-1 æ¨¡å‹è®­ç»ƒæ•´ä½“ä¸Šåˆ†ä¸ºä¸¤æ­¥:

    - 1. åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ åˆ°ä¸€ä¸ªé«˜å®¹é‡çš„è¯­è¨€æ¨¡å‹ï¼›
    - 2. åœ¨æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚å…¶ä¸­ç¬¬äºŒæ­¥æ˜¯é’ˆå¯¹å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚

  - **æ— ç›‘ç£é¢„è®­ç»ƒ**:

    - why `é¢„è®­ç»ƒ`å«åš`æ— ç›‘ç£è®­ç»ƒ`
      - å› ä¸ºæˆ‘ä»¬å…¶å®æ²¡æœ‰æ ‡æ³¨æ ·æœ¬ï¼Œè€Œæ˜¯æ‹¿ä¸‹ä¸€ä¸ªè¯å½“åšæ ‡ç­¾è¿›è¡Œæ¨¡å‹è®­ç»ƒ
      - è¿™ç§æ–¹å¼ä¹Ÿè¢«ç§°ä½œ`è‡ªç›‘ç£è®­ç»ƒ`ã€‚
    - æ€»ä½“è®­ç»ƒä»»åŠ¡ç›®æ ‡æ˜¯æ ¹æ®å·²çŸ¥çš„è¯é¢„æµ‹æœªçŸ¥çš„è¯ã€‚
    - åœ¨è¿™é‡Œè®¾å®šä¸€å®šçš„çª—å£å¤§å°ï¼Œå³æ ¹æ®æœ‰é™çš„è¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯:
    - ç»™å®šä¸€ä¸ªè¯­æ–™çš„å¥å­åºåˆ— $\mathcal{U}=\{u_{1},\cdots,u_{n}\}$
    - å·²çŸ¥å‰ $k$ ä¸ªè¯é¢„æµ‹å½“å‰è¯ $u_{i}$â€‹ï¼Œç”¨ä¸€ä¸ªæ ‡å‡†çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å»æå¤§åŒ–è¿™ä¸ªä¼¼ç„¶å‡½æ•°: $L_{1}(\mathcal{U})=\sum_{i} logP(u_{i}|u_{i-k},\cdots, u_{i-1};\Theta)$

    - å…¶ä¸­:

      - $k$ æ˜¯æ»‘åŠ¨çª—å£å¤§å°ï¼Œ $\Theta$ æ˜¯è¦ä¼˜åŒ–çš„å‚æ•°ã€‚
      - $P(u)$ çš„è®¡ç®—æ–¹æ³•æ˜¯:

        $$
        \begin{aligned}
        h_{0}&=UW_{e}+W_{p} \\
        h_{i}&=transformer\_block(h_{i-1})ï¼Œ \forall i \in[1,n] \\
        P(u)&=softmax(h_{n}W_{e}^{T})
        \end{aligned}
        $$

    - å…¶ä¸­:
      - $W_{e}$ â€‹ æ˜¯è¯å‘é‡çŸ©é˜µ(token embedding matrix)
      - $W_{p}$ â€‹ æ˜¯ä½ç½®å‘é‡çŸ©é˜µ(position embedding matrix)
      - $U=(u_{-k},\cdots,u_{-1})$ æ˜¯ tokens çš„ä¸Šä¸‹æ–‡å‘é‡
    - æºä»£ç ä¸­:

      - $u_{i}$ â€‹ éƒ½æ˜¯ one-hot ç¼–ç å‘é‡ï¼Œç›¸å½“äºåšä¸€ä¸ªæŸ¥è¯¢æ“ä½œ
      - $U$ å­˜å‚¨ç´¢å¼•
      - $W_{e}$ â€‹ å­˜å‚¨ç€è¯å‘é‡å€¼ï¼Œ $n$ æ˜¯ Decoder å±‚çš„æ•°é‡ã€‚

    - ä¸Šé¢æ˜¯è®ºæ–‡ä¸­çš„æè¿°ï¼Œä¸¾ä¸€ä¸ªä¾‹å­è¯´æ˜ GPT-1 å®é™…ä¸Šæ˜¯å¦‚ä½•è¿›è¡Œ`æ— ç›‘ç£é¢„è®­ç»ƒ`çš„ã€‚

      - ä¾‹å¦‚è¾“å…¥æ–‡æœ¬æ˜¯: ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘

        - è¿™æ®µæ–‡æœ¬ç»è¿‡åˆ‡è¯è½¬æ¢ä¸ºä¸€ä¸ªä¸ª token åï¼Œè¾“å…¥ GPT-1 çš„`transformer-decoder`ç»“æ„
        - åœ¨æœ€åä¸€å±‚ï¼Œä¼šè¾“å‡ºæ¯ä¸ª token å¯¹åº”çš„è¡¨å¾å‘é‡ï¼Œå³ä¸Šæ–‡çš„ $h_{n}\in R^{m\times d}$

          - å…¶ä¸­
          - $m$ æ˜¯ token æ•°é‡ï¼Œè¿™ä¸ªä¾‹å­ä¸­å°±æ˜¯ 5
          - $d$ æ˜¯æ¨¡å‹ç»´åº¦ï¼ŒGPT-1 ä¸­å°±æ˜¯ 768ï¼›

        - æ¥ä¸‹æ¥ï¼Œ $h_{n}$ â€‹ å†ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œç”Ÿæˆ $z_{n}\in R^{m\times v}$

          - å…¶ä¸­ $v$ æ˜¯è¯è¡¨çš„å¤§å°ï¼›

        - æœ€åï¼Œ $z_{n}$ â€‹ ä¼šç»è¿‡ softmax æ“ä½œï¼Œç„¶åé€‰å–å®ƒæ¯ä¸€è¡Œä¸­æ•°å€¼æœ€å¤§çš„ç´¢å¼•åˆ°è¯è¡¨ä¸­æœç´¢å¯¹åº”çš„ token

        - æœç´¢åˆ°çš„ token æ€ä¹ˆç”¨å‘¢ï¼Ÿ
          - æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹
          - è¾“å…¥æ˜¯ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¾“å‡ºä¹Ÿæ˜¯ 5 ä¸ª token
          - å› ä¸ºè¾“å…¥çš„ç¬¬ä¸€ä¸ª token æ˜¯ã€ä»Šã€‘ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„ç¬¬ä¸€ä¸ª token æ˜¯ã€å¤©ã€‘
          - è¾“å…¥çš„ç¬¬äºŒä¸ª token æ˜¯ã€å¤©ã€‘ï¼Œåˆ™å¸Œæœ›è¾“å‡ºçš„ç¬¬äºŒä¸ª token æ˜¯ã€å¾ˆã€‘
          - ä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°æœ€åä¸€ä¸ªè¾“å…¥ tokenã€å¿ƒã€‘
          - ä¸è¿‡å› ä¸ºå®ƒæ²¡æœ‰ä¸‹ä¸€ä¸ªè¯ï¼Œæ‰€ä»¥åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸åœ¨æˆ‘ä»¬çš„æŸå¤±è®¡ç®—èŒƒå›´å†…ã€‚
          - æ‰€ä»¥ï¼Œæˆ‘ä»¬ä¼šæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå°½å¯èƒ½çš„è®©æœ€ç»ˆçš„è¾“å‡º token çš„å‰å››ä¸ªå­—æ˜¯ã€å¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¿™å°±æ˜¯é¢„è®­ç»ƒä»»åŠ¡çš„æ•´ä½“æµç¨‹ã€‚

  - **ç›‘ç£è®­ç»ƒ**:

    - å½“å¾—åˆ°æ— ç›‘ç£çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬å°†å®ƒç›´æ¥åº”ç”¨åˆ°æœ‰ç›‘ç£ä»»åŠ¡ä¸­ç»§ç»­è®­ç»ƒã€‚

    - å¯¹äºä¸€ä¸ªæœ‰æ ‡ç­¾çš„æ•°æ®é›† $\mathcal{C}$
    - æ¯ä¸ªå®ä¾‹æœ‰ $m$ ä¸ªè¾“å…¥ token: $\{x^{1},\cdots,x^{m}\}$
    - å®ƒå¯¹åº”çš„æ ‡ç­¾æ˜¯ $y$ã€‚
    - é¦–å…ˆå°†è¿™äº› token è¾“å…¥åˆ°è®­ç»ƒå¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè·å–æœ€åä¸€ä¸ª transformer decoder çš„è¾“å‡ºï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾å‘é‡ $h_{l}^{m}$â€‹
    - ç„¶åå†é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å¾—åˆ°é¢„æµ‹ç»“æœ $y$: $P(y|x^{1},\cdots,x^{m})=softmax(h_{l}^{m}W_{y})$

    - å…¶ä¸­ $W_{y}$â€‹ ä¸ºå…¨è¿æ¥å±‚çš„å‚æ•°ã€‚
    - æœ‰ç›‘ç£çš„ç›®æ ‡åˆ™æ˜¯æœ€å¤§åŒ–ä¸‹å¼çš„å€¼: $L_{2}(\mathcal{C})=\sum_{x,y}P(y|x^{1},\cdots,x^{m})$

    - æ³¨æ„: è¿™é‡Œçš„ $h^m_l$ â€‹ æ˜¯æ¯ä¸€ä¸ªè¯å¯¹åº”çš„ Decoder è¾“å‡ºæ‹¼æ¥èµ·æ¥çš„ï¼Œ$h^m_l=\{h^{<1>}_l,\cdots,h^{<m>}_l\}, , h^{<i>}_l$ â€‹ å¯¹åº” $x^{i}$ çš„åµŒå…¥è¡¨ç¤ºã€‚
    - GPT-1 çš„å®éªŒä¸­å‘ç°ï¼ŒåŠ å…¥è¯­è¨€æ¨¡å‹å­¦ä¹ ç›®æ ‡ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æŸå¤±å‡½æ•°ä¸­åŠ å…¥ $L_{1}$ â€‹ èƒ½å¸¦æ¥ä¸¤ç‚¹å¥½å¤„:
      - 1. æå‡ç›‘ç£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›
      - 2. åŠ å¿«æ”¶æ•›ï¼›å› æ­¤ï¼Œæœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹( Î» \lambda Î» ä¸€èˆ¬å– 0.5): $L_{3}(\mathcal{C})=L_{2}(\mathcal{C})+\lambda L_{1}(\mathcal{C})$

**ä¸‹æ¸¸ä»»åŠ¡**:

- GPT-1 è®ºæ–‡ä¸­ç»™å‡ºäº†å››ä¸ªä¸‹æ¸¸é€‚é…ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯æ–‡æœ¬åˆ†ç±», è‡ªç„¶è¯­è¨€æ¨ç†, é—®ç­”, è¯­ä¹‰ç›¸ä¼¼åº¦
- åŒæ—¶ç»™å‡ºäº†é’ˆå¯¹è¿™å››ä¸ªä»»åŠ¡ï¼Œå¦‚ä½•è¿›è¡Œé’ˆå¯¹æ€§çš„å¾®è°ƒã€‚
- è¿™å››ä¸ªä»»åŠ¡è™½ç„¶æœ¬è´¨ä¸Šéƒ½æ˜¯å±äºè‡ªç„¶è¯­è¨€ç†è§£çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä½†æ˜¯ GPT-1 çš„ç»“æ„æ˜¯å¾ˆé€‚é…åšè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡çš„ã€‚

- GPT-1 å¦‚ä½•åœ¨ä¸Šè¿°å››ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/0c96d17a280347a6b80e71a0a4da483a.png#pic_center)

  - **åˆ†ç±»ä»»åŠ¡**:

    - å°†èµ·å§‹å’Œç»ˆæ­¢`token`åŠ å…¥åˆ°åŸå§‹åºåˆ—ä¸¤ç«¯
    - è¾“å…¥`transformer`ä¸­å¾—åˆ°ç‰¹å¾å‘é‡
    - æœ€åç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å¾—åˆ°é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼›

  - **è‡ªç„¶è¯­è¨€æ¨ç†**:

    - å°†å‰æ`(premise)`å’Œå‡è®¾`(hypothesis)`é€šè¿‡åˆ†éš”ç¬¦`(Delimiter)`éš”å¼€
    - ä¸¤ç«¯åŠ ä¸Šèµ·å§‹å’Œç»ˆæ­¢`token`
    - å†ä¾æ¬¡é€šè¿‡`transformer`å’Œå…¨è¿æ¥å¾—åˆ°é¢„æµ‹ç»“æœï¼›

  - **è¯­ä¹‰ç›¸ä¼¼åº¦**:

    - è¾“å…¥çš„ä¸¤ä¸ªå¥å­ï¼Œæ­£å‘å’Œåå‘å„æ‹¼æ¥ä¸€æ¬¡(ç”±äºç›¸ä¼¼æ€§è´¨æ˜¯å¯¹ç§°çš„ï¼Œä¸ºäº†æ¶ˆé™¤é¡ºåºçš„å½±å“)
    - ç„¶ååˆ†åˆ«è¾“å…¥ç»™`transformer`
    - å¾—åˆ°çš„ç‰¹å¾å‘é‡æ‹¼æ¥åå†é€ç»™å…¨è¿æ¥å¾—åˆ°é¢„æµ‹ç»“æœï¼›

  - **é—®ç­”å’Œå¸¸è¯†æ¨ç†**:

    - å°†ä¸ªé€‰é¡¹çš„é—®é¢˜æŠ½è±¡åŒ–ä¸ºä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œå³æ¯ä¸ªé€‰é¡¹åˆ†åˆ«å’Œå†…å®¹è¿›è¡Œæ‹¼æ¥
    - ç„¶åå„é€å…¥`transformer`å’Œå…¨è¿æ¥ä¸­
    - æœ€åé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ä½œä¸ºé¢„æµ‹ç»“æœã€‚

  - é€šè¿‡ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»çš„ä¾‹å­ï¼Œæ¥ä»‹ç»ä¸‹ GPT-1 åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¯å¦‚ä½•å¾®è°ƒçš„ã€‚

    - ä¸‹æ¸¸ä»»åŠ¡æ˜¯æƒ…æ„Ÿæ–‡æœ¬åˆ†ç±»ä»»åŠ¡
    - åŒ…æ‹¬å–œ, æ€’, å“€, æƒ§, å…¶ä»–äº”ä¸ªç±»åˆ«
    - å…¶ä¸­ä¸€ä¸ªæ ·æœ¬æ˜¯ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼ŒçœŸå®æ ‡ç­¾æ˜¯ã€å–œã€‘ã€‚
    - GPT-1 åœ¨ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒæŸå¤±å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†
      - ä¸€éƒ¨åˆ†æ˜¯ä¸é¢„è®­ç»ƒä¿æŒä¸€è‡´çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æŸå¤±ï¼Œ
      - å¦ä¸€éƒ¨åˆ†æ˜¯åˆ†ç±»æŸå¤±
    - å¯¹äºåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œæˆ‘ä»¬æœ€ç»ˆä¹Ÿä¼šè·å–åˆ° GPT-1 æœ€åä¸€å±‚çš„å‘é‡è¡¨å¾ $h_{l}\in R^{m\times d}$
      - å…¶ä¸­
        - $m$ æ˜¯ token æ•°é‡ï¼Œè¿™ä¸ªä¾‹å­ä¸­å°±æ˜¯ 5ï¼Œ
        - $d$ æ˜¯æ¨¡å‹ç»´åº¦ï¼ŒGPT-1 ä¸­å°±æ˜¯ 768ï¼Œ
        - $l$ æ˜¯æ¨¡å‹å±‚æ•°ï¼›
    - æ¥ä¸‹æ¥ï¼Œ $h_{l}$ â€‹ çš„æœ€åä¸€è¡Œå†ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚
    - (æ³¨æ„ï¼Œé¢„è®­ç»ƒä»»åŠ¡æ˜¯ $h_{l}$ â€‹ æ•´ä½“éƒ½è¦ç»è¿‡å…¨è¿æ¥å±‚ï¼Œæˆ‘ä»¬è¿™é‡Œåªéœ€ç”¨åˆ°æœ€åä¸€ä¸ª tokenï¼Œå³å›¾ç‰‡ä¸­çš„ Extract å¯¹åº”çš„å‘é‡è¡¨å¾)
    - ç”Ÿæˆ $z_{l}\in R^{c}$ï¼Œå…¶ä¸­ $c$ æ˜¯ç±»åˆ«æ•°ç›®ï¼›
    - æœ€å $z_{l}$ â€‹ ä¼šç»è¿‡ softmax æ“ä½œï¼Œè·å–ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘è¿™æ®µæ–‡æœ¬å¯¹åº”çš„æ¯ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼
    - æˆ‘ä»¬çš„æœŸæœ›æ˜¯ã€å–œã€‘çš„æ¦‚ç‡å€¼è¦å°½å¯èƒ½çš„å¤§ï¼Œä¹Ÿå°±æ˜¯ $z_{l}$ â€‹ çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„å€¼è¦å°½å¯èƒ½å¤§ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡ã€‚

**GPT-1 ç‰¹ç‚¹**:

- **ä¼˜ç‚¹**:
  - ç‰¹å¾æŠ½å–å™¨ä½¿ç”¨äº†å¼ºå¤§çš„ Transformerï¼Œèƒ½å¤Ÿæ•æ‰åˆ°æ›´é•¿çš„è®°å¿†ä¿¡æ¯ï¼Œä¸”è¾ƒä¼ ç»Ÿçš„ RNN æ›´æ˜“äºå¹¶è¡ŒåŒ–ï¼›
  - transformer çš„å¹¶è¡ŒåŒ–å¯ä»¥å‚è€ƒ[æµ…æ Transformer è®­ç»ƒæ—¶å¹¶è¡Œé—®é¢˜](https://zhuanlan.zhihu.com/p/368592551)ï¼›
- **ç¼ºç‚¹**: GPT-1 æœ€å¤§çš„é—®é¢˜å°±æ˜¯ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹æ˜¯å•å‘çš„ã€‚

**GPT-1 ä¸ ELMoï¼ŒBert çš„åŒºåˆ«**:

- **GPT-1 ä¸ ELMo çš„åŒºåˆ«**:

  - **æ¨¡å‹æ¶æ„ä¸åŒ**: ELMo æ˜¯æµ…å±‚çš„åŒå‘ RNNï¼›GPT-1 æ˜¯å¤šå±‚çš„ Transformer decoderï¼›
  - **é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å¤„ç†ä¸åŒ**: ELMo å°†è¯åµŒå…¥æ·»åŠ åˆ°ç‰¹å®šä»»åŠ¡ä¸­ï¼Œä½œä¸ºé™„åŠ åŠŸèƒ½ï¼›GPT åˆ™é’ˆå¯¹æ‰€æœ‰ä»»åŠ¡å¾®è°ƒç›¸åŒçš„åŸºæœ¬æ¨¡å‹ã€‚

- **GPT-1 ä¸ Bert çš„åŒºåˆ«**:
  - **é¢„è®­ç»ƒ**: GPT-1 é¢„è®­ç»ƒçš„æ–¹å¼å’Œä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ä¸€æ ·ï¼Œé€šè¿‡ä¸Šæ–‡ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼›Bert ä¼šåŒæ—¶åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼›
  - **æ¨¡å‹æ•ˆæœ**:
    - GPT-1 å› ä¸ºé‡‡ç”¨äº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹æ‰€ä»¥æ›´åŠ é€‚åˆç”¨äº`è‡ªç„¶è¯­è¨€ç”Ÿæˆç±»çš„ä»»åŠ¡ (NLG)`ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡é€šå¸¸æ˜¯æ ¹æ®å½“å‰ä¿¡æ¯ç”Ÿæˆä¸‹ä¸€åˆ»çš„ä¿¡æ¯ã€‚
    - è€Œ Bert æ›´é€‚åˆç”¨äº`è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ (NLU)`ã€‚
    - å½“ç„¶è¿™æ˜¯ä¹‹å‰çš„è¯´æ³•ï¼Œç°åœ¨ chatgpt å‡ºæ¥ä»¥åå“ªä¸ªæ›´é€‚åˆ NLU ä»»åŠ¡è¿˜çœŸä¸ä¸€å®šã€‚
    - GPT-1 çš„æ¨¡å‹å‚æ•°ä¸º L=12ï¼ŒH=768ï¼ŒA=12ï¼Œè¿™ä¸ªè®¾ç½®å’Œåæ¥ Bert-base ä¸€æ¨¡ä¸€æ ·ï¼Œä½†åè€…çš„æ•ˆæœè¦å¥½ä¸Šå¾ˆå¤šã€‚
    - åŸå› ä¹‹ä¸€æ˜¯ï¼ŒGPT-1 é‡‡ç”¨ Mask-Attention ç»“æ„ï¼Œå¯¹æ¨¡å‹å’Œè®­ç»ƒæ•°æ®çš„è¦æ±‚ä¼šæ›´é«˜ï¼Œå› ä¸ºæ¨¡å‹èƒ½è¯»åˆ°çš„ä¿¡æ¯åªæœ‰ä¸Šæ–‡ã€‚
    - è€Œé‡‡ç”¨æ™®é€š attention çš„ Bert åœ¨è®­ç»ƒé˜¶æ®µå°±èƒ½åŒæ—¶è¯»åˆ°ä¸Šä¸‹æ–‡ã€‚
    - è¿™ä¸ªæ€§è´¨å†³å®šäº† GPT æ¨¡å‹è¶Šæ¥è¶Šå¤§çš„è¶‹åŠ¿ã€‚ä½†æ˜¯ï¼Œé•¿è¿œæ¥çœ‹ï¼ŒMasked-Attention æ˜¯æ¨åŠ¨æ¨¡å‹æ›´å¥½ç†è§£æ–‡å­—çš„é‡è¦æ‰‹æ®µï¼Œæ¯•ç«Ÿåœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›åŸ¹å…»æ¨¡å‹çŸ¥ä¸Šæ–‡è¡¥ä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å•çº¯åœ°åšå®Œå½¢å¡«ç©ºã€‚
  - **æ¨¡å‹ç»“æ„**: GPT-1 é‡‡ç”¨äº† Transformer çš„ Decoderï¼Œè€Œ Bert é‡‡ç”¨äº† Transformer çš„ Encoderã€‚

**GPT-1 çš„æ•°æ®é›†**:

- GPT-1 ä½¿ç”¨äº† BooksCorpus æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†åŒ…å« 7000 æœ¬æ²¡æœ‰å‘å¸ƒçš„ä¹¦ç±ã€‚
- ä½œè€…é€‰è¿™ä¸ªæ•°æ®é›†çš„åŸå› æœ‰äºŒ:
  - 1. æ•°æ®é›†æ‹¥æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä½¿å¾—æ¨¡å‹èƒ½å­¦å¾—æ›´é•¿æœŸçš„ä¾èµ–å…³ç³»ï¼›
  - 2. è¿™äº›ä¹¦ç±å› ä¸ºæ²¡æœ‰å‘å¸ƒï¼Œæ‰€ä»¥å¾ˆéš¾åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šè§åˆ°ï¼Œæ›´èƒ½éªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

##### GPT-2

- GPT-1 å’Œ Bert çš„è®­ç»ƒéƒ½æ˜¯åˆ†ä¸¤æ­¥èµ°: `pre-training + supervised fine-tuning`ã€‚

  - ç¼ºç‚¹:
  - è™½ç„¶å€ŸåŠ©`é¢„è®­ç»ƒ`æå‡æ€§èƒ½ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šè¿˜æ˜¯éœ€è¦æœ‰ç›‘ç£çš„`Fine-Tuning`æ‰èƒ½ä½¿å¾—æ¨¡å‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼›
  - éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šé¢æœ‰æ ‡æ³¨çš„æ•°æ®ã€‚å½“åªæœ‰å¾ˆå°‘é‡çš„å¯ç”¨æ•°æ® (zero-shot çš„æƒ…å†µä¸‹) å¾ˆéš¾æœ‰å¾ˆå¥½çš„æ•ˆæœã€‚

  - å¦å¤–ï¼Œåœ¨ Bert æ¨¡å‹æå‡ºä¹‹åï¼ŒEncoder vs Decoderï¼ŒBert vs GPT-1ï¼Œä¸¤ä¸¤ä¹‹é—´çš„æ¯”è¾ƒå°±å¼€å§‹äº†
    - ä½†æ˜¯æ­¤æ—¶ GPT-1 ä»å¤„åœ¨åŠ£åŠ¿ã€‚
    - Bert æå‡ºä¹‹åï¼Œé™¤äº†ç”Ÿæˆä»»åŠ¡å¤–ï¼ŒNLP ä»»åŠ¡çš„èŒƒå¼åŸºæœ¬å°±æ˜¯ Bert çš„`é¢„è®­ç»ƒ+Fine-Tuning`äº†ã€‚
    - åŸºäº Decoder çš„æ¨¡å‹ï¼Œæ¨¡å‹å’Œæ•°æ®é‡è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚
    - å› æ­¤ï¼ŒOpenAI ä»è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œå¼•å…¥äº†`zero-shot`è¿™ä¸€åˆ›æ–°ç‚¹ï¼ŒGPT-2 è¯ç”Ÿäº†. [^Unsupervised_Multitask_Learners]

[^Unsupervised_Multitask_Learners]: Language Models are Unsupervised Multitask Learners, https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.

- è®ºæ–‡ä¸­è®¤ä¸ºç°åœ¨çš„è®­ç»ƒæ–¹å¼è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹åªèƒ½ç®—æ˜¯ä¸€ä¸ªå°ä»»åŠ¡ä¸Šçš„ä¸“å®¶ç³»ç»Ÿï¼Œè€Œä¸”è¿˜éƒ½ä¸å¤Ÿé²æ£’ã€‚

  - é€ æˆè¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯æ¨¡å‹éƒ½æ˜¯åœ¨å•ä¸€é¢†åŸŸå†…çš„å•ä¸€ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œç¼ºä¹æ³›åŒ–æ€§ã€‚
  - è·Ÿäººä¸€æ ·ï¼Œè§è¯†å’ŒçŸ¥è¯†å¤ªå°‘æ—¶ï¼Œå°±å¾ˆéš¾å¯¹äº‹æƒ…æœ‰å…¨é¢çš„äº†è§£ã€‚
  - è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªå¯è¡Œçš„æ€è·¯æ˜¯å¤šä»»åŠ¡å­¦ä¹ ï¼Œè€Œä¸”æ˜¯å¤§é‡ä¸åŒé¢†åŸŸçš„ä¸åŒä»»åŠ¡ã€‚
  - ä½†æ˜¯ï¼Œè¿™æ ·çš„å¤šä»»åŠ¡å­¦ä¹ æ˜¯æœ‰ç›‘ç£çš„è®­ç»ƒï¼Œéœ€è¦å¤§é‡çš„æ•°æ®ï¼Œè¿™ä¸ªå°±æ¯”è¾ƒéš¾å®ç°äº†ã€‚

- GPT-2 åœ¨ GPT-1 çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†æ–°çš„å‘å±•æ€è·¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
  - ç®€å•æ¥è¯´
    - GPT-2 çš„æ€è·¯å°±æ˜¯å……åˆ†ç›¸ä¿¡è¯­è¨€æ¨¡å‹
    - ä¸å†å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œ Fine-Tuning æˆ–è€…å¢åŠ ä»»åŠ¡å¤´äº†ï¼Œå°±ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥è§£å†³æ‰€æœ‰ä»»åŠ¡
    - ç›´æ¥åš zero-shot çš„ä»»åŠ¡ã€‚
  - å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ä¸Šé«˜è´¨é‡çš„å¤§æ•°æ®ï¼Œå †å æ›´å¤šçš„å‚æ•°ï¼Œä¸åŒä»»åŠ¡æ”¹é€ æˆç”Ÿæˆä»»åŠ¡ã€‚
    - GPT-2 æœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä½†æ˜¯ä¸ä¸€æ ·çš„æ˜¯ï¼Œå®ƒè¯æ˜äº†è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ zero-shot çš„æƒ…å†µä¸‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡
    - ä¹Ÿå°±æ˜¯è¯´ï¼ŒGPT-2 åœ¨åšä¸‹æ¸¸ä»»åŠ¡çš„æ—¶å€™å¯ä»¥æ— éœ€ä»»ä½•æ ‡æ³¨çš„ä¿¡æ¯ï¼Œä¹Ÿæ— éœ€ä»»ä½•å‚æ•°æˆ–æ¶æ„çš„ä¿®æ”¹ã€‚
    - åæ¥çš„ GPT-3 ä¹Ÿæ˜¯æ²¿ç”¨äº†è¿™ä¸ªæ€è·¯ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œå·²ç»å¯ä»¥çœ‹å‡ºä¸€äº› ChatGPT çš„å½±å­äº†ã€‚

**æ¨¡å‹ç»“æ„**:

- GPT-2 çš„æ¨¡å‹åœ¨ GPT-1 çš„åŸºç¡€ä¸Šåšäº†ä¸€äº›æ”¹è¿›ï¼Œå¦‚ä¸‹:

  - **ç»“æ„å˜åŒ–**:
    - å¯¹äºæ¯ä¸ª sub-block: ç¬¬ä¸€ä¸ª layer norm å±‚ç§»åˆ° sub-block çš„è¾“å…¥éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯ attention ä¹‹å‰ï¼Œç¬¬äºŒä¸ª layer norm å±‚ç§»åˆ° feed forward ä¹‹å‰ï¼›
    - å¯¹äºæ•´ä½“æ¨¡å‹æ¶æ„ï¼Œåœ¨æœ€åä¸€ä¸ª sub-block åå†åŠ ä¸€ä¸ª layer norm å±‚ï¼›
  - **æƒé‡å˜åŒ–**:
    - é‡‡ç”¨ä¸€ç§æ”¹è¿›çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†æ®‹å·®è·¯å¾„ä¸æ¨¡å‹æ·±åº¦çš„ç´¯ç§¯ã€‚
    - åœ¨åˆå§‹åŒ–æ—¶å°† residual layers çš„æƒé‡æŒ‰ $\sqrt{N}$ â€‹ çš„å› å­è¿›è¡Œç¼©æ”¾ï¼Œå…¶ä¸­ $N$ æ˜¯ residual layers çš„æ•°é‡ï¼›
    - å¤‡æ³¨: è¿™ä¸ªæ”¹åŠ¨å…¶å®æ²¡å¤ªçœ‹æ‡‚ï¼Œresidual layers å°±æ˜¯ä¸€ä¸ªç›¸åŠ æ“ä½œï¼Œæ€ä¹ˆä¼šæœ‰å‚æ•°å‘¢ï¼ŸæŸ¥é˜…äº†å¾ˆå¤šèµ„æ–™ï¼Œæºç ä¹Ÿçœ‹äº†[GPT-2](https://github.com/openai/gpt-2)ï¼Œæ²¡çœ‹åˆ°æƒé‡ç¼©æ”¾çš„æµç¨‹ã€‚åœ¨æ­¤ç»™ä¸€ä¸ªæœ¬äººçš„è§è§£: æ ¹æ®è¿™ä¸ªæ“ä½œçš„ç›®çš„å¯çŸ¥ï¼Œæ˜¯ä¸ºäº†é˜²æ­¢éšç€æ¨¡å‹æ·±åº¦çš„ç´¯ç§¯ï¼Œæ®‹å·®è¶ŠåŠ è¶Šå¤§ï¼Œå› æ­¤è®¤ä¸ºè¿™é‡Œçš„ç¼©æ”¾æŒ‡çš„æ˜¯æ¯æ¬¡è¿›è¡Œæ®‹å·®æ“ä½œä¹‹å‰(å³å°†è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œç›¸åŠ ä¹‹å‰)ï¼Œå…ˆå°†è¾“å…¥è¿›è¡Œç¼©æ”¾ï¼Œç¼©æ”¾å› å­è·Ÿå½“å‰æ˜¯æ•´ä½“ç»“æ„çš„ç¬¬å‡ å±‚æœ‰å…³ï¼Œå±‚æ•°è¶Šå¤§ï¼Œç´¯ç§¯çš„è¶Šå¤§ï¼Œæ‰€ä»¥åº”è¯¥ç¼©æ”¾çš„è¶Šå¤šã€‚æ¯”å¦‚ç°åœ¨æ˜¯æ•´ä½“ç»“æ„çš„ç¬¬äº”å±‚ï¼Œé‚£ä¹ˆç¼©æ”¾å› å­ $N$ å°±æ˜¯ 5ã€‚
  - **è¯è¡¨å˜åŒ–**: è¯è¡¨å¤§å°è®¾ç½®ä¸º 50257ï¼›
  - **è¾“å…¥å˜åŒ–**: æ— ç›‘ç£é¢„è®­ç»ƒå¯çœ‹åˆ°çš„ä¸Šä¸‹æ–‡çš„ context ç”± 512 æ‰©å±•ä¸º 1024ï¼›
  - **æ‰¹æ¬¡å˜åŒ–**: è®­ç»ƒæ—¶ï¼Œbatchsize å¤§å°ä» 64 è°ƒæ•´ä¸º 512ã€‚

  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/707f0f02c6aa4dd1854bf28312013182.png#pic_center)

  - è®ºæ–‡ç»™äº†ä¸åŒå±‚æ•°çš„æ¨¡å‹

    - æœ€å¤§çš„æ¨¡å‹ç§°ä¸º GPT-2 æ¨¡å‹ï¼Œå‚æ•°æœ‰ 1.5Bï¼›
    - æœ€å°çš„å³æ˜¯ GPT-1ï¼Œå¯¹æ ‡ Bert-baseï¼›
    - å€’æ•°ç¬¬äºŒå°çš„å¯¹æ ‡ Bert-largeã€‚

  - ä¸åŒæ¨¡å‹å¤§å°å¦‚ä¸‹:
  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/888b7f350a62461d84897fcd39e8b2a5.png#pic_center)

**æ¨¡å‹è®­ç»ƒ**:

- GPT-2 åªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ã€‚
- **æ— ç›‘ç£è®­ç»ƒ**:
  - GPT-2 çš„è®­ç»ƒæ–¹å¼å’Œ GPT-1 çš„è®­ç»ƒæ–¹å¼ç›¸æ¯”ï¼Œä¸¤è€…éƒ½æ˜¯æœ‰é¢„è®­ç»ƒè¿‡ç¨‹çš„
  - ä¸è¿‡ GPT-2 åªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¸é‡‡ç”¨ Fine-Tuning æ–¹æ³•ï¼Œè€Œæ˜¯é‡‡ç”¨è®ºæ–‡ä¸­æåˆ°çš„ zero-shot æ–¹æ³•ã€‚
  - GPT-2 é‡‡ç”¨è¿™ç§æ¨¡å¼ï¼Œå½’å› äº GPT-2 æå‡ºçš„æ ¸å¿ƒæ€æƒ³: å½“ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å®¹é‡è¶³å¤Ÿå¤§æ•°æ®é‡è¶³å¤Ÿä¸°å¯Œæ—¶ï¼Œå®ƒå°±è¶³ä»¥è¦†ç›–æ‰€æœ‰çš„æœ‰ç›‘ç£ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰€æœ‰çš„æœ‰ç›‘ç£å­¦ä¹ éƒ½æ˜¯æ— ç›‘ç£è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªå­é›†ï¼Œä»…ä»…é è®­ç»ƒè¯­è¨€æ¨¡å‹ä¾¿å¯ä»¥å®Œæˆå…¶ä»–æœ‰ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ã€‚

**GPT-2 çš„æ•°æ®é›†**:

- è®¸å¤šä¹‹å‰çš„å·¥ä½œæ˜¯åœ¨å•ä¸ªæ–‡æœ¬åŸŸä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚æ–°é—»æ–‡ç« ï¼Œç»´åŸºç™¾ç§‘æˆ–å°è¯´ç­‰ç­‰ã€‚GPT-2 åˆ™æ˜¯å¸Œæœ›ä½¿å¾—è®­ç»ƒæ•°æ®é›†çš„é¢†åŸŸå’Œä¸Šä¸‹æ–‡æ›´å¤šä¸€ç‚¹ã€‚
- åœ¨ç½‘ç«™ä¸Šçˆ¬å–æ–‡æœ¬æ˜¯ä¸€ä¸ªæ–¹æ¡ˆï¼Œæ¯”å¦‚è¯´ Common Crawl ç½‘ç«™ã€‚è™½ç„¶è¿™äº›ç½‘ç«™çš„æ•°æ®é›†åœ¨é‡çº§ä¸Šå¾ˆå¤§ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¥é‡çš„æ•°æ®è´¨é‡é—®é¢˜ï¼Œè¿™ä¸Šé¢çš„å†…å®¹æœ‰å¾ˆå¤šæ˜¯ä¿¡å™ªæ¯”å¾ˆä½çš„ï¼Œéš¾ä»¥ç†è§£çš„å†…å®¹ã€‚
- ä¸ºäº†è§£å†³æ•°æ®é›†è´¨é‡çš„é—®é¢˜ï¼ŒGPT-2 åªçˆ¬å–äººç±»è¿‡æ»¤ä¹‹åçš„ç½‘é¡µã€‚
- æ‰‹åŠ¨è¿‡æ»¤çš„ç½‘ç»œçˆ¬å–å¾ˆæ˜‚è´µï¼Œæ‰€ä»¥ GPT-2 ä»ç¤¾äº¤åª’ä½“å¹³å° Reddit ä¸ŠæŠ“å–äº†è‡³å°‘æ”¶åˆ°äº† 3 ä¸ª karma çš„é“¾æ¥ã€‚
  - karma å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§å¯å‘å¼æŒ‡æ ‡ï¼Œç”¨äºåˆ¤æ–­å…¶ä»–ç”¨æˆ·æ˜¯å¦è®¤ä¸ºè¯¥é“¾æ¥æœ‰è¶£, æœ‰æ•™è‚²æ„ä¹‰æˆ–åªæ˜¯æœ‰è¶£ã€‚
  - å¾—åˆ°çš„è¿™ä¸ªæ•°æ®é›†ç§°ä¹‹ä¸º WebTextï¼Œæ˜¯ä¸€ä¸ªåŒ…å«äº† 4500 ä¸‡ä¸ªé“¾æ¥çš„æ–‡æœ¬æ•°æ®é›†ã€‚
- ç»è¿‡é‡å¤æ•°æ®åˆ é™¤å’Œä¸€äº›åŸºäºå¯å‘å¼çš„æ¸…ç†åï¼Œå®ƒåŒ…å«ç•¥å¤šäº 800 ä¸‡ä¸ªæ–‡æ¡£ï¼Œæ€»æ–‡æœ¬å®¹é‡ä¸º 40GBã€‚
  - ä½œè€…ä» WebText ä¸­åˆ é™¤äº†æ‰€æœ‰ç»´åŸºç™¾ç§‘æ–‡æ¡£ï¼Œå› ä¸ºå®ƒå¯èƒ½æ¶‰åŠåˆ° test evaluation tasksã€‚
  - ç›®å‰å…¨é‡çš„æ•°æ®æ˜¯æ²¡æœ‰å¼€æ”¾ä¸‹è½½çš„ï¼Œå¯é€šè¿‡[GPT-2 è®­ç»ƒæ•°æ®é›†](https://link.zhihu.com/?target=https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/webtext.train.jsonl)ä¸‹è½½éƒ¨åˆ†è®­ç»ƒæ•°æ®ã€‚

**GPT-2 ç‰¹ç‚¹**:

- **ä¼˜ç‚¹**:

  - GPT-2 ç›¸å¯¹ GPT-1 æ¨¡å‹çš„äº®ç‚¹æ˜¯æ”¯æŒ zero-shot çš„è®¾ç½®ï¼ŒåŒæ—¶åœ¨ zero-shot çš„å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­å±•ç¤ºå‡ºä¸é”™çš„æ€§èƒ½ã€‚
  - GPT-2 é¦–å…ˆæ„é€ äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†: WebTextï¼Œå®ƒæ˜¯ä¸€ä¸ªæœ‰ç™¾ä¸‡çº§åˆ«æ–‡æœ¬çš„æ•°æ®é›†ã€‚
  - GPT-2 è‡ªå·±æ˜¯ä¸€ä¸ªæœ‰ç€ 1.5B å‚æ•°é‡çš„æ¨¡å‹ï¼›
  - GPT-2 æå‡ºäº†æ–°çš„ NLP èŒƒå¼ï¼Œå¼ºè°ƒé€šè¿‡æ›´å¤šçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®è®­ç»ƒé«˜å®¹é‡è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ— ç›‘ç£å®Œæˆä¸‹æ¸¸å¤šä»»åŠ¡ã€‚
  - å°è¯•ä»¥ä¸€ç§é€šç”¨çš„è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå»è§£å†³ç°æœ‰çš„å¤§éƒ¨åˆ† NLP ä»»åŠ¡ï¼›

- **ç¼ºç‚¹**: GPT-2 åœ¨æ¨¡å‹æœ¬èº«ä¸Šæ²¡å•¥å¤§çš„å˜åŒ–å’Œåˆ›æ–°ã€‚

---

##### GPT-3

- GPT-2 çš„æœ€å¤§è´¡çŒ®æ˜¯éªŒè¯äº†é€šè¿‡æµ·é‡æ•°æ®å’Œå¤§é‡å‚æ•°è®­ç»ƒå‡ºæ¥çš„è¯å‘é‡æ¨¡å‹æœ‰è¿ç§»åˆ°å…¶å®ƒç±»åˆ«ä»»åŠ¡ä¸­è€Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚

  - ä½†æ˜¯å¾ˆå¤šå®éªŒä¹Ÿè¡¨æ˜ï¼ŒGPT-2 çš„æ— ç›‘ç£å­¦ä¹ çš„èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œç”šè‡³åœ¨æœ‰äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ¯”éšæœºçš„å¥½ã€‚
  - å°½ç®¡åœ¨æœ‰äº› zero-shot çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸é”™ï¼Œä½†æ˜¯æˆ‘ä»¬ä»ä¸æ¸…æ¥š GPT-2 çš„è¿™ç§ç­–ç•¥ç©¶ç«Ÿèƒ½åšæˆä»€ä¹ˆæ ·å­ã€‚
  - GPT-2 è¡¨æ˜éšç€æ¨¡å‹å®¹é‡å’Œæ•°æ®é‡çš„å¢å¤§ï¼Œå…¶æ½œèƒ½è¿˜æœ‰è¿›ä¸€æ­¥å¼€å‘çš„ç©ºé—´ï¼ŒåŸºäºè¿™ä¸ªæ€æƒ³ï¼Œè¯ç”Ÿäº†æˆ‘ä»¬ä¸‹é¢è¦ä»‹ç»çš„ GPT-3[ã€ŠLanguage Models are Few-Shot Learnersã€‹](https://arxiv.org/pdf/2005.14165.pdf)ã€‚

- GPT-2 åœ¨ GPT-1 çš„åŸºç¡€ä¸Šå¾€å‰èµ°äº†ä¸€å¤§æ­¥: å®Œå…¨æŠ›å¼ƒäº†å¾®è°ƒï¼Œå¹¶é‡‡ç”¨äº† zero-shot çš„æ–¹å¼ã€‚
  - Zero-shot çš„æ–¹å¼è¢« GPT-2 è®¤è¯å¯è¡Œåï¼ŒOpenAI å°±ä¸å¾—ä¸å¼€å§‹è€ƒè™‘æ¨¡å‹æ˜¯å¦èƒ½çœŸæ­£åšåˆ°å¼ºå¤§äº†ï¼Œæ¯•ç«Ÿç°åœ¨åªæ˜¯å’Œ Bert æŒå¹³è€Œå·²ã€‚è¿™ä¸€åˆ» OpenAI å¼€å§‹æ‚Ÿè¿‡æ¥ï¼Œæ—¢ç„¶ LLM è¦ä¸€è·¯èµ°åˆ°åº•ï¼Œæ—¢ç„¶æ¨¡å‹å˜å¤§é¿å…ä¸äº†ï¼Œé‚£ä¸å¦‚æ¥å¾—æ›´å½»åº•ä¸€äº›ã€‚
  - GPT-3 æ²¿ç”¨äº†å»é™¤ Fine-Tuningï¼Œåªåšé€šç”¨è¯­è¨€æ¨¡å‹çš„æ€è·¯ï¼ŒåŒæ—¶æŠ€æœ¯ä¸Šå°åšæ›¿æ¢(`sparse Transformer`)ï¼›
    - å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨ä¸åšå¾®è°ƒçš„å‰æä¸‹é‡‡ç”¨äº† few-shot çš„æ–¹å¼(æ¯•ç«Ÿå®Œå…¨ä¸ç»™æ¨¡å‹ä»»ä½•æ˜¾æ€§æç¤ºï¼Œæ•ˆæœç¡®å®æ²¡è¾¾åˆ°é¢„æœŸ)ã€‚
    - æœ€ç»ˆç”Ÿæˆäº†ä¸€ä¸ªå¤§å°é«˜è¾¾ 175B çš„å¤§æ¨¡å‹ï¼Œå½“ç„¶æ•ˆæœä¹Ÿæ˜¯ä¸€éª‘ç»å°˜çš„ã€‚

**æ¨¡å‹ç»“æ„**: GPT-3 çš„æ¨¡å‹ä¸ GPT-2 çš„æ¨¡å‹åŸºæœ¬ä¸€è‡´ï¼Œä¸»è¦æ”¹è¿›åªæœ‰ä¸€ç‚¹:

- **Sparse Attention**:

  - åœ¨æ¨¡å‹ç»“æ„ä¸­çš„æ³¨æ„åŠ›å±‚ï¼ŒGPT-3 é‡‡ç”¨ Sparse Transformer ä¸­çš„ Sparse Attention æ–¹æ¡ˆ
  - sparse attention ä¸ä¼ ç»Ÿ self-attention(ç§°ä¸º dense attention)çš„åŒºåˆ«åœ¨äº:

    - **dense attention**: æ¯ä¸ª token ä¹‹é—´ä¸¤ä¸¤è®¡ç®— attentionï¼Œå¤æ‚åº¦ $O(n^{2})$
    - **sparse attention**: æ¯ä¸ª token åªä¸å…¶ä»– token çš„ä¸€ä¸ªå­é›†è®¡ç®— attentionï¼Œå¤æ‚åº¦ $O(n*logn)$

  - å…·ä½“æ¥è¯´ï¼Œsparse attention é™¤äº†ç›¸å¯¹è·ç¦»ä¸è¶…è¿‡ $k$ ä»¥åŠç›¸å¯¹è·ç¦»ä¸º $kï¼Œ2kï¼Œ3kï¼Œ\cdots$ çš„ tokenï¼Œå…¶ä»–æ‰€æœ‰ token çš„æ³¨æ„åŠ›éƒ½è®¾ä¸º 0

    - å¦‚ä¸‹å›¾æ‰€ç¤º:
    - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2190cc14dc354960b919117f658590c9.jpeg#pic_center)
    - å›¾ä¸­çš„ç¬¬äºŒè¡Œå°±æ˜¯æ¶‰åŠåˆ°çš„ attention çš„ token å†…å®¹ï¼Œå¯ä»¥çœ‹å‡ºé¦–å…ˆå…³æ³¨äº†é™„è¿‘å››ä¸ª tokenï¼Œå…¶æ¬¡æ˜¯$2k,3k 2k,3k$ è·ç¦»çš„ token

  - é‚£ä¹ˆä¸ºä»€ä¹ˆè¿™ä¹ˆåšå‘¢ï¼Ÿä½¿ç”¨ sparse attention çš„å¥½å¤„ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç‚¹:

    - å‡å°‘æ³¨æ„åŠ›å±‚çš„è®¡ç®—å¤æ‚åº¦ï¼ŒèŠ‚çº¦æ˜¾å­˜å’Œè€—æ—¶ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ï¼›
    - å…·æœ‰â€œå±€éƒ¨ç´§å¯†ç›¸å…³å’Œè¿œç¨‹ç¨€ç–ç›¸å…³â€çš„ç‰¹æ€§ï¼Œå¯¹äºè·ç¦»è¾ƒè¿‘çš„ä¸Šä¸‹æ–‡å…³æ³¨æ›´å¤šï¼Œå¯¹äºè·ç¦»è¾ƒè¿œçš„ä¸Šä¸‹æ–‡å…³æ³¨è¾ƒå°‘ã€‚

  - å…³äº Sparse Transformer çš„è¯¦ç»†ä»‹ç»å¯ä»¥å‚è§ OpenAI äº 2019 å¹´å‘è¡¨çš„è®ºæ–‡[ã€ŠGenerating Long Sequences with Sparse Transformersã€‹](https://arxiv.org/pdf/1904.10509.pdf)ã€‚

    - è®ºæ–‡ä¸­ä¾›è®­ç»ƒäº† 8 ä¸ªä¸é€šè§„æ¨¡çš„æ¨¡å‹ï¼Œæœ€å¤§çš„ä¸€ä¸ªç§°ä½œä¸º GPT-3:
    - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/49511955866740cab0d31d50b68cd8a0.png#pic_center)

**æ¨¡å‹è®­ç»ƒ**:

- GPT-3 ä¹Ÿåªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ã€‚
- **æ— ç›‘ç£è®­ç»ƒ**:
  - GPT-3 ä»é‡‡ç”¨ GPT-2 æå‡ºçš„ä»…åšé¢„è®­ç»ƒ, ä¸åšå¾®è°ƒçš„æ€è·¯ã€‚
  - GPT-3 é‡‡ç”¨äº† In-context learningã€‚å€Ÿç”¨ meta-learning(å…ƒå­¦ä¹ )çš„æ€æƒ³ï¼Œåœ¨ pre-training æœŸé—´è®©æ¨¡å‹å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½å’Œæ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œè€Œåœ¨æ¨ç†æœŸé—´åˆ©ç”¨è¿™äº›æŠ€èƒ½å’Œèƒ½åŠ›è¿…é€Ÿé€‚é…åˆ°æœŸæœ›çš„ä»»åŠ¡ä¸Šã€‚
  - GPT-3 ä¸­çš„ In-context learningã€‚
  - è¦ç†è§£ In-context learningï¼Œæˆ‘ä»¬éœ€è¦å…ˆç†è§£ meta-learning(å…ƒå­¦ä¹ )ã€‚
    - å¯¹äºä¸€ä¸ªå°‘æ ·æœ¬çš„ä»»åŠ¡æ¥è¯´ï¼Œæ¨¡å‹çš„åˆå§‹åŒ–å€¼éå¸¸é‡è¦
    - ä»ä¸€ä¸ªå¥½çš„åˆå§‹åŒ–å€¼ä½œä¸ºèµ·ç‚¹ï¼Œæ¨¡å‹èƒ½å¤Ÿå°½å¿«æ”¶æ•›ï¼Œä½¿å¾—åˆ°çš„ç»“æœéå¸¸å¿«çš„é€¼è¿‘å…¨å±€æœ€ä¼˜è§£ã€‚
    - å…ƒå­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³åœ¨äºé€šè¿‡å°‘é‡çš„æ•°æ®å¯»æ‰¾ä¸€ä¸ªåˆé€‚çš„åˆå§‹åŒ–èŒƒå›´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šå¿«é€Ÿæ‹Ÿåˆï¼Œå¹¶è·å¾—ä¸é”™çš„æ•ˆæœã€‚
  - è¿™é‡Œçš„ä»‹ç»ä½¿ç”¨çš„æ˜¯`MAML(Model-Agnostic Meta-Learning)`ç®—æ³•
    - æ­£å¸¸çš„ç›‘ç£å­¦ä¹ æ˜¯å°†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ‰“åŒ…æˆä¸€ä¸ª batch è¿›è¡Œå­¦ä¹ ã€‚
    - ä½†æ˜¯å…ƒå­¦ä¹ æ˜¯å°†ä¸€ä¸ªä¸ªä»»åŠ¡æ‰“åŒ…æˆ batchï¼Œæ¯ä¸ª batch åˆ†ä¸º`æ”¯æŒé›†(support set)`å’Œ`è´¨è¯¢é›†(query set)`ï¼Œç±»ä¼¼äºå­¦ä¹ ä»»åŠ¡ä¸­çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
  - å¯¹ä¸€ä¸ªç½‘ç»œæ¨¡å‹ $f$ï¼Œå…¶å‚æ•°è¡¨ç¤ºä¸º $\theta$ï¼Œå®ƒçš„åˆå§‹åŒ–å€¼è¢«å«åš meta-initializationã€‚
  - MAML çš„ç›®æ ‡åˆ™æ˜¯å­¦ä¹ ä¸€ç»„ meta-initializationï¼Œèƒ½å¤Ÿå¿«é€Ÿåº”ç”¨åˆ°å…¶å®ƒä»»åŠ¡ä¸­ã€‚
  - MAML çš„è¿­ä»£æ¶‰åŠä¸¤æ¬¡å‚æ•°æ›´æ–°ï¼Œåˆ†åˆ«æ˜¯`å†…å¾ªç¯(inner loop)`å’Œ`å¤–å¾ªç¯(outer loop)`ã€‚
    - å†…å¾ªç¯æ˜¯æ ¹æ®ä»»åŠ¡æ ‡ç­¾å¿«é€Ÿçš„å¯¹å…·ä½“çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ å’Œé€‚åº”
    - å¤–å­¦ä¹ åˆ™æ˜¯å¯¹ meta-initialization è¿›è¡Œæ›´æ–°ã€‚
  - ç›´è§‚çš„ç†è§£ï¼Œæˆ‘ç”¨ä¸€ç»„ meta-initialization å»å­¦ä¹ å¤šä¸ªä»»åŠ¡ï¼Œå¦‚æœæ¯ä¸ªä»»åŠ¡éƒ½å­¦å¾—æ¯”è¾ƒå¥½ï¼Œåˆ™è¯´æ˜è¿™ç»„ meta-initialization æ˜¯ä¸€ä¸ªä¸é”™çš„åˆå§‹åŒ–å€¼ï¼Œå¦åˆ™æˆ‘ä»¬å°±å»å¯¹è¿™ç»„å€¼è¿›è¡Œæ›´æ–°ã€‚
  - GPT-3 ä¸­ä»‹ç»çš„ In-context learning åˆ™æ˜¯å…ƒå­¦ä¹ çš„`å†…å¾ªç¯`ï¼ŒåŸºäºè¯­è¨€æ¨¡å‹çš„ SGD åˆ™ä¸‹å›¾æ‰€ç¤ºã€‚
  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/49030ab15e4b4045bc65c9b8b66ada22.png#pic_center)

**ä¸‹æ¸¸ä»»åŠ¡**:

- åœ¨`è®­ç»ƒé˜¶æ®µ`ï¼Œé¢„è®­ç»ƒé€šç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…·å¤‡è¯†åˆ«ä¸åŒ NLP ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ­¤æ—¶æ¨¡å‹å…·å¤‡äº†ä¸€å®šçš„ ICL èƒ½åŠ›ã€‚
- åœ¨`æ¨ç†é˜¶æ®µ`ï¼Œä¾èµ–äºæ¨¡å‹çš„ ICL èƒ½åŠ›ï¼Œé’ˆå¯¹å„ NLP ä»»åŠ¡ï¼Œå‘æ¨¡å‹ä¸­è¾“å…¥ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œä¸Šä¸‹æ–‡åŒ…æ‹¬ä»»åŠ¡æè¿°, è‹¥å¹²ä¸ªä»»åŠ¡æ ·æœ¬å’Œä»»åŠ¡æç¤ºï¼Œæ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ç»™å‡ºä»»åŠ¡è¾“å‡ºã€‚
- æ ¹æ®ä¸Šä¸‹æ–‡åŒ…å«çš„ä»»åŠ¡æ ·æœ¬æ•°é‡å¯è¿›ä¸€æ­¥å°†ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†ä¸º`Zero-Shot(æ— ä»»åŠ¡æ ·æœ¬)`, `One-Shot(ä»…ä¸€ä¸ªä»»åŠ¡æ ·æœ¬)`å’Œ`Few-Shot(å¤šä¸ªä»»åŠ¡æ ·æœ¬)`ä¸‰ç±»ã€‚

- **Fine-Tunning(FT)**:

  - FT åˆ©ç”¨æˆåƒä¸Šä¸‡çš„`ä¸‹æ¸¸ä»»åŠ¡æ ‡æ³¨æ•°æ®`æ¥æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æƒé‡ä»¥è·å¾—å¼ºå¤§çš„æ€§èƒ½ã€‚
  - ä½†æ˜¯ï¼Œè¯¥æ–¹æ³•ä¸ä»…å¯¼è‡´æ¯ä¸ªæ–°çš„ä¸‹æ¸¸ä»»åŠ¡éƒ½éœ€è¦å¤§é‡çš„æ ‡æ³¨è¯­æ–™ï¼Œè¿˜å¯¼è‡´æ¨¡å‹åœ¨æ ·æœ¬å¤–é¢„æµ‹çš„èƒ½åŠ›å¾ˆå¼±ã€‚
  - è™½ç„¶ GPT-3 ä»ç†è®ºä¸Šæ”¯æŒ FTï¼Œä½†è®ºæ–‡ä¸­æ²¡è¿™ä¹ˆåšï¼›

- **Few-Shot(FS)**:

  - æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µå¯ä»¥å¾—åˆ°å°‘é‡çš„`ä¸‹æ¸¸ä»»åŠ¡ç¤ºä¾‹`ä½œä¸ºé™åˆ¶æ¡ä»¶ï¼Œä½†æ˜¯ä¸å…è®¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æƒé‡ã€‚
  - FS è¿‡ç¨‹çš„ç¤ºä¾‹å¯ä»¥çœ‹ä¸‹å›¾ä¸­æ•´ç†çš„æ¡ˆä¾‹ã€‚
  - FS çš„**ä¸»è¦ä¼˜ç‚¹**æ˜¯
    - å¹¶`ä¸éœ€è¦å¤§é‡çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®`
    - åŒæ—¶ä¹Ÿé˜²æ­¢äº†æ¨¡å‹åœ¨ Fine-Tuning é˜¶æ®µçš„è¿‡æ‹Ÿåˆã€‚
  - FS çš„**ä¸»è¦ç¼ºç‚¹**æ˜¯
    - ä¸ Fine-Tuning çš„ SOTA æ¨¡å‹æ€§èƒ½å·®è·è¾ƒå¤§
    - ä¸”ä»éœ€è¦å°‘é‡çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®

- **One-Shot(1S)**:

  - æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µ`ä»…å¾—åˆ°1ä¸ªä¸‹æ¸¸ä»»åŠ¡ç¤ºä¾‹`ã€‚
  - æŠŠ 1S ç‹¬ç«‹äº Few-Shot å’Œ Zero-Shot è®¨è®ºæ˜¯å› ä¸º`è¿™ç§æ–¹å¼ä¸äººç±»æ²Ÿé€šçš„æ–¹å¼æœ€ç›¸ä¼¼`ï¼›

- **Zero-Shot(0S)**:

  - æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µ`ä»…å¾—åˆ°ä¸€æ®µä»¥è‡ªç„¶è¯­è¨€æè¿°çš„ä¸‹æ¸¸ä»»åŠ¡è¯´æ˜`ã€‚
  - 0S çš„ä¼˜ç‚¹æ˜¯æä¾›äº†æœ€å¤§ç¨‹åº¦çš„æ–¹ä¾¿æ€§, å°½å¯èƒ½å¤§çš„é²æ£’æ€§å¹¶å°½å¯èƒ½é¿å…äº†ä¼ªç›¸å…³æ€§ã€‚
  - 0S çš„æ–¹å¼æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜çš„ï¼Œå³ä½¿æ˜¯äººç±»æœ‰æ—¶å€™ä¹Ÿéš¾ä»¥ä»…ä¾èµ–ä»»åŠ¡æè¿°è€Œæ²¡æœ‰ç¤ºä¾‹çš„æƒ…å†µä¸‹ç†è§£ä¸€ä¸ªä»»åŠ¡ã€‚
  - ä½†æ¯«æ— ç–‘é—®ï¼Œ0S è®¾ç½®ä¸‹çš„æ€§èƒ½æ˜¯æœ€ä¸äººç±»çš„æ°´å¹³å…·æœ‰å¯æ¯”æ€§çš„ã€‚

- ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3fc26c6dcbfb4fff8d7fa67ad2a5c9ff.png#pic_center)

**GPT-3 çš„æ•°æ®é›†**:

- GPT-3 çš„è®­ç»ƒæ•°æ®åŒ…æ‹¬ä½è´¨é‡çš„ Common Crawlï¼Œé«˜è´¨é‡çš„ WebText2, Books1, Books2 å’Œ Wikipediaã€‚
- GPT-3 æ ¹æ®æ•°æ®é›†çš„ä¸åŒè´¨é‡èµ‹äºˆäº†ä¸åŒçš„æƒå€¼ï¼Œæƒå€¼è¶Šé«˜çš„åœ¨è®­ç»ƒçš„æ—¶å€™è¶Šå®¹æ˜“æŠ½æ ·åˆ°(è§ä¸‹å›¾)ã€‚

- ä¸ºäº†æ¸…ç†è„æ•°æ®ï¼ŒOpenAI åšäº†ä»¥ä¸‹çš„æ•°æ®å¤„ç†:

  - ä½¿ç”¨é«˜è´¨é‡æ•°æ®ä½œä¸ºæ­£ä¾‹ï¼Œè®­ç»ƒ LR åˆ†ç±»ç®—æ³•ï¼Œå¯¹ CommonCrawl çš„æ‰€æœ‰æ–‡æ¡£åšåˆæ­¥è¿‡æ»¤ï¼›
  - åˆ©ç”¨å…¬å¼€çš„ç®—æ³•åšæ–‡æ¡£å»é‡ï¼Œå‡å°‘å†—ä½™æ•°æ®ï¼›
  - åŠ å…¥å·²çŸ¥çš„é«˜è´¨é‡æ•°æ®é›†ï¼›

- æœ€ç»ˆå¤„ç†å®Œæˆåä½¿ç”¨çš„æ•°æ®è§„æ¨¡çº¦ 570Gã€‚
- ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b28940a5d66449cf9e9e415560b2bfe8.png#pic_center)
- å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåœ¨å®é™…å®éªŒè¿‡ç¨‹ä¸­ï¼Œå¯¹ä¸åŒæ•°æ®é›†æŒ‰ç…§ä¸€å®šçš„æ¯”ä¾‹è¿›è¡Œé‡‡æ ·ï¼Œè¿™ä¸ªæ¯”ä¾‹ä¸æ˜¯æŒ‰ç…§åŸå§‹æ•°æ®é‡å¤šå°‘æ¥åˆ’åˆ†çš„ï¼Œä¸ç„¶è¿™é‡ŒåŸºæœ¬é‡‡æ ·åˆ°çš„å°±éƒ½æ˜¯ Common Crawl çš„æ•°æ®äº†ï¼Œå¯ä»¥çœ‹åˆ°è¿™é‡Œ Common Crawl çš„æ•°æ®é‡æ¯”å…¶ä»–å‡ ä¸ªå¤šå¾ˆå¤šã€‚
- è¿›è¡Œé‡‡æ ·çš„åŸå› ä¸»è¦è€ƒè™‘åˆ°ï¼Œå°±ç®—åšäº†ä¸€äº›æ•°æ®æ¸…æ´—è¿˜æ˜¯è§‰å¾— Common Crawl çš„æ•°æ®è´¨é‡ä¸å¦‚å…¶ä»–å‡ ä¸ªã€‚
- æœ€ç»ˆé‡‡æ ·çš„æ—¶å€™ï¼Œè™½ç„¶ Common Crawl çš„æ•°æ®é‡æ˜¯å…¶ä»–å‡ ä¸ªæ•°æ®é›†çš„ä¸Šç™¾å€ï¼Œä½†æ˜¯å®é™…å æ¯”æ˜¯ 60%ï¼Œæœ‰ 40%çš„æ•°æ®æ˜¯èƒ½å¤Ÿä¿è¯è´¨é‡çš„ã€‚

**GPT-3 çš„ç‰¹ç‚¹**:

- **ä¼˜ç‚¹**:

  - GPT-3 çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚
  - ä¸éœ€è¦å¾®è°ƒï¼Œåªéœ€è¦åœ¨è¾“å…¥åºåˆ—é‡Œç”¨è‡ªç„¶è¯­è¨€è¡¨è¿°ä»»åŠ¡è¦æ±‚ï¼Œå°±å¯ä»¥è®©æ¨¡å‹æ‰§è¡Œä¸åŒçš„å­ä»»åŠ¡ã€‚
  - GPT-3 åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº† SOTAï¼Œå¹¶ä¸”éªŒè¯äº†æ¨¡å‹è§„æ¨¡è¶Šå¤§, ä»»åŠ¡æ•ˆæœè¶Šå¥½ï¼Œä¸”å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼ŒGPT-3 çš„ Few-Shot ä¼˜äº One-Shot å’Œ Zero-Shotã€‚
  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/da96b6d40b2c4523b21c63d0b87d1fdd.jpeg#pic_center)

- **ç¼ºç‚¹**:
  - ç”Ÿæˆçš„å†…å®¹å­˜åœ¨é‡å¤æˆ–ä¸åˆç†çš„å¥å­, æ®µè½ï¼Œç¼ºä¹å¸¸è¯†ï¼Œåœ¨ä¸€äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸€èˆ¬ï¼Œç”šè‡³å’Œéšæœºåˆ¤æ–­å·®ä¸å¤šï¼›
  - æ¨¡å‹ç»“æ„ä½¿ç”¨çš„ Transformer è§£ç å™¨æ˜¯ä¸€ä¸ªå•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥åœ¨ä¸€äº›éœ€è¦åŒå‘ç†è§£çš„ NLP ä»»åŠ¡(æ¯”å¦‚æ–‡æœ¬è•´å«)ä¸Šè¡¨ç°ä¸ä½³ï¼›
  - è¯­è¨€æ¨¡å‹åº•å±‚åŸç†è¿˜æ˜¯æ ¹æ®å‰åºè¯å…ƒé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒï¼Œæ²¡æœ‰è€ƒè™‘ä¸åŒè¯å…ƒçš„æƒé‡ï¼›
  - æ¨¡å‹è§„æ¨¡å¤ªå¤§ï¼Œè®¡ç®—èµ„æºæˆæœ¬è¾ƒé«˜ï¼Œåç»­çš„ä¸€ä¸ªæ–¹å‘æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼›
  - å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¼ºï¼›
  - æ­¤å¤–ï¼Œä½œè€…è¿˜å±•æœ›äº†ä¸€ä¸‹ GPT-3 å¯èƒ½å¸¦æ¥çš„ç¤¾ä¼šå½±å“ã€‚æ¯”å¦‚å®ƒå¯èƒ½è¢«æ‹¿æ¥ç”Ÿæˆå‡æ–°é—», åƒåœ¾é‚®ä»¶ï¼Œä»¥åŠè®ºæ–‡é€ å‡ã€‚ç”±äº GPT-3 çš„è®­ç»ƒæ•°æ®æ¥è‡ªç½‘ç»œï¼Œå…¶ä¸­åŒ…å«äº†ä¸€äº›æ€§åˆ«, å®—æ•™, ç§æ—æ­§è§†çš„ä¿¡æ¯ï¼Œå¯¼è‡´ GPT-3 ç”Ÿæˆçš„æ–‡æœ¬ä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ã€‚

---

#### 1.2 InstructGPT

GPT-3 è™½ç„¶åœ¨å„å¤§ NLP ä»»åŠ¡ä»¥åŠæ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ä¸Šä»¤äººæƒŠè‰³ï¼Œä½†æ˜¯ä»–ä»ç„¶è¿˜æ˜¯ä¼šç”Ÿæˆä¸€äº›å¸¦æœ‰åè§çš„ï¼Œä¸çœŸå®çš„ï¼Œæœ‰å®³çš„é€ æˆè´Ÿé¢ç¤¾ä¼šå½±å“çš„ä¿¡æ¯ï¼Œè€Œä¸”å¾ˆå¤šæ—¶å€™ï¼Œä»–å¹¶ä¸æŒ‰äººç±»å–œæ¬¢çš„è¡¨è¾¾æ–¹å¼å»è¯´è¯ã€‚

- åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼ŒOpenAI æå‡ºäº†ä¸€ä¸ªæ¦‚å¿µâ€œ`Alignment`â€
  - æ„æ€æ˜¯æ¨¡å‹è¾“å‡ºä¸äººç±»çœŸå®æ„å›¾å¯¹é½ï¼Œç¬¦åˆäººç±»åå¥½ã€‚
  - å› æ­¤ï¼Œä¸ºäº†è®©æ¨¡å‹è¾“å‡ºä¸ç”¨æˆ·æ„å›¾æ›´åŠ å¯¹é½ï¼Œå°±æœ‰äº† InstructGPT è¿™ä¸ªå·¥ä½œ[ã€ŠTraining language models to follow instructionswith human feedbackã€‹](https://arxiv.org/pdf/2203.02155.pdf)ã€‚
  - InstructGPT æå‡ºäº†ä¸€ä¸ªç†æƒ³åŒ–è¯­è¨€æ¨¡å‹çš„ä¸‰å¤§ç›®æ ‡:
    - **helpful**(èƒ½å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜),
    - **honest**(ä¸èƒ½æé€ äº‹å®ï¼Œä¸èƒ½è¯¯å¯¼ç”¨æˆ·),
    - **harmless**(ä¸èƒ½å¯¹ç”¨æˆ·æˆ–ç¯å¢ƒé€ æˆç‰©ç†, ç²¾ç¥, ç¤¾ä¼šå±‚é¢çš„ä¼¤å®³)ã€‚

ä¸ºäº†å®ç°ä¸Šè¿°çš„ç›®æ ‡ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºäººç±»åé¦ˆæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªç”¨æˆ·çš„æŒ‡ç¤ºï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„è´¨é‡å’Œå¯ä¿¡åº¦ã€‚

- åŸºæœ¬æµç¨‹åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤:

- **æ­¥éª¤ä¸€**:
  - ä» OpenAI API ä¸­è·å–ç”¨æˆ·æäº¤çš„`æŒ‡ä»¤prompt`(åé¢æåˆ°çš„`æŒ‡ä»¤prompt`éƒ½å¯ç†è§£ä¸ºé—®é¢˜)å’Œæ ‡æ³¨äººå‘˜ç¼–å†™çš„`æŒ‡ä»¤prompt`ä¸­æ”¶é›†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œ
  - ä»æ”¶é›†åˆ°çš„`æŒ‡ä»¤prompt`æ•°æ®é›†ä¸­å–å‡ºä¸€äº›`æŒ‡ä»¤prompt`ï¼Œç„¶åè®©æ ‡æ³¨äººå‘˜æ ‡æ³¨å¯¹åº”çš„ç­”æ¡ˆ
  - å†ç”¨è¿™äº›æ•°æ®å¾®è°ƒ GPT-3 å¾—åˆ° SFT æ¨¡å‹ï¼›
- **æ­¥éª¤äºŒ**:
  - è¾“å…¥`æŒ‡ä»¤prompt`ï¼Œè®©æ¨¡å‹è¾“å‡ºå‡ ä¸ªç­”æ¡ˆï¼Œç„¶åè®©æ ‡æ³¨äººå‘˜å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œ
  - ç”¨è¿™äº›æ’åºæ•°æ®è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ RMï¼Œèƒ½å¤Ÿå¯¹ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ï¼Œæ‰“åˆ†çš„å¤§å°é¡ºåºæ»¡è¶³è®­ç»ƒä½¿ç”¨çš„è¿™äº›ç­”æ¡ˆçš„é¡ºåºï¼›
- **æ­¥éª¤ä¸‰**:
  - å†è¾“å…¥ä¸€äº›`æŒ‡ä»¤prompt`è®© STF å»ç”Ÿæˆä¸€äº›ç­”æ¡ˆï¼ŒæŠŠç­”æ¡ˆæ”¾åˆ° RM é‡Œé¢å»æ‰“åˆ†ï¼Œ
  - ç„¶åç”¨ PPO ç®—æ³•å»ä¼˜åŒ– SFT çš„å‚æ•°ä½¿å¾—å®ƒç”Ÿæˆæ›´é«˜çš„åˆ†æ•°ï¼Œæœ€åå¾—åˆ° InstrctGPTã€‚

æœ€ç»ˆå¾—åˆ°çš„ InstrctGPT ç›¸è¾ƒäº GPT-3:

- å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·æŒ‡ç¤ºä¸­éšå«æˆ–æ˜¾å¼åœ°è¡¨è¾¾å‡ºæ¥çš„ç›®æ ‡, çº¦æŸå’Œåå¥½ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›å’Œéœ€æ±‚çš„è¾“å‡ºï¼›
- å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºä¸­æä¾›çš„ä¿¡æ¯æˆ–ç»“æ„ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œåˆç†æ¨æ–­æˆ–åˆ›é€ ã€‚
- å¯ä»¥æ›´ç¨³å®šåœ°ä¿æŒè¾“å‡ºè´¨é‡ï¼Œå¹¶å‡å°‘é”™è¯¯æˆ–å¤±è´¥ç‡ï¼›

InstructGPT æ•°æ®é›†æ„å»ºä»¥åŠè®­ç»ƒæµç¨‹ã€‚

- **InstructGPT æ•°æ®é›†æ„å»º**: å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µã€‚

  - ç¬¬ä¸€ä¸ªé˜¶æ®µ: æ„å»ºåˆå§‹çš„`æŒ‡ä»¤prompt`æ•°æ®é›†

    - å…·ä½“åšæ³•æ˜¯è®©æ ‡æ³¨äººå‘˜æ„å»ºä¸‹é¢ä¸‰ç§ prompt:
    - **Plain**: åªè¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸€ä¸ªä»»æ„çš„ä»»åŠ¡(ä¹Ÿå°±æ˜¯`æŒ‡ä»¤prompt`)ï¼Œå¹¶ä¿è¯ä»»åŠ¡æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼›
    - **Few-shot**: è¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸€ä¸ª`æŒ‡ä»¤prompt`ï¼Œå¹¶ç»™å‡ºå¤šä¸ªç¬¦åˆè¯¥æŒ‡ä»¤çš„ query/response ç»„åˆï¼›
    - **User-based**: åŸºäºç”¨æˆ·æœŸæœ› OpenAI API ä¿±å¤‡çš„èƒ½åŠ›æ‰€æå‡ºçš„ä¸€äº›ç”¨ä¾‹ï¼Œè¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸è¿™äº›ç”¨ä¾‹ç›¸å…³çš„`æŒ‡ä»¤prompt`

  - ç¬¬äºŒä¸ªé˜¶æ®µ:

    - åŸºäºä¸Šé¢ä¸‰ç§`æŒ‡ä»¤prompt`ï¼ŒOpenAI å›¢é˜Ÿè®­ç»ƒäº†åˆå§‹ç‰ˆæœ¬çš„ InstructGPT æ¨¡å‹ï¼Œç„¶åå°†è¿™ä¸ª InstructGPT æ¨¡å‹æ”¾åˆ° Playground(Playground å¯ç†è§£ä¸ºæµ‹è¯• APIï¼Œéç”Ÿäº§ API)é‡Œä¾›ç”¨æˆ·ä½¿ç”¨
    - ç”¨æˆ·åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œä¼šç»§ç»­é—®ä¸€äº›é—®é¢˜ï¼ŒOpenAI å›¢é˜Ÿå°†è¿™äº›é—®é¢˜æ”¶é›†å›æ¥ï¼Œå¹¶è¿›è¡Œè¿‡æ»¤ç­‰æ“ä½œ
    - å…·ä½“æ¥è¯´ï¼Œå°†æ¯ä¸ªç”¨æˆ· ID çš„å¯¹åº”çš„`æŒ‡ä»¤prompt`æ•°é‡é™åˆ¶ä¸º 200 ä¸ªï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸ªäººä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ· ID æ‹†åˆ†è®­ç»ƒ, éªŒè¯å’Œæµ‹è¯•é›†(åŒä¸€ä¸ªç”¨æˆ·é—®é¢˜ä¼šæ¯”è¾ƒç±»ä¼¼ï¼Œä¸é€‚åˆåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­)ã€‚

  - å¯ä»¥çœ‹å‡ºï¼Œç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µæ˜¯ä¸€ä¸ªå¾ªç¯è¿‡ç¨‹: å…ˆæ‹¿éƒ¨åˆ†æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç„¶åé€šè¿‡æ¨¡å‹è·å–æ–°æ•°æ®ï¼Œå†ç”¨æ–°æ•°æ®ç»§ç»­ä¼˜åŒ–æ¨¡å‹ï¼Œè¿™ç§æ€è·¯ä¹Ÿå¾ˆé€‚åˆæˆ‘ä»¬ä»¥åçš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

  - ç¬¬ä¸‰é˜¶æ®µ:

    - è‡³æ­¤ï¼Œé€šè¿‡ä¸Šè¿°ä¸¤é˜¶æ®µçš„å¤„ç†ï¼ŒOpenAI å›¢é˜Ÿå·²ç»è·å–äº†ä¸€å®šé‡çš„`æŒ‡ä»¤prompt`(åŒ…æ‹¬æ ‡æ³¨äººå‘˜æ„å»ºçš„ prompt ä»¥åŠä»ç”¨æˆ·ä¾§æ”¶é›†çš„ prompt)ï¼Œæ¥ä¸‹æ¥å³æ˜¯é’ˆå¯¹ä¸åŒè®­ç»ƒä»»åŠ¡æ„å»ºä¸åŒçš„æ•°æ®é›†
    - åŸºäºç¬¬äºŒé˜¶æ®µè·å–çš„`æŒ‡ä»¤prompt`ï¼Œæ„å»ºä¸‰ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºåç»­çš„ä¸‰ä¸ªè®­ç»ƒä»»åŠ¡`SFT, RM, RL`ã€‚

    - **SFT Dataset**:

      - æ ‡æ³¨äººå‘˜æ ¹æ®`æŒ‡ä»¤prompt`æ„é€ ç­”æ¡ˆï¼Œå°† prompt å’Œç­”æ¡ˆæ‹¼åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€æ®µå¯¹è¯(promptï¼Œanswer)ï¼Œç”¨äºè®­ç»ƒ SFT æ¨¡å‹ã€‚
      - æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦ 13kï¼ŒåŒ…æ‹¬äººå·¥æ ‡è®° prompt+ç”¨æˆ·æ”¶é›† promptã€‚

    - **RM Dataset**:

      - å…ˆå°† prompt è¾“å…¥ SFT æ¨¡å‹ï¼Œæ ‡æ³¨äººå‘˜å†å¯¹ SFT æ¨¡å‹è¾“å‡ºçš„ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œç„¶åç”¨è¿™äº›æ’åºæ•°æ®(promptï¼ŒRank)è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ RMï¼Œèƒ½å¤Ÿå¯¹ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ã€‚
      - æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦ 33kï¼ŒåŒ…æ‹¬äººå·¥æ ‡è®° prompt+ç”¨æˆ·æ”¶é›† promptã€‚

    - **RL Dataset**:

      - æ­¤éƒ¨åˆ†æ•°æ®é›†ä¸éœ€è¦æ ‡æ³¨ï¼Œåªéœ€è¦ä»`æŒ‡ä»¤prompt`æ•°æ®é›†é‡Œé¢è·å–éƒ¨åˆ†`æŒ‡ä»¤prompt`ï¼Œç„¶åä½¿ç”¨ SFT å’Œ RM æ¨¡å‹åˆ†åˆ«å¾—åˆ° answer å’Œ RM ç»™å‡ºçš„åˆ†æ•°ï¼Œæ„æˆä¸‰å…ƒç»„(promptï¼Œanswerï¼ŒRM ç»™å‡ºçš„åˆ†æ•°)ï¼Œç”¨äºè¿›ä¸€æ­¥é‡‡ç”¨ PPO ç®—æ³•å¾®è°ƒ SFT æ¨¡å‹ã€‚
      - æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦ 31kï¼ŒåªåŒ…æ‹¬ç”¨æˆ·æ”¶é›† promptã€‚

    - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/dd5b7ba2c2714e91a38f4ef6f9ccc705.png#pic_center)

    - SFT Dataset å’Œ RM Dataset éƒ½éœ€è¦äººå·¥æ ‡æ³¨ï¼ŒåŒºåˆ«åœ¨äºå‰è€…çš„ç”Ÿæˆå¼çš„æ ‡æ³¨è¦æ¯”åè€…çš„åˆ¤åˆ«å¼çš„æ ‡æ³¨è´µå¾ˆå¤šï¼ŒåŒæ ·çš„æ ‡æ³¨æ—¶é—´å’Œæˆæœ¬ï¼Œè”åˆå‰è€…å’Œåè€…å¾—åˆ°çš„æ•°æ®è¦æ¯”åªç”¨å‰è€…å¾—åˆ°çš„æ•°æ®å¤šå¾ˆå¤šï¼Œåœ¨è¿™ä¸Šé¢è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ€§èƒ½å¯èƒ½ä¼šå¥½ä¸€äº›ã€‚

- **InstructGPT è®­ç»ƒæµç¨‹**:

  - InstructGPT çš„è®­ç»ƒæµç¨‹åˆ†ä¸ºäº†ä¸‰ä¸ªæ­¥éª¤: æœ‰ç›‘ç£å¾®è°ƒï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
  - å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
  - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/8d822267b9b04a08bb20b999216c284f.png#pic_center)

  - å®é™…ä¸Šå¯ä»¥æŠŠå®ƒæ‹†åˆ†æˆä¸¤ç§æŠ€æœ¯æ–¹æ¡ˆ

    - ä¸€ä¸ªæ˜¯æœ‰ç›‘ç£å¾®è°ƒ(SFT)
    - ä¸€ä¸ªæ˜¯åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)

  - **æœ‰ç›‘ç£å¾®è°ƒ(SFT)**:

    - ä»¥ GPT-3 æ¨¡å‹ä¸ºåº•åº§ï¼Œåœ¨æ ‡æ³¨å¥½çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†(é—®é¢˜+ç­”æ¡ˆ)ä¸Šè¿›è¡Œè®­ç»ƒã€‚
    - å…·ä½“æ¥è¯´ï¼Œè¿­ä»£è½®æ•°ä½¿ç”¨ 16 ä¸ª epochï¼Œå­¦ä¹ ç‡ä½¿ç”¨ä½™å¼¦è¡°å‡ï¼Œæ¨¡å‹æ®‹å·®è¿æ¥ dropout ç‡ä¸º 0.2ã€‚
    - ç”±äºåªæœ‰ 13000 ä¸ªæ•°æ®ï¼Œ1 ä¸ª epoch å°±è¿‡æ‹Ÿåˆï¼Œä¸è¿‡è®ºæ–‡ä¸­è¯æ˜äº†è¿™ä¸ªæ¨¡å‹è¿‡æ‹Ÿåˆä¹Ÿæ²¡ä»€ä¹ˆå…³ç³»ï¼Œç”šè‡³è®­ç»ƒæ›´å¤šçš„ epoch å¯¹åç»­æ˜¯æœ‰å¸®åŠ©çš„ï¼Œæœ€ç»ˆè®­ç»ƒäº† 16 ä¸ª epochã€‚

  - **åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (RLHF)**:

    - RLHF è¿‡ç¨‹åŒ…å«ä¸¤ä¸ªé˜¶æ®µ

      - ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯`RMæ¨¡å‹è®­ç»ƒ`
      - ç¬¬äºŒé˜¶æ®µæ˜¯`åˆ©ç”¨PPOç®—æ³•ç»§ç»­å¾®è°ƒSFTæ¨¡å‹`

    - **å¥–åŠ±æ¨¡å‹(RM)**:

      - æ¨¡å‹ç»“æ„æ˜¯æŠŠ SFT æ¨¡å‹æœ€åçš„ unembedding å±‚(å°±æ˜¯å°†æ¨¡å‹è¾“å‡ºçš„ token embedding è½¬æ¢ä¸º logits çš„é‚£ä¸€å±‚)å»æ‰ï¼Œå³æœ€åä¸€å±‚ä¸ç”¨ softmaxï¼Œæ”¹æˆä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¿™æ ·è®­ç»ƒå¥½çš„ RM æ¨¡å‹å°±å¯ä»¥åšåˆ°è¾“å…¥é—®é¢˜+ç­”æ¡ˆï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡çš„åˆ†æ•°ã€‚
      - RM æ¨¡å‹ä½¿ç”¨ 6Bï¼Œè€Œä¸æ˜¯ 175Bï¼Œä¸»è¦åŸå› æ˜¯:
        - èŠ‚çœè®¡ç®—ï¼Œæ›´ä¾¿å®œï¼›
        - å¤§æ¨¡å‹ 175B-RM ä¸ç¨³å®š(å¤§æ¨¡å‹çš„é€šç—…ï¼Œæ¨¡å‹å‚æ•°å¾ˆå¤šï¼Œå¾ˆéš¾æ”¶æ•›)ï¼Œå› æ­¤ä¸å¤ªé€‚åˆåœ¨ RL æœŸé—´ç”¨ä½œå€¼å‡½æ•°ã€‚
      - RM æ•°æ®é›†åœ¨æ ‡æ³¨é˜¶æ®µï¼Œæ ‡æ³¨äººå‘˜è¢«è¦æ±‚å¯¹æ¯ä¸€ä¸ª prompt ä¸‹çš„ä¸åŒå›ç­”è¿›è¡Œæ’åºã€‚

        - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/4bf95ac06149479b8bb5189c7f38cc48.jpeg#pic_center)
        - å¦‚å›¾
          - æŸä¸ª prompt ä¸‹æœ‰`A, B, Cä¸‰ä¸ªå›ç­”`ï¼Œæ ‡æ³¨äººå‘˜è®¤ä¸º`A>B>C`ã€‚
          - åœ¨è®­ç»ƒé˜¶æ®µï¼Œå‡è®¾ä¸€ä¸ª prompt ä¸‹æœ‰ $K$ ä¸ªå›ç­”ï¼Œåˆ™ä¸¤ä¸¤å›ç­”ä¸€ç»„ï¼Œç»„æˆä¸€æ¡è®­ç»ƒæ•°æ®, ä¾‹å¦‚(prompt, A, B)ï¼Œä¸€å…±æœ‰ $C_{k}^{2}$ â€‹ æ¡è®­ç»ƒæ•°æ®ã€‚
          - è¿™äº›è®­ç»ƒæ•°æ®å°†ç»„æˆä¸€ä¸ª**batch**ï¼Œé€šè¿‡æ„é€ å¹¶æœ€å°åŒ– **Pairwise Ranking Loss**çš„æ–¹æ³•ï¼Œæ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œ

      - æ•´ä½“è¿‡ç¨‹å¦‚ä¸‹:

        - å…ˆä»¥ RM Dataset ä¸­çš„`æŒ‡ä»¤prompt`ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ç¬¬ä¸€é˜¶æ®µå¾®è°ƒå¥½çš„ SFT æ¨¡å‹ï¼Œç”Ÿæˆ $K$ ä¸ªä¸åŒçš„å›ç­”ï¼Œå½¢æˆ $<prompt,answer1>,<prompt,answer2>â€¦.<prompt,answerK>$ æ•°æ®ã€‚
        - ç„¶åï¼Œæ ‡æ³¨äººå‘˜æ ¹æ®ç›¸å…³æ€§, ä¿¡æ¯æ€§å’Œæœ‰å®³ä¿¡æ¯ç­‰æ ‡å‡†ï¼Œå¯¹ $K$ ä¸ªç»“æœè¿›è¡Œæ’åºï¼Œç”Ÿæˆæ’åºç»“æœæ•°æ®ã€‚
        - æ¥ä¸‹æ¥ï¼Œä½¿ç”¨è¿™ä¸ªæ’åºç»“æœæ•°æ®è¿›è¡Œ **pair-wise learning to rank** æ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒ RM æ¨¡å‹ã€‚
        - RM æ¨¡å‹æ¥å—ä¸€ä¸ªè¾“å…¥`<prompt,answer>`ï¼Œç»™å‡ºè¯„ä»·å›ç­”è´¨é‡é«˜ä½çš„å¥–åŠ±åˆ†æ•° scoreã€‚
        - å¯¹äºä¸€å¯¹è®­ç»ƒæ•°æ®`<answer1,answer2>`ï¼Œå‡è®¾äººå·¥æ’åºä¸­ answer1 æ’åœ¨ answer2 å‰é¢ï¼Œé‚£ä¹ˆ loss å‡½æ•°åˆ™é¼“åŠ± RM æ¨¡å‹å¯¹`<prompt,answer1>`çš„æ‰“åˆ†è¦æ¯”`<prompt,answer2>`çš„æ‰“åˆ†è¦é«˜ã€‚

      - RM æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

        - æ¨¡å‹çš„æŸå¤±å‡½æ•°**Pairwise Ranking Loss**è¡¨è¾¾å¼: $loss(\theta)=-\frac{1}{C_{k}^{2}}E_{(x,y_{w},y_{l})\sim D}[log(\sigma(r_{\theta}(x,y_{w})-r_{\theta}(x,y_{l})))]$
        - å…¶ä¸­

          - $x$ è¡¨ç¤ºæŸä¸ª promptï¼›
          - $y_{w}$ â€‹ å’Œ $y_{l}$ â€‹ åˆ†åˆ«è¡¨ç¤ºè¯¥ prompt ä¸‹çš„ä»»æ„ä¸€å¯¹å›ç­”ï¼Œå¹¶ä¸”å‡è®¾æ ‡æ³¨ä¸­ $y_{w}$ çš„æ’åºæ˜¯é«˜äº $y_{l}$ â€‹ çš„ï¼›
          - $D$ è¡¨ç¤ºè¯¥ prompt ä¸‹äººç±»æ ‡æ³¨æ’åºçš„æ‰€æœ‰ä¸¤ä¸¤å›ç­”ç»„åˆï¼›
          - $r_{\theta}$ â€‹ è¡¨ç¤ºå¥–åŠ±æ¨¡å‹ï¼›
          - $\sigma$ è¡¨ç¤º $sigmoid$ å‡½æ•°ã€‚

        - ä¹Ÿå¯ä»¥å‚è€ƒä¸‹å›¾(æœ‰ä¸ªå°é”™è¯¯ï¼Œå°±æ˜¯ $sigmoid$ å‡½æ•°æ˜¯å°†å€¼æ˜ å°„è‡³ $(0ï¼Œ1)$ è€Œä¸æ˜¯ $(-1ï¼Œ1)$ ä¸è¿‡æ— ä¼¤å¤§é›…)ã€‚

          - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b35eac803dbf4336b3a40eadd7881bee.png#pic_center)
          - è®ºæ–‡ä¸­æœŸæœ›å½“å›ç­” $y$ çš„æ’åºç›¸å¯¹è¾ƒé«˜æ—¶ï¼Œ $r_{\theta}(x,y)$ çš„å¾—åˆ†ä¹Ÿèƒ½è¶Šé«˜ã€‚
          - ä¸ºäº†ä¸è®© $K$ çš„ä¸ªæ•°å½±å“è®­ç»ƒæ¨¡å‹ï¼Œè®ºæ–‡ä¸­åœ¨å‰é¢ä¹˜ä¸Š $\frac{1}{C_{k}^{2}}$â€‹ï¼Œå°† loss å¹³å‡åˆ°æ¯ä¸€ä¸ªç­”æ¡ˆç»„åˆä¸Šã€‚

        - é™¤æ­¤ä¹‹å‰ï¼Œè¿˜æœ‰å‡ ç‚¹éœ€è¦æ³¨æ„:

          - **K å€¼çš„é€‰æ‹©**:

            - è®ºæ–‡ä¸­é‡‡ç”¨äº† K=9ï¼Œè€Œä¸æ˜¯æ›´å°çš„å€¼ï¼Œæ¯”å¦‚ 4ã€‚åŸå› åœ¨äº:
            - è¿›è¡Œæ ‡æ³¨çš„æ—¶å€™ï¼Œéœ€è¦èŠ±å¾ˆå¤šæ—¶é—´å»ç†è§£é—®é¢˜ï¼Œä½†ç­”æ¡ˆå’Œç­”æ¡ˆæ¯”è¾ƒç›¸è¿‘ï¼Œå¯¹ 9 ä¸ªç­”æ¡ˆåšæ’åºç›¸è¾ƒäºå¯¹ 4 ä¸ªç­”æ¡ˆåšæ’åºå¤šèŠ±çš„æ—¶é—´ä¸åˆ°ä¸€å€ã€‚
            - åŒæ—¶ K=9 ç”Ÿæˆçš„é—®ç­”å¯¹æ˜¯ K=4 çš„ 6 å€ ${C_{9}^{2}}=36ï¼Œ {C_{4}^{2}}=6)$ éå¸¸åˆ’ç®—ï¼›
            - K=9 æ—¶ï¼Œæ¯æ¬¡è®¡ç®— RM æ¨¡å‹çš„ loss æ—¶éœ€è¦éƒ½æœ‰ 36 é¡¹ $r_{\theta}(x,y)$ è¦è®¡ç®—ï¼Œè¿™ä¸ªè®¡ç®—æ¯”è¾ƒè´µï¼Œä½†å¯ä»¥é€šè¿‡é‡å¤åˆ©ç”¨ä¹‹å‰ç®—è¿‡çš„å€¼ï¼Œä½¿å¾—åªè¦è®¡ç®— 9 æ¬¡å°±è¡Œï¼Œä¹Ÿå°±æ˜¯è¯´å°† 9 ä¸ªç­”æ¡ˆå¯¹åº”çš„ $r_{\theta}(x,y)$è®¡ç®—å‡ºæ¥ä¹‹åï¼Œåé¢è®¡ç®—æŸå¤±æ—¶ï¼Œä¸¤ä¸¤ç»„åˆå°±å¯ä»¥äº†ï¼Œè¿™æ ·å°±å¯ä»¥çœä¸‹å¾ˆå¤šæ—¶é—´ã€‚

          - **è®­ç»ƒæ•°æ®è¾“å…¥æ¨¡å¼é€‰æ‹©**:

            - è®ºæ–‡ä¸­å°† $(x,y_{w},y_{l})\sim D$ å½“æˆä¸€ä¸ª batch åŒæ—¶é€å…¥æ¨¡å‹ï¼Œè€Œä¸æ˜¯å°†å•æ¡ $(x,y_{w},y_{l})$ æ•°æ®åˆ†åˆ«é€å…¥æ¨¡å‹ï¼ŒåŸå› åœ¨äº:
            - ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆã€‚å¯¹äºæŸä¸€å¯¹ $(x,y_{w},y_{l})$ ä¸­çš„ä¸€ä¸ªæ ·æœ¬ $(x,y)$ ï¼Œç”¨ batch æ–¹å¼æ—¶ï¼Œå®ƒåªå‚ä¸ä¸€æ¬¡æ¢¯åº¦è®¡ç®—ï¼›ç”¨å•æ¡æ–¹å¼æ—¶ï¼Œå®ƒéœ€è¦å‚ä¸ K-1 æ¬¡æ¢¯åº¦è®¡ç®—ã€‚æ¨¡å‹è¶…è¿‡ä¸€ä¸ª epoch åä¼šè¿‡æ‹Ÿåˆï¼Œåœ¨ä¸€ä¸ª epoch ä¸­åå¤ä½¿ç”¨æ•°æ®æ›´ä¼šè¿‡æ‹Ÿåˆäº†ï¼›
            - ä¸ºäº†æå‡è®¡ç®—æ•ˆç‡ã€‚åœ¨æ¨¡å‹ forward çš„è¿‡ç¨‹ä¸­ï¼Œæœ€è€—æ—¶çš„æ­¥éª¤æ˜¯è®¡ç®— $r_{\theta}(x,y)$ã€‚ç”¨ batch æ–¹å¼æ—¶ï¼Œè¯¥è®¡ç®—åªéœ€æ‰§è¡Œ K æ¬¡(å› ä¸ºæ¨¡å‹å‚æ•°æ²¡æœ‰æ›´æ–°ï¼Œç›¸åŒçš„(x, y)å¯ä»¥é‡å¤ä½¿ç”¨)ï¼›é‡‡ç”¨å•æ¡æ–¹å¼æ—¶ï¼Œéœ€è¦è®¡ç®— K(K-1)æ¬¡(å› ä¸ºä¸€æ¡è®¡ç®—æ›´æ–°ä¸€æ¬¡æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°ï¼Œç›¸åŒçš„(x,y)éœ€è¦é‡æ–°è®¡ç®— $r_{\theta}(x,y)$)ã€‚
            - å› æ­¤ï¼ŒK è¶Šå¤§æ—¶ï¼Œé‡‡ç”¨ batch çš„æ–¹å¼è¶Šåˆ’ç®—ï¼Œå®ƒåœ¨ä¿è¯ç›¸å¯¹æ’åºä¿¡æ¯ä¸°å¯Œçš„åŒæ—¶ï¼ŒåˆèŠ‚çœäº†è®¡ç®—æ•ˆç‡ã€‚

          - **è®­ç»ƒ epoch é€‰æ‹©**:
            - æ¨¡å‹è®­ç»ƒè¶…è¿‡ä¸€ä¸ª epoch åä¼šè¿‡æ‹Ÿåˆï¼Œæ•…åªè®­ç»ƒä¸€ä¸ª epochã€‚

    - **å¼ºåŒ–å­¦ä¹ æ¨¡å‹(RL)**:

      - è¿™ä¸ªé˜¶æ®µå…ˆå°† RL æ¨¡å‹çš„æƒé‡åˆå§‹åŒ–ä¸º SFT æ¨¡å‹çš„æƒé‡ï¼Œç„¶åé€šè¿‡æ”¹è‰¯åçš„ PPO ç®—æ³•(PPO-ptx ç®—æ³•)ç»§ç»­å¯¹ RL æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå¾—åˆ° InstructGPTã€‚
      - å¤§è‡´æµç¨‹å¯ä»¥æ€»ç»“ä¸º: æ¨¡å‹åœ¨åšå‡ºè¡ŒåŠ¨åï¼Œéœ€è¦äººæ¥å¯¹æ¨¡å‹è¿›è¡Œåé¦ˆï¼Œç„¶åæ¨¡å‹åšå‡ºå¯¹åº”çš„æ›´æ–°ã€‚
      - å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­è®­ç»ƒ RM å°±æ˜¯ä¸ºäº†å­¦ä¹ äººæ¥å¯¹æ¨¡å‹è¿›è¡Œåé¦ˆï¼ŒSFT æ¨¡å‹åœ¨æ‹¿åˆ° prompt å¹¶ç”Ÿæˆå¯¹åº”çš„ç­”æ¡ˆåï¼Œç”± RM è¿›è¡Œæ‰“åˆ†ï¼Œå†æ ¹æ®è¿™ä¸ªæ‰“åˆ†å»æ›´æ–°æ¨¡å‹ï¼Œç„¶åç”¨æ›´æ–°çš„æ¨¡å‹ç”Ÿæˆæ–°çš„ç­”æ¡ˆï¼Œå¹¶è¿›è¡Œä¸‹ä¸€æ­¥å­¦ä¹ ï¼Œè¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ã€‚
      - å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å‡½æ•° $objective(\phi)$ å¦‚ä¸‹æ‰€ç¤ºï¼ŒRL æ¨¡å‹æœ€ç»ˆçš„è®­ç»ƒç›®æ ‡æ˜¯è®© $objective(\phi)$ è¶Šå¤§è¶Šå¥½ã€‚

      $$
      \begin{aligned}
      objective(\phi)= &E_{(x,y)\sim D_{\pi_{\phi}^{RL}}}[r_{\theta}(x,y)-\beta log(\pi_{\phi}^{RL}(y|x)/\pi^{SFT}(y|x))] \\ + \\ &\gamma E_{x\sim D_{pretrain}}[log(\pi_{\phi}^{RL}(x))]
      \end{aligned}
      $$

      - å…¶ä¸­:
        - $\pi^{SFT}$ : å³ç¬¬ä¸€é˜¶æ®µï¼Œç»è¿‡ supervised fine-tuning çš„ GPT-3 æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯ SFT æ¨¡å‹ï¼›
        - $\pi_{\phi}^{RL}$ â€‹: å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¨¡å‹ç§°åš policyï¼Œ $\pi_{\phi}^{RL}$ â€‹ å°±æ˜¯éœ€è¦å­¦ä¹ çš„æ¨¡å‹ï¼Œå³æœ€ç»ˆçš„æ¨¡å‹ã€‚
      - åˆå§‹æ—¶:
        - $\pi_{\phi}^{RL}$ â€‹\= $\pi^{SFT}$ ï¼›
        - $r_{\theta}$ â€‹: å³ç¬¬äºŒé˜¶æ®µè®­ç»ƒçš„ RM æ¨¡å‹ã€‚
      - æ•´ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸Šè¿°çš„ç›®æ ‡å‡½æ•°:

        - $(x,y)\sim D_{\pi_{\phi}^{RL}}$ â€‹â€‹:
          - $x$ æ˜¯ç¬¬ RL Dataset æ•°æ®é›†ä¸­çš„é—®é¢˜(`æŒ‡ä»¤prompt`)
          - $y$ æ˜¯ $x$ é€šè¿‡ $\pi_{\phi}^{RL}$ â€‹ æ¨¡å‹å¾—åˆ°çš„ç­”æ¡ˆ;
        - $r_{\theta}(x,y)$: å¯¹é—®é¢˜ $x$ + ç­”æ¡ˆ $y$ï¼Œè¾“å…¥ RM æ¨¡å‹è¿›è¡Œæ‰“åˆ†ï¼Œç›®æ ‡æ˜¯å¸Œæœ›è¿™ä¸ªåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼›
        - $\pi_{\phi}^{RL}(y|x)$ :
          - é—®é¢˜ $x$ é€šè¿‡ $\pi_{\phi}^{RL}$ â€‹ å¾—åˆ°ç­”æ¡ˆ $y$çš„æ¦‚ç‡ï¼Œ
          - å…·ä½“æ¥è¯´ $\pi(y|x)$ æ˜¯æŠŠæ¨¡å‹è¾“å‡º $y$çš„æ¯ä¸€ä¸ª token å¯¹åº”çš„ softmax æ¦‚ç‡ç›¸ä¹˜å¾—åˆ°çš„ç»“æœï¼Œä¸‹åŒï¼›
        - $\pi^{SFT}(y|x)$ :
          - é—®é¢˜ $x$ é€šè¿‡ $\pi^{SFT}$ å¾—åˆ°ç­”æ¡ˆ $y$çš„æ¦‚ç‡ï¼›
        - $log(\pi_{\phi}^{RL}(y|x)/\pi^{SFT}(y|x))$ :

          - KL æ•£åº¦ï¼Œå–å€¼èŒƒå›´>=0ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ˜¯å¦ç›¸ä¼¼
          - KL å€¼è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šä¸ç›¸ä¼¼ï¼Œåˆ†å¸ƒç›¸åŒæ—¶ KL=0ã€‚

          - åœ¨æœ¬é˜¶æ®µï¼Œè®ºæ–‡å¸Œæœ›å¼ºåŒ–å­¦ä¹ åå¾—åˆ°çš„æ¨¡å‹ï¼Œåœ¨èƒ½å¤Ÿç†è§£äººç±»æ„å›¾çš„åŸºç¡€ä¸Šï¼Œåˆä¸è¦å’Œæœ€åŸå§‹çš„æ¨¡å‹è¾“å‡ºç›¸å·®å¤ªè¿œ
            - åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åï¼Œ $\pi_{\phi}^{RL}$ â€‹ ä¼šå‘ç”Ÿå˜åŒ–ï¼Œ
            - $x$ é€šè¿‡ $\pi_{\phi}^{RL}$ â€‹ ç”Ÿæˆçš„ $y$ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œ
            - è€Œ $r_{\theta}$ â€‹ æ‰“åˆ†æ¨¡å‹æ˜¯æ ¹æ® $\pi^{SFT}$ æ¨¡å‹çš„æ•°æ®è®­ç»ƒè€Œæ¥ï¼Œå¦‚æœ $\pi_{\phi}^{RL}$ â€‹ å’Œ $\pi^{SFT}$ å·®çš„å¤ªå¤šï¼Œåˆ™ä¼šå¯¼è‡´ $r_{\theta}$ â€‹ çš„åˆ†æ•°è¾“å‡ºä¸å‡†ç¡®ã€‚
            - å› æ­¤éœ€è¦é€šè¿‡ KL æ•£åº¦æ¥è®¡ç®— $\pi_{\phi}^{RL}$ â€‹ ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒå’Œ $\pi^{SFT}$ ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¸è¦å·®çš„å¤ªè¿œã€‚)ã€‚
            - å‚æ•° $\beta$ åˆ™è¡¨ç¤ºå¯¹è¿™ç§åå·®çš„å®¹å¿ç¨‹åº¦ã€‚åç¦»è¶Šè¿œï¼Œå°±è¦ä»å¥–åŠ±æ¨¡å‹çš„åŸºç¡€ä¸Šå¾—åˆ°è¶Šå¤šçš„æƒ©ç½šï¼›

        - $x\sim D_{pretrain}$ â€‹: $x$ æ˜¯æ¥è‡ª GPT-3 é¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®ï¼›
        - $log(\pi_{\phi}^{RL}(x))$ : è¡¨ç¤ºå°†æ¥è‡ªåˆå§‹ GPT-3 ä¸­çš„æ•°æ®é€å…¥å½“å‰å¼ºåŒ–æ¨¡å‹ä¸‹ï¼ŒåŒæ ·ï¼Œè®ºæ–‡ä¸­å¸Œæœ›åœ¨è®­ç»ƒå¾—åˆ°æ–°æ¨¡å‹ä¹‹åï¼Œä¸èƒ½é™ä½åœ¨åŸå§‹ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå³ä¸èƒ½å¤ªåç¦»åŸå§‹ä»»åŠ¡ï¼Œä¿è¯æ–°æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚
        - $\gamma$: åˆ™æ˜¯å¯¹è¿™ç§åç¦»çš„æƒ©ç½šç¨‹åº¦ã€‚

        - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/ec1c753c065148ecbe75efbbf240120f.png#pic_center)

      - æœ€åå†ç»™å‡ºå¯¹ç›®æ ‡å‡½æ•°çš„ç†è§£

        - ä¼˜åŒ–ç›®æ ‡æ˜¯ä½¿å¾—ä¸Šè¿°ç›®æ ‡å‡½æ•°è¶Šå¤§è¶Šå¥½ï¼Œ
        - $objective(\phi)$ å¯åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼Œ`RMæ‰“åˆ†éƒ¨åˆ†` + `KLæ•£åº¦éƒ¨åˆ†` + `GPT-3é¢„è®­ç»ƒéƒ¨åˆ†`:

        - å°† RL Dataset æ•°æ®é›†ä¸­çš„é—®é¢˜ $x$ ï¼Œé€šè¿‡ $\pi_{\phi}^{RL}$ â€‹ æ¨¡å‹å¾—åˆ°ç­”æ¡ˆ $y$ï¼›
        - æŠŠä¸€å¯¹ $(x,y)$ é€è¿› RM æ¨¡å‹è¿›è¡Œæ‰“åˆ†ï¼Œå¾—åˆ° $r_{\theta}(x,y)$ï¼Œå³ç¬¬ä¸€éƒ¨åˆ†æ‰“åˆ†éƒ¨åˆ†ï¼Œè¿™ä¸ªåˆ†æ•°è¶Šé«˜å°±ä»£è¡¨æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆè¶Šå¥½ï¼›
        - åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åï¼Œ $\pi_{\phi}^{RL}$ â€‹ ä¼šå‘ç”Ÿå˜åŒ–ï¼Œ $x$ é€šè¿‡ $\pi_{\phi}^{RL}$ â€‹ ç”Ÿæˆçš„ $y$ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–
        - $r_{\theta}(x,y)$ æ‰“åˆ†æ¨¡å‹æ˜¯æ ¹æ® $\pi^{SFT}$ æ¨¡å‹çš„æ•°æ®è®­ç»ƒè€Œæ¥ï¼Œå¦‚æœ $\pi_{\phi}^{RL}$ â€‹ å’Œ $\pi^{SFT}$ å·®çš„å¤ªå¤šï¼Œåˆ™ä¼šå¯¼è‡´ $r_{\theta}(x,y)$çš„åˆ†æ•°ä¼°ç®—ä¸å‡†ç¡®ã€‚
        - å› æ­¤éœ€è¦é€šè¿‡ KL æ•£åº¦æ¥è®¡ç®— $\pi_{\phi}^{RL}$ â€‹ ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒå’Œ $\pi^{SFT}$ ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¸è¦å·®çš„å¤ªè¿œã€‚
          - æˆ‘ä»¬å¸Œæœ›ä¸¤ä¸ªæ¨¡å‹çš„å·®è·è¶Šå°è¶Šå¥½ï¼Œå³ KL æ•£åº¦è¶Šå°è¶Šå¥½ï¼Œå‰é¢éœ€è¦åŠ ä¸€ä¸ªè´Ÿå·ï¼Œä½¿å¾— $objective(\phi)$ è¶Šå¤§è¶Šå¥½ã€‚è¿™ä¸ªå°±æ˜¯ KL æ•£åº¦éƒ¨åˆ†ï¼›
        - å¦‚æœæ²¡æœ‰ç¬¬ä¸‰éƒ¨åˆ†ï¼Œé‚£ä¹ˆæ¨¡å‹æœ€ç»ˆå¯èƒ½åªå¯¹è¿™ä¸€ä¸ªä»»åŠ¡èƒ½å¤Ÿåšå¥½ï¼Œåœ¨åˆ«çš„ä»»åŠ¡ä¸Šä¼šå‘ç”Ÿæ€§èƒ½ä¸‹é™ã€‚æ‰€ä»¥ç¬¬ä¸‰éƒ¨åˆ†å°±æŠŠåŸå§‹çš„`GPT-3ç›®æ ‡å‡½æ•°`åŠ äº†ä¸Šå»ï¼Œä½¿å¾—å‰é¢ä¸¤ä¸ªéƒ¨åˆ†åœ¨æ–°çš„æ•°æ®é›†ä¸Šåšæ‹Ÿåˆï¼ŒåŒæ—¶ä¿è¯åŸå§‹çš„æ•°æ®ä¹Ÿä¸è¦ä¸¢ï¼Œè¿™ä¸ªå°±æ˜¯ç¬¬ä¸‰éƒ¨åˆ† GPT-3 é¢„è®­ç»ƒéƒ¨åˆ†ï¼›
        - å½“ $\gamma =0$ æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åš**PPO**ï¼Œ
        - å½“ $\gamma$ ä¸ä¸º 0 æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åš**PPO-ptx**ã€‚
        - InstructGPT æ›´åå‘äºä½¿ç”¨ PPO-ptxï¼›
        - æœ€ç»ˆä¼˜åŒ–åçš„ $\pi_{\phi}^{RL}$ â€‹ æ¨¡å‹å°±æ˜¯ InstructGPT çš„æ¨¡å‹ã€‚

InstructGPT çš„è®­ç»ƒæµç¨‹ï¼Œå…±åŒ…å«ä¸¤æ¬¡å¯¹æ¨¡å‹çš„å¾®è°ƒ:

- GPT-3 æ¨¡å‹ $\Rightarrow$ SFT æ¨¡å‹ $\Rightarrow$ RL æ¨¡å‹
- å…¶å®è¿™é‡Œå§‹ç»ˆéƒ½æ˜¯åŒä¸€ä¸ªæ¨¡å‹ï¼Œåªæ˜¯ä¸åŒè¿‡ç¨‹ä¸­åç§°ä¸ä¸€æ ·ã€‚
- é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨ SFT æ¨¡å‹ $\Rightarrow$ RL æ¨¡å‹é˜¶æ®µï¼Œè¿˜ä¼šä¾èµ–äºå¦ä¸€ä¸ªåœ¨ SFT æ¨¡å‹åŸºç¡€ä¸Šè®­ç»ƒçš„ RM æ¨¡å‹ã€‚

InstructGPT è®­ç»ƒ SFT, RM, RL ä¸‰ä¸ªæ¨¡å‹çš„åŸå› ä¸º:

- **éœ€è¦ SFT æ¨¡å‹çš„åŸå› **:
  - GPT-3 æ¨¡å‹ä¸ä¸€å®šèƒ½å¤Ÿä¿è¯æ ¹æ®äººçš„æŒ‡ç¤º, æœ‰å¸®åŠ©çš„, å®‰å…¨çš„ç”Ÿæˆç­”æ¡ˆï¼Œéœ€è¦äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼›
- **éœ€è¦ RM æ¨¡å‹çš„åŸå› **:
  - æ ‡æ³¨æ’åºçš„åˆ¤åˆ«å¼æ ‡æ³¨ï¼Œæˆæœ¬è¿œè¿œä½äºç”Ÿæˆç­”æ¡ˆçš„ç”Ÿæˆå¼æ ‡æ³¨ï¼›
- **éœ€è¦ RL æ¨¡å‹çš„åŸå› **:
  - è®©æ¨¡å‹å€ŸåŠ©å¼ºåŒ–å­¦ä¹ çš„èƒ½åŠ›ï¼Œæ›´å¥½çš„ç†è§£äººç±»çš„æ„å›¾ã€‚

InstructGPT æ€§èƒ½å¯¹æ¯”ç»“æœ

- å‚æ•°é‡ä¸º 13B çš„ InstructGPT æ¨¡å‹ï¼Œæ€§èƒ½éƒ½è¦è¿œè¿œå¥½äºå‚æ•°é‡ä¸º 175B çš„ GPT-3 æ¨¡å‹
- ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b1e168ac492f4c478d978d06548a1018.png#pic_center)

---

#### 1.3 ChatGPT

- **InstructGPT**æ˜¯åœ¨ GPT-3 çš„åŸºç¡€ä¸Šé€šè¿‡`SFT+RLHF`ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå®Œæˆï¼›
- **ChatGPT**åˆ™æ˜¯åœ¨ GPT-3.5 çš„åŸºç¡€ä¸Šé€šè¿‡`SFT+RLHF`ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå®Œæˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ã€‚

GPT-3 å’Œ GPT-3.5 æ˜¯ä¸¤ä¸ªæ¨¡å‹ç³»åˆ—

- åˆ†åˆ«ç§°ä¸º GPT-3 ç³»åˆ—å’Œ GPT-3.5 ç³»åˆ—
- å‚è€ƒç»¼è¿°[æ‹†è§£è¿½æº¯ GPT-3.5 å„é¡¹èƒ½åŠ›çš„èµ·æº](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)
- OpenAI å›¢é˜Ÿæ‰€æ„å»ºçš„ GPT-3 ç³»åˆ—å’Œ GPT-3.5 ç³»åˆ—æ˜¯å¦‚ä½•è¿›åŒ–çš„:

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/44f3c3bb1edb409682f8b2103ebd4ec6.jpeg#pic_center)

---

#### 1.4 GPT-4

- 2022 å¹´ 3 æœˆï¼ŒOpenAI å›¢é˜Ÿå‘å¸ƒäº†æ›´å¼ºçš„ LLM: GPT-4ã€‚
- è™½ç„¶æ— ä»å¾—çŸ¥ GPT-4 çš„è®­ç»ƒç»†èŠ‚ï¼Œä½†æ˜¯å¯ä»¥è‚¯å®šçš„æ˜¯ï¼ŒGPT-4 é‡‡ç”¨äº†æ›´å¤§çš„æ¨¡å‹ç»“æ„ï¼Œå¢åŠ äº†æ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚
- æˆ‘ä»¬å¯ä»¥é€šè¿‡å®˜æ–¹åšå®¢[GPT-4](https://openai.com/research/gpt-4)äº†è§£ä¸‹ GPT-4 çš„å¼ºå¤§èƒ½åŠ›ã€‚
- ç›®å‰ GPT-4 çš„ä¸»è¦èƒ½åŠ›ç‚¹å¦‚ä¸‹:

  - GPT-4 æ˜¯å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå¯æ”¯æŒå›¾ç‰‡æˆ–æ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºæ˜¯æ–‡æœ¬ï¼›
  - GPT-4 çš„è¾“å…¥å¯æ¥å— 8192 ä¸ª tokenã€‚å¦å­˜åœ¨å˜ä½“æ¨¡å‹ï¼Œå¯æ¥å—è¾“å…¥ 32768 ä¸ª tokenï¼›
  - GPT-4 ç›¸è¾ƒäº ChatGPTï¼Œå…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œé™¤äº†èŠå¤©æœºå™¨äººä¹‹å¤–ï¼Œè¿˜åŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆ, æ‘˜è¦, ç¿»è¯‘, é—®ç­”ç³»ç»Ÿç­‰å¤šä¸ªé¢†åŸŸã€‚è€Œ ChatGPT ä¸»è¦é’ˆå¯¹èŠå¤©æœºå™¨äººé¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ—¥å¸¸å¯¹è¯, é—®é¢˜è§£ç­”, ä¿¡æ¯æŸ¥è¯¢ç­‰æœåŠ¡ã€‚

---

### 2 å…¶ä»–å¤§æ¨¡å‹

| Model Name  | Team     | Publish Time | Size | OpenSource                   | Hugging Face                                                   | Github                                                                                                                        |
| ----------- | -------- | ------------ | ---- | ---------------------------- | -------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| ChatGLM-6B  | æ¸…åå¤§å­¦ | 2023         | 6B   | å·²å¼€æºï¼Œä¸å¯å•†ç”¨(è·å–è®¸å¯è¯) | [ChatGLM-6B](https://www.huggingface.co/THUDM/chatglm-6b)      | [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)                                                                             |
| ChatGLM2-6B | æ¸…åå¤§å­¦ | 2023         | 6B   | å·²å¼€æºï¼Œä¸å¯å•†ç”¨(è·å–è®¸å¯è¯) | [ChatGLM2-6B](https://www.huggingface.co/THUDM/chatglm2-6b)    | [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)                                                                           |
| LLaMA2-7B   | Meta     | 2023         | 7B   | å·²å¼€æºï¼Œå¯å•†ç”¨               | [LLaMA2-7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)   | [LLaMA](https://github.com/facebookresearch/llama), [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) |
| baichuan-7B | ç™¾å·æ™ºèƒ½ | 2023         | 7B   | å·²å¼€æºï¼Œå¯å•†ç”¨               | [baichuan-7B](https://huggingface.co/baichuan-inc/baichuan-7B) | [baichuan-7B](https://github.com/baichuan-inc/baichuan-7B)                                                                    |
| æ–‡å¿ƒä¸€è¨€    | ç™¾åº¦     | 2023         | åƒäº¿ | æœªå¼€æºï¼Œä¸å¯å•†ç”¨             | æš‚æ—                                                            | æš‚æ—                                                                                                                           |

---

#### 2.1 ChatGLM

- `ChatGLM`æ˜¯ä¸€ä¸ªåŸºäºåƒäº¿åŸºåº§æ¨¡å‹ GLM-130B å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰é—®ç­”, å¤šè½®å¯¹è¯å’Œä»£ç ç”ŸæˆåŠŸèƒ½ã€‚
- ç›®å‰ï¼ŒChatGLM æœ‰ä¸¤ä¸ªç‰ˆæœ¬:
  - åƒäº¿å‚æ•°çš„ ChatGLM-130B(å†…æµ‹ç‰ˆ)
  - 62 äº¿å‚æ•°çš„ ChatGLM-6B(å¼€æºç‰ˆï¼Œå®˜æ–¹ Github æ˜¯[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B))ã€‚
- ChatGLM-6B æ˜¯åœ¨ 2023 å¹´ 3 æœˆ 14 æ—¥æ­£å¼å¼€æºçš„ï¼Œç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²(INT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜)ã€‚
- ChatGLM çš„æŠ€æœ¯åŸºç¡€æ˜¯ GLM-130Bï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç›®æ ‡å‡½æ•°çš„è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–åƒäº¿è§„æ¨¡çš„æ¨¡å‹ã€‚
- ChatGLM çš„æ€§èƒ½è¡¨ç°ä¹Ÿååˆ†å‡ºè‰²ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒ(SFT), åé¦ˆè‡ªåŠ©(RW), äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)ç­‰æŠ€æœ¯ï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚è€Œåƒäº¿å‚æ•°çš„ ChatGLM åˆ™æ›´è¿›ä¸€æ­¥ï¼Œåœ¨é—®ç­”å’Œå¯¹è¯æ–¹é¢å…·æœ‰æ›´å¼ºå¤§çš„èƒ½åŠ›ã€‚
- 2023 å¹´ 6 æœˆ 25 æ—¥ï¼Œæ¸…åå¤§å­¦å‘å¸ƒäº† ChatGLM2-6Bï¼ŒChatGLM-6B çš„å‡çº§ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…, éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§:

- **æ›´å¼ºå¤§çš„æ€§èƒ½**:
  - åŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œå…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚
  - ChatGLM2-6B ä½¿ç”¨äº† GLM çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLU(+23%), CEval(+33%), GSM8K(+571%) , BBH(+60%)ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ï¼›
- **æ›´é•¿çš„ä¸Šä¸‹æ–‡**:
  - åŸºäº FlashAttention æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦(Context Length)ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒï¼Œå…è®¸æ›´å¤šè½®æ¬¡çš„å¯¹è¯ã€‚
  - ä½†å½“å‰ç‰ˆæœ¬çš„ ChatGLM2-6B å¯¹å•è½®è¶…é•¿æ–‡æ¡£çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­è¿­ä»£å‡çº§ä¸­ç€é‡è¿›è¡Œä¼˜åŒ–ï¼›
- **æ›´é«˜æ•ˆçš„æ¨ç†**:

  - åŸºäº Multi Query Attention æŠ€æœ¯ï¼ŒChatGLM2-6B æœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨: åœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚

- æœ‰å…³ ChatGLM2-6B æ›´å¤šçš„ç»†èŠ‚ï¼Œå®˜æ–¹ Githubï¼Œ[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)ã€‚

åŸå§‹ GLM çš„æ¨¡å‹ç»“æ„

- GLM(General Language Model)æ˜¯æ¸…åå¤§å­¦åœ¨ 2022 å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ä¸­[ã€ŠGLM: General Language Model Pretraining with Autoregressive Blank Infillingã€‹](https://aclanthology.org/2022.acl-long.26.pdf)æå‡ºçš„æ¨¡å‹ã€‚

- GLM æ¨¡å‹è¢«æå‡ºä¹‹å‰ï¼ŒNLP é¢†åŸŸä¸»æµçš„é¢„è®­ç»ƒæ¡†æ¶å¯ä»¥åˆ†ä¸ºä¸‰ç§:

  - **auto-regressive è‡ªå›å½’æ¨¡å‹(AR æ¨¡å‹)**:

    - ä»£è¡¨æ˜¯ GPTï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä»å·¦åˆ°å³çš„è¯­è¨€æ¨¡å‹ï¼Œå¸¸ç”¨äºæ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡(unconditional generation)ï¼Œåœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œæ¯”å¦‚è‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG)é¢†åŸŸçš„ä»»åŠ¡ã€‚
    - å½“æ‰©å±•åˆ°åäº¿çº§åˆ«å‚æ•°æ—¶ï¼Œè¡¨ç°å‡ºäº†å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚
    - ç¼ºç‚¹æ˜¯å•å‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ NLU ä»»åŠ¡ä¸­ï¼Œæ— æ³•å®Œå…¨æ•æ‰ä¸Šä¸‹æ–‡çš„ä¾èµ–å…³ç³»ï¼›

  - **auto-encoding è‡ªç¼–ç æ¨¡å‹(AE æ¨¡å‹)**:

    - ä»£è¡¨æ˜¯ Bertï¼Œæ˜¯é€šè¿‡æŸä¸ªé™å™ªç›®æ ‡(å¦‚æ©ç è¯­è¨€æ¨¡å‹)è®­ç»ƒçš„è¯­è¨€ç¼–ç å™¨ã€‚
    - è‡ªç¼–ç æ¨¡å‹æ“…é•¿è‡ªç„¶è¯­è¨€ç†è§£(NLU)ä»»åŠ¡ï¼Œå¸¸è¢«ç”¨æ¥ç”Ÿæˆå¥å­çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä½†ä¸èƒ½ç›´æ¥åº”ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼›

  - **encoder-decoder(Seq2Seq æ¨¡å‹)**:
    - ä»£è¡¨ä½œ T5ï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ Transformer ç»“æ„ï¼ŒåŒ…å«ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚
    - é‡‡ç”¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šå¸¸ç”¨äºæ¡ä»¶ç”Ÿæˆä»»åŠ¡(conditional generation)ï¼Œæ¯”å¦‚æ–‡æœ¬æ‘˜è¦, æœºå™¨ç¿»è¯‘ç­‰ã€‚2019ã€‚
    - å®ƒä»¬é€šå¸¸è¢«éƒ¨ç½²åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦å’Œå›åº”ç”Ÿæˆã€‚
    - T5 é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç»Ÿä¸€äº† NLU å’Œæœ‰æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œä½†éœ€è¦æ›´å¤šçš„å‚æ•°æ¥åŒ¹é…åŸºäº BRET çš„æ¨¡å‹çš„æ€§èƒ½ã€‚

- ä¸Šè¿°ä¸‰ç§é¢„è®­ç»ƒæ¶æ„çš„è®­ç»ƒç›®æ ‡ä¹Ÿç•¥æœ‰ä¸åŒ:

  - `GPT`çš„è®­ç»ƒç›®æ ‡æ˜¯ä»å·¦åˆ°å³çš„æ–‡æœ¬ç”Ÿæˆï¼›
  - `Bert`çš„è®­ç»ƒç›®æ ‡æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œéšæœºæ©ç ï¼Œç„¶åé¢„æµ‹è¢«æ©ç çš„è¯ï¼›
  - `T5`åˆ™æ˜¯æ¥å—ä¸€æ®µæ–‡æœ¬ï¼Œä»å·¦åˆ°å³çš„ç”Ÿæˆå¦ä¸€æ®µæ–‡æœ¬ã€‚

  - ä¸‰ç§é¢„è®­ç»ƒæ¡†æ¶å„æœ‰åˆ©å¼Šï¼Œæ²¡æœ‰ä¸€ç§æ¡†æ¶åœ¨ä»¥ä¸‹ä¸‰ç§é¢†åŸŸçš„è¡¨ç°æœ€ä½³: è‡ªç„¶è¯­è¨€ç†è§£(NLU), æ— æ¡ä»¶ç”Ÿæˆä»¥åŠæ¡ä»¶ç”Ÿæˆã€‚
  - GLM åŸºäºä»¥ä¸ŠèƒŒæ™¯è¯ç”Ÿäº†ã€‚GLM æ¨¡å‹æ ¸å¿ƒæ˜¯`Autoregressive Blank Infilling`ï¼Œç»“åˆäº†ä¸Šè¿°ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹çš„æ€æƒ³ã€‚

åŸå§‹ GLM é¢„è®­ç»ƒåŸç†

- **é¢„è®­ç»ƒç›®æ ‡**:

  - **Autoregressive Blank Infilling(è‡ªå›å½’çš„ç©ºç™½å¡«å……)**:

    - GLM æ˜¯é€šè¿‡ä¼˜åŒ–è‡ªå›å½’`ç©ºç™½å¡«å……ç›®æ ‡`æ¥è®­ç»ƒçš„ã€‚
    - ç»™å®šä¸€ä¸ªè¾“å…¥æ–‡æœ¬ $x = [x_{1}, \cdots, x_{n}]$
    - å¤šä¸ªæ–‡æœ¬è·¨åº¦(æ–‡æœ¬ç‰‡æ®µ) $\{s_{1},\cdots, s_{m}\}$ è¢«é‡‡æ ·
    - å…¶ä¸­æ¯ä¸ªè·¨åº¦ $s_{i}$ â€‹ å¯¹åº”äº $x$ ä¸­ä¸€ç³»åˆ—è¿ç»­çš„ token: $[s_{i,1}, \cdots, s_{i,l_{i}}]$
      - $l_{i}$ â€‹ ä»£è¡¨è·¨åº¦
      - $s_{i}$ â€‹ çš„é•¿åº¦ã€‚
    - $x$ ä¸­çš„æ¯ä¸€ä¸ªè·¨åº¦éƒ½ä¼šè¢«ä¸€ä¸ª`[Mask]`æ›¿æ¢æ‰ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªè¢«ç ´åçš„ $x_{corrupt}$
    - GLM æ¨¡å‹ä»¥è‡ªå›å½’çš„æ–¹å¼é¢„æµ‹è¢«ç ´åçš„æ–‡æœ¬ä¸­ç¼ºå°‘çš„ tokenï¼Œè¿™æ„å‘³ç€å½“é¢„æµ‹ä¸€ä¸ªè·¨åº¦ä¸­ç¼ºå°‘çš„ token æ—¶ï¼ŒGLM æ—¢å¯ä»¥è®¿é—®è¢«ç ´åçš„æ–‡æœ¬ $x_{corrupt}$ â€‹ï¼Œåˆå¯ä»¥è®¿é—®è·¨åº¦ä¸­ä¹‹å‰å·²ç»è¢«é¢„æµ‹çš„ tokenã€‚

    - ä¸ºäº†å……åˆ†æ•æ‰ä¸åŒè·¨åº¦ä¹‹é—´çš„ç›¸äº’ä¾å­˜å…³ç³»ï¼ŒGLM éšæœºæ’åˆ—è·¨åº¦çš„é¡ºåºã€‚
    - å½¢å¼ä¸Šï¼Œè®© $Z_{m}$ â€‹ è¡¨ç¤ºé•¿åº¦ä¸º $m$ çš„ç´¢å¼•åºåˆ— $[1, 2, \cdots, m]$ æ‰€æœ‰å¯èƒ½æ’åˆ—çš„é›†åˆï¼Œ $s_{z_{<i}}$ ä»£è¡¨ $[s_{z_{1}}, \cdots, s_{z_{i-1}}]$
    - æ­¤æ—¶ï¼Œå¯å®šä¹‰é¢„è®­ç»ƒç›®æ ‡ä¸º:
      $\max_{\theta}E_{z\sim Z_{m}}[\sum_{i=1}^{m}log\ p_{\theta}(s_{z_{i}}|x_{corrupt}, s_{z_{<i}})]$

    - å…¶ä¸­:
      - $z$ ä»£è¡¨ $Z_{m}$ â€‹ ä¸­ä»»æ„ä¸€ä¸ªæ’åˆ—ï¼Œä¹Ÿå°±æ˜¯ç´¢å¼•é›†åˆï¼›
      - $\{z_{1},\cdots,z_{m}\}$ ä»£è¡¨ $z$ ä¸­çš„ç´¢å¼•å…ƒç´ ï¼›
      - $s_{z_{i}}$ â€‹ ä»£è¡¨ $\{s_{1},\cdots, s_{m}\}$ ä¸­ç¬¬ $z_{i}$ â€‹ ä¸ªè·¨åº¦ã€‚
    - ä¸Šè¿°å…¬å¼çš„å«ä¹‰å°±æ˜¯:
      - ç”¨è¢«ç ´åçš„ $x_{corrupt}$ â€‹ï¼Œä¸ $s_{z_{i}}$ â€‹ ä¹‹å‰çš„è·¨åº¦ $[s_{z_{1}}, \cdots, s_{z_{i-1}}]$ è¿›è¡Œæ‹¼æ¥ï¼Œ
      - é¢„æµ‹ç”Ÿæˆçš„æ–‡æœ¬æ˜¯è·¨åº¦ $s_{z_{i}}$ â€‹ çš„æ¦‚ç‡è¶Šå¤§è¶Šå¥½ï¼Œè¿™ä¹Ÿæ˜¯å…¸å‹çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å‡½æ•°ã€‚
    - å¦å¤–è®ºæ–‡ä¸­æåˆ°ï¼Œç”Ÿæˆä»»åŠ¡éƒ½æ˜¯æŒ‰ç…§ä»å·¦åˆ°å³çš„é¡ºåºç”Ÿæˆæ¯ä¸ªç©ºç™½å¤„çš„æ ‡è®°ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç”Ÿæˆè·¨åº¦ $s_{i}$ â€‹ çš„æ¦‚ç‡è¢«åˆ†è§£ä¸º:
      $p_{\theta}(s_{z_{i}}|x_{corrupt}, s_{z_{<i}})=\prod_{j=1}^{l_{i}}p(s_{i,j}|x_{corrupt}, s_{z_{<i}},s_{i<j})$

    - åœ¨æ„å»ºå¥½ä¼˜åŒ–ç›®æ ‡åï¼Œè®ºæ–‡ä¸­é€šè¿‡ä»¥ä¸‹æŠ€æœ¯å®ç°è¯¥ç›®æ ‡ï¼Œå³ä¸Šè¿°çš„è‡ªå›å½’ç©ºç™½å¡«è¡¥ç›®æ ‡ã€‚

      - è¾“å…¥çš„ $x$ è¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚

        - Part A æ˜¯è¢«ç ´åçš„æ–‡æœ¬ $x_{corrupt}$ â€‹ï¼Œ
        - Part B ç”±è¢«`mask`çš„è·¨åº¦ç»„æˆã€‚

      - ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º
        - å‡è®¾åŸå§‹çš„æ–‡æœ¬åºåˆ—ä¸º $x = [x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}]$
        - é‡‡æ ·çš„ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µä¸º $[x_{3}]$ å’Œ $[x_{5}, x_{6}]$
        - é‚£ä¹ˆæ©ç åçš„æ–‡æœ¬åºåˆ— $x_{corrupt}$ â€‹ ä¸º $[x_{1}, x_{2}, [M], x_{4}, [M]]$ ï¼Œä¹Ÿå°±æ˜¯ Part Aã€‚
        - æ–‡æœ¬ç‰‡æ®µ $[x_{3}]$ å’Œ $[x_{5}, x_{6}]$ ç”¨äºç»„æˆ Part Bï¼ŒåŒæ—¶éœ€è¦å¯¹ Part B çš„ç‰‡æ®µè¿›è¡Œ shuffleï¼Œä¹Ÿå°±æ˜¯æ‰“ä¹±æ–‡æœ¬ç‰‡æ®µçš„é¡ºåº(æ³¨æ„ä¸æ˜¯æ–‡æœ¬ç‰‡æ®µçš„å†…éƒ¨é¡ºåºï¼Œè€Œæ˜¯æ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„é¡ºåº)ï¼Œå¹¶ä¸”æ¯ä¸ªç‰‡æ®µä½¿ç”¨ $[S]$ å¡«å……åœ¨å¼€å¤´ä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨ $[E]$ å¡«å……åœ¨æœ«å°¾ä½œä¸ºè¾“å‡ºã€‚
        - æœ€åï¼Œä»å¼€å§‹æ ‡è®° $[S]$ å¼€å§‹ä¾æ¬¡è§£ç å‡ºè¢«æ©ç çš„æ–‡æœ¬ç‰‡æ®µï¼Œç›´è‡³ç»“æŸæ ‡è®° $[E]$ ã€‚
        - ä»¥ä¸Šæ˜¯å®ç°è‡ªå›å½’ç©ºç™½å¡«è¡¥ç›®æ ‡çš„å¤§ä½“æµç¨‹ã€‚

    - é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰æœ‰ä¸¤ç‚¹éœ€è¦æ³¨æ„

      - ä¸€ä¸ªæ˜¯`self-attention mask`çš„è®¾è®¡ï¼Œ
      - ä¸€ä¸ªæ˜¯`[Mask]`æ–‡æœ¬ç‰‡æ®µçš„é‡‡æ ·è®¾è®¡ã€‚

      - **self-attention mask**:

        - Part A ä¸­çš„è¯å½¼æ­¤å¯è§ï¼Œä½†ä¸å¯è§ Part B ä¸­çš„è¯(ä¸‹å›¾(d)ä¸­è“è‰²æ¡†ä¸­çš„åŒºåŸŸ)ï¼›
        - Part B ä¸­çš„è¯å•å‘å¯è§(ä¸‹å›¾(d)é»„è‰²å’Œç»¿è‰²çš„åŒºåŸŸã€‚é»„è‰²å’Œç»¿è‰²åˆ†åˆ«å¯¹åº” $[x_{3}]$ å’Œ $[x_{5}, x_{6}]$ ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œä¸‹åŒ)ï¼›
        - Part B å¯è§ Part A(ä¸‹å›¾(d)ä¸­é»„è‰²å’Œç»¿è‰²çš„åŒºåŸŸ)ï¼›
        - å…¶ä½™ä¸å¯è§(ä¸‹å›¾(d)ä¸­ç°è‰²çš„åŒºåŸŸ)

      - **`[Mask]`æ–‡æœ¬ç‰‡æ®µé‡‡æ ·**:

        - è®ºæ–‡ä¸­éšæœºå¯¹è·¨åº¦çš„é•¿åº¦é‡‡æ ·ï¼Œé‡‡æ ·åˆ†å¸ƒå±äºæ³Šæ¾åˆ†å¸ƒï¼Œå…¶ä¸­ $\lambda=3$ï¼Œç›´åˆ°è‡³å°‘ 15%çš„åŸå§‹ token è¢« maskã€‚
        - æ ¹æ®ç»éªŒï¼Œè®ºæ–‡ä¸­å‘ç°ï¼Œ15%çš„æ¯”ä¾‹å¯¹äºä¸‹æ¸¸ NLU ä»»åŠ¡çš„è‰¯å¥½è¡¨ç°è‡³å…³é‡è¦ã€‚

      - æœ€ç»ˆé€šè¿‡ä»¥ä¸Šæ–¹å¼ï¼ŒGLM è‡ªåŠ¨å­¦ä¹ ä¸€ä¸ªåŒå‘ç¼–ç å™¨(Part A)å’Œä¸€ä¸ªå•å‘è§£ç å™¨(Part B)ç»Ÿä¸€çš„æ¨¡å‹ã€‚
      - ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/71aa1a29c9804f36b06c7b4a82a96235.png#pic_center)

    - **Multi-Task Pretraining(å¤šä»»åŠ¡é¢„è®­ç»ƒ)**:

      - ä¸Šè¿°ä¾‹å­ä¸­ï¼ŒGLM æ©ç›–äº†çŸ­è·¨åº¦ï¼Œé€‚ç”¨äº NLU ä»»åŠ¡ã€‚
      - è€Œè®ºæ–‡çš„å…³æ³¨ç‚¹æ˜¯é¢„è®­ç»ƒä¸€ä¸ªèƒ½åŒæ—¶å¤„ç† NLU å’Œæ–‡æœ¬ç”Ÿæˆçš„æ¨¡å‹ã€‚
      - å› æ­¤ï¼Œè®ºæ–‡ç ”ç©¶äº†ä¸€ä¸ªå¤šä»»åŠ¡é¢„è®­ç»ƒçš„è®¾ç½®ã€‚
      - åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œå¢åŠ ä¸€ä¸ªç”Ÿæˆè¾ƒé•¿æ–‡æœ¬çš„ç›®æ ‡ï¼Œä¸ç©ºç™½å¡«å……ç›®æ ‡å…±åŒä¼˜åŒ–ã€‚
      - å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­è€ƒè™‘ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡ã€‚

        - **æ–‡æ¡£çº§**:

          - ä»æ–‡æ¡£ä¸­é‡‡æ ·ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µè¿›è¡Œ maskï¼Œä¸”ç‰‡æ®µé•¿åº¦ä¸ºæ–‡æ¡£é•¿åº¦çš„ 50%ï½ 100%ã€‚
          - è¯¥ç›®æ ‡æ—¨åœ¨ç”Ÿæˆé•¿æ–‡æœ¬ï¼›

        - **å¥å­çº§**:
          - é™åˆ¶è¢« mask çš„æ–‡æœ¬ç‰‡æ®µå¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ã€‚
          - å¤šä¸ªæ–‡æœ¬ç‰‡æ®µ(å¥å­)è¢«å–æ ·ï¼Œä»¥è¦†ç›– 15%çš„åŸå§‹ tokenã€‚
          - è¿™ä¸€ç›®æ ‡æ˜¯é’ˆå¯¹ seq2seq ä»»åŠ¡ï¼Œå…¶é¢„æµ‹å¾€å¾€æ˜¯å®Œæ•´çš„å¥å­æˆ–æ®µè½ã€‚
        - è¿™ä¸¤ä¸ªæ–°ç›®æ ‡çš„å®šä¹‰ä¸åŸç›®æ ‡ç›¸åŒã€‚å”¯ä¸€ä¸åŒçš„æ˜¯çš„è·¨åº¦æ•°é‡å’Œè·¨åº¦é•¿åº¦ã€‚

- **æ¨¡å‹ç»“æ„**:
  - GLM ä½¿ç”¨ Transformer æ¶æ„ï¼Œå¹¶å¯¹æ¶æ„è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ã€‚
  - å…¶ä¸­ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œæ˜¯äºŒç»´ä½ç½®ç¼–ç ã€‚
    - **Layer Normalization**: é‡æ–°è°ƒæ•´äº† LayerNorm å’Œæ®‹å·®è¿æ¥çš„é¡ºåº(å…ˆè¿›è¡Œ LayerNormï¼Œå†è¿›è¡Œæ®‹å·®è¿æ¥ï¼Œç±»ä¼¼äº Pre-LNï¼Œä¸è¿‡ GLM-130B è®­ç»ƒæ—¶åˆè°ƒæ•´ä¸º DeepNorm äº†)ï¼›
    - **è¾“å‡ºå±‚**: ä½¿ç”¨å•ä¸ªçº¿æ€§å±‚è¿›è¡Œè¾“å‡º token é¢„æµ‹ï¼›
    - **æ¿€æ´»å‡½æ•°**: ä½¿ç”¨ GeLU æ›¿æ¢ ReLU æ¿€æ´»å‡½æ•°ï¼›
    - **äºŒç»´ä½ç½®ç¼–ç **:
      - è‡ªå›å½’ç©ºç™½å¡«å……ä»»åŠ¡çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å¦‚ä½•å¯¹ä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚
      - Transformer ä¾é ä½ç½®ç¼–ç æ¥æ³¨å…¥ token çš„ç»å¯¹å’Œç›¸å¯¹ä½ç½®ã€‚
      - è®ºæ–‡ä¸­æå‡ºäº†äºŒç»´ä½ç½®ç¼–ç æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚
      - å¦‚ä¸Šé¢å›¾ç‰‡æ‰€ç¤ºï¼Œå…·ä½“æ¥è¯´ï¼Œæ¯ä¸ª token éƒ½æœ‰ä¸¤ä¸ªä½ç½®æ ‡è¯†ç¼–ç ã€‚
      - ç¬¬ä¸€ä¸ªä½ç½®æ ‡è¯†ä»£è¡¨åœ¨è¢«ç ´åæ–‡æœ¬ $x_{corrupt}$ â€‹ ä¸­çš„ä½ç½®ã€‚
      - å¯¹äºè¢« mask çš„è·¨åº¦ï¼Œå®ƒæ˜¯ç›¸åº”çš„`[Mask]`çš„ä½ç½®ã€‚
      - ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†ä»£è¡¨è·¨åº¦å†…çš„ä½ç½®ã€‚
        - å¯¹äº A éƒ¨åˆ†çš„ tokenï¼Œå®ƒä»¬çš„ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†æ˜¯ 0ï¼›
        - å¯¹äº B éƒ¨åˆ†çš„ tokenï¼Œå®ƒä»¬çš„ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†æ˜¯ä» 1 åˆ°è·¨åº¦çš„é•¿åº¦ã€‚
      - è¿™ä¸¤ä¸ªä½ç½®æ ‡è¯†é€šè¿‡å¯å­¦ä¹ çš„åµŒå…¥è¡¨æ˜ å°„ä¸ºä¸¤ä¸ªå‘é‡ï¼Œè¿™ä¸¤ä¸ªå‘é‡éƒ½è¢«æ·»åŠ åˆ°è¾“å…¥ token çš„ embedding è¡¨è¾¾ä¸­ã€‚

æ›´å¤šç»†èŠ‚

- [GLM: è‡ªå›å½’ç©ºç™½å¡«å……çš„é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ](https://zhuanlan.zhihu.com/p/560559133)
- [ChatGLM å®˜æ–¹åšå®¢](https://chatglm.cn/blog)ã€‚

å¦‚ä½•åœ¨ GLM åŸºç¡€ä¸Šè¿›è¡Œçš„ä¼˜åŒ–è°ƒæ•´ã€‚

- **ç›¸å¯¹åŸå§‹ GLM çš„ä¼˜åŒ–è°ƒæ•´**:

  - **Layer Normalization**: ä½¿ç”¨ DeepNorm(Post-LN çš„å‡çº§ç‰ˆ)æ¥æä¾›æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ä¸‹é¢æ˜¯ä¸‰ç§ LN æ¨¡å¼ï¼Œå…¶ä¸­ $f$ ä»£è¡¨ FFN æˆ– Attention å±‚ã€‚

    - **Post-LN**: åŸå§‹ Bert, GPT-1 é‡‡ç”¨çš„ Layer Normalization å½¢å¼ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œä½†æ•ˆæœè¾ƒå¥½ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹: $x=LayerNorm(x+f(x))$
    - **Pre-LN**: GPT-2, GPT-3 ä»¥åŠ LLaMA é‡‡ç”¨çš„ Layer Normalization å½¢å¼éƒ½è¿‘ä¼¼äº Pre-LNï¼Œæ•ˆæœä¸å¦‚ Post-LNï¼Œä½†ç¨³å®šæ€§è¾ƒå¥½ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹: $x=x+LayerNorm(f(x))$
    - **DeepNorm**: é›†æˆ Post-LN å’Œ Pre-LN çš„ä¼˜ç‚¹ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹: $x=LayerNorm(\alpha x+f(x)), (\alpha>1)$

  - **Position Embedding**: ä½¿ç”¨ RoPE(æ—‹è½¬ä½ç½®ç¼–ç )æ›¿æ¢ 2D Position Embeddingï¼›

  - **Feed Forward Network(FFN)**: ä½¿ç”¨ GeGLU æ›¿æ¢ GeLU ã€‚

- **GLM-130B é¢„è®­ç»ƒé…ç½®**:

  - **è‡ªç›‘ç£ç©ºç™½å¡«å……(95% tokens)**: é€šè¿‡ä¸åŒçš„ Mask ç­–ç•¥ï¼Œæ¥ä½¿æ¨¡å‹è·å¾—è‡ªç¼–ç å’Œè‡ªå›å½’çš„èƒ½åŠ›ï¼Œå…·ä½“æ¥è¯´:

    - **è¯ Mask**: 30%çš„è®­ç»ƒ token è¿›è¡Œè¯çº§åˆ«çš„ Maskï¼ŒMask æ–¹å¼å‚è€ƒå‰æ–‡çš„è·¨åº¦é‡‡æ ·æ–¹æ³•: è·¨åº¦é•¿åº¦éµå¾ªæ³Šæ¾åˆ†å¸ƒ(Î»=3)ï¼Œæ¯ä¸ªæ ·æœ¬çš„è·¨åº¦é•¿åº¦åŠ èµ·æ¥æœ€å¤šä¸ºè¯¥æ ·æœ¬é•¿åº¦çš„ 15%ï¼›

    - **å¥å­åŠæ–‡æ¡£ Mask**: å‰©ä¸‹ 70%çš„ token è¿›è¡Œå¥å­æˆ–æ–‡æ¡£çº§åˆ«çš„ Maskã€‚

  - **å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒ(MIPï¼Œ5%tokens)**: T5 å’Œ ExT5 ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒä¸­çš„å¤šä»»åŠ¡å­¦ä¹ æ¯”å¾®è°ƒæ›´æœ‰å¸®åŠ©ã€‚å› æ­¤ï¼ŒGLM-130B åœ¨é¢„è®­ç»ƒä¸­åŒ…å«å„é¡¹æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¯­æ„ç†è§£, ç”Ÿæˆå’Œä¿¡æ¯æŠ½å–ã€‚ä¸ºäº†ä¿è¯æ¨¡å‹çš„å…¶ä»–ç”Ÿæˆèƒ½åŠ›ä¸å—å½±å“ï¼Œç”¨äº MIP è®­ç»ƒçš„æ•°æ®é›†åªå äº† 5%ã€‚

---

#### 2.2 LLaMA

= 2023 å¹´ 7 æœˆï¼ŒMeta æ¨å‡ºäº†å®Œå…¨å¯å•†ç”¨çš„å¼€æºå¤§æ¨¡å‹ LLaMA2ã€‚è¿™é‡Œç®€å•ä»‹ç»ä¸‹ä¸¤ä»£ LLaMA çš„å…±æœ‰ç»“æ„ä»¥åŠ LLaMA2 ç›¸è¾ƒäºåˆä»£ LLaMA çš„ä¼˜åŒ–ç‚¹ã€‚

- **é€šç”¨ç»“æ„**: LLaMA çš„å…·ä½“ç»“æ„è§ä¸‹å›¾ã€‚

  - **Layer Normalization**:

    - ä½¿ç”¨å‰ç½®çš„`RMSNorm`ï¼›
    - åœ¨ BERT, GPT ç­‰æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„`LayerNorm`æ˜¯å¦‚ä¸‹å½¢å¼: $y=W*\frac{x-Mean(x)}{\sqrt{Var(x)+\epsilon}}+b$

    - RMSNorm(root mean square)å‘ç° LayerNorm çš„ä¸­å¿ƒåç§»æ²¡ä»€ä¹ˆç”¨(å‡å»å‡å€¼ç­‰æ“ä½œ)ã€‚
    - å°†å…¶å»æ‰ä¹‹åï¼Œæ•ˆæœå‡ ä¹ä¸å˜ï¼Œä½†æ˜¯é€Ÿåº¦æå‡äº† 40%ã€‚
    -
    - RMSNorm å…¬å¼ä¸º: $y=W*\frac{x}{\sqrt{Mean(x^{2})+\epsilon}}$
    -
    - æ³¨æ„é™¤äº†æ²¡æœ‰å‡å‡å€¼ï¼ŒåŠ åç½®ä»¥å¤–ï¼Œåˆ†æ¯ä¸Šæ±‚çš„æ˜¯ RMS è€Œä¸æ˜¯æ–¹å·®ã€‚
    - å¦å¤– LLaMA åœ¨ Attention Layer å’Œ MLP çš„è¾“å…¥ä¸Šä½¿ç”¨äº† RMSNormï¼Œç›¸æ¯”åœ¨è¾“å‡ºä¸Šä½¿ç”¨ï¼Œè®­ç»ƒä¼šæ›´åŠ ç¨³å®šï¼Œç±»ä¼¼äº Pre-LN æ–¹å¼ã€‚

    - **ä½ç½®ç¼–ç **: åœ¨ Q, K ä¸Šä½¿ç”¨ RoPE æ—‹è½¬å¼ä½ç½®ç¼–ç ï¼›
    - **Causal Mask**: ä½¿ç”¨`causal mask`ä¿è¯æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å‰é¢çš„ tokensï¼›
    - **æ¿€æ´»å‡½æ•°**: ä½¿ç”¨ SwiGLU æ›¿ä»£ ReLUã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3351f0c56f4f48c29d9ca152aee90f9f.png#pic_center)

- **LLaMA2 ä¼˜åŒ–ç‚¹**:
  - **æ›´å¤šçš„è®­ç»ƒè¯­æ–™**: é¢„è®­ç»ƒè¯­æ–™ä» 1 ä¸‡äº¿å¢åŠ åˆ° 2 ä¸‡äº¿ tokensï¼›
  - **æ›´é•¿çš„ä¸Šä¸‹æ–‡**: ä¸Šä¸‹æ–‡é•¿åº¦ä» 2048 å¢åŠ åˆ° 4096ï¼›
  - **æ–°å¢ SFT è¿‡ç¨‹**: æ”¶é›†äº† 10 ä¸‡äººç±»æ ‡æ³¨æ•°æ®è¿›è¡Œ SFTï¼›
  - **æ–°å¢ RLHF è¿‡ç¨‹**: æ”¶é›†äº† 100 ä¸‡äººç±»åå¥½æ•°æ®è¿›è¡Œ RLHFï¼›
  - **è°ƒæ•´ Attention æœºåˆ¶**: å’Œ Falcon ä¸€æ ·ï¼Œä½¿ç”¨äº† Group Query Attentionï¼ŒèŠ‚çœæ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶æå‡è®¡ç®—é€Ÿåº¦ã€‚
    - **Multi Head Attention(MHA)**: åŸå§‹å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€æœ‰å¤´å„è‡ªä¿å­˜ç‹¬ç«‹çš„ Q, K, V çŸ©é˜µï¼›
    - **Multi Query Attention(MQA)**: æ‰€æœ‰çš„å¤´ä¹‹é—´å…±äº«åŒä¸€ä»½ K å’Œ V çŸ©é˜µï¼Œæ¯ä¸ªå¤´åªå•ç‹¬ä¿ç•™äº†ä¸€ä»½ Q çŸ©é˜µå‚æ•°ï¼Œä»è€Œå¤§å¤§å‡å°‘ K å’Œ V çŸ©é˜µçš„å‚æ•°é‡ï¼›
    - **Group Query Attention(GQA)**: æ²¡æœ‰åƒ MQA ä¸€æ ·æç«¯ï¼Œè€Œæ˜¯å°† Q åˆ†ç»„ï¼Œç»„å†…å…±äº« K, V çŸ©é˜µã€‚å½“ group=1 æ—¶ï¼ŒGQA ç­‰ä»·äº MQAã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/206eaadf3abe41c4a911e0e46e6b9583.png#pic_center)

å‚è€ƒ

- [LLaMA2 vs LLaMA](https://zhuanlan.zhihu.com/p/636784644)
- [LLaMA2 ä»‹ç»](https://zhuanlan.zhihu.com/p/647862867)

.
