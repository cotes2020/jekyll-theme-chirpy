---
title: AIML - AI
date: 2023-04-24 11:11:11 -0400
description:
categories: [51AIML]
# img: /assets/img/sample/rabbit.png
tags: [AIML]
---

# AIML - AI

- [AIML - AI](#aiml---ai)
  - [Overall](#overall)
  - [AI](#ai)
    - [Divisions of AI](#divisions-of-ai)
      - [Artificial Narrow Intelligence (ANI)](#artificial-narrow-intelligence-ani)
      - [Artificial General Intelligence (AGI)](#artificial-general-intelligence-agi)
  - [Traditional AIML vs GenAI](#traditional-aiml-vs-genai)
    - [RNN - Recurrent neural networks](#rnn---recurrent-neural-networks)
    - [Attention is all you need](#attention-is-all-you-need)
      - [High alignment](#high-alignment)
      - [Multi-Headed Attention](#multi-headed-attention)
  - [GenAI](#genai)
    - [Large Language Model](#large-language-model)
    - [Features of LLMs](#features-of-llms)
      - [Translation](#translation)
      - [Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡](#automating-mundane-tasks-è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡)
      - [Emergent Abilities æ–°å…´èƒ½åŠ›](#emergent-abilities-æ–°å…´èƒ½åŠ›)
    - [Drawbacks of LLMs](#drawbacks-of-llms)
      - [Hallucination](#hallucination)
      - [Bias](#bias)
      - [Glitch tokens](#glitch-tokens)
      - [LLM Generation Inefficient](#llm-generation-inefficient)
        - [Speculative æ¨æµ‹çš„ Decoding](#speculative-æ¨æµ‹çš„-decoding)
    - [LLM Subject](#llm-subject)
    - [Generative configuration](#generative-configuration)
    - [Tuning å¾®è°ƒ](#tuning-å¾®è°ƒ)
      - [Fine-Tuningï¼ˆå¾®è°ƒï¼‰](#fine-tuningå¾®è°ƒ)
      - [Prompt-Tuningï¼ˆæç¤ºå¾®è°ƒï¼‰](#prompt-tuningæç¤ºå¾®è°ƒ)
        - [In-context learning (ICL)ï¼ˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰](#in-context-learning-iclä¸Šä¸‹æ–‡å­¦ä¹ )
        - [Prompting and prompt engineering](#prompting-and-prompt-engineering)
        - [Pattern-Verbalizer-Pairï¼ˆPVPï¼‰](#pattern-verbalizer-pairpvp)
        - [Prompt-Tuning](#prompt-tuning)
      - [2.4 Prompt-Tuning vs Fine-Tuning](#24-prompt-tuning-vs-fine-tuning)
    - [3ã€Instruction-Tuningï¼ˆæŒ‡ç¤ºå¾®è°ƒï¼‰](#3instruction-tuningæŒ‡ç¤ºå¾®è°ƒ)
      - [3.1ã€Instruction-Tuningçš„æå‡º](#31instruction-tuningçš„æå‡º)
      - [3.2ã€Fine-Tuning vs Prompt-Tuning vs Instruction-Tuning](#32fine-tuning-vs-prompt-tuning-vs-instruction-tuning)
    - [4ã€Chain-of-Thoughtï¼ˆæ€ç»´é“¾ï¼‰](#4chain-of-thoughtæ€ç»´é“¾)
      - [4.1ã€Manual-CoTï¼ˆäººå·¥æ€ç»´é“¾ï¼‰](#41manual-cotäººå·¥æ€ç»´é“¾)
      - [4.2ã€Zero-shot-CoTï¼ˆé›¶ç¤ºä¾‹æ€ç»´é“¾ï¼‰](#42zero-shot-coté›¶ç¤ºä¾‹æ€ç»´é“¾)
      - [4.3ã€Auto-CoTï¼ˆè‡ªåŠ¨æ€ç»´é“¾ï¼‰](#43auto-cotè‡ªåŠ¨æ€ç»´é“¾)
    - [5ã€Parameter-Efficient Fine-Tuning (PEFTï¼Œå‚æ•°æœ‰æ•ˆæ€§å¾®è°ƒ)](#5parameter-efficient-fine-tuning-peftå‚æ•°æœ‰æ•ˆæ€§å¾®è°ƒ)
      - [5.1ã€PEFTä»‹ç»](#51peftä»‹ç»)
      - [5.2ã€PEFTå®è·µ](#52peftå®è·µ)
      - [5.3ã€å¤§æ¨¡å‹Fine-Tuningä¹‹åˆ†å¸ƒå¼è®­ç»ƒ](#53å¤§æ¨¡å‹fine-tuningä¹‹åˆ†å¸ƒå¼è®­ç»ƒ)
      - [5.4ã€å¤§æ¨¡å‹çŸ¥è¯†é—®ç­”](#54å¤§æ¨¡å‹çŸ¥è¯†é—®ç­”)
  - [äºŒã€LLMç®€ä»‹](#äºŒllmç®€ä»‹)
    - [1ã€GPTç³»åˆ—ï¼ˆOpenAIï¼‰](#1gptç³»åˆ—openai)
      - [1.1 GPT-1ã€GPT-2ã€GPT-3](#11-gpt-1gpt-2gpt-3)
      - [1.2 InstructGPT](#12-instructgpt)
      - [1.3 ChatGPT](#13-chatgpt)
      - [1.4 GPT-4](#14-gpt-4)
    - [2ã€å…¶ä»–å¤§æ¨¡å‹](#2å…¶ä»–å¤§æ¨¡å‹)
      - [2.1 ChatGLM](#21-chatglm)
      - [2.2 LLaMA](#22-llama)
  - [ä¸‰ã€è¡¥å……çŸ¥è¯†](#ä¸‰è¡¥å……çŸ¥è¯†)
    - [1ã€LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoder onlyæ¶æ„ï¼Ÿ](#1llmä¸ºä»€ä¹ˆéƒ½ç”¨decoder-onlyæ¶æ„)
    - [2ã€NLPå°çŸ¥è¯†ç‚¹](#2nlpå°çŸ¥è¯†ç‚¹)
    - [3ã€åè¯è§£é‡Š](#3åè¯è§£é‡Š)
    - [4ã€å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„ä¸€äº›ä¸ªäººæ„Ÿæƒ³](#4å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„ä¸€äº›ä¸ªäººæ„Ÿæƒ³)
  - [æ€»ç»“](#æ€»ç»“)
    - [â€œç›¸å…³æ¨èâ€å¯¹ä½ æœ‰å¸®åŠ©ä¹ˆï¼Ÿ](#ç›¸å…³æ¨èå¯¹ä½ æœ‰å¸®åŠ©ä¹ˆ)
    - [çƒ­é—¨æ–‡ç« ](#çƒ­é—¨æ–‡ç« )
    - [æœ€æ–°è¯„è®º](#æœ€æ–°è¯„è®º)
    - [æ‚¨æ„¿æ„å‘æœ‹å‹æ¨èâ€œåšå®¢è¯¦æƒ…é¡µâ€å—ï¼Ÿ](#æ‚¨æ„¿æ„å‘æœ‹å‹æ¨èåšå®¢è¯¦æƒ…é¡µå—)
    - [æœ€æ–°æ–‡ç« ](#æœ€æ–°æ–‡ç« )
    - [ç›®å½•](#ç›®å½•)
    - [ç›®å½•](#ç›®å½•-1)
    - [æœ€æ–°æ–‡ç« ](#æœ€æ–°æ–‡ç« -1)
    - [ç›®å½•](#ç›®å½•-2)
    - [Transformers](#transformers)
      - [Transformers architecture](#transformers-architecture)
      - [encoder vs decoder](#encoder-vs-decoder)
      - [How the model works](#how-the-model-works)
      - [Overall prediction process](#overall-prediction-process)
        - [Data preprocessing / embedding](#data-preprocessing--embedding)
        - [Prompt construction / retrieval](#prompt-construction--retrieval)
        - [Prompt execution / inference](#prompt-execution--inference)
      - [AI agents frameworks](#ai-agents-frameworks)
    - [LLM Tools](#llm-tools)
      - [Medusa](#medusa)
        - [Medusa heads](#medusa-heads)
        - [Tree attention](#tree-attention)
        - [Typical acceptance](#typical-acceptance)
        - [accelerate models](#accelerate-models)
        - [Ablation Study æ¶ˆèç ”ç©¶](#ablation-study-æ¶ˆèç ”ç©¶)
    - [Confidence score for ML model](#confidence-score-for-ml-model)
    - [Transparency](#transparency)
      - [The Foundation Model Transparency Index](#the-foundation-model-transparency-index)
    - [Generative AI project lifecycle](#generative-ai-project-lifecycle)
  - [Hugging Face](#hugging-face)
    - [Generative AI Time Series Forecasting](#generative-ai-time-series-forecasting)
    - [Multivariate Time Series Forecasting](#multivariate-time-series-forecasting)
    - [Generative AI Transformers for Time Series Forecasting](#generative-ai-transformers-for-time-series-forecasting)
    - [Falcon 40b](#falcon-40b)
    - [CodeParrot](#codeparrot)
    - [TAPEX](#tapex)
  - [Juptyper](#juptyper)


ref:
- [OWAPS Top10 for LLM v1](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0.pdf)
- https://www.freecodecamp.org/news/large-language-models-and-cybersecurity/
- https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html
- https://docs.whylabs.ai/docs/integrations-llm-whylogs-container
- https://hackernoon.com/security-threats-to-high-impact-open-source-large-language-models
- https://a16z.com/emerging-architectures-for-llm-applications/
- [Examining Zero-Shot Vulnerability Repair with Large Language Models](https://www.connectedpapers.com/main/a5731122200fbb8b37f048010a1e1ca4474aa606/Examining-Zero%20Shot-Vulnerability-Repair-with-Large-Language-Models/graph)
- [medusa](https://together.ai/blog/medusa)
- [awesome-generative-ai](https://github.com/steven2358/awesome-generative-ai)
- [Googleâ€™s Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)
- [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/fmti.pdf)


Link:
- [Browse State-of-the-Art](https://paperswithcode.com/sota)

---

## Overall

> Research in artificial intelligence is increasing at an exponential rate. Itâ€™s difficult for AI experts to keep up with everything new being published, and even harder for beginners to know where to start.

â€œAI Canonâ€
- a curated list of resources weâ€™ve relied on to get smarter about modern AI
- because these papers, blog posts, courses, and guides have had an outsized impact on the field over the past several years.


**Data pipelines**
- Databricks
- Airflow
- Unstructured

**Embedding model**
- OpenAI
- Cohere
- Hugging Face

**Vector database**
- Pinecone
- Weaviate
- ChromaDB
- pgvector

**Playground**
- OpenAI
- nat.dev
- Humanloop

**Orchestration**
- Langchain
- LlamaIndex
- ChatGPT


**APIs/plugins**
- Serp
- Wolfram
- Zapier

**LLM cache**
- Redis
- SQLite
- GPTCache

**Logging / LLMops**
- Weights & Biases
- MLflow
- PromptLayer
- Helicone

**Validation**
- Guardrails
- Rebuff
- Microsoft Guidance
- LMQL

**App hosting**
- Vercel
- Steamship
- Streamlit
- Modal

**LLM APIs (proprietary)**
- OpenAI
- Anthropic

**LLM APIs (open)**
- Hugging Face
- Replicate

**Cloud providers**
- AWS
- GCP
- Azure
- CoreWeave

**Opinionated clouds**
- Databricks
- Anyscale
- Mosaic
- Modal
- RunPod



**OpenSource**
- hugging face
- OpenAI
- Generative AI (answers for everything)

**Programming**
- python
- panda

**AI modeling**
- pyTorch
- Tensor flow (Google)

**ML platforms**
- Jupyter Notebooks

**Time series**
- Forecasting and predictive Analytics

**Use case**
- Supply Chain Management with GenAI

OpenSource -> fine tuning -> custom result


---


## AI

- Artificial Intelligence refers to `the ability of computers to perform tasks that typically require human-level intellect`.
- AI is useful in many contexts, from automation to problem solving and merely trying to understand how humans think.

- But it is important to note that AI is only concerned with human intelligence for now â€“ it could possibly go beyond that.

  - Many people correlate the word â€˜Intelligenceâ€™ with only â€˜Human Intelligenceâ€™. Just because a chicken may not be able to solve a mathematical equation doesnâ€™t mean it wonâ€™t run when you chase it. It is â€˜Intelligentâ€™ enough to know it doesnâ€™t want you to catch it ğŸ”ğŸ—.

  - Intelligence spans a much wider spectrum, and practically expands to any living thing that can make decisions or carry out actions autonomously, even plants.


### Divisions of AI

- Artificial Intelligence is `centered around computers and their ability to mimic human actions and thought processes`.

- Programming and experiments have allowed humans to produce ANI systems. These can do things like classifying items, sorting large amounts of data, looking for trends in charts and graphs, code debugging, and knowledge representation and expression. But computers donâ€™t think like humans, they merely mimic humans.

- This is evident in voice assistants such as `Googleâ€™s Assistant, Appleâ€™s Siri, Amazonâ€™s Alexa, and Microsoftâ€™s Cortana`. They are basic ANI programs that add â€˜the human touchâ€™. In fact, people are known to be polite to these systems simply because they combine computerized abilities with a human feel.

- These assistants have gotten better over the years but fail to reach high levels of sophistication when compared to their AGI counterparts.


There are two major divisions of AI:


#### Artificial Narrow Intelligence (ANI)

- focused on a small array of similar tasks or a small task that is programmed only for one thing.
- ANI is not great in dynamic and complex environments and is used in only areas specific to it.
- Examples include self-driving cars, as well as facial and speech recognition systems.


#### Artificial General Intelligence (AGI)

- focused on a wide array of tasks and human activities.
- AGI is currently theoretical and is proposed to adapt and carry out most tasks in many dynamic and complex environments.
- Examples include J.A.R.V.I.S from Marvelâ€™s _Iron Man_ and Ava from _Ex-Machina_.




---

## Traditional AIML vs GenAI

**Traditional AIML**
- good at **identify pattern**
- learning from the pattern
- limit success with close supervised learning of very large amount of data
- must have human involved


**GenAI**
- produces 'content' (text, image, music, art, forecasts, etc...)
- use 'transformers' (Encoders/Decoders) based on pre-trained data using small amount of fine tuning data
  - encode and decode at the same time
    - less data and faster
    - GenAI use small data and uses encoders and decoders and Transformers to take that smaller data and be able to use it for other types of models. (Pre training)
    - then add on top of it small amounts of fine tuning data
    - and then get a a training model.
  - As perceptions, not neurons.

- Generative AI is a subset of traditional machine learning.
  - And the generative AI machine learning models have learned these abilities by `finding statistical patterns in massive datasets of content` that was originally generated by humans.


choosing between LLMs or layout-based Traditional AIML[^Choosing_an_extraction_approach]
- recommends using LLM prompts for free-form, highly variable documents
- layout-based or "rule-based" queries for structured, less-variable documents.

![Screenshot 2023-11-13 at 16.21.05](/assets/img/Screenshot%202023-11-13%20at%2016.21.05.png)

[^Choosing_an_extraction_approach]: Choosing an extraction approach, https://docs.sensible.so/docs/author


---

### RNN - Recurrent neural networks

> generative algorithms are not new.

recurrent neural networks - RNNs
- Previous generations of language models made use of an architecture called RNNs.
- RNNs were limited by the amount of `compute and memory` needed to perform well at generative tasks.


- With just one previous words seen by the model, the prediction can't be very good.
  - scale the RNN implementation to be able to see more of the preceding words in the text,
  - significantly scale the resources that the model uses.
  - As for the prediction, Even though scale the model, it still hasn't seen enough of the input to make a good prediction.

![Screenshot 2023-10-10 at 00.31.57](/assets/img/post/Screenshot%202023-10-10%20at%2000.31.57.png)


- To successfully predict the next word,
  - models need to see more than just the previous few words.
  - Models needs to have an understanding of the whole sentence or even the whole document.


> How can an algorithm make sense of human language if sometimes we can't?
> in 2017, after the publication of this paper, `Attention is All You Need`, from Google and the University of Toronto, everything changed.
> The transformer architecture had arrived.
> - It can be scaled efficiently to use multi-core GPUs,
> - parallel process input data, making use of much larger training datasets, and crucially,
> - it's able to learn to pay attention to the meaning of the words it's processing.

---

### Attention is all you need

![Screenshot 2023-10-10 at 00.33.00](/assets/img/post/Screenshot%202023-10-10%20at%2000.33.00.png)

---

#### High alignment

![Screenshot 2023-09-06 at 22.55.58](/assets/img/post/Screenshot%202023-09-06%20at%2022.55.58.png)

![Screenshot 2023-09-06 at 22.56.59](/assets/img/post/Screenshot%202023-09-06%20at%2022.56.59.png)

---

#### Multi-Headed Attention

![Screenshot 2023-09-06 at 23.10.28](/assets/img/post/Screenshot%202023-09-06%20at%2023.10.28.png)




---

## GenAI

With the rise in popularity of Foundation Models, new models and tools are released almost every week and yet

![Screenshot 2023-11-13 at 22.11.36](/assets/img/Screenshot%202023-11-13%20at%2022.11.36.png)

---

### Large Language Model

> "... a language model is a Turing-complete weird machine running programs written in natural language; when you do retrieval, you are not 'plugging updated facts into the AI', you are actually downloading random new unsigned blobs of code from the Internet (many written by adversaries) and casually executing them on the LM with full privileges. This does not end well." - [Gwern Branwen on LessWrong](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K)

- a deep learning model which consists of a `neural network` with billions of parameters, trained on distinctively large amounts of unlabelled data using self-supervised learning.

- At the core of all AI are algorithms. Algorithms are procedures or steps to carry out a specific task. The more complex the algorithm, the more tasks can be carried out and the more widely it can be applied. The aim of AI developers is to find the most complex algorithms that can solve and perform a wide array of tasks.

- The procedure to create a basic fruit recognition model using an simple analogy:

  1. There are two people: A teacher and a bot creator
  2. The bot creator creates random bots, and the teacher teaches and tests them on identifying some fruits
  3. The bot with the highest test score is then sent back to the creator as a base to make new upgraded bots
  4. These new upgraded bots are sent back to the teacher for teaching and testing, and the one with the highest test score is sent back to the bot creator to make new better bots.

  - This is an oversimplification of the process, but nevertheless it relays the concept. The Model/Algorithm/Bot is continuously trained, tested, and modified until it is found to be satisfactory. More data and higher complexity means more training time required and more possible modifications.

- the developer of the model can tweak a few things about the model but may not know how those tweaks might affect the results.
- A common example of this are neural networks, which have hidden layers whose deepest layers and workings even the creator may not fully understand.

- Self-supervised learning means that rather than the teacher and the bot creator being two separate people, it is one highly skilled person that can both create bots and teach them.
  - This makes the process much faster and practically autonomous.
  - The result is a bot or set of bots that are both sophisticated and complex enough to recognise fruit in dynamic and different environments.

- In the case of LLMs, the data here are human text, and possibly in various languages. The reason why the data are large is because the LLMs take in huge amounts of text data with the aim of finding connections and patterns between words to derive context, meaning, probable replies, and actions to these text.

- The results are models that seem to understand language and carry out tasks based on prompts they're given.

  - **ChatGPT**: the greatest achievement in this field as it amassed 100 million active users in 2 months from the day of its release.
  - GPT-4 by OpenAI ğŸ”¥
  - LLaMA by Meta ğŸ¦™
  - AlexaTM by Amazon ğŸ«
  - Minerva by Google âœ–ï¸â•

---


### Features of LLMs

#### Translation

- LLMs that are trained on an array of languages rather than just one can be used for translation from one language to another.
- It's even theorised that large enough LLMs can find patterns and connections in other languages to derive meaning from unknown and lost languages, despite not knowing what each individual word may mean.


#### Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡

- Task automation has always been a major aim of AI development. Language models have always been able to carry out syntax analysis, finding patterns in text and responding appropriately.

- Large language models have an advantage with semantic analysis è¯­ä¹‰åˆ†æ, enabling the model to understand the underlying meaning and context, giving it a higher level of accuracy.

- This can be applied to a number of basic tasks like `text summarising, text rephrasing, and text generation`.


#### Emergent Abilities æ–°å…´èƒ½åŠ›

- Emergent Abilities are `unexpected but impressive` abilities LLMs have due to the high amount of data they are trained on.

- These behaviours are usually discovered when the model is used rather than when it is programmed.

- Examples include multi-step arithmetic, taking college-level exams, and chain-of-thought prompting. æ€ç»´é“¾æç¤º



---

### Drawbacks of LLMs


#### Hallucination

- An infamous outcome of Microsoftâ€™s Sydney were instances when the AI gave responses that were either bizarre å¼‚ä¹å¯»å¸¸, untrue, or seemed sentient æœ‰æ„Ÿæƒ….
- These instances are termed Hallucination, where the model gives answers or makes claims that are not based on its training data.


#### Bias

- Sometimes, the data could be the source of the problem. If a model is trained on data that is discriminatory to a person, group, race, or class, the results would also tend to be discriminatory.

- Sometimes, as the model is being used, the bias could change to fit what users tend to input. Microsoftâ€™s Tay in 2016 was a great example of how bias could go wrong.

#### Glitch tokens

- Also known as adversarial examples å¯¹æŠ—æ€§ç¤ºä¾‹, glitch tokens are inputs given to a model to intentionally make it malfunction and be inaccurate when delivering answers.


#### LLM Generation Inefficient

> From a systems perspective, LLM generation follows a memory-bound computational pattern with the main latency bottleneck arising from memory reads/writes rather than arithmetic computations. This issue is rooted in the inherently sequential nature of the auto-regressive decoding process. Each forward pass necessitates the transfer of the entire model's parameters from High-Bandwidth Memory (HBM) to the accelerator's compute units. This operation, while only producing a single token for each sample, fails to fully utilize the arithmetic computation capabilities of modern accelerators, resulting in inefficiency.


Before the rise of LLMs, a common mitigation for this inefficiency was to `simply increase the batch size, enabling the parallel production of more tokens`.


But the **situation becomes far more complicated with LLMs**.
- Increasing the batch size in this context not only introduces higher latency but also substantially `inflates è†¨èƒ€ the memory requirements` for the Transformer model's key-value cache.
  - This trade-off makes the use of large batches impractical for many applications where low latency is a critical requirement.

- also, for cost structures, as of September 2023, generation costs approximately 2x higher for GPT-4 and roughly 3x for Claude 2, compared to merely processing prompts.

![Screenshot 2023-09-20 at 17.51.23](/assets/img/post/Screenshot%202023-09-20%20at%2017.51.23.png)


##### Speculative æ¨æµ‹çš„ Decoding

> Given the challenges outlined, one appealing strategy to accelerate **text generation** is `more efficient computational utilizationâ€”specifically`, by `processing more tokens in parallel`.

speculative decoding

- The methodology employs a streamlined "draft" model to generate a batch of token candidates at each step quickly. These candidates are then validated by the original, full-scale language model to identify the most reasonable text continuations.

- The underlying logic hinges on an intriguing å¼•èµ·å…´è¶£çš„ assumption:

  - the draft model, although smaller, should be proficient enough to churn out sequences that the original model will find acceptable.

  - the draft model can rapidly produce token sequences while the original model efficiently vets å®¡æŸ¥ multiple tokens in parallel, which maximizing computational throughput.

  - Recent research indicates that with a well-tuned draft model, speculative decoding can cut latency by an impressive factor of up to 2.5x.

- However, the approach is not without its challenges:

  - Finding the Ideal Draft Model: Identifying a "small yet mighty" draft model that aligns well with the original model is easier said than done.

  - System Complexity: Hosting two distinct models in one system introduces layers of complexity, both computational and operational, especially in distributed settings.

  - Sampling Inefficiency: When doing sampling with speculative decoding, an importance sampling scheme needs to be used. This introduces additional overhead on generation, especially at higher sampling temperatures.


- These complexities and trade-offs have limited the broader adoption of speculative decoding techniques. So speculative decoding isn't widely adopted.

- Remark: We use speculative decoding to refer to those methods that require an independent draft model here. In a broader sense, our method can also be viewed as speculative decoding, while the draft model is entangled with the original model.




---


### LLM Subject


**Large language models**
- Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power.
- These foundation models with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve.

**foundation models (base models)**
- their relative size in terms of their parameters.
- ![Screenshot 2023-10-09 at 15.57.07](/assets/img/post/Screenshot%202023-10-09%20at%2015.57.07.png)
- **parameters**:
  - the model's `memory`.
  - the more parameters a model has, the more memory, the more `sophisticated` the tasks it can perform.
- By either using these models as they are or by applying fine tuning techniques to adapt them to the specific use case, you can rapidly `build customized solutions without the need to train a new model` from scratch.


**Augmenting LLMs**
- connecting LLM to external data sources or using them to invoke external APIs.
- use this ability to provide the model with information it doesn't know from its pre-training and to enable the model to power interactions with the real-world.


**Interact**
- `other machine learning and programming paradigms`: write computer code with formalized syntax to interact with `libraries and APIs`.
- `large language models`: able to take natural language or human written instructions and perform tasks much as a human would.

**prompt**
- The text that you pass to an LLM
- The space or memory that is available to the prompt is called the `context window`, and this is typically large enough for a few thousand words, but differs from model to model.

- example
  - ask the model to determine where Ganymede is located in the solar system.
  - The prompt is passed to the model, the model then predicts the next words, and because the prompt contained a question, this model generates an answer.
- The output of the model is called a `completion`, and the act of using the model to generate text is known as `inference`.
- The completion is comprised of the text contained in the original prompt, followed by the generated text.

![Screenshot 2023-10-10 at 00.25.13](/assets/img/post/Screenshot%202023-10-10%20at%2000.25.13.png)



**GPU**
- cloud class instance (from NVIDIA)
  - google colab
  - kaggle
  - amazon sagemaker
  - gradient
  - microsoft azure

> Tesla is graphics cards from NVIDIA for AI



**pyTorch**
- it does these heavy mathematical computation very easily with libraries
- it got a whole set of APIs and utilities that let you manipulate all of these different tensors



**tensors**
- a tensor is a computer data object (data structure) that represents numeric data,
- it could be floating point data values or data objects within data objects.
- 1d tensors (column)
- 2d tensors (xy)
- 3d tensors (xyz)
- 4d tensors (**cube**)
- 5d tensors
- 6d tensors


---

---

### Generative configuration

- Each model exposes a set of configuration parameters that can influence the model's output during inference.
  - training parameters: learned during training time.
  - configuration parameters: invoked at inference time and give control over things like the maximum number of tokens in the completion, and how creative the output is.


![Screenshot 2023-10-21 at 11.40.09](/assets/img/Screenshot%202023-10-21%20at%2011.40.09.png)


- **Max new tokens**: limit the number of tokens that the model will generate.
  - putting a cap on the number of times the model will go through the selection process.
  - the length of the completion is shorter
    - because another stop condition was reached, such as the model predicting and end of sequence token.
    - it's max new tokens, not a hard number of new tokens generated.
    - ![Screenshot 2023-10-21 at 11.46.18](/assets/img/Screenshot%202023-10-21%20at%2011.46.18.png)

  - The output from the `transformer's softmax layer` is a `probability distribution` across the entire dictionary of words that the model uses.
    - Here you can see a selection of words and their probability score next to them.
    - this is a list that carries on to the complete dictionary.

- **controls**

  - to generate text that's more natural, more creative and avoids repeating words, you need to use some other controls.
    - in some implementations, you may need to disable greedy and enable random sampling explicitly.
    - For example, the Hugging Face transformers implementation that we use in the lab requires that we set do sample to equal true.

  - `greedy decoding`
    - Most large language models by default will operate with `greedy decoding`.
    - the simplest form of next-word prediction
    - the model will always choose the word with the highest probability.
    - This method can work very well for short generation but is susceptible to repeated words or repeated sequences of words.

  - `Random sampling`
    - the easiest way to introduce some variability.
    - Instead of selecting the most probable word every time with random sampling, the model `chooses an output word at random using the probability distribution to weight the selection`.
    - depending on the setting, there is a possibility that the output may be too creative, producing words that cause the generation to wander off into topics or words that just don't make sense.

    - For example, in the illustration, the word banana has a probability score of 0.02. With random sampling, this equates to a 2% chance that this word will be selected. By using this sampling technique, we reduce the likelihood that words will be repeated.


    - ![Screenshot 2023-10-21 at 11.52.25](/assets/img/Screenshot%202023-10-21%20at%2011.52.25.png)


- **top k, top p sampling techniques**
  - help `limit the random sampling` and `increase the chance that the output will be sensible`.
    - With `top k`, you specify the number of tokens to randomly choose from
    - with `top p`, you specify the total probability that you want the model to choose from.

  - `top k` value:
    - limit the options while still allowing some variability
    - instructs the model to choose from only the k tokens with the highest probability.
    - In this example here, k is set to three, so you're restricting the model to choose from these three options. The model then selects from these options using the probability weighting and in this case, it chooses donut as the next word.
    - This method can help the model have some randomness while preventing the selection of highly improbable completion words.
    - This in turn makes the text generation more likely to sound reasonable and to make sense.
    - ![Screenshot 2023-10-21 at 12.04.16](/assets/img/Screenshot%202023-10-21%20at%2012.04.16.png)

  - `top p` setting:
    - limit the random sampling to the predictions whose combined probabilities do not exceed p.
    - For example, if you set p to equal 0.3, the options are cake and donut since their probabilities of 0.2 and 0.1 add up to 0.3. The model then uses the random probability weighting method to choose from these tokens.
    - ![Screenshot 2023-10-21 at 12.04.53](/assets/img/Screenshot%202023-10-21%20at%2012.04.53.png)


- **temperature**
  - control the randomness of the model output
  - can be adjusted to either increase or decrease randomness within the model output layer (softmax layer)

  - influences `the shape of the probability distribution` that the model calculates for the next token.
    - The temperature value is a `scaling factor that's applied within the final softmax layer of the model` that impacts `the shape of the probability distribution of the next token`.
    - In contrast to the `top k & p` parameters, changing the temperature actually alters the predictions that the model will make.

  - Broadly speaking, the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness.
  - ![Screenshot 2023-10-21 at 12.07.00](/assets/img/Screenshot%202023-10-21%20at%2012.07.00.png)

  - low value of temperature, say less than one,
    - the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words.
    - blue bars in the table show a probability bar chart turned on its side.
    - Most of the probability here is concentrated on the word cake. The model will select from this distribution using random sampling and the resulting text will be less random and will more closely follow the most likely word sequences that the model learned during training.


  - if set the temperature to a higher value, say, greater than one, then the model will calculate a broader flatter probability distribution for the next token.
    - Notice that in contrast to the blue bars, the probability is more evenly spread across the tokens.
    - This leads the model to generate text with a `higher degree of randomness and more variability` in the output compared to a `cool temperature setting`.
    - This can help you generate text that sounds more creative.

  - If leave the temperature value equal to one, this will leave the softmax function as default and the unaltered probability distribution will be used.


```py
generation_config = GenerationConfig(max_new_tokens=50)
# generation_config = GenerationConfig(max_new_tokens=10)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)

inputs = tokenizer(few_shot_prompt, return_tensors='pt')
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        generation_config=generation_config,
    )[0],
    skip_special_tokens=True
)

print(dash_line)
print(f'MODEL GENERATION - FEW SHOT:\n{output}')
print(dash_line)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
```



---

### Tuning å¾®è°ƒ

ç›®å‰å­¦æœ¯ç•Œä¸€èˆ¬å°†NLPä»»åŠ¡çš„å‘å±•åˆ†ä¸ºå››ä¸ªé˜¶æ®µï¼Œå³NLPå››èŒƒå¼:

- **ç¬¬ä¸€èŒƒå¼**: åŸºäºã€Œ`ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹`ã€çš„èŒƒå¼ï¼Œå¦‚TF-IDFç‰¹å¾+æœ´ç´ è´å¶æ–¯ç­‰æœºå™¨ç®—æ³•ï¼›
- **ç¬¬äºŒèŒƒå¼**: åŸºäºã€Œ`æ·±åº¦å­¦ä¹ æ¨¡å‹`ã€çš„èŒƒå¼ï¼Œå¦‚word2vecç‰¹å¾+LSTMç­‰æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œç›¸æ¯”äºç¬¬ä¸€èŒƒå¼ï¼Œæ¨¡å‹å‡†ç¡®æœ‰æ‰€æé«˜ï¼Œç‰¹å¾å·¥ç¨‹çš„å·¥ä½œä¹Ÿæœ‰æ‰€å‡å°‘ï¼›

- **ç¬¬ä¸‰èŒƒå¼**: åŸºäºã€Œ`é¢„è®­ç»ƒæ¨¡å‹+fine-tuning`ã€çš„èŒƒå¼ï¼Œå¦‚Bert+fine-tuningçš„NLPä»»åŠ¡ï¼Œç›¸æ¯”äºç¬¬äºŒèŒƒå¼ï¼Œæ¨¡å‹å‡†ç¡®åº¦æ˜¾è‘—æé«˜ï¼Œæ¨¡å‹ä¹Ÿéšä¹‹å˜å¾—æ›´å¤§ï¼Œä½†å°æ•°æ®é›†å°±å¯è®­ç»ƒå‡ºå¥½æ¨¡å‹ï¼›

- **ç¬¬å››èŒƒå¼**: åŸºäºã€Œ`é¢„è®­ç»ƒæ¨¡å‹+Prompt+é¢„æµ‹`ã€çš„èŒƒå¼ï¼Œå¦‚Bert+Promptçš„èŒƒå¼ç›¸æ¯”äºç¬¬ä¸‰èŒƒå¼ï¼Œæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„è®­ç»ƒæ•°æ®æ˜¾è‘—å‡å°‘ã€‚

åœ¨æ•´ä¸ªNLPé¢†åŸŸï¼Œä½ ä¼šå‘ç°æ•´ä¸ªå‘å±•æ˜¯æœç€ç²¾åº¦æ›´é«˜ã€å°‘ç›‘ç£ï¼Œç”šè‡³æ— ç›‘ç£çš„æ–¹å‘å‘å±•çš„ã€‚ä¸‹é¢æˆ‘ä»¬å¯¹ç¬¬ä¸‰èŒƒå¼ã€ç¬¬å››èŒƒå¼è¿›è¡Œè¯¦ç»†ä»‹ç»ã€‚

- æ€»çš„æ¥è¯´
  - åŸºäºFine-Tuningçš„æ–¹æ³•æ˜¯è®©é¢„è®­ç»ƒæ¨¡å‹å»è¿å°±ä¸‹æ¸¸ä»»åŠ¡ã€‚
  - åŸºäºPrompt-Tuningçš„æ–¹æ³•å¯ä»¥è®©ä¸‹æ¸¸ä»»åŠ¡å»è¿å°±é¢„è®­ç»ƒæ¨¡å‹ã€‚





---

#### Fine-Tuningï¼ˆå¾®è°ƒï¼‰

- Fine-Tuningæ˜¯ä¸€ç§è¿ç§»å­¦ä¹ ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼ŒFine-Tuningæ˜¯ç”¨äºå°†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹é€‚åº”äºç‰¹å®šä»»åŠ¡æˆ–é¢†åŸŸã€‚

- åŸºæœ¬æ€æƒ³æ˜¯é‡‡ç”¨å·²ç»åœ¨å¤§é‡æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç„¶ååœ¨å°è§„æ¨¡çš„ä»»åŠ¡ç‰¹å®šæ–‡æœ¬ä¸Šç»§ç»­è®­ç»ƒå®ƒã€‚

- Fine-Tuningçš„æ¦‚å¿µå·²ç»å­˜åœ¨å¾ˆå¤šå¹´ï¼Œå¹¶åœ¨å„ç§èƒŒæ™¯ä¸‹è¢«ä½¿ç”¨ã€‚
  - Fine-Tuningåœ¨NLPä¸­æœ€æ—©çš„å·²çŸ¥åº”ç”¨æ˜¯åœ¨ç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œå…¶ä¸­ç ”ç©¶äººå‘˜ä½¿ç”¨é¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¥åˆå§‹åŒ–ä¸€ä¸ªæ›´å°çš„ç½‘ç»œçš„æƒé‡ï¼Œç„¶åå¯¹å…¶è¿›è¡Œäº†ç‰¹å®šçš„ç¿»è¯‘ä»»åŠ¡çš„å¾®è°ƒã€‚

- ç»å…¸çš„Fine-Tuningæ–¹æ³•åŒ…æ‹¬å°†é¢„è®­ç»ƒæ¨¡å‹ä¸å°‘é‡ç‰¹å®šä»»åŠ¡æ•°æ®ä¸€èµ·ç»§ç»­è®­ç»ƒã€‚
  - åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡è¢«æ›´æ–°ï¼Œä»¥æ›´å¥½åœ°é€‚åº”ä»»åŠ¡ã€‚
  - æ‰€éœ€çš„Fine-Tuningé‡å–å†³äºé¢„è®­ç»ƒè¯­æ–™åº“å’Œä»»åŠ¡ç‰¹å®šè¯­æ–™åº“ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
  - å¦‚æœä¸¤è€…ç›¸ä¼¼ï¼Œå¯èƒ½åªéœ€è¦å°‘é‡çš„Fine-Tuningï¼Œå¦‚æœä¸¤è€…ä¸ç›¸ä¼¼ï¼Œåˆ™å¯èƒ½éœ€è¦æ›´å¤šçš„Fine-Tuningã€‚

- Bertæ¨¡å‹2018å¹´æ¨ªç©ºå‡ºä¸–ä¹‹åï¼Œå°†Fine-Tuningæ¨å‘äº†æ–°çš„é«˜åº¦ã€‚ä¸è¿‡ç›®å‰æ¥çœ‹ï¼ŒFine-Tuningé€æ¸é€€å‡ºäº†tuningç ”ç©¶çš„èˆå°ä¸­å¿ƒ: **LLMè“¬å‹ƒå‘å±•ï¼ŒFine-Tuningè¿™ç§å¤§è§„æ¨¡æ›´æ–°å‚æ•°çš„èŒƒå¼å±å®æ— æ³•ç«™ç¨³è„šè·Ÿ**ã€‚è€Œæ›´é€‚åº”äºLLMçš„tuningèŒƒå¼ï¼Œä¾¿æ˜¯æ¥ä¸‹æ¥æˆ‘ä»¬è¦ä»‹ç»çš„Prompt-Tuningã€Instruction-Tuningç­‰ã€‚

---

#### Prompt-Tuningï¼ˆæç¤ºå¾®è°ƒï¼‰

**prompt learning**:
- Prompt-Tuningå’ŒIn-context learningæ˜¯prompt learningçš„ä¸¤ç§æ¨¡å¼ã€‚

- In-context learning
  - æŒ‡åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡Œæ¨ç†æ—¶ï¼Œä¸éœ€è¦æå‰åœ¨ä¸‹æ¸¸ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå³ä¸æ”¹å˜é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å°±å¯å®ç°æ¨ç†ï¼Œ
  - å…¶è®¤ä¸ºè¶…å¤§è§„æ¨¡çš„æ¨¡å‹åªè¦é…åˆå¥½åˆé€‚çš„æ¨¡æ¿å°±å¯ä»¥æå¤§åŒ–åœ°å‘æŒ¥å…¶æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚
- å¸¸ç”¨çš„In-context learningæ–¹æ³•æœ‰`few-shotã€one-shotã€zero-shot`ï¼›

- Prompt-Tuning
  - æŒ‡åœ¨ä¸‹æ¸¸ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œæ¨ç†å‰ï¼Œéœ€è¦å¯¹å…¨éƒ¨æˆ–è€…éƒ¨åˆ†å‚æ•°è¿›è¡Œæ›´æ–°
  - **å…¨éƒ¨/éƒ¨åˆ†**çš„åŒºåˆ«å°±åœ¨äºé¢„è®­ç»ƒæ¨¡å‹å‚æ•°æ˜¯å¦æ”¹å˜ï¼ˆå…¶å®æœ¬è´¨ä¸Šçš„Prompt-Tuningæ˜¯ä¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹å‚æ•°çš„ï¼Œè¿™é‡Œæœ‰ä¸ªç‰¹ä¾‹æ–¹æ³•ç§°ä¸ºPrompt-Oriented Fine-Tuningï¼Œå…¶å®è¯¥æ–¹æ³•æ›´é€‚åˆç§°ä¸ºå‡çº§ç‰ˆçš„Fine-Tuningï¼Œåé¢ä¼šè¯¦ç»†ä»‹ç»è¿™ä¸ªæ–¹æ³•ï¼‰ã€‚

- æ— è®ºæ˜¯In-context learningè¿˜æ˜¯Prompt-Tuningï¼Œå®ƒä»¬çš„ç›®æ ‡éƒ½æ˜¯å°†ä¸‹æ¸¸ä»»åŠ¡è½¬æ¢ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œä»¥æ­¤æ¥å¹¿æ³›æ¿€å‘å‡ºé¢„è®­ç»ƒæ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚

- ä»¥äºŒåˆ†ç±»çš„æƒ…æ„Ÿåˆ†æä½œä¸ºä¾‹å­:
  - ç»™å®šä¸€ä¸ªå¥å­ `[CLS]` I like the Disney films very much. `[SEP]` ï¼Œ

  - ä¼ ç»Ÿçš„Fine-Tuningæ–¹æ³•:
    - å°†å…¶é€šè¿‡Bertè·å¾— `[CLS]`è¡¨å¾ä¹‹åå†å–‚å…¥æ–°å¢åŠ çš„`MLP`åˆ†ç±»å™¨è¿›è¡ŒäºŒåˆ†ç±»ï¼Œé¢„æµ‹è¯¥å¥å­æ˜¯ç§¯æçš„ï¼ˆpositiveï¼‰è¿˜æ˜¯æ¶ˆæçš„ï¼ˆnegativeï¼‰
    - å› æ­¤éœ€è¦ä¸€å®šé‡çš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒã€‚

  - è€ŒPrompt-Tuningåˆ™æ‰§è¡Œå¦‚ä¸‹æ­¥éª¤:

    - **æ„å»ºæ¨¡æ¿ï¼ˆTemplate Constructionï¼‰**:
      - é€šè¿‡äººå·¥å®šä¹‰ã€è‡ªåŠ¨æœç´¢ã€æ–‡æœ¬ç”Ÿæˆç­‰æ–¹æ³•ï¼Œç”Ÿæˆä¸ç»™å®šå¥å­ç›¸å…³çš„ä¸€ä¸ªå«æœ‰`[Mask]`æ ‡è®°çš„æ¨¡æ¿ã€‚ä¾‹å¦‚It was `[Mask]`
      - å¹¶æ‹¼æ¥åˆ°åŸå§‹çš„æ–‡æœ¬ä¸­ï¼Œè·å¾—Prompt-Tuningçš„è¾“å…¥: `[CLS]` I like the Disney films very much. It was `[Mask]`. `[SEP]`ã€‚
      - å°†å…¶å–‚å…¥Bæ¨¡å‹ä¸­ï¼Œå¹¶å¤ç”¨é¢„è®­ç»ƒå¥½çš„MLMåˆ†ç±»å™¨ï¼ˆåœ¨huggingfaceä¸­ä¸ºBertForMaskedLMï¼‰ï¼Œå³å¯ç›´æ¥å¾—åˆ°`[Mask]`é¢„æµ‹çš„å„ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼›

    - **æ ‡ç­¾è¯æ˜ å°„ï¼ˆLabel Word Verbalizerï¼‰**:
      - å› ä¸º`[Mask]`éƒ¨åˆ†æˆ‘ä»¬åªå¯¹éƒ¨åˆ†è¯æ„Ÿå…´è¶£ï¼Œå› æ­¤éœ€è¦å»ºç«‹ä¸€ä¸ªæ˜ å°„å…³ç³»ã€‚
      - ä¾‹å¦‚å¦‚æœ`[Mask]`é¢„æµ‹çš„è¯æ˜¯â€œgreatâ€ï¼Œåˆ™è®¤ä¸ºæ˜¯positiveç±»ï¼Œå¦‚æœæ˜¯â€œterribleâ€ï¼Œåˆ™è®¤ä¸ºæ˜¯negativeç±»ï¼›
      - ä¸åŒçš„å¥å­åº”è¯¥æœ‰ä¸åŒçš„templateå’Œlabel wordï¼Œå› ä¸ºæ¯ä¸ªå¥å­å¯èƒ½æœŸæœ›é¢„æµ‹å‡ºæ¥çš„label wordéƒ½ä¸åŒï¼Œå› æ­¤å¦‚ä½•æœ€å¤§åŒ–çš„å¯»æ‰¾å½“å‰ä»»åŠ¡æ›´åŠ åˆé€‚çš„templateå’Œlabel wordæ˜¯Prompt-Tuningéå¸¸é‡è¦çš„æŒ‘æˆ˜ï¼›

    - **è®­ç»ƒ**:
      - æ ¹æ®Verbalizerï¼Œåˆ™å¯ä»¥è·å¾—æŒ‡å®šlabel wordçš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨äº¤å‰ä¿¡æ¯ç†µè¿›è¡Œè®­ç»ƒã€‚
      - æ­¤æ—¶å› ä¸ºåªå¯¹é¢„è®­ç»ƒå¥½çš„MLM headè¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥é¿å…äº†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚

---


##### In-context learning (ICL)ï¼ˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰

- ICLåˆç§°ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæœ€æ—©æ˜¯åœ¨GPT-3[ã€ŠLanguage Models are Few-Shot Learnersã€‹](https://arxiv.org/pdf/2005.14165.pdf)ä¸­è¢«æå‡ºæ¥çš„ã€‚
- ICLçš„å…³é”®æ€æƒ³æ˜¯ä»ç±»æ¯”ä¸­å­¦ä¹ ã€‚

- ä¸‹å›¾ç»™å‡ºäº†ä¸€ä¸ªæè¿°è¯­è¨€æ¨¡å‹å¦‚ä½•ä½¿ç”¨ICLè¿›è¡Œå†³ç­–çš„ä¾‹å­ã€‚
  - é¦–å…ˆï¼ŒICLéœ€è¦ä¸€äº›ç¤ºä¾‹æ¥å½¢æˆä¸€ä¸ªæ¼”ç¤ºä¸Šä¸‹æ–‡ã€‚è¿™äº›ç¤ºä¾‹é€šå¸¸æ˜¯ç”¨è‡ªç„¶è¯­è¨€æ¨¡æ¿ç¼–å†™çš„ã€‚
  - ç„¶åICLå°†æŸ¥è¯¢çš„é—®é¢˜ï¼ˆå³ä½ éœ€è¦é¢„æµ‹æ ‡ç­¾çš„inputï¼‰å’Œä¸€ä¸ªä¸Šä¸‹æ–‡æ¼”ç¤ºï¼ˆä¸€äº›ç›¸å…³çš„casesï¼‰è¿æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆå¸¦æœ‰æç¤ºçš„è¾“å…¥ï¼ˆå¯ç§°ä¹‹ä¸ºpromptï¼‰ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œé¢„æµ‹ã€‚
  - å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸éœ€è¦ä½¿ç”¨åå‘æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°çš„è®­ç»ƒé˜¶æ®µçš„ç›‘ç£å­¦ä¹ ä¸åŒï¼ŒICLä¸éœ€è¦å‚æ•°æ›´æ–°ï¼Œå¹¶ç›´æ¥å¯¹é¢„å…ˆè®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆè¿™æ˜¯ä¸Prompt-Tuningä¸åŒçš„åœ°æ–¹ï¼ŒICLä¸éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­Prompt-Tuningæˆ–Fine-Tuningï¼‰ã€‚
  - å®ƒå¸Œæœ›æ¨¡å‹èƒ½è‡ªåŠ¨å­¦ä¹ éšè—åœ¨æ¼”ç¤ºä¸­çš„æ¨¡å¼ï¼Œå¹¶æ®æ­¤åšå‡ºæ­£ç¡®çš„é¢„æµ‹ã€‚
  - ![ICL](https://img-blog.csdnimg.cn/27eb61b06a0b4bbdbbf58b9cee910844.png#pic_center)

- use LLMs off the shelf (i.e., without any fine-tuning), then control their behavior through clever prompting and conditioning on private â€œcontextualâ€ data.

- itâ€™s usually easier than the alternative: training or fine-tuning the LLM itself.

- It also tends to outperform fine-tuning for relatively small datasetsâ€”since a specific piece of information needs to occur at least ~10 times in the training set before an LLM will remember it through fine-tuningâ€”and can incorporate new data in near real time.


- Example:
  - building a chatbot to answer questions about a set of legal documents.
    - `naive approach`: paste all the documents into a ChatGPT or GPT-4 prompt, then ask a question about them at the end. This may work for very small datasets, but it doesnâ€™t scale. The biggest GPT-4 model can only process ~50 pages of input text, and performance (measured by inference time and accuracy) degrades badly when approach the limit `context window`.
    - `In-context learning`: instead of sending all the documents with each LLM prompt, it sends only a handful of the most relevant documents. And the most relevant documents are determined with the help of . . . you guessed it . . . LLMs.

in-context learning method
- One shot: creating an initial prompt that states the task to be completed and includes a single example question with answer followed by a second question to be answered by the LLM

- **In-context learningçš„ä¼˜åŠ¿**:
  - è‹¥å¹²ç¤ºä¾‹ç»„æˆçš„æ¼”ç¤ºæ˜¯ç”¨è‡ªç„¶è¯­è¨€æ’°å†™çš„ï¼Œè¿™æä¾›äº†ä¸€ä¸ªè·ŸLLMäº¤æµçš„å¯è§£é‡Šæ€§æ‰‹æ®µï¼Œé€šè¿‡è¿™äº›ç¤ºä¾‹è·Ÿæ¨¡ç‰ˆè®©è¯­è¨€æ¨¡å‹æ›´å®¹æ˜“åˆ©ç”¨åˆ°äººç±»çš„çŸ¥è¯†ï¼›
  - ç±»ä¼¼äºäººç±»ç±»æ¯”å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹ï¼Œä¸¾ä¸€åä¸‰ï¼›
  - ç›¸æ¯”äºç›‘ç£å­¦ä¹ ï¼Œå®ƒä¸éœ€è¦æ¨¡å‹è®­ç»ƒï¼Œå‡å°äº†è®¡ç®—æ¨¡å‹é€‚é…æ–°ä»»åŠ¡çš„è®¡ç®—æˆæœ¬ï¼Œæ›´å®¹æ˜“åº”ç”¨åˆ°æ›´å¤šçœŸå®åœºæ™¯ã€‚

- **In-context learningçš„æµç¨‹**:
  - In-context learningå¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œåˆ†ä¸ºä½œç”¨äºtrainingè·Ÿinferenceé˜¶æ®µ:

  - **Training**:

    - åœ¨æ¨ç†å‰ï¼Œé€šè¿‡æŒç»­å­¦ä¹ è®©è¯­è¨€æ¨¡å‹çš„ICLèƒ½åŠ›å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸º**model warmup**ï¼ˆæ¨¡å‹é¢„çƒ­ï¼‰ï¼Œmodel warmupä¼šä¼˜åŒ–è¯­è¨€æ¨¡å‹å¯¹åº”å‚æ•°æˆ–è€…æ–°å¢å‚æ•°ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„Fine-Tuningï¼ŒFine-Tuningæ—¨åœ¨æå‡LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè€Œmodel warmupåˆ™æ˜¯æå‡æ¨¡å‹æ•´ä½“çš„ICLæ€§èƒ½ã€‚

    - **Supervised in-context training**: ä¸ºäº†å¢å¼ºICLçš„èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†

      - é€šè¿‡æ„å»ºin-contextè®­ç»ƒæ•°æ®ï¼Œè¿›è€Œè¿›è¡Œä¸€ç³»åˆ—æœ‰ç›‘ç£in-contextå¾®è°ƒä»¥åŠå¤šä»»åŠ¡è®­ç»ƒã€‚ç”±äºé¢„è®­ç»ƒç›®æ ‡å¯¹äºIn-context learningå¹¶ä¸æ˜¯æœ€ä¼˜çš„ï¼ŒSewon Minç­‰äººæå‡ºäº†ä¸€ç§æ–¹æ³• `MetaICL`[ã€ŠMetaICL: Learning to Learn In Contextã€‹](https://github.com/facebookresearch/MetaICL)ï¼Œä»¥æ¶ˆé™¤é¢„è®­ç»ƒå’Œä¸‹æ¸¸ICLä½¿ç”¨ä¹‹é—´çš„å·®è·ã€‚é¢„è®­ç»ƒLLMåœ¨å…·æœ‰æ¼”ç¤ºæ ·ä¾‹çš„å¹¿æ³›çš„ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™æé«˜äº†å…¶few-shotèƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œ`MetaICL`è·å¾—çš„æ€§èƒ½ä¸åœ¨52ä¸ªç‹¬åŠ›æ•°æ®é›†ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒç›¸å½“ã€‚

      - æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼Œå³æœ‰ç›‘ç£æŒ‡ä»¤å¾®è°ƒï¼Œä¹Ÿå°±æ˜¯åé¢è¦è®²åˆ°çš„Instruction-Tuningã€‚æŒ‡ä»¤å¾®è°ƒé€šè¿‡å¯¹ä»»åŠ¡æŒ‡ä»¤è¿›è¡Œè®­ç»ƒå¢å¼ºäº†LLMçš„ICLèƒ½åŠ›ã€‚ä¾‹å¦‚Googleæå‡ºçš„`FLAN`æ–¹æ³•[ã€ŠFINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERSã€‹](https://openreview.net/pdf?id=gEZrGCozdqR): é€šè¿‡åœ¨ç”±è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¨¡æ¿æ„å»ºçš„60å¤šä¸ªNLPæ•°æ®é›†ä¸Šè°ƒæ•´137Bå‚æ•°é‡çš„LaMDA-PTæ¨¡å‹ï¼ŒFLANæ–¹æ³•å¯ä»¥æ”¹å–„zero-shotå’Œfew-shot ICLæ€§èƒ½ï¼ˆå…·ä½“å¯å‚è€ƒ[Finetuned Language Models are Zero-shot Learners](https://zhuanlan.zhihu.com/p/538013856)ã€[ç¬”è®° - Instruction Tuning æ—¶ä»£çš„æ¨¡å‹](https://zhuanlan.zhihu.com/p/616830127)ï¼‰ã€‚ä¸MetaICLä¸ºæ¯ä¸ªä»»åŠ¡æ„å»ºè‹¥å¹²æ¼”ç¤ºæ ·ä¾‹ç›¸æ¯”ï¼ŒæŒ‡ä»¤å¾®è°ƒä¸»è¦è€ƒè™‘å¯¹ä»»åŠ¡çš„è§£é‡Šï¼Œå¹¶ä¸”æ˜“äºæ‰©å±•ã€‚

    - **Self-supervised in-context training**:

      - Supervised LearningæŒ‡çš„æ˜¯æœ‰ä¸€ä¸ªmodelï¼Œè¾“å…¥æ˜¯ x x xï¼Œè¾“å‡ºæ˜¯ y y yï¼Œè¦æœ‰labelï¼ˆæ ‡ç­¾ï¼‰æ‰å¯ä»¥è®­ç»ƒSupervised Learningï¼Œ

      - æ¯”å¦‚è®©æœºå™¨çœ‹ä¸€ç¯‡æ–‡ç« ï¼Œå†³å®šæ–‡ç« æ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼Œå¾—å…ˆæ‰¾ä¸€å¤§å †æ–‡ç« ï¼Œæ ‡æ³¨æ–‡ç« æ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼Œæ­£é¢è´Ÿé¢å°±æ˜¯labelã€‚

      - Self-Supervised Learningå°±æ˜¯æœºå™¨è‡ªå·±åœ¨æ²¡æœ‰labelçš„æƒ…å†µä¸‹ï¼Œæƒ³åŠæ³•åšSupervised Learningã€‚
        - æ¯”å¦‚æŠŠæ²¡æœ‰æ ‡æ³¨çš„è¯­æ–™åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œä¸€éƒ¨åˆ†ä½œä¸ºæ¨¡å‹çš„è¾“å‡ºï¼Œæ¨¡å‹çš„è¾“å‡ºå’Œlabelè¶Šæ¥è¿‘è¶Šå¥½ï¼Œå…·ä½“å‚è§[2022æå®æ¯…æœºå™¨å­¦ä¹ æ·±åº¦å­¦ä¹ å­¦ä¹ ç¬”è®°ç¬¬å››å‘¨â€“Self-Supervised Learning](https://blog.csdn.net/qq_45612705/article/details/124755797)ã€‚
        - å¼•ç”³åˆ°self-supervised in-context trainingï¼Œæ˜¯æ ¹æ®ICLçš„æ ¼å¼å°†åŸå§‹æ•°æ®è½¬æ¢æˆinput-outputçš„pairå¯¹æ•°æ®ååˆ©ç”¨å››ä¸ªè‡ªç›‘ç£ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬æ©`[Mask]`é¢„æµ‹ï¼Œåˆ†ç±»ä»»åŠ¡ç­‰ã€‚
            Â 
      - `Supervised ICT`è·Ÿ`self-supervised ICT`æ—¨åœ¨é€šè¿‡å¼•å…¥æ›´åŠ æ¥è¿‘äº`ICT`çš„è®­ç»ƒç›®æ ‡ä»è€Œ`ç¼©å°é¢„è®­ç»ƒè·ŸICLä¹‹é—´çš„å·®è·`ã€‚
        - æ¯”èµ·éœ€è¦ç¤ºä¾‹çš„In-context learningï¼Œåªæ¶‰åŠä»»åŠ¡æè¿°çš„Instruction-Tuningæ›´åŠ ç®€å•ä¸”å—æ¬¢è¿ã€‚
        - å¦å¤–ï¼Œåœ¨model warmupè¿™ä¸ªé˜¶æ®µï¼Œè¯­è¨€æ¨¡å‹åªéœ€è¦ä»å°‘é‡æ•°æ®è®­ç»ƒå°±èƒ½æ˜æ˜¾æå‡ICLèƒ½åŠ›ï¼Œä¸æ–­å¢åŠ ç›¸å…³æ•°æ®å¹¶ä¸èƒ½å¸¦æ¥ICLèƒ½åŠ›çš„æŒç»­æå‡ã€‚
        - ä»æŸç§è§’åº¦ä¸Šçœ‹ï¼Œè¿™äº›æ–¹æ³•é€šè¿‡æ›´æ–°æ¨¡å‹å‚æ•°å¯ä»¥æå‡ICLèƒ½åŠ›ä¹Ÿè¡¨æ˜äº†åŸå§‹çš„LLMå…·å¤‡è¿™ç§æ½œåŠ›ã€‚
        - è™½ç„¶ICLä¸è¦æ±‚model warmupï¼Œä½†æ˜¯ä¸€èˆ¬æ¨èåœ¨æ¨ç†å‰å¢åŠ ä¸€ä¸ªmodel warmupè¿‡ç¨‹
        - ICLæœ€åˆçš„å«ä¹‰æŒ‡çš„æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¶Œç°å‡ºä¸€ç§èƒ½åŠ›: ä¸éœ€è¦æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»…ä»…ä¿®æ”¹è¾“å…¥promptå³æ·»åŠ ä¸€äº›ä¾‹å­å°±å¯ä»¥æå‡æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚ICLç›¸æ¯”ä¹‹å‰éœ€è¦å¯¹æ¨¡å‹åœ¨æŸä¸ªç‰¹å®šä¸‹æ¸¸ä»»åŠ¡è¿›è¡ŒFine-Tuningå¤§å¤§èŠ‚çœäº†æˆæœ¬ã€‚ä¹‹åICLé—®é¢˜æ¼”å˜æˆç ”ç©¶æ€ä¹ˆæå‡æ¨¡å‹ä»¥å…·å¤‡æ›´å¥½æ›´é€šç”¨çš„ICLèƒ½åŠ›ï¼Œè¿™é‡Œå°±å¯ä»¥ç”¨ä¸Šä¹‹å‰Fine-Tuningçš„æ–¹å¼ï¼Œå³æŒ‡model warmupé˜¶æ®µå¯¹æ¨¡å‹æ›´æ–°å‚æ•°

    - **Inference**:

      - å¾ˆå¤šç ”ç©¶è¡¨æ˜LLMçš„ICLæ€§èƒ½ä¸¥é‡ä¾èµ–äºæ¼”ç¤ºç¤ºä¾‹çš„æ ¼å¼ï¼Œä»¥åŠç¤ºä¾‹é¡ºåºç­‰ç­‰ï¼Œåœ¨ä½¿ç”¨ç›®å‰å¾ˆå¤šLLMæ¨¡å‹æ—¶æˆ‘ä»¬ä¹Ÿä¼šå‘ç°ï¼Œåœ¨æ¨ç†æ—¶ï¼ŒåŒä¸€ä¸ªé—®é¢˜å¦‚æœåŠ ä¸Šä¸åŒçš„ç¤ºä¾‹ï¼Œå¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„æ¨¡å‹ç”Ÿæˆç»“æœã€‚

      - **Demonstration Selection**: å¯¹äºICLè€Œè¨€ï¼Œå“ªäº›æ ·æœ¬æ˜¯å¥½çš„ï¼Ÿè¯­è¨€æ¨¡å‹çš„è¾“å…¥é•¿åº¦æ˜¯æœ‰é™åˆ¶çš„ï¼Œå¦‚ä½•ä»ä¼—å¤šçš„æ ·æœ¬ä¸­æŒ‘é€‰å…¶ä¸­åˆé€‚çš„éƒ¨åˆ†ä½œä¸ºç¤ºä¾‹è¿™ä¸ªè¿‡ç¨‹éå¸¸é‡è¦ã€‚æŒ‰ç…§é€‰æ‹©çš„æ–¹æ³•ä¸»è¦å¯ä»¥åˆ†ä¸ºæ— ç›‘ç£è·Ÿæœ‰ç›‘ç£ä¸¤ç§ã€‚

        - **æ— ç›‘ç£æ–¹æ³•**: é¦–å…ˆå°±æ˜¯æ ¹æ®å¥å‘é‡è·ç¦»æˆ–è€…äº’ä¿¡æ¯ç­‰æ–¹å¼é€‰æ‹©è·Ÿå½“å‰è¾“å…¥xæœ€ç›¸ä¼¼çš„æ ·æœ¬ä½œä¸ºæ¼”ç¤ºç¤ºä¾‹ï¼Œå¦å¤–è¿˜æœ‰åˆ©ç”¨è‡ªé€‚åº”æ–¹æ³•å»é€‰æ‹©æœ€ä½³çš„ç¤ºä¾‹æ’åˆ—ï¼Œæœ‰çš„æ–¹æ³•è¿˜ä¼šè€ƒè™‘åˆ°æ¼”ç¤ºç¤ºä¾‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°½å¯èƒ½å»æé«˜ç¤ºä¾‹çš„å¤šæ ·æ€§ã€‚é™¤äº†ä¸Šè¿°è¿™äº›ä»äººå·¥æ’°å†™çš„æ ·æœ¬ä¸­é€‰æ‹©ç¤ºä¾‹çš„æ–¹å¼å¤–ï¼Œè¿˜å¯ä»¥åˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«å»ç”Ÿæˆåˆé€‚çš„æ¼”ç¤ºç¤ºä¾‹ã€‚

        - **ç›‘ç£æ–¹æ³•**: ç¬¬ä¸€ç§æ˜¯å…ˆåˆ©ç”¨æ— ç›‘ç£æ£€ç´¢å™¨å¬å›è‹¥å¹²ç›¸ä¼¼çš„æ ·æœ¬ï¼Œå†é€šè¿‡ç›‘ç£å­¦ä¹ è®­ç»ƒçš„Efficient Prompt Retrieverè¿›è¡Œæ‰“åˆ†ï¼Œä»è€Œç­›é€‰å‡ºæœ€åˆé€‚çš„æ ·æœ¬ã€‚æ­¤å¤–è¿˜æœ‰åŸºäºPrompt Tuningè·Ÿå¼ºåŒ–å­¦ä¹ çš„æ–¹å¼å»é€‰æ‹©æ ·æœ¬ã€‚

      - **Demonstration Ordering**: æŒ‘é€‰å®Œæ¼”ç¤ºç¤ºä¾‹åï¼Œå¦‚ä½•å¯¹å…¶è¿›è¡Œæ’åºä¹Ÿéå¸¸é‡è¦ã€‚æ’åºçš„æ–¹æ³•æ—¢æœ‰ä¸éœ€è¦è®­ç»ƒçš„ï¼Œä¹Ÿæœ‰æ ¹æ®ç¤ºä¾‹è·Ÿå½“å‰è¾“å…¥è·ç¦»è¿œè¿‘è¿›è¡Œæ’åºçš„ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è‡ªå®šä¹‰çš„ç†µæŒ‡æ ‡è¿›è¡Œé‡æ’ã€‚

      - **Demonstration Formatting**:
        - å¦‚ä½•è®¾è®¡æ¼”ç¤ºç¤ºä¾‹çš„æ ¼å¼ï¼Ÿæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯å°†ç¤ºä¾‹ä»¬çš„` ( x , y ) (x,y) (x,y)`å¯¹æŒ‰ç…§é¡ºåºç›´æ¥æ‹¼æ¥åˆ°ä¸€èµ·ã€‚
        - ä½†æ˜¯å¯¹äºå¤æ‚çš„æ¨ç†é—®é¢˜ï¼Œè¯­è¨€æ¨¡å‹å¾ˆéš¾ç›´æ¥æ ¹æ® x x xæ¨ç†å‡º y y yï¼Œè¿™ç§æ ¼å¼å°±ä¸é€‚ç”¨äº†ã€‚
        - å¦å¤–ï¼Œæœ‰çš„ç ”ç©¶æ—¨åœ¨è®¾è®¡æ›´å¥½çš„ä»»åŠ¡æŒ‡ä»¤instructionä½œä¸ºæ¼”ç¤ºå†…å®¹ï¼ˆå³Instruction-Tuningï¼‰ã€‚
        - å¯¹äºè¿™ä¸¤ç±»åœºæ™¯ï¼Œé™¤äº†äººå·¥æ’°å†™çš„æ–¹å¼å¤–ï¼Œè¿˜å¯ä»¥åˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«å»ç”Ÿæˆå¯¹åº”çš„æ¼”ç¤ºå†…å®¹ã€‚

- **In-context learningçš„æ¨¡å¼**:
  - In-context learningåŒ…æ‹¬ä¸‰ç§æ¨¡å¼ï¼Œåˆ†åˆ«ç§°ä½œfew-shotã€one-shotä»¥åŠzero-shotï¼Œ
  - ä¸‰è€…çš„ä¸»è¦åŒºåˆ«æ˜¯promptä¸­åŒ…å«çš„æ ·æœ¬ç¤ºä¾‹æ•°é‡
  - **Few-Shot**: å¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œæä¾›å¤šæ¡æ•°æ®æ ·ä¾‹ï¼Œè®ºæ–‡ä¸­æŒ‡å‡ºä¸€èˆ¬æ˜¯10-100æ¡ï¼›
  - **One-Shot**: few-shotçš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œåªæä¾›ä¸€æ¡æ•°æ®æ ·ä¾‹ï¼›
  - **Zero-Shot**: æ˜¯ä¸€ç§æç«¯æƒ…å†µï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¸æä¾›æ•°æ®æ ·ä¾‹ï¼Œåªæä¾›ä»»åŠ¡æè¿°ã€‚


å‚è€ƒè®ºæ–‡:
- [ã€ŠA Survey on In-context Learningã€‹](https://arxiv.org/pdf/2301.00234.pdf)
- [ã€ŠA Survey for In-context Learningã€‹ç¿»è¯‘](https://blog.csdn.net/qq_28385535/article/details/128789038)
- [ã€Šè¯‘ Prompt Engineering: å¾ªå¾ªå–„è¯±ã€‹](https://zhuanlan.zhihu.com/p/526299013)

---

##### Prompting and prompt engineering

å¯¹äºIn-context learningåŠåé¢ä¼šè®²åˆ°çš„Instruction-Tuningæ–¹æ³•æ¥è¯´ï¼Œå¦‚ä½•è®¾è®¡è¾“å…¥çš„promptæ˜¯å¾ˆé‡è¦çš„ä¸€ç‚¹

![Screenshot 2023-10-21 at 11.30.26](/assets/img/Screenshot%202023-10-21%20at%2011.30.26.png)

![Screenshot 2023-10-21 at 11.34.54](/assets/img/Screenshot%202023-10-21%20at%2011.34.54.png)

- failed with 5-6 example, fune tune the model
- Typically, above five or six shots, so full prompt and then completions, you really don't gain much after that. Either the model can do it or it can't do it

---

##### Pattern-Verbalizer-Pairï¼ˆPVPï¼‰

- ICLæ–¹æ³•æ˜¯åœ¨GPT-3ä¸­è¢«æå‡ºçš„ï¼Œè¿™ç±»æ–¹æ³•æœ‰ä¸€ä¸ªæ˜æ˜¾çš„ç¼ºé™·æ˜¯, å…¶å»ºç«‹åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸Šï¼Œæ­¤æ—¶çš„æ¨¡å‹å‚æ•°æ•°é‡é€šå¸¸è¶…è¿‡100äº¿ï¼Œåœ¨çœŸå®åœºæ™¯ä¸­å¾ˆéš¾åº”ç”¨ï¼Œå› æ­¤ä¼—å¤šç ”ç©¶è€…å¼€å§‹æ¢ç´¢GPT-3çš„è¿™å¥—æ€è·¯åœ¨å°è§„æ¨¡çš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Bertï¼‰ä¸Šè¿˜æ˜¯å¦é€‚ç”¨ï¼Ÿäº‹å®ä¸Šï¼Œè¿™å¥—æ–¹æ³•åœ¨å°è§„æ¨¡çš„è¯­è¨€æ¨¡å‹ä¸Šæ˜¯å¯è¡Œçš„ï¼Œä½†æ˜¯éœ€è¦æ³¨æ„:

  - æ¨¡å‹å‚æ•°è§„æ¨¡å°äº†ï¼Œpromptç›´æ¥ç”¨åœ¨zero-shotä¸Šæ•ˆæœä¼šä¸‹é™ï¼ˆè™½ç„¶GPT-3åœ¨zero-shotä¸Šæ•ˆæœä¹Ÿæ²¡æœ‰å¾ˆæƒŠè‰³ï¼Œè¿™ä¹Ÿæ˜¯åæ¥Instruction-Tuningå‡ºç°çš„åŸå› ï¼‰ï¼Œå› æ­¤éœ€è¦è€ƒè™‘å°†In-context learningåº”ç”¨åœ¨Fine-Tuningé˜¶æ®µï¼Œä¹Ÿå°±æ˜¯åé¢è¦è®²åˆ°çš„Prompt-Tuningã€‚

Pattern-Verbalizer-Pairï¼ˆPVPï¼‰
- å®ç°Prompt-Tuningçš„é‡è¦ç»„ä»¶
- Pattern-Verbalizer-Pair æ¨¡å¼æ¥æºäºå¤§åé¼é¼çš„PETæ¨¡å‹ï¼ŒPETï¼ˆPattern-Exploiting Trainingï¼‰[ã€ŠExploiting Cloze Questions for Few Shot Text Classification and Natural Language Inferenceã€‹](https://aclanthology.org/2021.eacl-main.20.pdf)ã€‚
  - ç”±äºåœ¨å®é™…ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¾€å¾€åªä¼šæ¥è§¦åˆ°å°‘é‡çš„labeled examples(few-shot learning)ï¼Œè€Œç›´æ¥å°†ç›‘ç£å­¦ä¹ è¿ç”¨åˆ°å°æ ·æœ¬å­¦ä¹ ä¼šä½¿å¾—æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œé’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡ä¸­æå‡ºäº†Pattern-Exploiting Training (PET)
  - ä½¿ç”¨natural language patternså°†input examplesè§„èŒƒä¸ºå®Œå‹å¡«ç©ºå½¢å¼çš„åŠç›‘ç£è®­ç»ƒæœºåˆ¶ã€‚
  - é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒæˆåŠŸåœ°åœ¨few-shot settingsä¸Šå°†task descriptionsä¸æ ‡å‡†ç›‘ç£å­¦ä¹ ç»“åˆã€‚

  - å…·ä½“çš„æ­¥éª¤æ˜¯:
    - æ„å»ºä¸€ç»„patternï¼Œå¯¹äºæ¯ä¸€ä¸ªpattern, ä¼šä½¿ç”¨ä¸€ä¸ªPLMåœ¨å°æ ·æœ¬è®­ç»ƒé›†ä¸Šè¿›è¡ŒFine-Tuningï¼›
    - è®­ç»ƒåçš„æ‰€æœ‰æ¨¡å‹çš„é›†åˆä¼šè¢«ç”¨æ¥åœ¨å¤§è§„æ¨¡unlabeled datasetæ ‡æ³¨soft labelsï¼›
    - åœ¨soft labelsæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ ‡å‡†åˆ†ç±»å™¨ã€‚

  - å¦å¤–åœ¨è¯¥è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºï¼Œåœ¨æ¯ä¸€ä¸ªPLMä¸Šåªè¿›è¡Œä¸€æ¬¡å¾®è°ƒ+soft labelsç”Ÿæˆï¼Œé€šå¸¸å¾—åˆ°çš„æ–°çš„æ•°æ®é›†ï¼ˆå³ç”¨soft labelsæ ‡è®°çš„unlabeled datasetï¼‰ä¼šæœ‰å¾ˆå¤šé”™è¯¯çš„æ•°æ®ï¼Œå› æ­¤æ‰©å±•æå‡ºiPETæ¨¡å‹ï¼ˆIterative PETï¼‰ï¼Œå³æ·»åŠ äº†è¿­ä»£è¿‡ç¨‹:
  - é¦–å…ˆéšæœºä»é›†æˆçš„é¢„è®­ç»ƒæ¨¡å‹é›†åˆä¸­æŠ½å–éƒ¨åˆ†é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨æœªæ ‡æ³¨æ•°æ®é›†ï¼ˆunlabeled datasetï¼‰D ä¸Šæ ‡æ³¨æ•°æ®ï¼Œå¹¶æ‰©å¢åˆ°åˆå§‹æœ‰æ ‡ç­¾æ•°æ®é›† T ä¸Šï¼Œå…¶æ¬¡å†æ ¹æ®æ‰©å¢åçš„ T åˆ†åˆ«å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸Šè¿°è¿‡ç¨‹ä¸€ç›´è¿­ä»£å¤šæ¬¡[^è¿­ä»£å¤šæ¬¡]

[^è¿­ä»£å¤šæ¬¡]: è¿­ä»£å¤šæ¬¡, https://blog.csdn.net/qq_39439006/article/details/130796416

- [è®ºæ–‡è§£è¯»: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://wjn1996.blog.csdn.net/article/details/120788059)
- [è®ºæ–‡é˜…è¯»: PETç³»åˆ—](https://zhuanlan.zhihu.com/p/440692428)ã€‚


PETæœ€æ ¸å¿ƒçš„éƒ¨åˆ†Pattern-Verbalizer-Pairï¼ˆPVPï¼‰ï¼ŒPETè®¾è®¡äº†ä¸¤ä¸ªå¾ˆé‡è¦çš„ç»„ä»¶:

- **Patternï¼ˆTemplateï¼‰**:
  - è®°ä½œ T ï¼Œå³ä¸Šæ–‡æåˆ°çš„Templateï¼Œå…¶ä¸ºé¢å¤–æ·»åŠ çš„å¸¦æœ‰`[mask]`æ ‡è®°çš„çŸ­æ–‡æœ¬ï¼Œé€šå¸¸ä¸€ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªPatternï¼ˆå› ä¸ºæˆ‘ä»¬å¸Œæœ›åªæœ‰1ä¸ªè®©æ¨¡å‹é¢„æµ‹çš„`[mask]`æ ‡è®°ï¼‰ã€‚
  - ç”±äºä¸åŒçš„ä»»åŠ¡ã€ä¸åŒçš„æ ·æœ¬å¯èƒ½ä¼šæœ‰å…¶æ›´åŠ åˆé€‚çš„patternï¼Œå› æ­¤å¦‚ä½•æ„å»ºåˆé€‚çš„patternæ˜¯Prompt-Tuningçš„ç ”ç©¶ç‚¹ä¹‹ä¸€ï¼›

- **Verbalizer**:
  - è®°ä½œ Vï¼Œå³æ ‡ç­¾è¯çš„æ˜ å°„ï¼Œå¯¹äºå…·ä½“çš„åˆ†ç±»ä»»åŠ¡ï¼Œéœ€è¦é€‰æ‹©æŒ‡å®šçš„æ ‡ç­¾è¯ï¼ˆlabel wordï¼‰ã€‚
  - ä¾‹å¦‚æƒ…æ„Ÿåˆ†æä¸­ï¼Œæˆ‘ä»¬æœŸæœ›Verbalizerå¯èƒ½æ˜¯:  V ( positive ) = great, V ( negative ) = terribleï¼ˆpositiveå’Œnegativeæ˜¯ç±»æ ‡ç­¾ï¼‰ã€‚
  - åŒæ ·ï¼Œä¸åŒçš„ä»»åŠ¡æœ‰å…¶ç›¸åº”çš„label wordï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒVerbalizerçš„æ„å»ºéœ€è¦å–å†³äºå¯¹åº”çš„Patternã€‚å› æ­¤å¦‚ä½•æ„å»ºVerbalizeræ˜¯å¦ä¸€ä¸ªç ”ç©¶æŒ‘æˆ˜ã€‚
  - ä¸Šè¿°ä¸¤ä¸ªç»„ä»¶å³ä¸ºPattern-Verbalizer-Pairï¼ˆPVPï¼‰ï¼Œä¸€èˆ¬è®°ä½œ P = ( T , V ) åœ¨åç»­çš„å¤§å¤šæ•°ç ”ç©¶ä¸­å‡é‡‡ç”¨è¿™ç§PVPç»„ä»¶ã€‚å­¦åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬é¢ä¸´çš„æœ€å¤§ç–‘é—®: å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ä½•æŒ‘é€‰åˆé€‚çš„Patternå’ŒVerbalizerï¼Ÿè‡ª2020å¹´åº•è‡³ä»Šï¼Œå­¦æœ¯ç•Œå·²ç»æ¶Œç°å‡ºå„ç§æ–¹æ¡ˆè¯•å›¾æ¢ç´¢å¦‚ä½•è‡ªåŠ¨æ„å»ºPVPã€‚å…¶å®ä¹Ÿè®¸åœ¨å¤§å¤šæ•°äººä»¬çš„å°è±¡ä¸­ï¼Œåˆé€‚çš„Patternæ‰æ˜¯å½±å“ä¸‹æ¸¸ä»»åŠ¡æ•ˆæœçš„å…³é”®ï¼ŒVerbalizerå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“å¹¶ä¸å¤§ï¼Œè€Œä¸‹é¢è¿™ä¸ªå®éªŒä¾¿å¾ˆå¥½çš„è¯æ˜äº†Verbalizerçš„ä½œç”¨: å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»¥SST-2ä¸ºä¾‹ï¼Œç›¸åŒçš„æ¨¡æ¿æ¡ä»¶ä¸‹ï¼Œä¸åŒçš„label wordå¯¹åº”çš„æŒ‡æ ‡å·®å¼‚å¾ˆå¤§ã€‚
  - ![Verbalizerè®¾è®¡å¯¹æ¯”å®éªŒ](https://img-blog.csdnimg.cn/ed70449e04b643529a4d4be71a6c074b.png#pic_center)
  - æ„å»ºVerbalizerçš„æ–¹æ³•ä¹Ÿæœ‰å¾ˆå¤š [Prompt-Tuningâ€”â€”æ·±åº¦è§£è¯»ä¸€ç§æ–°çš„å¾®è°ƒèŒƒå¼](https://blog.csdn.net/qq_36426650/article/details/120607050)ï¼Œé‡Œé¢è¯´æ˜çš„æ¯”è¾ƒè¯¦ç»†ã€‚

---

##### Prompt-Tuning

Prompt-Tuningæ˜¯ç”¨æ¥è‡ªåŠ¨æ„å»ºpatternçš„æ–¹æ³•ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ ¹æ®ä½¿ç”¨åœºæ™¯çš„ä¸åŒï¼Œåˆ†åˆ«ä»‹ç»å‡ ç§æˆç†Ÿçš„Prompt-Tuningæ–¹æ³•ã€‚

- **Prompt-Oriented Fine-Tuning**:
  - è¿™ä¸ªå°±æ˜¯å‰é¢æåˆ°çš„éœ€è¦æ›´æ–°å…¨éƒ¨å‚æ•°ï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼‰çš„Prompt-Tuningæ–¹æ³•ã€‚
  - è®­ç»ƒæ–¹æ³•çš„æœ¬è´¨æ˜¯`å°†ç›®æ ‡ä»»åŠ¡`è½¬æ¢ä¸º`é€‚åº”é¢„è®­ç»ƒæ¨¡å‹`çš„`é¢„è®­ç»ƒä»»åŠ¡`ï¼Œä»¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹çš„å­¦ä¹ ä½“ç³»ã€‚
  - ä¾‹å¦‚æˆ‘ä»¬åœ¨Bertæ¨¡å‹ä¸Šåšæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼Œ
    - æ­£å¸¸çš„Fine-Tuningæµç¨‹ï¼Œæ˜¯å°†è®­ç»ƒæ–‡æœ¬ç»è¿‡Bertç¼–ç åï¼Œç”Ÿæˆå‘é‡è¡¨å¾ï¼Œå†åˆ©ç”¨è¯¥å‘é‡è¡¨å¾ï¼Œè¿æ¥å…¨è¿æ¥å±‚ï¼Œå®ç°æœ€ç»ˆçš„æƒ…æ„Ÿç±»åˆ«è¯†åˆ«ã€‚
    - è¿™ç§æ–¹å¼å­˜åœ¨ä¸€ä¸ªæ˜¾å¼çš„å¼Šç«¯: é¢„è®­ç»ƒä»»åŠ¡ä¸ä¸‹æ¸¸ä»»åŠ¡å­˜åœ¨**gap**ï¼Œæˆ‘ä»¬çŸ¥é“Bertçš„é¢„è®­ç»ƒä»»åŠ¡åŒ…æ‹¬ä¸¤ä¸ª: MLMä¸NSPï¼ˆå…·ä½“å¯å‚è€ƒ[Berté¢„è®­ç»ƒçš„ä»»åŠ¡MLMå’ŒNSP](https://zhuanlan.zhihu.com/p/562352255)ï¼‰ï¼Œç®€å•æ¥è¯´ï¼ŒMLMä»»åŠ¡æ˜¯é€šè¿‡åˆ†ç±»æ¨¡å‹è¯†åˆ«è¢«MASKæ‰çš„è¯ï¼Œç±»åˆ«å¤§å°å³ä¸ºæ•´ä¸ªè¯è¡¨å¤§å°ï¼›NSPä»»åŠ¡æ˜¯é¢„æµ‹ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼›è€ŒPrompt-Oriented Fine-Tuningè®­ç»ƒæ–¹æ³•ï¼Œæ˜¯å°†æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡è½¬æ¢ä¸ºç±»ä¼¼äºMLMä»»åŠ¡çš„`[Mask]`é¢„æµ‹ä»»åŠ¡ï¼Œå…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºå¦‚ä¸‹çš„promptæ–‡æœ¬: `prompt = It was [MASK].`ï¼Œå°†promptæ–‡æœ¬ä¸è¾“å…¥textæ–‡æœ¬`text = The film is attractive.`è¿›è¡Œæ‹¼æ¥ç”Ÿæˆ`It was [MASK].The film is attractive.`ï¼Œè¾“å…¥è‡³é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè®­ç»ƒä»»åŠ¡ç›®æ ‡å’ŒMLMä»»åŠ¡çš„ç›®æ ‡ä¸€è‡´ï¼Œå³è¯†åˆ«è¢«`[Mask]`æ‰çš„è¯ã€‚
    - é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å°†ä¸‹æ¸¸ä»»åŠ¡è½¬æ¢ä¸ºå’Œé¢„è®­ç»ƒä»»åŠ¡è¾ƒä¸ºä¸€è‡´çš„ä»»åŠ¡ï¼Œå·²æœ‰å®éªŒè¯æ˜ï¼ŒPrompt-Oriented Fine-Tuningç›¸å¯¹äºå¸¸è§„çš„Fine-Tuningï¼Œæ•ˆæœç¡®å®ä¼šå¾—åˆ°æå‡ï¼ˆ[Promptè¿›è¡Œæƒ…æ„Ÿåˆ†ç±»](https://blog.csdn.net/wf19971210/article/details/120543015)ï¼‰ã€‚

- é€šè¿‡ä»¥ä¸Šæè¿°æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼ŒPrompt-Oriented Fine-Tuningæ–¹æ³•ä¸­ï¼Œé¢„è®­ç»ƒæ¨¡å‹å‚æ•°æ˜¯å¯å˜çš„ã€‚å…¶å®å°†Prompt-Oriented Fine-Tuningæ–¹æ³•æ”¾åœ¨Prompt-Tuningè¿™ä¸ªéƒ¨åˆ†åˆç†ä¹Ÿä¸åˆç†ï¼Œå› ä¸ºå®ƒå…¶å®æ˜¯Prompt-Tuning+Fine-Tuningçš„ç»“åˆä½“ï¼Œå°†å®ƒè§†ä¸ºFine-Tuningçš„å‡çº§ç‰ˆæ˜¯æœ€åˆé€‚çš„ã€‚Prompt-Oriented Fine-Tuningæ–¹æ³•åœ¨Bertç±»ç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†æ˜¯éšç€æ¨¡å‹è¶Šæ¥è¶Šå¤§ï¼Œå¦‚æœæ¯æ¬¡é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œéƒ½éœ€è¦æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œèµ„æºæˆæœ¬åŠæ—¶é—´æˆæœ¬éƒ½ä¼šå¾ˆé«˜ï¼Œå› æ­¤åç»­é™†ç»­æå‡ºäº†ä¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œå•çº¯åªé’ˆå¯¹promptè¿›è¡Œè°ƒä¼˜çš„æ–¹æ³•ï¼Œä¾‹å¦‚**Hard Prompt**å’Œ**Soft Prompt**ã€‚

- è¿™é‡Œå†ç»™å‡ºä¸€äº›å¸¸è§ä¸‹æ¸¸ä»»åŠ¡çš„promptè®¾è®¡:

![å¸¸è§ä»»åŠ¡çš„Promptè®¾è®¡](https://img-blog.csdnimg.cn/4a5f862ee964472189079e88a73c23f3.png#pic_center)

- **Hard Prompt & Soft Prompt**: æ‰¿æ¥ä¸Šæ–‡ï¼ŒHard Promptå’ŒSoft Promptçš„æå‡ºï¼Œæ˜¯ä¸ºäº†è§£å†³é¢„è®­ç»ƒæ¨¡å‹è¿‡å¤§ï¼Œéš¾ä»¥é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè®­ç»ƒçš„ç—›ç‚¹ã€‚ç›®å‰å¸¸è§çš„Hard Promptå’ŒSoft Promptæ–¹æ³•ï¼Œåˆ†ä¸ºä»¥ä¸‹äº”ç§:

    - **äººå·¥æ„å»ºï¼ˆManual Templateï¼‰**: æœ€ç®€å•çš„æ„å»ºæ¨¡æ¿æ–¹æ³•ï¼›
    - **å¯å‘å¼æ³•ï¼ˆHeuristic-based Templateï¼‰**: é€šè¿‡è§„åˆ™ã€å¯å‘å¼æœç´¢ç­‰æ–¹æ³•æ„å»ºåˆé€‚çš„æ¨¡æ¿ï¼›
    - **ç”Ÿæˆï¼ˆGenerationï¼‰**: æ ¹æ®ç»™å®šçš„ä»»åŠ¡è®­ç»ƒæ•°æ®ï¼ˆé€šå¸¸æ˜¯å°æ ·æœ¬åœºæ™¯ï¼‰ï¼Œç”Ÿæˆå‡ºåˆé€‚çš„æ¨¡æ¿ï¼›
    - **è¯å‘é‡å¾®è°ƒï¼ˆWord Embeddingï¼‰**: æ˜¾å¼åœ°å®šä¹‰ç¦»æ•£å­—ç¬¦çš„æ¨¡æ¿ï¼Œä½†åœ¨è®­ç»ƒæ—¶è¿™äº›æ¨¡æ¿å­—ç¬¦çš„è¯å‘é‡å‚ä¸æ¢¯åº¦ä¸‹é™ï¼Œåˆå§‹å®šä¹‰çš„ç¦»æ•£å­—ç¬¦ç”¨äºä½œä¸ºå‘é‡çš„åˆå§‹åŒ–ï¼›
    - **ä¼ªæ ‡è®°ï¼ˆPseudo Tokenï¼‰**: ä¸æ˜¾å¼åœ°å®šä¹‰ç¦»æ•£çš„æ¨¡æ¿ï¼Œè€Œæ˜¯å°†æ¨¡æ¿ä½œä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚
- **Hard Prompt**: å‰é¢ä¸‰ç§ç§°ä¸ºç¦»æ•£çš„æ¨¡æ¿æ„å»ºæ³•ï¼ˆè®°ä½œHard Templateã€Hard Promptã€Discrete Templateã€Discrete Promptï¼‰ï¼Œå…¶æ—¨åœ¨ç›´æ¥ä¸åŸå§‹æ–‡æœ¬æ‹¼æ¥æ˜¾å¼ç¦»æ•£çš„å­—ç¬¦ï¼Œä¸”åœ¨è®­ç»ƒä¸­å§‹ç»ˆä¿æŒä¸å˜ã€‚è¿™é‡Œçš„ä¿æŒä¸å˜æ˜¯æŒ‡è¿™äº›ç¦»æ•£å­—ç¬¦çš„è¯å‘é‡ï¼ˆWord Embeddingï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒå›ºå®šã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œç¦»æ•£æ³•ä¸éœ€è¦å¼•å…¥ä»»ä½•å‚æ•°ã€‚ä¸»è¦é€‚ç”¨åœºæ™¯æ˜¯GPT-3ç±»ç›¸å¯¹è¾ƒå¤§çš„æ¨¡å‹ï¼ŒBertç±»ç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨ï¼Œåªæ˜¯ä¸ªäººè§‰å¾—Bertç­‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒçš„æˆæœ¬å¹¶ä¸æ˜¯å¾ˆé«˜ï¼Œå®Œå…¨å¯ä»¥åŒæ—¶å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚ä¸Šè¿°ä¸‰ç§Hard Promptæ–¹æ³•ï¼Œå®é™…åœºæ™¯ä¸­ç”¨çš„æ¯”è¾ƒå°‘ï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€ä»‹ç»äº†ï¼Œå¤§å®¶æœ‰å…´è¶£å¯ä»¥å‚è€ƒ[Prompt-Tuningâ€”â€”æ·±åº¦è§£è¯»ä¸€ç§æ–°çš„å¾®è°ƒèŒƒå¼](https://blog.csdn.net/qq_36426650/article/details/120607050)ã€‚

- **Soft Prompt**: åé¢ä¸¤ç§åˆ™è¢«ç§°ä¸ºè¿ç»­çš„æ¨¡æ¿æ„å»ºæ³•ï¼ˆè®°ä½œSoft Templateã€Soft Promptã€Continuous Templateã€Continuous Promptï¼‰ï¼Œå…¶æ—¨åœ¨è®©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®å…·ä½“çš„ä¸Šä¸‹æ–‡è¯­ä¹‰å’Œä»»åŠ¡ç›®æ ‡å¯¹æ¨¡æ¿å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚åè§‚Hard Promptæ–¹æ³•ï¼Œä¸è®ºæ˜¯å¯å‘å¼æ–¹æ³•ï¼Œè¿˜æ˜¯é€šè¿‡ç”Ÿæˆçš„æ–¹æ³•ï¼Œéƒ½éœ€è¦ä¸ºæ¯ä¸€ä¸ªä»»åŠ¡å•ç‹¬è®¾è®¡å¯¹åº”çš„æ¨¡æ¿ï¼Œå› ä¸ºè¿™äº›æ¨¡æ¿éƒ½æ˜¯å¯è¯»çš„ç¦»æ•£çš„tokenï¼Œè¿™å¯¼è‡´å¾ˆéš¾å¯»æ‰¾åˆ°æœ€ä½³çš„æ¨¡æ¿ã€‚å¦å¤–ï¼Œå³ä¾¿æ˜¯åŒä¸€ä¸ªä»»åŠ¡ï¼Œä¸åŒçš„å¥å­ä¹Ÿä¼šæœ‰å…¶æ‰€è°“æœ€ä½³çš„æ¨¡æ¿ï¼Œè€Œä¸”æœ‰æ—¶å€™ï¼Œå³ä¾¿æ˜¯äººç±»ç†è§£çš„ç›¸ä¼¼çš„æ¨¡æ¿ï¼Œä¹Ÿä¼šå¯¹æ¨¡å‹é¢„æµ‹ç»“æœäº§ç”Ÿå¾ˆå¤§å·®å¼‚ã€‚ä¾‹å¦‚ä¸‹å›¾ï¼Œä»¥SNLIæ¨æ–­ä»»åŠ¡ä¸ºä¾‹ï¼Œä»…ä»…åªæ˜¯ä¿®æ”¹äº†æ¨¡æ¿ï¼Œæµ‹è¯•ç»“æœå·®å¼‚å¾ˆæ˜æ˜¾ï¼Œå› æ­¤ç¦»æ•£çš„æ¨¡æ¿å­˜åœ¨æ–¹å·®å¤§ã€ä¸ç¨³å®šç­‰é—®é¢˜ã€‚![Hard Promptè®¾è®¡å¯¹æ¯”å®éªŒ](https://img-blog.csdnimg.cn/4cc12829dc2b4c2a920c22b447435821.png#pic_center)
    â€ƒâ€ƒå¦‚ä½•é¿å…è¿™ç§é—®é¢˜å‘¢ï¼ŒSoft Promptæ–¹æ³•ä¾¿æ˜¯æ¥è§£å†³è¿™ç§é—®é¢˜çš„ï¼Œå…¶å°†æ¨¡æ¿è½¬æ¢ä¸ºå¯ä»¥è¿›è¡Œä¼˜åŒ–çš„è¿ç»­å‘é‡ï¼Œæ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼åœ°æŒ‡å®šè¿™äº›æ¨¡æ¿ä¸­å„ä¸ªtokenå…·ä½“æ˜¯ä»€ä¹ˆï¼Œåªéœ€è¦åœ¨è¯­ä¹‰ç©ºé—´ä¸­è¡¨ç¤ºä¸€ä¸ªå‘é‡å³å¯ï¼Œè¿™æ ·ï¼Œä¸åŒçš„ä»»åŠ¡ã€æ•°æ®å¯ä»¥è‡ªé€‚åº”åœ°åœ¨è¯­ä¹‰ç©ºé—´ä¸­å¯»æ‰¾è‹¥å¹²åˆé€‚çš„å‘é‡ï¼Œæ¥ä»£è¡¨æ¨¡æ¿ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œç›¸è¾ƒäºæ˜¾å¼çš„tokenï¼Œè¿™ç±»tokenç§°ä¸ºä¼ªæ ‡è®°ï¼ˆPseudo Tokenï¼‰ã€‚ä¸‹é¢ç»™å‡ºåŸºäºSoft Promptçš„æ¨¡æ¿å®šä¹‰:

    > å‡è®¾é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥å¥å­ x x xï¼Œè¿ç»­æç¤ºçš„æ¨¡æ¿å¯ä»¥å®šä¹‰ä¸º:
    > T = \[ x \] , \[ v 1 \] , \[ v 2 \] ï¼Œ â€¦ ï¼Œ \[ v m \] \[ M A S K \] Â  \\mathcal{T} =\[x\],\[v\_{1}\],\[v\_{2}\]ï¼Œâ€¦ï¼Œ\[v\_{m}\]`[Mask]`\\ T\=\[x\],\[v1â€‹\],\[v2â€‹\]ï¼Œâ€¦ï¼Œ\[vmâ€‹\]`[Mask]`Â å…¶ä¸­ \[ v 1 \] \[v\_{1}\] \[v1â€‹\]åˆ™æ˜¯ä¼ªæ ‡è®°ï¼Œå…¶ä»…ä»£è¡¨ä¸€ä¸ªæŠ½è±¡çš„tokenï¼Œå¹¶æ²¡æœ‰å®é™…çš„å«ä¹‰ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå‘é‡ã€‚

    â€ƒâ€ƒ**æ€»ç»“æ¥è¯´**: Soft Promptæ–¹æ³•ï¼Œæ˜¯å°†æ¨¡æ¿å˜ä¸ºå¯è®­ç»ƒçš„å‚æ•°ï¼Œä¸åŒçš„æ ·æœ¬å¯ä»¥åœ¨è¿ç»­çš„å‘é‡ç©ºé—´ä¸­å¯»æ‰¾åˆé€‚çš„ä¼ªæ ‡è®°ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œè¿ç»­æ³•éœ€è¦å¼•å…¥å°‘é‡çš„å‚æ•°å¹¶åœ¨è®­ç»ƒæ—¶è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä½†é¢„è®­ç»ƒæ¨¡å‹å‚æ•°æ˜¯ä¸å˜çš„ï¼Œå˜çš„æ˜¯prompt tokenå¯¹åº”çš„è¯å‘é‡ï¼ˆWord Embeddingï¼‰è¡¨å¾åŠå…¶ä»–å¼•å…¥çš„å°‘é‡å‚æ•°ã€‚ä¸»è¦é€‚ç”¨åœºæ™¯åŒHard Promptä¸€è‡´ã€‚ç›®å‰å…·æœ‰ä»£è¡¨æ€§çš„ä¸‰ç§Soft Promptæ–¹æ³•å¦‚ä¸‹ï¼Œä¸‹é¢æˆ‘ä»¬è¿›è¡Œé€ä¸€ä»‹ç»:

    - **Parameter-Efficient Prompt Tuning**: è¯¥æ–¹æ³•ç‡å…ˆæå‡ºäº†ä¼ªæ ‡è®°å’Œè¿ç»­æç¤ºçš„æ¦‚å¿µï¼Œæ”¯æŒæ¨¡å‹èƒ½å¤ŸåŠ¨æ€åœ°å¯¹æ¨¡æ¿åœ¨è¯­ä¹‰ç©ºé—´å†…è¿›è¡Œè°ƒæ•´ã€‚ä¸»è¦é’ˆå¯¹çš„æ˜¯NLUä»»åŠ¡ï¼Œå½¢å¼åŒ–çš„æè¿°å¦‚ä¸‹:

        > ç»™å®š n n nä¸ªtokenï¼Œè®°ä½œ x 1 , . . . , x n x\_{1}, ..., x\_{n} x1â€‹,...,xnâ€‹ï¼Œé€šè¿‡ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„embedding tableï¼Œå°† n n nä¸ªtokenè¡¨å¾ä¸ºå‘é‡çŸ©é˜µ X e âˆˆ R n Ã— e X\_{e} \\in R^{n\\times e} Xeâ€‹âˆˆRnÃ—eï¼Œå…¶ä¸­ e e eæ˜¯å‘é‡çš„ç»´åº¦ï¼ˆå…¶ä¸é¢„è®­ç»ƒæ¨¡å‹çš„é…ç½®æœ‰å…³ï¼Œä¾‹å¦‚Bert-baseæ˜¯768ï¼‰ã€‚è¿ç»­æ¨¡æ¿ä¸­çš„æ¯ä¸ªä¼ªæ ‡è®° v i v\_{i} viâ€‹å¯ä»¥è§†ä¸ºå‚æ•°ï¼Œä¹Ÿå¯ä»¥è§†ä¸ºä¸€ä¸ªtokenï¼Œå› æ­¤ï¼Œå¯ä»¥é€šè¿‡å¦ä¸€ä¸ªembedding tableå°† p p pä¸ªä¼ªæ ‡è®°tokenè¡¨å¾ä¸ºå‘é‡çŸ©é˜µ P e âˆˆ R p Ã— e P\_{e} \\in R^{p\\times e} Peâ€‹âˆˆRpÃ—e ã€‚å°†æ–‡æœ¬å’Œpromptè¿›è¡Œæ‹¼æ¥è·å¾—æ–°çš„è¾“å…¥ \[ P e : X e \] âˆˆ R ( p + n ) Ã— e \[P\_{e} :X\_{e}\] \\in R^{(p+n) \\times e} \[Peâ€‹:Xeâ€‹\]âˆˆR(p+n)Ã—eã€‚è¿™ä¸ªæ–°çš„è¾“å…¥å°†ä¼šè¿›å…¥T5çš„encoder-decoderç»“æ„æ¥è®­ç»ƒå’Œæ¨ç†ã€‚æ³¨æ„ï¼Œåªæœ‰promptå¯¹åº”çš„å‘é‡è¡¨å¾å‚æ•° P e P\_{e} Peâ€‹ä¼šéšç€è®­ç»ƒè¿›è¡Œæ›´æ–°ã€‚

        â€ƒâ€ƒè®ºæ–‡ä¸­æåˆ°ï¼Œæ¯ä¸ªä¼ªæ ‡è®°çš„åˆå§‹åŒ–å¯ä»¥æœ‰ä¸‹åˆ—ä¸‰ç§æƒ…å†µï¼Œåˆ†åˆ«æ˜¯Random Uniformï¼ŒSampled Vocabå’ŒClass Labelã€‚

        - **Random Uniform**: ä»å‡åŒ€åˆ†å¸ƒä¸­éšæœºè¿›è¡Œåˆå§‹åŒ–ï¼›
        - **Sampled Vocab**: ä»T5çš„è¯­æ–™åº“ä¸­é€‰æ‹©æœ€å¸¸è§çš„5000ä¸ªè¯æ±‡ï¼Œå¹¶ä»ä¸­é€‰æ‹©è¯æ±‡åµŒå…¥ä½œä¸ºåˆå§‹åŒ–ï¼›
        - **Class Label**: æ˜¯å°†ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡ç­¾å¯¹åº”çš„å­—ç¬¦ä¸²è¡¨ç¤ºçš„åµŒå…¥ä½œä¸ºåˆå§‹åŒ–ï¼Œå¦‚æœä¸€ä¸ªç±»æœ‰å¤šä¸ªè¯ï¼Œå–è¯åµŒå…¥çš„å¹³å‡è¡¨ç¤ºä½œä¸ºä¸€ä¸ªpromptã€‚å‡å¦‚æ ‡ç­¾æ•°ç›®ä¸è¶³ï¼Œåˆ™ä»Sampled Vocabæ–¹æ¡ˆä¸­ç»§ç»­é‡‡æ ·è¡¥è¶³ã€‚
            Â 

        â€ƒâ€ƒæœ€åå‘ç°ï¼Œééšæœºåˆå§‹åŒ–æ–¹æ³•è¦æ˜¾è‘—å¥½äºéšæœºåˆå§‹åŒ–ï¼Œè€ŒClass Labelæ•ˆæœç›¸å¯¹æ›´å¥½ï¼Œå½“ç„¶ï¼Œåªè¦æ¨¡å‹è¶³å¤Ÿå¤§ï¼Œè¿™å‡ ç§åˆå§‹åŒ–æ–¹æ³•çš„å·®å¼‚å°±æ¯”è¾ƒå°äº†ã€‚å…·ä½“è®ºæ–‡å‚è€ƒ2021å¹´è°·æ­Œå‘è¡¨çš„[ã€ŠThe Power of Scale for Parameter-Efficient Prompt Tuningã€‹](https://aclanthology.org/2021.emnlp-main.243.pdf)ã€‚

    - **P-Tuning**: P-Tuningæ˜¯å¦ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„è¿ç»­æç¤ºæ–¹æ³•ï¼Œä¸»è¦é’ˆå¯¹çš„æ˜¯NLUä»»åŠ¡ï¼Œæ–¹æ³•å›¾å¦‚ä¸‹æ‰€ç¤ºï¼ˆå›¾ä¸­çš„ P i P\_{i} Piâ€‹ç­‰ä»·äºä¸Šæ–‡çš„ v i v\_{i} viâ€‹ï¼Œè¡¨ç¤ºä¼ªæ ‡è®°ï¼‰ï¼Œè°·æ­Œäº2021å¹´å‘è¡¨ã€‚![P-Tuningç»“æ„](https://img-blog.csdnimg.cn/8356a2a18e0b4b4d8b64b9d947ed4423.png#pic_center)
        P-Tuningæ–¹æ³•ä¸­çš„å››ä¸ªæŠ€å·§ç‚¹:

        - è€ƒè™‘åˆ°è¿™äº›ä¼ªæ ‡è®°çš„ç›¸äº’ä¾èµ–å…³ç³»: è®¤ä¸º \[ P 1 \] \[P\_{1}\] \[P1â€‹\]ä¸ \[ P 2 \] \[P\_{2}\] \[P2â€‹\]æ˜¯æœ‰å…ˆåå…³ç³»çš„ï¼Œè€Œtransformeræ— æ³•æ˜¾å¼åœ°åˆ»ç”»è¿™å±‚å…³ç³»ï¼Œå› æ­¤å¼•å…¥Prompt Encoderï¼ˆBiLSTM+MLPï¼‰ï¼›
        - æŒ‡å®šä¸Šä¸‹æ–‡è¯: å¦‚æœæ¨¡æ¿å…¨éƒ¨æ˜¯ä¼ªæ ‡è®°ï¼Œåœ¨è®­ç»ƒæ—¶æ— æ³•å¾ˆå¥½åœ°æ§åˆ¶è¿™äº›æ¨¡æ¿æœç€ä¸å¯¹åº”å¥å­ç›¸ä¼¼çš„è¯­ä¹‰ä¸Šä¼˜åŒ–ï¼Œå› æ­¤é€‰å®šéƒ¨åˆ†å…·æœ‰ä¸å½“å‰å¥å­è¯­ä¹‰ä»£è¡¨æ€§çš„ä¸€äº›è¯ä½œä¸ºä¸€äº›ä¼ªæ ‡è®°çš„åˆå§‹åŒ–ï¼ˆä¾‹å¦‚ä¸Šå›¾ä¸­â€œcapitalâ€ã€â€œBritainâ€ç­‰ï¼‰ï¼›
        - é‡å‚æ•°ï¼ˆReparameterizationï¼‰: å…·ä½“åˆ°ä»£ç å®ç°ä¸Šï¼ŒP-Tuningå…ˆé€šè¿‡ä¸€ä¸ªPrompt Encoderè¡¨å¾è¿™äº›ä¼ªæ ‡è®°åï¼Œç›´æ¥å°†è¿™äº›æ–°çš„è¡¨å¾è¦†ç›–åˆ°å¯¹åº”çš„embedding tableä¸Šï¼Œæ¢å¥è¯è¯´ï¼ŒPrompt Encoderåªåœ¨è®­ç»ƒæ—¶å€™ä¼šä½¿ç”¨åˆ°ï¼Œè€Œåœ¨æ¨ç†é˜¶æ®µåˆ™ä¸å†ä½¿ç”¨ï¼Œç›´æ¥ä½¿ç”¨æ„å»ºå¥½çš„embedding tableï¼›
        - æ··åˆæç¤ºï¼ˆHydride Promptï¼‰: å°†è¿ç»­æç¤ºä¸ç¦»æ•£tokenè¿›è¡Œæ··åˆï¼Œä¾‹å¦‚ \[ x \] \[ i t \] \[ v 1 \] \[ m a s k \] \[x\]\[it\]\[v1\]\[mask \] \[x\]\[it\]\[v1\]`[mask]`ã€‚
            Â 

        â€ƒâ€ƒå…·ä½“å¯å‚è€ƒ: 2021å¹´å‘è¡¨çš„[ã€ŠGPT Understands, Tooã€‹](https://arxiv.org/pdf/2103.10385.pdf)ã€[ã€Šè®ºæ–‡è§£è¯»: GPT Understands, Tooã€‹](https://wjn1996.blog.csdn.net/article/details/120802305)ã€[ã€Šç»†è¯»ç»å…¸: P-Tuningã€‹](https://zhuanlan.zhihu.com/p/391992466)

    - **PPTï¼ˆPre-trained Prompt Tuningï¼‰**: Prompt-Tuningé€šå¸¸é€‚ç”¨äºä½èµ„æºåœºæ™¯ï¼Œä½†æ˜¯ç”±äºè¿ç»­çš„æ¨¡æ¿æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œå³å…¶å­˜åœ¨æ–°çš„å‚æ•°ï¼Œå°‘é‡æ ·æœ¬å¯èƒ½ä¾ç„¶å¾ˆéš¾ç¡®ä¿è¿™äº›æ¨¡æ¿è¢«å¾ˆå¥½åœ°ä¼˜åŒ–ã€‚å› æ­¤ç®€å•çš„æ–¹æ³•å°±æ˜¯å¯¹è¿™äº›è¿ç»­çš„æ¨¡æ¿è¿›è¡Œé¢„è®­ç»ƒã€‚PPTæ—¨åœ¨é€šè¿‡å…ˆè®©è¿™äº›è¿ç»­æç¤ºåœ¨å¤§é‡æ— æ ‡æ³¨çš„é¢„è®­ç»ƒè¯­æ–™è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å¯¹åº”ä¸‹æ¸¸ä»»åŠ¡çš„PLMä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…å¯¹3ç§Prompt-Tuningçš„ä¼˜åŒ–ç­–ç•¥åœ¨few-shot learningé—®é¢˜ä¸Šåˆ†åˆ«è¿›è¡Œäº†æ•ˆæœå¯¹æ¯”ï¼ŒåŒ…æ‹¬hard promptå’Œsoft promptç»“åˆã€labelåˆ°textæ˜ å°„æ–¹æ³•é€‰æ‹©ä»¥åŠä½¿ç”¨çœŸå®å•è¯çš„embeddingè¿›è¡Œsoft promptçš„éšæœºåˆå§‹åŒ–ã€‚é€šè¿‡å¯¹æ¯”å®éªŒå‘ç°ï¼Œhard+soft promptç»“åˆçš„æ–¹æ³•å¯ä»¥æå‡æ•ˆæœï¼Œä½†æ˜¯ä»ç„¶æ¯”finetuneæ•ˆæœå·®ã€‚Labelåˆ°textçš„æ˜ å°„æ–¹æ³•å¯¹äºæ•ˆæœå½±å“å¾ˆå¤§ï¼Œé€‰æ‹©èƒ½å¤Ÿè¡¨è¾¾labelå¯¹åº”å«ä¹‰çš„å¸¸ç”¨å•è¯ä¼šå¸¦æ¥æœ€å¥½æ•ˆæœã€‚è€Œä½¿ç”¨å•è¯embeddingè¿›è¡Œsoft promptçš„åˆå§‹åŒ–åœ¨å¤§æ¨¡å‹ä¸Šå¹¶æ²¡æœ‰æ˜æ˜¾çš„æ•ˆæœæå‡ã€‚
        â€ƒâ€ƒåŸºäºä»¥ä¸Šå®éªŒç»“æœï¼Œä½œè€…æå‡ºäº†Pre-trained Pormpt Tuningè§£å†³few-shot learningé—®é¢˜ï¼Œæ ¸å¿ƒæ€è·¯æ˜¯å¯¹soft promptè¿›è¡Œé¢„è®­ç»ƒï¼Œå¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„soft promptåˆå§‹åŒ–è¡¨ç¤ºã€‚å¯¹äºæ¯ç§ç±»å‹çš„ä»»åŠ¡ï¼Œè®¾è®¡ä¸€ä¸ªå’Œå…¶åŒ¹é…çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¾—åˆ°soft prompt embeddingçš„é¢„è®­ç»ƒè¡¨ç¤ºã€‚
        â€ƒâ€ƒè®ºæ–‡ä¸­ä»¥sentence-pair classificationã€multiple-choice classificationã€single sentence classificationä¸‰ç§ä»»åŠ¡ä»‹ç»äº†å¦‚ä½•é’ˆå¯¹æ¯ç§ä¸‹æ¸¸ä»»åŠ¡è®¾è®¡é¢„è®­ç»ƒä»»åŠ¡å­¦ä¹ soft prompt embeddingã€‚ä¾‹å¦‚å¯¹äºsentence-pair classificationï¼Œä½œè€…è®¾è®¡äº†å¦‚ä¸‹é¢„è®­ç»ƒä»»åŠ¡ã€‚å°†2ä¸ªå¥å­å¯¹æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¦‚æœä¸¤ä¸ªå¥å­æ¥è‡ªåŒä¸€ä¸ªæ–‡æ¡£ç›¸é‚»ä¸¤å¥è¯ï¼Œåˆ™labelä¸ºyesï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ï¼›å¦‚æœä¸¤ä¸ªå¥å­æ¥è‡ªåŒä¸€ä¸ªæ–‡æ¡£ä½†è·ç¦»è¾ƒè¿œï¼Œåˆ™labelä¸ºmaybeï¼›å…¶ä»–å¥å­å¯¹labelä¸ºnoï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆå›¾ä¸­çš„ P P På³è¿ç»­çš„æç¤ºæ¨¡æ¿ï¼Œ < x > <x> <x\>è¡¨ç¤ºmask tokenã€‚æœ€ä¸Šé¢çš„ä»»åŠ¡æ˜¯é¢„è®­ç»ƒä»»åŠ¡ï¼Œä¸‹é¢ä¸‰ä¸ªä»»åŠ¡ä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼‰ã€‚![PPTæ ¸å¿ƒæ€æƒ³](https://img-blog.csdnimg.cn/49c6411ed3de4466a794ca92e3335168.png#pic_center) Â 

        â€ƒâ€ƒå¦å¤–è®ºæ–‡ä¸­è¿˜ç»™å‡ºäº†å››ç§å¾®è°ƒæ–¹æ¡ˆï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ\[a\]å±•ç¤ºäº†æ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œ\[b\]å’Œ\[c\]å±•ç¤ºäº†ä¸¤ç§ä¸»æµçš„Fine-Tuningæ–¹æ³•ï¼ˆå‰æ–‡å·²ç»ä»‹ç»è¿‡ï¼‰ï¼Œ\[d\]å±•ç¤ºäº†æç¤ºå­¦ä¹ ( Prompt Tuning, PT )æ–¹æ³•ï¼Œå…·ä½“å¯å‚è€ƒ2022å¹´æ¸…åå¤§å­¦å‘è¡¨çš„[ã€ŠPPT: Pre-trained Prompt Tuning for Few-shot Learningã€‹](https://aclanthology.org/2022.acl-long.576.pdf)ã€[å°æ ·æœ¬å­¦ä¹ : Pre-trained Prompt Tuning for Few-shot Learning](https://zhuanlan.zhihu.com/p/617006511)ï¼Œ[Prompt å¦‚ä½•æ›´å¥½åœ°åº”ç”¨äºå·¥ä¸šç•Œï¼Ÿ](https://www.zhihu.com/question/495040812/answer/2438217999)ã€‚![Tuningæ–¹æ¡ˆ](https://img-blog.csdnimg.cn/94f3d30b97b54f47a0b39bc82bc610a8.png#pic_center)


#### 2.4 Prompt-Tuning vs Fine-Tuning

â€ƒâ€ƒè‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»æ·±å…¥äº†è§£äº†Fine-Tuningå’ŒPrompt-Tuningä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œä¹Ÿæˆ–å¤šæˆ–å°‘èƒ½è§‚å¯Ÿåˆ°äºŒè€…ä¹‹é—´çš„åŒºåˆ«ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œè¿›è¡Œä¸‹æ€»ç»“ã€‚ä¼—å¤šå‘¨çŸ¥ï¼ŒPrompt-Tuningæ˜¯åœ¨Fine-Tuningåå‘å±•èµ·æ¥çš„ï¼Œå¯ä»¥è¯´æ˜¯è§£å†³NLPé¢†åŸŸå„ç§ä¸‹æ¸¸é—®é¢˜æ›´å¥½çš„ä¸€ç§æ–¹å¼ã€‚è¦æå‡ºä¸€ä¸ªå¥½çš„æ–¹å¼é‚£å¿…ç„¶æ˜¯ç”¨æ¥ã€Œè§£å†³å¦ä¸€ç§æ–¹å¼å­˜åœ¨çš„ç¼ºé™·æˆ–ä¸è¶³ã€ï¼Œé‚£æˆ‘ä»¬å°±å…ˆä»é¢„è®­ç»ƒæ¨¡å‹PLM+Fine-TuningèŒƒå¼è¯´èµ·ï¼Œè¿™ä¸ªèŒƒå¼å¸¸ç”¨çš„ç»“æ„æ˜¯Bert+Fine-Tuningï¼Œè¿™ç§èŒƒå¼è‹¥æƒ³è¦é¢„è®­ç»ƒæ¨¡å‹æ›´å¥½çš„åº”ç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼Œéœ€è¦åˆ©ç”¨ä¸‹æ¸¸æ•°æ®å¯¹æ¨¡å‹å‚æ•°å¾®è°ƒï¼›é¦–å…ˆï¼Œæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™ï¼Œé‡‡ç”¨çš„è®­ç»ƒå½¢å¼: è‡ªå›å½’ã€è‡ªç¼–ç ï¼Œè¿™ä¸ä¸‹æ¸¸ä»»åŠ¡å½¢å¼å­˜åœ¨æå¤§çš„ gapï¼Œä¸èƒ½å®Œå…¨å‘æŒ¥é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«çš„èƒ½åŠ›ï¼Œå¿…ç„¶å¯¼è‡´: è¾ƒå¤šçš„æ•°æ®æ¥é€‚åº”æ–°çš„ä»»åŠ¡å½¢å¼ï¼ˆå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›å·®ã€å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚å…¶æ¬¡ï¼Œç°åœ¨çš„é¢„è®­ç»ƒæ¨¡å‹å‚æ•°é‡è¶Šæ¥è¶Šå¤§ï¼Œä¸ºäº†ä¸€ä¸ªç‰¹å®šçš„ä»»åŠ¡å»Fine-Tuningä¸€ä¸ªæ¨¡å‹ï¼Œä¼šå ç”¨ç‰¹åˆ«å¤šçš„è®­ç»ƒèµ„æºï¼Œå¯¹ä¸€äº›ä¸­å°ä¼ä¸šæˆ–è€…ç”¨æˆ·æ¥è¯´å¹¶ä¸ç°å®ï¼Œä¹Ÿä¼šé€ æˆèµ„æºçš„ä¸€å®šæµªè´¹ã€‚
â€ƒâ€ƒè€ŒPrompt-Tuningåˆ™å¾ˆå¥½çš„è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå®ƒå°†æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡ç»Ÿä¸€æˆé¢„è®­ç»ƒä»»åŠ¡ï¼Œä»¥ç‰¹å®šçš„æ¨¡æ¿ï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®è½¬æˆè‡ªç„¶è¯­è¨€å½¢å¼ï¼Œå……åˆ†æŒ–æ˜é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«çš„èƒ½åŠ›ã€‚æœ¬è´¨ä¸Šå°±æ˜¯è®¾è®¡ä¸€ä¸ªæ¯”è¾ƒå¥‘åˆä¸Šæ¸¸é¢„è®­ç»ƒä»»åŠ¡çš„æ¨¡æ¿ï¼Œé€šè¿‡æ¨¡æ¿çš„è®¾è®¡æ¥æŒ–æ˜å‡ºä¸Šæ¸¸é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›ï¼Œè®©ä¸Šæ¸¸çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å°½é‡ä¸éœ€è¦æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æ¯”è¾ƒå¥½çš„å®Œæˆä¸‹æ¸¸çš„ä»»åŠ¡ï¼Œå³åªéœ€è¦å°‘é‡æ•°æ®çš„ Prompt Tuningï¼Œå°±å¯ä»¥å®ç°å¾ˆå¥½çš„æ•ˆæœï¼Œå…·æœ‰è¾ƒå¼ºçš„é›¶æ ·æœ¬/å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚å…·ä½“å¯å‚è€ƒ[Prompt-Tuning VS Fine-Tuning](https://www.zhihu.com/question/504324484?utm_id=0)ã€‚

### 3ã€Instruction-Tuningï¼ˆæŒ‡ç¤ºå¾®è°ƒï¼‰

â€ƒâ€ƒå‰æ–‡ä¸­å·²ç»å¤šæ¬¡æåˆ°è¿‡Instruction-Tuningï¼Œå¯ä»¥è¯´åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢†åŸŸï¼Œå®ƒæ˜¯ç›®å‰æœ€ç«çš„ç ”ç©¶èŒƒå¼ï¼Œæ€§èƒ½è¶…è¿‡åŒ…æ‹¬In-context learningåœ¨å†…çš„prompt learningã€‚

#### 3.1ã€Instruction-Tuningçš„æå‡º

â€ƒâ€ƒå›é¡¾Instruction-Tuningçš„å‘å±•å†ç¨‹ï¼Œé¦–å…ˆæ˜¯Google2021å¹´çš„FLANæ¨¡å‹[ã€ŠFINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERSã€‹](https://openreview.net/pdf?id=gEZrGCozdqR)ï¼Œè¿™ç¯‡æ–‡ç« æ˜ç¡®æå‡ºInstruction-Tuningï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰çš„æŠ€æœ¯ï¼Œå®ƒçš„æœ¬è´¨ç›®çš„æ˜¯æƒ³å°† NLP ä»»åŠ¡è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå†å°†å…¶æŠ•å…¥æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡ç»™æ¨¡å‹æä¾›æŒ‡ä»¤å’Œé€‰é¡¹çš„æ–¹å¼ï¼Œä½¿å…¶èƒ½å¤Ÿæå‡Zero-Shotä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚
â€ƒâ€ƒInstruction-Tuningæå‡ºçš„åŠ¨æœºåœ¨äºå¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å¦‚GPT-3å¯ä»¥éå¸¸å¥½åœ°å­¦ä¹ few-shotï¼Œä½†å®ƒåœ¨zero-shotä¸Šå´ä¸é‚£ä¹ˆæˆåŠŸã€‚ä¾‹å¦‚ï¼Œ GPT-3åœ¨é˜…è¯»ç†è§£ã€é—®é¢˜å›ç­”å’Œè‡ªç„¶è¯­è¨€æ¨ç†ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾ˆä¸€èˆ¬ï¼Œä½œè€…è®¤ä¸ºä¸€ä¸ªæ½œåœ¨çš„åŸå› æ˜¯ï¼Œå¦‚æœåœ¨æ²¡æœ‰å°‘é‡ç¤ºä¾‹çš„zero-shotæ¡ä»¶ä¸‹ï¼Œæ¨¡å‹å¾ˆéš¾åœ¨promptsä¸Šè¡¨ç°å¾ˆå¥½ï¼Œå› ä¸ºpromptså¯èƒ½å’Œé¢„è®­ç»ƒæ•°æ®çš„æ ¼å¼ç›¸å·®å¾ˆå¤§ã€‚
â€ƒâ€ƒæ—¢ç„¶å¦‚æ­¤ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤åšè¾“å…¥å‘¢ï¼Ÿé€šè¿‡è®¾è®¡instructionï¼Œè®©å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç†è§£æŒ‡ä»¤ï¼Œè¿›è€Œå®Œæˆä»»åŠ¡ç›®æ ‡ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¾æ®æ¼”ç¤ºå®ä¾‹åšæ–‡æœ¬ç”Ÿæˆã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸ç®¡æ˜¯commonsense reasoningä»»åŠ¡è¿˜æ˜¯machine translationä»»åŠ¡ï¼Œéƒ½å¯ä»¥å˜ä¸ºinstructionçš„å½¢å¼ï¼Œç„¶ååˆ©ç”¨å¤§æ¨¡å‹è¿›è¡Œå­¦ä¹ ã€‚åœ¨è¿™ç§æ–¹å¼ä¸‹ï¼Œå½“ä¸€ä¸ªunseen taskè¿›å…¥æ—¶ï¼Œé€šè¿‡ç†è§£å…¶è‡ªç„¶è¯­è¨€è¯­ä¹‰å¯ä»¥è½»æ¾å®ç°zero-shotçš„æ‰©å±•ï¼Œå¦‚natural language inferenceä»»åŠ¡ã€‚
![FLAN](https://img-blog.csdnimg.cn/cea53e43f97e4cc1ab88b45df7047831.png#pic_center)
![FLAN](https://img-blog.csdnimg.cn/8fc5c313663b439d8b08e8a27623d7bb.png#pic_center)
â€ƒâ€ƒæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»ä¸‹FLANçš„å…·ä½“è®­ç»ƒæµç¨‹ã€‚
â€ƒâ€ƒå…·ä½“æ¥è¯´ï¼Œä½œè€…æå‡ºçš„Finetuned Language Netï¼ˆFLANï¼‰æ¨¡å‹å°†62ä¸ªNLPä»»åŠ¡åˆ†ä¸º12ä¸ªç°‡ï¼ŒåŒä¸€ä¸ªç°‡å†…æ˜¯ç›¸åŒçš„ä»»åŠ¡ç±»å‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
![FLAN-TASK](https://img-blog.csdnimg.cn/97a976f658714fd8b5f6fe23aca839ba.png#pic_center)
â€ƒâ€ƒå¯¹äºæ¯ä¸ªtaskï¼Œå°†ä¸ºå…¶æ‰‹åŠ¨æ„å»º10ä¸ªç‹¬ç‰¹templateï¼Œä½œä¸ºä»¥è‡ªç„¶è¯­è¨€æè¿°è¯¥ä»»åŠ¡çš„instructionsã€‚ä¸ºäº†å¢åŠ å¤šæ ·æ€§ï¼Œå¯¹äºæ¯ä¸ªæ•°æ®é›†ï¼Œè¿˜åŒ…æ‹¬æœ€å¤šä¸‰ä¸ªâ€œturned the task around/å˜æ›´ä»»åŠ¡â€çš„æ¨¡æ¿ï¼ˆä¾‹å¦‚ï¼Œå¯¹äºæƒ…æ„Ÿåˆ†ç±»ï¼Œè¦æ±‚å…¶ç”Ÿæˆç”µå½±è¯„è®ºçš„æ¨¡æ¿ï¼‰ã€‚æ‰€æœ‰æ•°æ®é›†çš„æ··åˆå°†ç”¨äºåç»­é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åšInstruction-Tuningï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®é›†çš„templateéƒ½æ˜¯éšæœºé€‰å–çš„ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒPremiseã€Hypothesisã€Optionsä¼šè¢«å¡«å……åˆ°ä¸åŒçš„templateä¸­ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚
![FLAN-Template](https://img-blog.csdnimg.cn/45b63fc37974479c8bc3d0f7079890a1.png#pic_center)
â€ƒâ€ƒæœ€ååŸºäºLaMDA-PTæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚LaMDA-PTæ˜¯ä¸€ä¸ªåŒ…å«137Bå‚æ•°çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹åœ¨webæ–‡æ¡£ï¼ˆåŒ…æ‹¬ä»£ç ï¼‰ã€å¯¹è¯æ•°æ®å’Œç»´åŸºç™¾ç§‘ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ŒåŒæ—¶æœ‰å¤§çº¦10%çš„æ•°æ®æ˜¯éè‹±è¯­æ•°æ®ã€‚ç„¶åFLANæ··åˆäº†æ‰€æœ‰æ„é€ çš„æ•°æ®é›†åœ¨128æ ¸çš„TPUv3èŠ¯ç‰‡ä¸Šå¾®è°ƒäº†60ä¸ªå°æ—¶ã€‚
â€ƒâ€ƒè‡³æ­¤ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†åŒ…æ‹¬FLANåœ¨å†…çš„Instruction-Tuningæ–¹æ³•ï¼Œæ€»ç»“æ¥è¯´ï¼ŒInstruction-Tuningä¹Ÿæ˜¯In-context learningçš„ä¸€ç§ï¼Œåªæ˜¯Instruction-Tuningæ˜¯å°†å¤§æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæå‡å¤§æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œæœ€ç»ˆå®ç°åœ¨æ–°ä»»åŠ¡ä¸Šçš„zero-shotã€‚ç›®å‰å¦å¤–ä¸€ä¸ªé‡‡ç”¨äº†Instruction-TuningæŠ€æœ¯çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ˜¯instructGPTï¼Œåé¢æˆ‘ä»¬ä¼šè¯¦ç»†ä»‹ç»instructGPTçš„å…·ä½“å®ç°æ–¹å¼ã€‚

#### 3.2ã€Fine-Tuning vs Prompt-Tuning vs Instruction-Tuning

- **Fine-Tuning**: å…ˆåœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†åœ¨æŸä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¦‚Bert+Fine-Tuningï¼›

- **Prompt-Tuning**: å…ˆé€‰æ‹©æŸä¸ªé€šç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åä¸ºå…·ä½“çš„ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªpromptæ¨¡æ¿ä»¥é€‚åº”å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¦‚GPT-3+Prompt-Tuningï¼›

- **Instruction-Tuning**: ä»ç„¶åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå…ˆåœ¨å¤šä¸ªå·²çŸ¥ä»»åŠ¡ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œç„¶ååœ¨æŸä¸ªæ–°ä»»åŠ¡ä¸Šè¿›è¡Œzero-shotï¼Œå¦‚GPT-3+Instruction-Tuningï¼›

- **Prompt-Tuning vs Instruction-Tuning**: Promptå’Œinstructionéƒ½æ˜¯æŒ‡å¯¼è¯­è¨€æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µï¼Œä½†å®ƒä»¬æœ‰ç€ä¸åŒçš„å«ä¹‰å’Œç”¨é€”ã€‚

    - Prompté€šå¸¸æ˜¯ä¸€ç§çŸ­æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œç”¨äºæŒ‡å¯¼è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”ã€‚Promptæä¾›ä¸Šä¸‹æ–‡å’Œä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£è¦æ±‚ï¼Œå¹¶ç”Ÿæˆæ­£ç¡®çš„è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œåœ¨é—®ç­”ä»»åŠ¡ä¸­ï¼Œpromptå¯èƒ½åŒ…å«é—®é¢˜æˆ–è¯é¢˜çš„æè¿°ï¼Œä»¥å¸®åŠ©æ¨¡å‹ç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆã€‚Prompté€šå¸¸æ˜¯äººç±»è®¾è®¡çš„ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ç‰¹å®šä»»åŠ¡æˆ–é¢†åŸŸï¼›
    - Instructioné€šå¸¸æ˜¯ä¸€ç§æ›´è¯¦ç»†çš„æ–‡æœ¬ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šæ“ä½œæˆ–å®Œæˆä»»åŠ¡ã€‚Instructionå¯ä»¥æ˜¯è®¡ç®—æœºç¨‹åºæˆ–è„šæœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯äººç±»ç¼–å†™çš„æŒ‡å¯¼æ€§æ–‡æœ¬ã€‚Instructionçš„ç›®çš„æ˜¯å‘Šè¯‰æ¨¡å‹å¦‚ä½•å¤„ç†æ•°æ®æˆ–æ‰§è¡ŒæŸä¸ªæ“ä½œï¼Œè€Œä¸æ˜¯ç®€å•åœ°æä¾›ä¸Šä¸‹æ–‡æˆ–ä»»åŠ¡ç›¸å…³ä¿¡æ¯ã€‚
        Â 

    â€ƒâ€ƒå› æ­¤ï¼ŒPromptå’Œinstructionéƒ½æ˜¯ç”¨äºæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬çš„ç›®çš„å’Œä½¿ç”¨æ–¹å¼æ˜¯ä¸åŒçš„ã€‚Promptæ›´å¤šåœ°ç”¨äºå¸®åŠ©æ¨¡å‹ç†è§£ä»»åŠ¡å’Œä¸Šä¸‹æ–‡ï¼Œè€ŒInstructionåˆ™æ›´å¤šåœ°ç”¨äºæŒ‡å¯¼æ¨¡å‹æ‰§è¡Œå…·ä½“æ“ä½œæˆ–å®Œæˆä»»åŠ¡ã€‚
    ![FT vs PT vs IT](https://img-blog.csdnimg.cn/8ac41efdf9884f1ea7876ef8886cdbd5.png#pic_center)

    â€ƒâ€ƒå¯¹äºPrompt-Tuningå’ŒInstruction-Tuningè¿˜æœ‰ä¸€ä¸ªä¸åŒç‚¹ï¼Œå°±æ˜¯promptåœ¨æ²¡ç²¾è°ƒçš„æ¨¡å‹ä¸Šä¹Ÿèƒ½æœ‰ä¸€å®šæ•ˆæœï¼ˆæ¨¡å‹ä¸ç»è¿‡Prompt-Tuningï¼Œç›´æ¥é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œæ¨ç†ï¼‰ï¼Œè€ŒInstruction-Tuningåˆ™å¿…é¡»å¯¹æ¨¡å‹ç²¾è°ƒï¼Œè®©æ¨¡å‹çŸ¥é“è¿™ç§æŒ‡ä»¤æ¨¡å¼ã€‚ä½†æ˜¯ï¼Œpromptä¹Ÿæœ‰ç²¾è°ƒï¼Œç»è¿‡Prompt-Tuningä¹‹åï¼Œæ¨¡å‹ä¹Ÿå°±å­¦ä¹ åˆ°äº†è¿™ä¸ªpromptæ¨¡å¼ï¼Œç²¾è°ƒä¹‹åè·ŸInstruction-Tuningæœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿè¿™å°±æ˜¯Instruction-Tuningå·§å¦™çš„åœ°æ–¹äº†ï¼ŒPrompt-Tuningéƒ½æ˜¯é’ˆå¯¹ä¸€ä¸ªä»»åŠ¡çš„ï¼Œæ¯”å¦‚åšä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡çš„Prompt-Tuningï¼Œç²¾è°ƒå®Œçš„æ¨¡å‹åªèƒ½ç”¨äºæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼Œè€Œç»è¿‡Instruction-Tuningå¤šä»»åŠ¡ç²¾è°ƒåï¼Œå¯ä»¥ç”¨äºå…¶ä»–ä»»åŠ¡çš„zero-shotã€‚
    â€ƒâ€ƒè¿™é‡ŒèŠä¸€èŠè‡ªå·±çš„è§è§£ï¼Œä¸¤è€…çš„å¯¹æ¯”ä¸»è¦æ˜¯åŸºäºå¤§æ¨¡å‹ã€‚Promptæ˜¯é€šè¿‡å¯¹ä»»åŠ¡è¿›è¡Œä¸€å®šçš„æè¿°ï¼Œæˆ–è€…ç»™ä¸€äº›ç¤ºä¾‹ï¼ˆICLï¼‰ï¼Œæ¥å®Œæˆæ—¢å®šä»»åŠ¡ç›®æ ‡ï¼Œä½†æ˜¯å¦‚æœä¸ç»™æ¨¡å‹ç¤ºä¾‹ï¼ˆzero-shotï¼‰ï¼Œpromptè¡¨ç°çš„å¾ˆä¸€èˆ¬ï¼Œè¿™æ€ä¹ˆåŠå‘¢ï¼Ÿèƒ½ä¸èƒ½è®©å¤§æ¨¡å‹ç†è§£ä»»åŠ¡æ˜¯åšä»€ä¹ˆçš„ï¼Œè¿™æ ·ä¸ç”¨ç¤ºä¾‹ä¹Ÿèƒ½å®Œæˆä»»åŠ¡ç›®æ ‡ï¼Œinstructionå°±æ˜¯æ¥åšè¿™ä¸ªä»»åŠ¡çš„ï¼Œå®ƒä¸ºäº†è®©æ¨¡å‹å…·å¤‡ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼Œé‡‡ç”¨å¤§é‡çš„æŒ‡ä»¤æ•°æ®ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå³Instruction-Tuningã€‚å› æ­¤ï¼Œinstructionå’Œpromptçš„ä¸åŒä¹‹å¤„åœ¨äº: instructionæ˜¯åœ¨promptçš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æŒ–æ˜æ¨¡å‹ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ã€‚ï¼ˆä»…ä¾›å‚è€ƒï¼‰


### 4ã€Chain-of-Thoughtï¼ˆæ€ç»´é“¾ï¼‰

â€ƒâ€ƒéšç€LLMçš„è¶Šæ¥è¶Šå¤§ï¼Œä»¥åŠtuningæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒLLMåœ¨åŒ…æ‹¬æƒ…æ„Ÿåˆ†æåœ¨å†…çš„ä¼ ç»Ÿè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°è¶Šæ¥è¶Šå¥½ï¼Œä½†æ˜¯å•çº¯çš„æ‰©å¤§LLMæ¨¡å‹çš„å‚æ•°é‡æ— æ³•è®©æ¨¡å‹åœ¨ç®—æœ¯æ¨ç†/å¸¸è¯†æ¨ç†/ç¬¦å·æ¨ç†ç­‰æ¨ç†ä»»åŠ¡ä¸Šå–å¾—ç†æƒ³çš„æ•ˆæœã€‚ å¦‚ä½•æå‡LLMåœ¨è¿™äº›æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½å‘¢ï¼Ÿåœ¨æ­¤å‰å…³äºLLMçš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œæœ‰ä¸¤ç§æ–¹æ³•:

- é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›
- ä¸ºæ¨¡å‹æä¾›å°‘é‡çš„è¾“å…¥è¾“å‡ºæ ·ä¾‹è¿›è¡Œå­¦ä¹ ã€‚

ä½†æ˜¯è¿™ä¸¤ç§æ–¹æ³•éƒ½æœ‰ç€å±€é™æ€§ï¼Œå‰è€…å¾®è°ƒè®¡ç®—æˆæœ¬å¤ªé«˜ï¼Œåè€…é‡‡ç”¨ä¼ ç»Ÿçš„è¾“å…¥è¾“å‡ºæ ·ä¾‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šæ•ˆæœå¾ˆå·®ï¼Œè€Œä¸”ä¸ä¼šéšç€è¯­è¨€æ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œæœ‰å®è´¨æ€§çš„æ”¹å–„ã€‚æ­¤æ—¶ï¼ŒChain-of-Thoughtåº”è¿è€Œç”Ÿã€‚ä¸‹é¢æˆ‘ä»¬æ ¹æ®ä¸‰ç¯‡æ¯”è¾ƒæœ‰ä»£è¡¨æ€§çš„è®ºæ–‡ï¼Œè¯¦ç»†ä»‹ç»CoTçš„å‘å±•å†ç¨‹ã€‚

#### 4.1ã€Manual-CoTï¼ˆäººå·¥æ€ç»´é“¾ï¼‰

â€ƒâ€ƒManual-CoTæ˜¯Chain-of-ThoughtæŠ€æœ¯çš„å¼€å±±ä¹‹ä½œï¼Œç”±Googleåœ¨2022å¹´åˆæå‡º[ã€ŠChain-of-Thought Prompting Elicits Reasoning in Large Language Modelsã€‹](https://arxiv.org/pdf/2201.11903.pdf)ã€‚å…¶æ—¨åœ¨è¿›ä¸€æ­¥æé«˜è¶…å¤§è§„æ¨¡æ¨¡å‹åœ¨ä¸€äº›å¤æ‚ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚å…¶è®¤ä¸ºç°æœ‰çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¯èƒ½å­˜åœ¨ä¸‹é¢æ½œåœ¨çš„é—®é¢˜:

- å¢å¤§æ¨¡å‹å‚æ•°è§„æ¨¡å¯¹äºä¸€äº›å…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ç®—æœ¯ã€å¸¸è¯†æ¨ç†å’Œç¬¦å·æ¨ç†ï¼‰çš„æ•ˆæœå¹¶æœªè¯æ˜æœ‰æ•ˆï¼›
- æœŸæœ›æ¢ç´¢å¦‚ä½•å¯¹å¤§æ¨¡å‹è¿›è¡Œæ¨ç†çš„ç®€å•æ–¹æ³•ã€‚

â€ƒâ€ƒé’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†chain of thought ï¼ˆCoTï¼‰è¿™ç§æ–¹æ³•æ¥åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ±‚è§£æ¨ç†ä»»åŠ¡ã€‚
â€ƒâ€ƒä¸‹é¢è¿™ä¸ªä¾‹å­å¯ä»¥å¾ˆå¥½çš„è¯´æ˜æ€ç»´é“¾åˆ°åº•åœ¨åšä»€ä¹ˆã€‚å·¦å›¾æ˜¯ä¼ ç»Ÿçš„one-shot promptingï¼Œå°±æ˜¯æ‹¼æ¥ä¸€ä¸ªä¾‹å­åœ¨queryçš„å‰é¢ã€‚å³å›¾åˆ™æ˜¯CoTçš„æ”¹è¿›ï¼Œå°±æ˜¯å°†exampleä¸­çš„Answeréƒ¨åˆ†çš„ä¸€ç³»åˆ—çš„æ¨ç†æ­¥éª¤ï¼ˆäººå·¥æ„å»ºï¼‰å†™å‡ºæ¥åï¼Œå†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚é€»è¾‘å°±æ˜¯å¸Œæœ›æ¨¡å‹å­¦ä¼šä¸€æ­¥ä¸€æ­¥çš„è¾“å‡ºæ¨ç†æ­¥éª¤ï¼Œç„¶åç»™å‡ºç»“æœã€‚
![CoT](https://img-blog.csdnimg.cn/c16b0de140e845c6b94bccbe03437d51.png#pic_center)
â€ƒâ€ƒè®ºæ–‡ä¸­é¦–å…ˆåœ¨ç®—æ•°æ¨ç†ï¼ˆarithmetic reasoningï¼‰é¢†åŸŸåšäº†å®éªŒï¼Œä½¿ç”¨äº†5ä¸ªæ•°å­¦ç®—æœ¯æ¨ç†æ•°æ®é›†: GSM8K / SVAMP / ASDiv / AQuA / MAWPSï¼Œå…·ä½“çš„å®éªŒè¿‡ç¨‹è¿™é‡Œä¸å†èµ˜è¿°ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç›´æ¥å‚è€ƒè®ºæ–‡ï¼Œè¿™é‡Œç›´æ¥ç»™å‡ºå®éªŒç»“è®ºï¼ˆå¦‚ä¸‹å›¾ï¼‰:
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/f4cda75074fb4d3698b9d6d8e7d5905d.png#pic_center)

- **CoTå¯¹å°æ¨¡å‹ä½œç”¨ä¸å¤§**: æ¨¡å‹å‚æ•°è‡³å°‘è¾¾åˆ°10Bæ‰æœ‰æ•ˆæœï¼Œè¾¾åˆ°100Bæ•ˆæœæ‰æ˜æ˜¾ã€‚å¹¶ä¸”ä½œè€…å‘ç°ï¼Œåœ¨è¾ƒå°è§„æ¨¡çš„æ¨¡å‹ä¸­äº§ç”Ÿäº†æµç•…ä½†ä¸ç¬¦åˆé€»è¾‘çš„ CoTï¼Œå¯¼è‡´äº†æ¯”Standard promptæ›´ä½çš„è¡¨ç°ï¼›
- **CoTå¯¹å¤æ‚çš„é—®é¢˜çš„æ€§èƒ½å¢ç›Šæ›´å¤§**: ä¾‹å¦‚ï¼Œå¯¹äºGSM8Kï¼ˆbaseline æ€§èƒ½æœ€ä½çš„æ•°æ®é›†ï¼‰ï¼Œæœ€å¤§çš„GPT ï¼ˆ175B GPTï¼‰å’ŒPaLM ï¼ˆ540B PaLMï¼‰æ¨¡å‹çš„æ€§èƒ½æé«˜äº†ä¸€å€ä»¥ä¸Šã€‚è€Œå¯¹äºSingleOpï¼ˆMAWPSä¸­æœ€ç®€å•çš„å­é›†ï¼Œåªéœ€è¦ä¸€ä¸ªæ­¥éª¤å°±å¯ä»¥è§£å†³ï¼‰ï¼Œæ€§èƒ½çš„æé«˜è¦ä¹ˆæ˜¯è´Ÿæ•°ï¼Œè¦ä¹ˆæ˜¯éå¸¸å°ï¼›
- **CoTè¶…è¶ŠSOTA**: åœ¨175Bçš„GPTå’Œ540Bçš„PaLMæ¨¡å‹ä¸‹ï¼ŒCoTåœ¨éƒ¨åˆ†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„SOTAï¼ˆä¹‹å‰çš„SOTA é‡‡ç”¨çš„æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸‹å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ¨¡å¼ï¼‰ã€‚

â€ƒâ€ƒé™¤æ­¤ä¹‹å¤–ï¼Œè®ºæ–‡ä¸­ä¸ºäº†è¯æ˜CoTçš„æœ‰æ•ˆæ€§ï¼Œç›¸ç»§åšäº†æ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰ã€é²æ£’æ€§å®éªŒï¼ˆ Robustness of Chain of Thoughtï¼‰ã€å¸¸è¯†æ¨ç†ï¼ˆCommonsense Reasoningï¼‰å®éªŒã€ç¬¦å·æ¨ç†ï¼ˆSymbolic Reasoningï¼‰å®éªŒï¼Œä¸‹é¢åˆ†åˆ«åšä»¥ç®€å•ä»‹ç»:

- **æ¶ˆèå®éªŒ**: æˆ‘ä»¬çŸ¥é“ï¼Œæ¶ˆèå®éªŒæ˜¯é€šè¿‡ç ”ç©¶ç§»é™¤æŸä¸ªç»„ä»¶ä¹‹åçš„æ€§èƒ½ï¼Œè¯æ˜è¯¥ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡ä¸­é€šè¿‡å¼•å…¥CoTçš„ä¸‰ä¸ªå˜ç§ï¼Œè¯æ˜CoTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœå¦‚ä¸‹å›¾æ‰€ç¤º:
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/be47c4e8e3c64d558480c9322de2f645.png#pic_center)

    - **Equation only**: æŠŠCoTä¸­çš„æ–‡å­—å»æ‰ï¼Œåªä¿ç•™å…¬å¼éƒ¨åˆ†ã€‚ç»“è®º: æ•ˆæœå¯¹äºåŸå§‹promptç•¥æœ‰æå‡ï¼Œå¯¹ç®€å•ä»»åŠ¡æå‡è¾ƒå¤šï¼Œä½†å’ŒCoTæ²¡æ³•æ¯”ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œå‡ ä¹æ²¡æœ‰æå‡ã€‚
    - **Variable compute only**: æŠŠCoTä¸­çš„tokenå…¨æ¢æˆç‚¹ï¼ˆâ€¦ï¼‰ã€‚ è¿™æ˜¯ä¸ºäº†éªŒè¯é¢å¤–çš„è®¡ç®—é‡æ˜¯å¦æ˜¯å½±å“æ¨¡å‹æ€§èƒ½çš„å› ç´ ã€‚ç»“è®º: å…¨æ¢æˆç‚¹ï¼ˆâ€¦ï¼‰åæ•ˆæœå’ŒåŸå§‹promptæ²¡ä»€ä¹ˆåŒºåˆ«ï¼Œè¿™è¯´æ˜è®¡ç®—é‡ç”¨çš„å¤šäº†å¯¹ç»“æœå½±å“å¾ˆå°ï¼ˆå‡ ä¹æ²¡æœ‰å½±å“ï¼‰ï¼Œä¹Ÿè¯´æ˜äº†äººå·¥æ„å»ºçš„CoTï¼ˆtoken sequenceï¼‰å¯¹ç»“æœå½±å“å¾ˆå¤§ã€‚
    - **Chain of thought after answer**: æŠŠæ€ç»´é“¾æ”¾åˆ°ç”Ÿæˆç»“æœä¹‹åã€‚ è¿™æ ·åšçš„åŸå› æ˜¯: çŒœæµ‹CoTå¥æ•ˆçš„åŸå› å¯èƒ½ä»…ä»…æ˜¯è¿™äº›CoTç®€å•çš„è®©æ¨¡å‹æ›´å¥½çš„è®¿é—®äº†é¢„è®­ç»ƒæœŸé—´è·å¾—çš„ç›¸å…³çŸ¥è¯†ï¼Œè€Œä¸æ¨ç†æ²¡å•¥å¤ªå¤§å…³ç³»ã€‚ç»“è®º: CoTæ”¾åˆ°ç”Ÿæˆçš„ç­”æ¡ˆä¹‹åçš„æ•ˆæœå’Œbenchmarkæ²¡å¤ªå¤§åŒºåˆ«ï¼Œè¯´æ˜CoTçš„é¡ºåºé€»è¾‘æ¨ç†è¿˜æ˜¯èµ·åˆ°äº†å¾ˆå¤§ä½œç”¨çš„ï¼ˆä¸ä»…ä»…æ˜¯æ¿€æ´»çŸ¥è¯†ï¼‰ï¼Œæ¢å¥è¯è¯´ï¼Œæ¨¡å‹ç¡®å®æ˜¯ä¾èµ–äºç”Ÿæˆçš„æ€ç»´é“¾ä¸€æ­¥ä¸€æ­¥å¾—åˆ°çš„æœ€ç»ˆç»“æœã€‚
- **é²æ£’æ€§å®éªŒ**: è®ºæ–‡ä¸­é€šè¿‡annotatorsï¼ˆæ ‡æ³¨è€…ï¼‰ï¼Œexemplarsï¼ˆæ ·ä¾‹é€‰æ‹©ï¼‰å’Œmodelsï¼ˆæ¨¡å‹ï¼‰ä¸‰ä¸ªæ–¹é¢å¯¹CoTè¿›è¡Œäº†é²æ£’æ€§åˆ†æã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ€»ä½“ç»“è®ºæ˜¯æ€ç»´é“¾æ™®éæœ‰æ•ˆï¼Œä½†æ˜¯**ä¸åŒçš„CoTæ„å»ºæ–¹å¼/exemplarsçš„é€‰æ‹©/exemplarsçš„æ•°é‡/exemplarsçš„é¡ºåº**ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå½±å“ç€CoTçš„æ•ˆæœã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/aaea0032da834412bd55e5ab13d3ed3e.png#pic_center)

    - **ä¸åŒäººæ„å»ºCoT**: å°½ç®¡æ¯ä¸ªäººæ„å»ºçš„CoTéƒ½ä¸ç›¸åŒï¼Œä½†éƒ½å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿäº†æ­£é¢çš„å½±å“ï¼Œè¯´æ˜CoTç¡®å®æœ‰æ•ˆã€‚ä½†æ˜¯å¦ä¸€æ–¹é¢ï¼Œä¸åŒäººç»™å‡ºçš„ä¸åŒçš„CoTå¯¹æœ€ç»ˆç»“æœçš„å½±å“ç¨‹åº¦è¿˜æ˜¯æœ‰å¾ˆå¤§ä¸åŒçš„ï¼Œè¯´æ˜å¦‚ä½•æ›´å¥½çš„æ„å»ºCoTæ˜¯ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼›
    - **Exemplarsæ ·æœ¬çš„é€‰æ‹©**: ä¸åŒçš„é€‰æ‹©éƒ½ä¼šæœ‰æå‡ï¼Œä½†æ˜¯å·®å¼‚æ˜æ˜¾ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šé€‰æ‹©çš„exemplarså¯ä»¥ç”¨åœ¨å…¶ä»–æ•°æ®é›†ä¸Šï¼Œæ¯”å¦‚è®ºæ–‡ä¸­çš„å®éªŒè®¾ç½®ï¼Œå¯¹äºåŒä¸€ç§ç±»å‹çš„é—®é¢˜ï¼Œå¦‚ç®—æœ¯æ¨ç†ï¼Œå°½ç®¡åœ¨å¤šä¸ªä¸åŒçš„æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œä½†ä½¿ç”¨çš„æ˜¯8ä¸ªç›¸åŒçš„exemplarsï¼Œç»“æœæ²¡æœ‰ç‰¹åˆ«å¤§çš„å·®å¼‚ï¼Œè¯´æ˜exemplarsä¸éœ€è¦æ»¡è¶³å’Œtest setæœ‰ç›¸åŒçš„åˆ†å¸ƒï¼›
    - **Exemplarsæ ·æœ¬çš„é¡ºåº**: æ•´ä½“å½±å“ä¸å¤§ï¼Œé™¤äº†coin flip taskï¼Œå¯èƒ½çš„åŸå› æ˜¯: åŒä¸€ä¸ªç±»åˆ«çš„å¤šä¸ªexemplarsè¿ç»­è¾“å…¥æ¨¡å‹ä½¿å…¶è¾“å‡ºäº§ç”Ÿäº†åå·®ï¼ˆbiasï¼‰ï¼Œä¾‹å¦‚æŠŠ4ä¸ªè´Ÿæ ·æœ¬æ”¾åˆ°4ä¸ªæ­£æ ·æœ¬çš„åé¢è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ›´åŠ å€¾å‘äºè¾“å‡ºè´Ÿlabelï¼›
    - **Exemplarsæ ·æœ¬çš„æ•°é‡**: å¯¹äºæ ‡å‡†promptï¼Œå¢åŠ exemplarsçš„æ•°é‡å¯¹æœ€ç»ˆç»“æœçš„å½±å“ä¸å¤§ã€‚å¯¹äºCoTï¼Œå¢åŠ exemplarså¯¹æ¨¡å‹æœ‰å½±å“ï¼ˆåœ¨æŸäº›æ•°æ®é›†ä¸Šï¼‰ï¼ŒåŒæ—¶ä¹Ÿä¸æ˜¯è¶Šå¤§è¶Šå¥½ï¼›
    - **ä¸åŒLLMä¸Šçš„æ•ˆæœ**:  å¯¹äºä¸€ä¸ªLLMæ•ˆæœå¥½çš„CoT exemplars setæ¢åˆ°å…¶ä»–LLMä¸Šæ•ˆæœä¸ä¸€å®šå¥½ï¼Œä¹Ÿå°±æ˜¯è¯´CoTå¯¹æ¨¡å‹çš„æå‡æ˜¯æ— æ³•åœ¨ä¸åŒçš„LLMä¸Šä¼ é€’çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå±€é™ã€‚
        Â 

    â€ƒâ€ƒå…³äºé²æ£’æ€§å®éªŒï¼Œè®ºæ–‡ä¸­æœ€åæŒ‡å‡º: **Prompt Engineering**ä»ç„¶å¾ˆé‡è¦ï¼Œä¸åŒçš„promptï¼ˆCoTï¼‰çš„è®¾è®¡/æ•°é‡/é¡ºåºéƒ½ä¼šå¯¹æ¨¡å‹äº§ç”Ÿä¸åŒçš„å½±å“ï¼Œä¸”æ–¹å·®è¿˜æ˜¯å¾ˆå¤§çš„ã€‚ å› æ­¤æœªæ¥çš„ä¸€ä¸ªæ–¹å‘å¯èƒ½æ˜¯æ¢ç´¢ä¸€ç§èƒ½å¤Ÿè·å–ç¨³å¥CoTï¼ˆPromptsï¼‰çš„èŒƒå¼ã€‚ æˆ–è®¸å¯ä»¥ç”¨ä¸€ä¸ªLLMè‡ªåŠ¨ç”ŸæˆCoTç”¨äºPromptingï¼Œåé¢æˆ‘ä»¬å°†ä»‹ç»è¿™ç§æŠ€æœ¯: Auto-CoTã€‚

- **å¸¸è¯†æ¨ç†å®éªŒ & ç¬¦å·æ¨ç†å®éªŒ**: æ­¤å¤„æˆ‘ä»¬ä¸åšè¿‡å¤šä»‹ç»ï¼Œè¿™é‡Œç»™å‡ºä¸‰ç§æ¨ç†æ¨¡å¼çš„exemplarsç¤ºä¾‹ï¼ˆç»¿è‰²: ç®—æ•°æ¨ç†ï¼Œæ©™è‰²: å¸¸è¯†æ¨ç†ï¼Œè“è‰²: ç¬¦å·æ¨ç†ï¼‰ï¼Œä¾›å¤§å®¶å‚è€ƒ:
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3f2139a40193402895d649a4e9bf7b62.jpeg#pic_center)


â€ƒâ€ƒè¿™ç¯‡CoTå¼€å±±ä¹‹ä½œé¦–æ¬¡æå‡ºæ€ç»´é“¾(CoT)çš„æ¦‚å¿µï¼Œæ€ç»´é“¾ç®€å•çš„è¯´å°±æ˜¯ä¸€ç³»åˆ—ä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¿™ç¯‡è®ºæ–‡æœ€å¤§çš„è´¡çŒ®å°±æ˜¯å‘ç°äº†åœ¨LLMç”Ÿæˆæ¨ç†ä»»åŠ¡çš„ç»“æœä¹‹å‰ï¼Œå…ˆç”Ÿæˆæ€ç»´é“¾ï¼Œä¼šä½¿æ¨¡å‹çš„æ¨ç†æ€§èƒ½æœ‰å¤§å¹…åº¦çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šï¼Œä½†æ˜¯æœ‰ä¸ªå‰æå°±æ˜¯LLMçš„è§„æ¨¡è¦å¤§äº10Bï¼Œå¦åˆ™CoTæ²¡ç”¨ç”šè‡³èµ·å‰¯ä½œç”¨ã€‚CoTçš„ä¸€å¤§å¥½å¤„æ˜¯æ— éœ€å¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä»…ä»…æ˜¯æ”¹å˜è¾“å…¥å°±å¯ä»¥æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½ã€‚éšç€LLMè¶Šæ¥è¶Šå¤§ï¼Œé«˜æ ¡å’Œå°ä¼ä¸šå¯èƒ½æ— æ³•æ‰¿æ‹…è®­ç»ƒLLMçš„æˆæœ¬ï¼Œå› æ­¤æ— æ³•å‚ä¸å…¶ä¸­è¿›è¡Œç§‘ç ”ä¸å®è·µï¼Œä½†CoTè¿™ä¸ªç ”ç©¶æ–¹å‘ä»ç„¶å¯ä»¥åšã€‚å¯¹äºCoTçš„æ›´å¤šç»†èŠ‚ï¼Œå¤§å®¶å¯å‚è€ƒ[ã€ŠChain-of-Thought Prompting Elicits Reasoning in Large Language Modelsã€‹](https://arxiv.org/pdf/2201.11903.pdf)å’Œ[æ€ç»´é“¾(Chain-of-Thought, CoT)çš„å¼€å±±ä¹‹ä½œ
](https://zhuanlan.zhihu.com/p/612136862?utm_id=0)

#### 4.2ã€Zero-shot-CoTï¼ˆé›¶ç¤ºä¾‹æ€ç»´é“¾ï¼‰

â€ƒâ€ƒ2022å¹´6æœˆä¸œäº¬å¤§å­¦å’Œè°·æ­Œå…±åŒå‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡[ã€ŠLarge Language Models are Zero-Shot Reasonersã€‹](https://arxiv.org/pdf/2205.11916v2.pdf)ï¼Œè¿™æ˜¯ä¸€ç¯‡å…³äºé¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPretrained Large Language Models, LLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ¢ç©¶è®ºæ–‡ã€‚ç›®å‰ï¼ŒLLMsè¢«å¹¿æ³›è¿ç”¨åœ¨å¾ˆå¤šNLPä»»åŠ¡ä¸Šã€‚åŒæ—¶ï¼Œåœ¨æä¾›äº†ç‰¹å®šä»»åŠ¡çš„ç¤ºä¾‹ä¹‹åï¼ŒLLMsæ˜¯ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„å­¦ä¹ è€…ã€‚éšç€æ€è€ƒé“¾çš„æç¤ºæ–¹å¼ï¼ˆchain of thought prompting, CoTï¼‰è¢«æå‡ºï¼Œå¯¹LLMsæ¨ç†èƒ½åŠ›çš„æ¢ç©¶ä¸Šå‡åˆ°ä¸€ä¸ªæ–°çš„é«˜åº¦ï¼Œè¿™ç§æç¤ºæ–¹å¼å¯ä»¥å¼•å¯¼æ¨¡å‹é€šè¿‡ç¤ºä¾‹ä¸­ä¸€æ­¥ä¸€æ­¥çš„æ¨ç†æ–¹å¼ï¼Œå»è§£å†³å¤æ‚çš„å¤šæ­¥æ¨ç†ï¼Œåœ¨æ•°å­¦æ¨ç†ï¼ˆarithmetic reasoningï¼‰å’Œç¬¦å·æ¨ç†ï¼ˆsymbolic reasoningï¼‰ä¸­å–å¾—äº†SOTAçš„æˆæœã€‚ä½œè€…åœ¨ç ”ç©¶ä¸­å‘ç°ï¼Œå¯¹æ‹¥æœ‰175Bå‚æ•°çš„GPT-3ï¼Œé€šè¿‡ç®€å•çš„æ·»åŠ â€Letâ€™s think step by stepâ€œï¼Œå¯ä»¥æå‡æ¨¡å‹çš„zero-shotèƒ½åŠ›ã€‚Zero-shot-CoTçš„å…·ä½“æ ¼å¼å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè®ºæ–‡ä¸­çš„å…·ä½“ç»†èŠ‚è¿™é‡Œä¸åšè¿‡å¤šèµ˜è¿°ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯è¯¦è¯»è®ºæ–‡å†…å®¹ã€‚éœ€è¦æ³¨æ„ä¸€ç‚¹çš„æ˜¯ï¼ŒåŒç­‰æ¡ä»¶ä¸‹ï¼ŒZero-shot-CoTçš„æ€§èƒ½æ˜¯ä¸åŠManual-CoTçš„ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/6dcd286feadf4fcea7951b6f4ede0bed.jpeg#pic_center)

#### 4.3ã€Auto-CoTï¼ˆè‡ªåŠ¨æ€ç»´é“¾ï¼‰

â€ƒâ€ƒå‰æ–‡å·²ç»æåˆ°è¿‡ï¼Œä¼ ç»ŸCoTçš„ä¸€ä¸ªæœªæ¥ç ”ç©¶æ–¹å‘: å¯ä»¥ç”¨ä¸€ä¸ªLLMè‡ªåŠ¨ç”ŸæˆCoTç”¨äºPromptingï¼Œææ²è€å¸ˆå›¢é˜Ÿåœ¨2022å¹´10æœˆå‘è¡¨çš„è®ºæ–‡[ã€ŠAUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELSã€‹](https://arxiv.org/pdf/2210.03493.pdf)è¯æ˜äº†è¿™ä¸€æŠ€æœ¯æ–¹å‘çš„æœ‰æ•ˆæ€§ï¼Œç§°ä¸º**Auto-CoT**ã€‚
â€ƒâ€ƒç›®å‰è¾ƒä¸ºæµè¡Œçš„CoTæ–¹æ³•æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯Manual-CoTï¼Œä¸€ç§æ˜¯Zero-shot-CoTï¼Œä¸¤ç§æ–¹å¼çš„è¾“å…¥æ ¼å¼å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å‰æ–‡æˆ‘ä»¬æåˆ°è¿‡ï¼ŒManual-CoTçš„æ€§èƒ½æ˜¯è¦ä¼˜äºZero-shot-CoTçš„ï¼Œå…³é”®åŸå› åœ¨äºManual-CoTåŒ…å«ä¸€äº›**äººå·¥è®¾è®¡çš„é—®é¢˜**ã€**æ¨ç†æ­¥éª¤**åŠ**ç­”æ¡ˆ**ï¼Œä½†æ˜¯è¿™éƒ¨åˆ†è¦èŠ±è´¹ä¸€å®šçš„äººå·¥æˆæœ¬ï¼Œè€ŒAuto-CoTåˆ™è§£å†³äº†è¿™ä¸€ç—›ç‚¹ï¼Œå…·ä½“åšæ³•æ˜¯:
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/275057c23ba04cda92006c176e89e8f2.png#pic_center)

- é€šè¿‡å¤šæ ·æ€§é€‰å–æœ‰ä»£è¡¨æ€§çš„é—®é¢˜ï¼›
- å¯¹äºæ¯ä¸€ä¸ªé‡‡æ ·çš„é—®é¢˜æ‹¼æ¥ä¸Šâ€œLetâ€™s think step by stepâ€ï¼ˆç±»ä¼¼äº Zero-shot-CoT ï¼‰è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ï¼Œè®©è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤å’Œç­”æ¡ˆï¼Œç„¶åæŠŠè¿™äº›æ‰€æœ‰é‡‡æ ·çš„é—®é¢˜ä»¥åŠè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸­é—´æ¨ç†æ­¥éª¤å’Œç­”æ¡ˆå…¨éƒ¨æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œæ„æˆå°‘æ ·æœ¬å­¦ä¹ çš„æ ·ä¾‹ï¼Œæœ€åå†æ‹¼æ¥ä¸Šéœ€è¦æ±‚è§£çš„é—®é¢˜ä¸€èµ·è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œç»­å†™ï¼Œæœ€ç»ˆæ¨¡å‹ç»­å†™å‡ºäº†ä¸­é—´çš„æ¨ç†æ­¥éª¤ä»¥åŠç­”æ¡ˆã€‚

â€ƒâ€ƒæ€»ä½“æ¥è¯´ï¼ŒAuto-CoTæ˜¯Manual-CoTå’ŒZero-shot-CoTçš„ç»“åˆä½“ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å®éªŒè¯æ˜ï¼Œåœ¨åä¸ªæ•°æ®é›†ä¸ŠAuto-CoTæ˜¯å¯ä»¥åŒ¹é…ç”šè‡³è¶…è¶ŠManual-CoTçš„æ€§èƒ½ï¼Œä¹Ÿå°±è¯´æ˜è‡ªåŠ¨æ„é€ çš„CoTçš„**é—®é¢˜**ã€**ä¸­é—´æ¨ç†æ­¥éª¤**å’Œ**ç­”æ¡ˆ**æ ·ä¾‹æ¯”äººå·¥è®¾è®¡çš„è¿˜è¦å¥½ï¼Œè€Œä¸”è¿˜èŠ‚çœäº†äººå·¥æˆæœ¬ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/c650251cf31149848b7ff2c4f21f8a6a.png#pic_center)
â€ƒâ€ƒè‡³æ­¤ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä¸‰ç§CoTæŠ€æœ¯: Manual-CoTã€Zero-shot-CoTä»¥åŠAuto-CoTï¼Œæœ‰å…³CoTçš„æŠ€æœ¯è¿˜æœ‰å¾ˆå¤šï¼Œéœ€è¦æˆ‘ä»¬æ…¢æ…¢å­¦ä¹ ï¼Œåç»­æŒç»­æ›´æ–°ã€‚

### 5ã€Parameter-Efficient Fine-Tuning (PEFTï¼Œå‚æ•°æœ‰æ•ˆæ€§å¾®è°ƒ)

â€ƒâ€ƒé€šè¿‡å‰æ–‡çš„ä»‹ç»ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠTuningåˆ†ä¸ºä¸¤ç±»:

- **å…¨å‚æ•°å¾®è°ƒ**: è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°åŒ…æ‹¬æ¨¡å‹åœ¨å†…çš„æ‰€æœ‰å‚æ•°ï¼Œä¾‹å¦‚Fine-Tuningã€Prompt-Orient Fine-Tuningç­‰ï¼›
- **éƒ¨åˆ†å‚æ•°å¾®è°ƒ**: è®­ç»ƒè¿‡ç¨‹ä¸­åªæ›´æ–°éƒ¨åˆ†æ¨¡å‹å‚æ•°ï¼Œæˆ–è€…å›ºå®šæ¨¡å‹å‚æ•°ï¼Œåªæ›´æ–°å°‘é‡é¢å¤–æ·»åŠ çš„å‚æ•°ï¼Œå¦‚Parameter-Efficient Prompt Tuningã€P-Tuningç­‰ã€‚

â€ƒâ€ƒæˆ‘ä»¬çŸ¥é“ï¼Œéƒ¨åˆ†å‚æ•°å¾®è°ƒæ¨¡å¼çš„æå‡ºï¼Œä¸€æ–¹é¢æ˜¯ç”±äºèµ„æºé™åˆ¶ï¼Œæ— æ³•æ›´æ–°æ•´ä½“å¤§æ¨¡å‹å‚æ•°ï¼Œå¦ä¸€æ–¹é¢ï¼Œè¦ä¿è¯åœ¨èµ„æºæœ‰é™çš„æ¡ä»¶ä¸‹ï¼Œèƒ½å¤Ÿå°½å¯èƒ½çš„æå‡å¤§æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚ç›®å‰ï¼Œé’ˆå¯¹éƒ¨åˆ†å‚æ•°å¾®è°ƒçš„ç ”ç©¶ï¼Œæ­£å¤„äºè“¬å‹ƒå‘å±•é˜¶æ®µï¼Œè¿™ä¸ªç ”ç©¶é¢†åŸŸæœ‰ä¸ªç»Ÿä¸€çš„åç§°: **Parameter-Efficient Fine-Tuning (PEFT)**ï¼Œå³**å‚æ•°æœ‰æ•ˆæ€§å¾®è°ƒ**ï¼ŒPEFTæ–¹æ³•ä»…å¾®è°ƒå°‘é‡æˆ–é¢å¤–çš„æ¨¡å‹å‚æ•°ï¼Œå›ºå®šå¤§éƒ¨åˆ†é¢„è®­ç»ƒå‚æ•°ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼ŒåŒæ—¶æœ€å…ˆè¿›çš„ PEFT æŠ€æœ¯ä¹Ÿèƒ½å®ç°äº†ä¸å…¨é‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚å‰æ–‡æåˆ°çš„Prompt-Tuningï¼ŒåŒ…æ‹¬P-Tuningç­‰ï¼Œéƒ½å¯ä»¥è§†ä¸ºPEFTçš„ä¸€ç§ã€‚æ€»ä½“æ¥è¯´ï¼Œå‚æ•°æœ‰æ•ˆæ€§å¾®è°ƒå¯åˆ†ä¸ºä¸‰ä¸ªç±»åˆ«:

- **Prompt-Tuning**: åœ¨æ¨¡å‹çš„è¾“å…¥æˆ–éšå±‚æ·»åŠ ä¸ªé¢å¤–å¯è®­ç»ƒçš„å‰ç¼€ tokensï¼ˆè¿™äº›å‰ç¼€æ˜¯è¿ç»­çš„ä¼ªtokensï¼Œä¸å¯¹åº”çœŸå®çš„tokensï¼‰ï¼Œåªè®­ç»ƒè¿™äº›å‰ç¼€å‚æ•°ï¼ŒåŒ…æ‹¬prefix-tuningã€parameter-efficient Prompt Tuningã€P-Tuningç­‰ï¼›
- **Adapter-Tuning**: å°†è¾ƒå°çš„ç¥ç»ç½‘ç»œå±‚æˆ–æ¨¡å—æ’å…¥é¢„è®­ç»ƒæ¨¡å‹çš„æ¯ä¸€å±‚ï¼Œè¿™äº›æ–°æ’å…¥çš„ç¥ç»æ¨¡å—ç§°ä¸ºadapterï¼ˆé€‚é…å™¨ï¼‰ï¼Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ—¶ä¹Ÿåªè®­ç»ƒè¿™äº›é€‚é…å™¨å‚æ•°ï¼›
- **LoRA**: é€šè¿‡å­¦ä¹ å°å‚æ•°çš„ä½ç§©çŸ©é˜µæ¥è¿‘ä¼¼æ¨¡å‹æƒé‡çŸ©é˜µçš„å‚æ•°æ›´æ–°ï¼Œè®­ç»ƒæ—¶åªä¼˜åŒ–ä½ç§©çŸ©é˜µå‚æ•°ã€‚

â€ƒâ€ƒæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹å…¶ä¸­æµè¡Œçš„PEFTç®—æ³•è¿›è¡Œè¯¦ç»†ä»‹ç»ã€‚

#### 5.1ã€PEFTä»‹ç»

- **Prefix-Tuning**: Prefix-Tuningä¹Ÿæ˜¯ä¸€ç§Prompt-Tuningï¼Œæ˜¯æœ€æ—©æå‡ºsoft-promptçš„è®ºæ–‡ä¹‹ä¸€[ã€ŠPrefix-Tuning: Optimizing Continuous Prompts for Generationã€‹](https://aclanthology.org/2021.acl-long.353.pdf)ï¼Œæ–¯å¦ç¦å¤§å­¦äº2021å¹´å‘è¡¨ã€‚Prefix-Tuningåœ¨æ¨¡å‹è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªè¿ç»­çš„ä¸”ä»»åŠ¡ç‰¹å®šçš„å‘é‡åºåˆ—ï¼ˆcontinuous task-specific vectorsï¼‰ï¼Œç§°ä¹‹ä¸ºå‰ç¼€ï¼ˆprefixï¼‰ã€‚å‰ç¼€åŒæ ·æ˜¯ä¸€ç³»åˆ—â€œè™šæ‹Ÿ tokensâ€ï¼Œå³æ²¡æœ‰çœŸå®è¯­ä¹‰ã€‚ä¸æ›´æ–°æ‰€æœ‰ PLM å‚æ•°çš„å…¨é‡å¾®è°ƒä¸åŒï¼ŒPrefix-Tuningå›ºå®šPLMçš„æ‰€æœ‰å‚æ•°ï¼Œåªæ›´æ–°ä¼˜åŒ–ç‰¹å®šä»»åŠ¡çš„prefixã€‚Prefix-Tuningä¸ä¼ ç»ŸFine-Tuningçš„å¯¹æ¯”å›¾å¦‚ä¸‹æ‰€ç¤º:
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/27aa031746bc403793e27a7ef70833b6.png#pic_center)
    â€ƒâ€ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒPrefix-Tuningæœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯è‡ªå›å½’æ¨¡å‹ï¼ˆä¾‹å¦‚GPT-2ï¼‰ï¼Œåœ¨è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªå‰ç¼€å¾—åˆ° \[ P R E F I X ; x ; y \] \[PREFIX;x;y\] \[PREFIX;x;y\]ï¼›å¦ä¸€ç§æ˜¯encoder-decoderæ¨¡å‹ï¼ˆä¾‹å¦‚Bartï¼‰ï¼Œåœ¨ç¼–ç å™¨å’Œè§£ç å™¨å‰åŠ å‰ç¼€å¾—åˆ° \[ P R E F I X ; x ; P R E F I X â€² ; y \] \[PREFIX;x;PREFIX^{'};y\] \[PREFIX;x;PREFIXâ€²;y\]ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä»¥GPT-2çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸ºä¾‹ï¼Œä»‹ç»ä¸‹Prefix-Tuningçš„æµç¨‹ã€‚
    â€ƒâ€ƒé¦–å…ˆï¼Œå¯¹äºä¼ ç»Ÿçš„GPT-2æ¨¡å‹æ¥è¯´ï¼Œå°†è¾“å…¥ x x xå’Œè¾“å‡º y y yæ‹¼æ¥ä¸º z = \[ x ; y \] z=\[x;y\] z\=\[x;y\]ï¼Œå…¶ä¸­ X i d x X\_{idx} Xidxâ€‹å’Œ Y i d x Y\_{idx} Yidxâ€‹åˆ†åˆ«ä¸ºè¾“å…¥å’Œè¾“å‡ºåºåˆ—çš„ç´¢å¼•ï¼Œ h i âˆˆ R d h\_{i} \\in R^{d} hiâ€‹âˆˆRdæ˜¯æ¯ä¸ªæ—¶é—´æ­¥ i i iä¸‹çš„æ¿€æ´»å‘é‡ï¼ˆéšè—å±‚å‘é‡ï¼‰ï¼Œ h i = \[ h i ( 1 ) ; â€¦ â€¦ ; h i ( n ) \] h\_{i}=\[h\_{i}^{(1)}; â€¦â€¦;h\_{i}^{(n)}\] hiâ€‹\=\[hi(1)â€‹;â€¦â€¦;hi(n)â€‹\]è¡¨ç¤ºåœ¨å½“å‰æ—¶é—´æ­¥çš„æ‰€æœ‰æ¿€æ´»å±‚çš„æ‹¼æ¥ï¼Œ h i ( j ) h\_{i}^{(j)} hi(j)â€‹æ˜¯æ—¶é—´æ­¥ i i içš„ç¬¬ j j jå±‚æ¿€æ´»å±‚ã€‚è‡ªå›å½’æ¨¡å‹é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®— h i h\_{i} hiâ€‹ï¼Œå…¶ä¸­ Ï• \\phi Ï•æ˜¯æ¨¡å‹å‚æ•°:
    h i = L M Ï• ( z i , h < i ) Â  h\_{i} =LM\_{\\phi}(z\_{i},h\_{<i})\\ hiâ€‹\=LMÏ•â€‹(ziâ€‹,h<iâ€‹)Â 
    h i h\_{i} hiâ€‹çš„æœ€åä¸€å±‚ï¼Œç”¨æ¥è®¡ç®—ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ:
    p Ï• ( z i + 1 âˆ£ h â‰¤ i ) = s o f t m a x ( W Ï• h i ( n ) ) Â  p\_{\\phi}(z\_{i+1}|h\_{â‰¤i}) =softmax(W\_{\\phi}h\_{i}^{(n)})\\ pÏ•â€‹(zi+1â€‹âˆ£hâ‰¤iâ€‹)\=softmax(WÏ•â€‹hi(n)â€‹)Â 
    å…¶ä¸­ W Ï• W\_{\\phi} WÏ•â€‹æ˜¯å°† h i ( n ) h\_{i}^{(n)} hi(n)â€‹æ ¹æ®è¯è¡¨å¤§å°è¿›è¡Œæ˜ å°„ã€‚
    â€ƒâ€ƒåœ¨é‡‡ç”¨Prefix-TuningæŠ€æœ¯åï¼Œåˆ™åœ¨è¾“å…¥å‰æ·»åŠ å‰ç¼€ï¼Œå³å°†prefixå’Œè¾“å…¥ä»¥åŠè¾“å‡ºè¿›è¡Œæ‹¼æ¥å¾—åˆ° z = \[ P R E F I X ; x ; y \] z=\[PREFIX;x;y\] z\=\[PREFIX;x;y\]ï¼Œ P i d x P\_{idx} Pidxâ€‹ä¸ºå‰ç¼€åºåˆ—çš„ç´¢å¼•ï¼Œ âˆ£ P i d x âˆ£ |P\_{idx}| âˆ£Pidxâ€‹âˆ£ä¸ºå‰ç¼€åºåˆ—çš„é•¿åº¦ï¼Œè¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒPrefix-Tuningæ˜¯åœ¨æ¨¡å‹çš„æ¯ä¸€å±‚éƒ½æ·»åŠ prefixï¼ˆæ³¨æ„ä¸æ˜¯åªæœ‰è¾“å…¥å±‚ï¼Œä¸­é—´å±‚ä¹Ÿä¼šæ·»åŠ prefixï¼Œç›®çš„å¢åŠ å¯è®­ç»ƒå‚æ•°ï¼‰ã€‚å‰ç¼€åºåˆ—ç´¢å¼•å¯¹åº”ç€ç”± Î¸ \\theta Î¸å‚æ•°åŒ–çš„å‘é‡çŸ©é˜µ P Î¸ P\_{\\theta} PÎ¸â€‹ï¼Œç»´åº¦ä¸º âˆ£ P i d x âˆ£ Ã— d i m ( h i ) |P\_{idx}|\\times dim(h\_{i}) âˆ£Pidxâ€‹âˆ£Ã—dim(hiâ€‹)ã€‚éšå±‚è¡¨ç¤ºçš„è®¡ç®—å¦‚ä¸‹å¼æ‰€ç¤ºï¼Œè‹¥ç´¢å¼•ä¸ºå‰ç¼€ç´¢å¼• P i d x P\_{idx} Pidxâ€‹ï¼Œç›´æ¥ä» P Î¸ P\_{\\theta} PÎ¸â€‹å¤åˆ¶å¯¹åº”çš„å‘é‡ä½œä¸º h i h\_{i} hiâ€‹ï¼ˆåœ¨æ¨¡å‹æ¯ä¸€å±‚éƒ½æ·»åŠ å‰ç¼€å‘é‡ï¼‰ï¼›å¦åˆ™ç›´æ¥é€šè¿‡LMè®¡ç®—å¾—åˆ°ï¼ŒåŒæ—¶ï¼Œç»è¿‡LMè®¡ç®—çš„ h i h\_{i} hiâ€‹ä¹Ÿä¾èµ–äºå…¶å·¦ä¾§çš„å‰ç¼€å‚æ•° P Î¸ P\_{\\theta} PÎ¸â€‹ï¼Œå³é€šè¿‡å‰ç¼€æ¥å½±å“åç»­çš„åºåˆ—æ¿€æ´»å‘é‡å€¼ï¼ˆéšå±‚å‘é‡å€¼ï¼‰ã€‚
    h i = { P Î¸ \[ i , : \] ifÂ Â Â  i âˆˆ P i d x L M Ï• ( z i , h < i ) otherwise h\_{i}= \\begin{cases} P\_{\\theta}\[i,:\]& \\text{if} \\ \\ \\ i\\in P\_{idx}\\\\ LM\_{\\phi}(z\_{i},h\_{<i})& \\text{otherwise} \\end{cases} hiâ€‹\={PÎ¸â€‹\[i,:\]LMÏ•â€‹(ziâ€‹,h<iâ€‹)â€‹ifÂ Â Â iâˆˆPidxâ€‹otherwiseâ€‹
    â€ƒâ€ƒåœ¨è®­ç»ƒæ—¶ï¼ŒPrefix-Tuningçš„ä¼˜åŒ–ç›®æ ‡ä¸æ­£å¸¸å¾®è°ƒç›¸åŒï¼Œä½†åªéœ€è¦æ›´æ–°å‰ç¼€å‘é‡çš„å‚æ•°ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…å‘ç°ç›´æ¥æ›´æ–°å‰ç¼€å‘é‡çš„å‚æ•°ä¼šå¯¼è‡´è®­ç»ƒçš„ä¸ç¨³å®šä¸ç»“æœçš„ç•¥å¾®ä¸‹é™ï¼Œå› æ­¤é‡‡ç”¨äº†é‡å‚æ•°åŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸€ä¸ªæ›´å°çš„çŸ©é˜µ P Î¸ â€² P\_{\\theta}^{'} PÎ¸â€²â€‹å’Œä¸€ä¸ªå¤§å‹å‰é¦ˆç¥ç»ç½‘ç»œ MLP Î¸ \\text{MLP}\_{\\theta} MLPÎ¸â€‹å¯¹ P Î¸ P\_{\\theta} PÎ¸â€‹è¿›è¡Œé‡å‚æ•°åŒ–: P Î¸ \[ i , : \] = MLP Î¸ ( P Î¸ â€² \[ i , : \] ) P\_{\\theta}\[i,:\]=\\text{MLP}\_{\\theta}(P\_{\\theta}^{'}\[i,:\]) PÎ¸â€‹\[i,:\]\=MLPÎ¸â€‹(PÎ¸â€²â€‹\[i,:\])ï¼Œå¯è®­ç»ƒå‚æ•°åŒ…æ‹¬ P Î¸ â€² P\_{\\theta}^{'} PÎ¸â€²â€‹å’Œ MLP Î¸ \\text{MLP}\_{\\theta} MLPÎ¸â€‹çš„å‚æ•°ï¼Œå…¶ä¸­ï¼Œ P Î¸ P\_{\\theta} PÎ¸â€‹å’Œ P Î¸ â€² P\_{\\theta}^{'} PÎ¸â€²â€‹æœ‰ç›¸åŒçš„è¡Œç»´åº¦ï¼ˆä¹Ÿå°±æ˜¯ç›¸åŒçš„prefix lengthï¼‰, ä½†ä¸åŒçš„åˆ—ç»´åº¦ã€‚åœ¨è®­ç»ƒæ—¶ï¼ŒLM çš„å‚æ•° Ï• \\phi Ï•è¢«å›ºå®šï¼Œåªæœ‰å‰ç¼€å‚æ•° P Î¸ â€² P\_{\\theta}^{'} PÎ¸â€²â€‹å’Œ MLP Î¸ \\text{MLP}\_{\\theta} MLPÎ¸â€‹çš„å‚æ•°ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚è®­ç»ƒå®Œæˆåï¼Œ P Î¸ â€² P\_{\\theta}^{'} PÎ¸â€²â€‹å’Œ MLP Î¸ \\text{MLP}\_{\\theta} MLPÎ¸â€‹çš„å‚æ•°è¢«ä¸¢æ‰ï¼Œåªæœ‰å‰ç¼€å‚æ•° P Î¸ P\_{\\theta} PÎ¸â€‹è¢«ä¿å­˜ã€‚
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/f1daf9e5ba2047dc992df48fb965abe7.png#pic_center)
    â€ƒâ€ƒä¸Šè¿°å†…å®¹è¯¦ç»†ä»‹ç»äº†Prefix-Tuningçš„ä¸»è¦è®­ç»ƒæµç¨‹ï¼Œä¸‹é¢æˆ‘ä»¬ç»™å‡ºè®ºæ–‡ä¸­é€šè¿‡å®éªŒå¾—å‡ºçš„ä¸‰ä¸ªä¸»è¦ç»“è®º:

    - **æ–¹æ³•æœ‰æ•ˆæ€§**: ä½œè€…é‡‡ç”¨äº†Table-To-Textä¸Summarizationä½œä¸ºå®éªŒä»»åŠ¡ï¼Œåœ¨Table-To-Textä»»åŠ¡ä¸Šï¼ŒPrefix-Tuningåœ¨ä¼˜åŒ–ç›¸åŒå‚æ•°çš„æƒ…å†µä¸‹ç»“æœå¤§å¹…ä¼˜äºAdapterï¼Œå¹¶ä¸å…¨å‚æ•°å¾®è°ƒå‡ ä¹ç›¸åŒã€‚è€Œåœ¨Summarizationä»»åŠ¡ä¸Šï¼ŒPrefix-Tuningæ–¹æ³•åœ¨ä½¿ç”¨2%å‚æ•°ä¸0.1%å‚æ•°æ—¶ç•¥å¾®å·®äºå…¨å‚æ•°å¾®è°ƒï¼Œä½†ä»ä¼˜äºAdapterå¾®è°ƒï¼›
    - **Full vs Embedding-only**: Embedding-onlyæ–¹æ³•åªåœ¨embeddingå±‚æ·»åŠ å‰ç¼€å‘é‡å¹¶ä¼˜åŒ–ï¼Œè€ŒFullä»£è¡¨çš„Prefix-Tuningä¸ä»…åœ¨embeddingå±‚æ·»åŠ å‰ç¼€å‚æ•°ï¼Œè¿˜åœ¨æ¨¡å‹æ‰€æœ‰å±‚æ·»åŠ å‰ç¼€å¹¶ä¼˜åŒ–ã€‚å®éªŒå¾—åˆ°ä¸€ä¸ªä¸åŒæ–¹æ³•çš„è¡¨è¾¾èƒ½åŠ›å¢å¼ºé“¾æ¡: discrete prompting < embedding-only < Prefix-Tuningã€‚åŒæ—¶ï¼ŒPrefix-Tuningå¯ä»¥ç›´æ¥ä¿®æ”¹æ¨¡å‹æ›´æ·±å±‚çš„è¡¨ç¤ºï¼Œé¿å…äº†è·¨è¶Šç½‘ç»œæ·±åº¦çš„é•¿è®¡ç®—è·¯å¾„é—®é¢˜ï¼›
    - **Prefix-Tuning vs Infix-Tuning**: é€šè¿‡å°†å¯è®­ç»ƒçš„å‚æ•°æ”¾ç½®åœ¨ x x xå’Œ y y yçš„ä¸­é—´æ¥ç ”ç©¶å¯è®­ç»ƒå‚æ•°ä½ç½®å¯¹æ€§èƒ½çš„å½±å“ï¼Œå³ \[ x ; I n f i x ; y \] \[x;Infix;y\] \[x;Infix;y\]ï¼Œè¿™ç§æ–¹å¼æˆä¸ºinfix-tuningã€‚å®éªŒè¡¨æ˜Prefix-Tuningæ€§èƒ½å¥½äº infix-tuningï¼Œå› ä¸ºprefixèƒ½å¤ŸåŒæ—¶å½±å“ x x xå’Œ y y yçš„éšå±‚å‘é‡ï¼Œè€Œinfixåªèƒ½å¤Ÿå½±å“ y y yçš„éšå±‚å‘é‡ã€‚
        Â 

    â€ƒâ€ƒæˆ‘ä»¬å›é¡¾ä¸‹å‰æ–‡æåˆ°çš„parameter-efficient prompt tuningï¼ˆä¸‹é¢ç®€ç§°ä¸ºPrompt Tuningï¼‰ï¼Œå…¶è®ºæ–‡ä¸­æœ‰æåˆ°ï¼Œå®ƒå¯ä»¥çœ‹ä½œæ˜¯Prefix-Tuningçš„ç®€åŒ–ç‰ˆã€‚æ€»ç»“ä¸‹ä¸¤è€…çš„ä¸åŒç‚¹:

    - **å‚æ•°æ›´æ–°ç­–ç•¥ä¸åŒ**: Prompt Tuningåªå¯¹è¾“å…¥å±‚(Embedding)è¿›è¡Œå¾®è°ƒï¼Œè€ŒPrefix-Tuningæ˜¯å¯¹æ¯ä¸€å±‚å…¨éƒ¨è¿›è¡Œå¾®è°ƒã€‚å› æ­¤parameter-efficient prompt tuningçš„å¾®è°ƒå‚æ•°é‡çº§è¦æ›´å°ï¼ˆå¦‚ä¸‹å›¾ï¼‰ï¼Œä¸”ä¸éœ€è¦ä¿®æ”¹åŸå§‹æ¨¡å‹ç»“æ„ï¼›
    - **å‚æ•°ç”Ÿæˆæ–¹å¼ä¸åŒ**: Prompt Tuningä¸Prefix-TuningåŠP-Tuningä¸åŒçš„æ˜¯ï¼Œæ²¡æœ‰é‡‡ç”¨ä»»ä½•çš„promptæ˜ å°„å±‚ï¼ˆå³Prefix-Tuningä¸­çš„é‡å‚æ•°åŒ–å±‚ä¸P-Tuningä¸­çš„prompt encoderï¼‰ï¼Œè€Œæ˜¯ç›´æ¥å¯¹prompt tokenå¯¹åº”çš„embeddingè¿›è¡Œäº†è®­ç»ƒï¼›
    - **é¢å‘ä»»åŠ¡ä¸åŒ**: Pompt Tuningã€P-Tuningä»¥åŠåé¢è¦ä»‹ç»çš„P-Tuning v2éƒ½æ˜¯é¢å‘çš„NLUä»»åŠ¡è¿›è¡Œæ•ˆæœä¼˜åŒ–åŠè¯„æµ‹çš„ï¼Œè€ŒPrefix-Tuningé’ˆå¯¹çš„åˆ™æ˜¯NLGä»»åŠ¡ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3f8b40dff5184a439dce772593efe61b.png#pic_center)

- **P-Tuning v2**: P-Tuning v2æ˜¯2022å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡[ã€ŠP-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasksã€‹](https://arxiv.org/pdf/2110.07602.pdf)ï¼Œæ€»ç»“æ¥è¯´æ˜¯åœ¨Prefix-Tuningå’ŒP-Tuningçš„åŸºç¡€ä¸Šè¿›è¡Œçš„ä¼˜åŒ–ã€‚ä¸‹é¢æˆ‘ä»¬ç®€å•ä»‹ç»ä¸‹P-Tuning v2æ–¹æ³•ã€‚
    - **P-Tuning v2é’ˆå¯¹Prefix-Tuningã€P-Tuningè§£å†³çš„é—®é¢˜**:

        - Prefix-Tuningæ˜¯é’ˆå¯¹äºç”Ÿæˆä»»åŠ¡è€Œè¨€çš„ï¼Œä¸èƒ½å¤„ç†å›°éš¾çš„åºåˆ—æ ‡æ³¨ä»»åŠ¡ã€æŠ½å–å¼é—®ç­”ç­‰ï¼Œç¼ºä¹æ™®éæ€§ï¼›
        - å½“æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œç‰¹åˆ«æ˜¯å°äº100äº¿ä¸ªå‚æ•°æ—¶ï¼Œå®ƒä»¬ä»ç„¶ä¸å¦‚Fine-Tuningã€‚
    - **P-Tuning v2çš„ä¼˜ç‚¹**:

        - P-Tuning v2åœ¨ä¸åŒçš„æ¨¡å‹è§„æ¨¡ï¼ˆä»300Måˆ°100Bçš„å‚æ•°ï¼‰å’Œå„ç§å›°éš¾çš„NLUä»»åŠ¡ï¼ˆå¦‚é—®ç­”å’Œåºåˆ—æ ‡æ³¨ï¼‰ä¸Šçš„è¡¨ç°ä¸Fine-Tuningç›¸åŒ¹é…ï¼›
        - ä¸Fine-Tuningç›¸æ¯”ï¼ŒP-Tuning v2æ¯ä¸ªä»»åŠ¡çš„å¯è®­ç»ƒå‚æ•°ä¸º0.1%åˆ°3%ï¼Œè¿™å¤§å¤§é™ä½äº†è®­ç»ƒæ—¶é—´çš„å†…å­˜æ¶ˆè€—å’Œæ¯ä¸ªä»»åŠ¡çš„å­˜å‚¨æˆæœ¬ã€‚
    - **P-Tuning v2çš„æ ¸å¿ƒç‚¹**:

        - **NLUä»»åŠ¡ä¼˜åŒ–**: ä¸»è¦é’ˆå¯¹NLUä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œæå‡P-Tuning v2åœ¨NLUä»»åŠ¡ä¸Šçš„æ•ˆæœï¼›
        - **æ·±åº¦æç¤ºä¼˜åŒ–**: å‚è€ƒPrefix-Tuningï¼Œä¸åŒå±‚åˆ†åˆ«å°†promptä½œä¸ºå‰ç¼€tokenåŠ å…¥åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œå½¼æ­¤ç›¸äº’ç‹¬ç«‹ï¼ˆæ³¨æ„ï¼Œè¿™éƒ¨åˆ†tokençš„å‘é‡è¡¨å¾æ˜¯äº’ä¸ç›¸åŒçš„ï¼Œå³åŒPrefix-Tuningä¸€è‡´ï¼Œä¸æ˜¯å‚æ•°å…±äº«æ¨¡å¼ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä¸€æ–¹é¢ï¼ŒP-Tuning v2æœ‰æ›´å¤šçš„å¯ä¼˜åŒ–çš„ç‰¹å®šä»»åŠ¡å‚æ•°ï¼ˆä»0.01%åˆ°0.1%-3%ï¼‰ï¼Œä»¥ä¿è¯å¯¹ç‰¹å®šä»»åŠ¡æœ‰æ›´å¤šçš„å‚æ•°å®¹é‡ï¼Œä½†ä»ç„¶æ¯”è¿›è¡Œå®Œæ•´çš„Fine-Tuningä»»åŠ¡å‚æ•°é‡å°å¾—å¤šï¼›å¦ä¸€æ–¹é¢ï¼Œæ·»åŠ åˆ°æ›´æ·±å±‚çš„æç¤ºï¼Œå¯ä»¥å¯¹è¾“å‡ºé¢„æµ‹äº§ç”Ÿæ›´ç›´æ¥çš„å½±å“ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/8b6e7e05931c45c9b04528de9162298e.png#pic_center)
    - **P-Tuning v2çš„å…¶ä»–ä¼˜åŒ–åŠå®æ–½ç‚¹**:

        - **é‡å‚æ•°åŒ–**: ä»¥å‰çš„æ–¹æ³•åˆ©ç”¨é‡å‚æ•°åŒ–åŠŸèƒ½æ¥æé«˜è®­ç»ƒé€Ÿåº¦ã€é²æ£’æ€§å’Œæ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒMLPçš„Prefix-Tuningå’ŒLSTMçš„P-Tuningï¼‰ã€‚ç„¶è€Œï¼Œå¯¹äºNLUä»»åŠ¡ï¼Œè®ºæ–‡ä¸­è¡¨æ˜è¿™ç§æŠ€æœ¯çš„å¥½å¤„å–å†³äºä»»åŠ¡å’Œæ•°æ®é›†ã€‚å¯¹äºä¸€äº›æ•°æ®é›†ï¼ˆå¦‚RTEå’ŒCoNLL04ï¼‰ï¼ŒMLPçš„é‡æ–°å‚æ•°åŒ–å¸¦æ¥äº†æ¯”åµŒå…¥æ›´ç¨³å®šçš„æ”¹å–„ï¼›å¯¹äºå…¶ä»–çš„æ•°æ®é›†ï¼Œé‡å‚æ•°åŒ–å¯èƒ½æ²¡æœ‰æ˜¾ç¤ºå‡ºä»»ä½•æ•ˆæœï¼ˆå¦‚BoolQï¼‰ï¼Œæœ‰æ—¶ç”šè‡³æ›´ç³Ÿï¼ˆå¦‚CoNLL12ï¼‰ã€‚éœ€æ ¹æ®ä¸åŒæƒ…å†µå»å†³å®šæ˜¯å¦ä½¿ç”¨ï¼›
        - **æç¤ºé•¿åº¦**: æç¤ºé•¿åº¦åœ¨æç¤ºä¼˜åŒ–æ–¹æ³•çš„è¶…å‚æ•°æœç´¢ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚è®ºæ–‡ä¸­è¡¨æ˜ä¸åŒçš„ç†è§£ä»»åŠ¡é€šå¸¸ç”¨ä¸åŒçš„æç¤ºé•¿åº¦æ¥å®ç°å…¶æœ€ä½³æ€§èƒ½ï¼Œæ¯”å¦‚ä¸€äº›ç®€å•çš„taskå€¾å‘æ¯”è¾ƒçŸ­çš„promptï¼ˆless than 20ï¼‰ï¼Œè€Œä¸€äº›æ¯”è¾ƒéš¾çš„åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œé•¿åº¦éœ€æ±‚æ¯”è¾ƒå¤§ï¼›
        - **å¤šä»»åŠ¡å­¦ä¹ **: å¤šä»»åŠ¡å­¦ä¹ å¯¹P-Tuning v2æ–¹æ³•æ¥è¯´æ˜¯å¯é€‰çš„ï¼Œä½†å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„ã€‚åœ¨å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œç”¨å…±äº«çš„promptså»è¿›è¡Œå¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œå¯ä»¥è®©promptsæœ‰æ¯”è¾ƒå¥½çš„åˆå§‹åŒ–ï¼›
        - **åˆ†ç±»æ–¹å¼é€‰æ‹©**: å¯¹æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œç”¨åŸå§‹çš„CLS+linear headæ¨¡å¼æ›¿æ¢Prompt-TuningèŒƒå¼ä¸­ä½¿ç”¨çš„Verbalizer+LM headæ¨¡å¼ï¼Œä¸è¿‡æ•ˆæœå¹¶ä¸æ˜æ˜¾ï¼Œå¦‚ä¸‹å›¾ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/80409db3a7174e59a1c8263b430f7080.png#pic_center)
- **Adapter-Tuning**: [ã€ŠParameter-Efficient Transfer Learning for NLPã€‹](https://arxiv.org/pdf/1902.00751.pdf)è¿™é¡¹2019å¹´çš„å·¥ä½œç¬¬ä¸€æ¬¡æå‡ºäº†Adapteræ–¹æ³•ã€‚ä¸Prefix-Tuningå’ŒPrompt Tuningè¿™ç±»åœ¨è¾“å…¥å‰æ·»åŠ å¯è®­ç»ƒprompt embeddingå‚æ•°æ¥ä»¥å°‘é‡å‚æ•°é€‚é…ä¸‹æ¸¸ä»»åŠ¡çš„æ–¹å¼ä¸é€šï¼ŒAdapter-Tuning åˆ™æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹å†…éƒ¨çš„ç½‘ç»œå±‚ä¹‹é—´æ·»åŠ æ–°çš„ç½‘ç»œå±‚æˆ–æ¨¡å—æ¥é€‚é…ä¸‹æ¸¸ä»»åŠ¡ã€‚å‡è®¾é¢„è®­ç»ƒæ¨¡å‹å‡½æ•°è¡¨ç¤ºä¸º Ï• w ( x ) \\phi\_{w}(x) Ï•wâ€‹(x)ï¼Œå¯¹äºAdapter-Tuningï¼Œæ·»åŠ é€‚é…å™¨ä¹‹åæ¨¡å‹å‡½æ•°æ›´æ–°ä¸º:  Ï• w , w 0 ( x ) \\phi\_{w,w\_{0}}(x) Ï•w,w0â€‹â€‹(x)ï¼Œ w w wæ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œ w 0 w\_{0} w0â€‹æ˜¯æ–°æ·»åŠ çš„é€‚é…å™¨çš„å‚æ•°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ w w wè¢«å›ºå®šï¼Œåªæœ‰ w 0 w\_{0} w0â€‹è¢«æ›´æ–°ã€‚ âˆ£ w 0 âˆ£ â‰ª âˆ£ w âˆ£ |w\_{0}|\\ll|w| âˆ£w0â€‹âˆ£â‰ªâˆ£wâˆ£ï¼Œè¿™ä½¿å¾—ä¸åŒä¸‹æ¸¸ä»»åŠ¡åªéœ€è¦æ·»åŠ å°‘é‡å¯è®­ç»ƒçš„å‚æ•°å³å¯ï¼ŒèŠ‚çœè®¡ç®—å’Œå­˜å‚¨å¼€é”€ï¼ŒåŒæ—¶å…±äº«å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å†»ç»“åœ¨ä¿ç•™åŸæ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å¯¹å·²æœ‰ç»“æ„æ·»åŠ ä¸€äº›é¢å¤–å‚æ•°ï¼Œå¯¹è¯¥éƒ¨åˆ†å‚æ•°è¿›è¡Œè®­ç»ƒä»è€Œè¾¾åˆ°å¾®è°ƒçš„æ•ˆæœã€‚
    â€ƒâ€ƒè®ºæ–‡ä¸­é‡‡ç”¨Bertä½œä¸ºå®éªŒæ¨¡å‹ï¼ŒAdapteræ¨¡å—è¢«æ·»åŠ åˆ°æ¯ä¸ªtransformerå±‚ä¸¤æ¬¡ã€‚é€‚é…å™¨æ˜¯ä¸€ä¸ª bottleneckï¼ˆç“¶é¢ˆï¼‰ç»“æ„çš„æ¨¡å—ï¼Œç”±ä¸€ä¸ªä¸¤å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆç”±å‘ä¸‹æŠ•å½±çŸ©é˜µã€éçº¿æ€§å‡½æ•°å’Œå‘ä¸ŠæŠ•å½±çŸ©é˜µæ„æˆï¼‰å’Œä¸€ä¸ªè¾“å…¥è¾“å‡ºä¹‹é—´çš„æ®‹å·®è¿æ¥ç»„æˆã€‚å…¶æ€»ä½“ç»“æ„å¦‚ä¸‹ï¼ˆè·Ÿè®ºæ–‡ä¸­çš„ç»“æ„æœ‰äº›å‡ºå…¥ï¼Œç›®å‰æ²¡æœ‰ç†è§£è®ºæ–‡ä¸­çš„ç»“æ„æ˜¯æ€ä¹ˆæ„å»ºå‡ºæ¥çš„ï¼Œä¸ªäººè§‰å¾—ä¸‹å›¾æ›´å‡†ç¡®çš„åˆ»ç”»äº†adapterçš„ç»“æ„ï¼Œæœ‰ä¸åŒè§è§£å¯åœ¨è¯„è®ºåŒºæ²Ÿé€šï¼‰: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/7707eedb17c34e01bfb94486bb014b27.png#pic_center)
    â€ƒâ€ƒAdapterç»“æ„æœ‰ä¸¤ä¸ªç‰¹ç‚¹: è¾ƒå°‘çš„å‚æ•°ã€åœ¨åˆå§‹åŒ–æ—¶ä¸åŸç»“æ„ç›¸ä¼¼çš„è¾“å‡ºã€‚åœ¨å®é™…å¾®è°ƒæ—¶ï¼Œç”±äºé‡‡ç”¨äº†down-projectä¸up-projectçš„æ¶æ„ï¼Œåœ¨è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒAdapterä¼šå…ˆå°†ç‰¹å¾è¾“å…¥é€šè¿‡down-projectæ˜ å°„åˆ°è¾ƒä½ç»´åº¦ï¼Œå†é€šè¿‡up-projectæ˜ å°„å›é«˜ç»´åº¦ï¼Œä»è€Œå‡å°‘å‚æ•°é‡ã€‚Adapter-Tuningåªéœ€è¦è®­ç»ƒåŸæ¨¡å‹0.5%-8%çš„å‚æ•°é‡ï¼Œè‹¥å¯¹äºä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œåªéœ€è¦å¯¹ä¸åŒçš„ä»»åŠ¡ä¿ç•™å°‘é‡Adapterç»“æ„çš„å‚æ•°å³å¯ã€‚ç”±äºAdapterä¸­å­˜åœ¨æ®‹å·®è¿æ¥ç»“æ„ï¼Œé‡‡ç”¨åˆé€‚çš„å°å‚æ•°å»åˆå§‹åŒ–Adapterå°±å¯ä»¥ä½¿å…¶å‡ ä¹ä¿æŒåŸæœ‰çš„è¾“å‡ºï¼Œä½¿å¾—æ¨¡å‹åœ¨æ·»åŠ é¢å¤–ç»“æ„çš„æƒ…å†µä¸‹ä»ç„¶èƒ½åœ¨è®­ç»ƒçš„åˆå§‹é˜¶æ®µè¡¨ç°è‰¯å¥½ã€‚åœ¨GLUEæµ‹è¯•é›†ä¸Šï¼ŒAdapterç”¨äº†æ›´å°‘é‡çš„å‚æ•°è¾¾åˆ°äº†ä¸ä¼ ç»ŸFine-Tuningæ–¹æ³•æ¥è¿‘çš„æ•ˆæœã€‚
- **LoRA**: LoRAæ˜¯åˆä¸€ç§PEFTæ–¹æ³•ï¼Œå¾®è½¯äº2022å¹´å‘è¡¨[ã€ŠLORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELSã€‹](https://arxiv.org/pdf/2106.09685.pdf)ã€‚æˆ‘ä»¬ä¾ç…§ä¸‹å›¾ä»¥åŠè®ºæ–‡ï¼Œç®€å•ä»‹ç»ä¸‹LoRAçš„å®ç°åŸç†ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/f3c74f46e06242cd96e01da393d6bfb2.png#pic_center)
    â€ƒâ€ƒLoRAåŸç†å…¶å®å¹¶ä¸å¤æ‚ã€‚ç®€å•ç†è§£ä¸€ä¸‹ï¼Œå°±æ˜¯åœ¨æ¨¡å‹çš„Linearå±‚çš„æ—è¾¹ï¼Œå¢åŠ ä¸€ä¸ªâ€œæ—æ”¯â€ï¼Œè¿™ä¸ªâ€œæ—æ”¯â€çš„ä½œç”¨ï¼Œå°±æ˜¯ä»£æ›¿åŸæœ‰çš„å‚æ•°çŸ©é˜µ W W Wè¿›è¡Œè®­ç»ƒã€‚ç»“åˆä¸Šå›¾ï¼Œæˆ‘ä»¬æ¥ç›´è§‚åœ°ç†è§£ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ï¼Œè¾“å…¥ x âˆˆ R d x\\in R^{d} xâˆˆRdï¼Œä¸¾ä¸ªä¾‹å­ï¼Œåœ¨æ™®é€šçš„transformeræ¨¡å‹ä¸­ï¼Œè¿™ä¸ª x x xå¯èƒ½æ˜¯embeddingçš„è¾“å‡ºï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯ä¸Šä¸€å±‚transformer layerçš„è¾“å‡ºï¼Œè€Œ d d dä¸€èˆ¬å°±æ˜¯768æˆ–è€…1024ã€‚æŒ‰ç…§åŸæœ¬çš„è·¯çº¿ï¼Œå®ƒåº”è¯¥åªèµ°å·¦è¾¹çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯åŸæœ‰çš„æ¨¡å‹éƒ¨åˆ†ã€‚
    â€ƒâ€ƒè€Œåœ¨LoRAçš„ç­–ç•¥ä¸‹ï¼Œå¢åŠ äº†å³ä¾§çš„â€œæ—æ”¯â€ï¼Œä¹Ÿå°±æ˜¯å…ˆç”¨ä¸€ä¸ªLinearå±‚ A A Aï¼Œå°†æ•°æ®ä» d d dç»´é™åˆ° r r rï¼Œè¿™ä¸ª r r rä¹Ÿå°±æ˜¯LoRAçš„ç§©ï¼Œæ˜¯LoRAä¸­æœ€é‡è¦çš„ä¸€ä¸ªè¶…å‚æ•°ã€‚ä¸€èˆ¬ä¼šè¿œè¿œå°äº d d dï¼Œå°¤å…¶æ˜¯å¯¹äºç°åœ¨çš„å¤§æ¨¡å‹ï¼Œ d d då·²ç»ä¸æ­¢æ˜¯768æˆ–è€…1024ï¼Œä¾‹å¦‚LLaMA-7Bï¼Œæ¯ä¸€å±‚transformeræœ‰32ä¸ªheadï¼Œè¿™æ ·ä¸€æ¥ d d då°±è¾¾åˆ°äº†4096ã€‚æ¥ç€å†ç”¨ç¬¬äºŒä¸ªLinearå±‚ B B Bï¼Œå°†æ•°æ®ä» r r rå˜å› d d dç»´ã€‚æœ€åå†å°†å·¦å³ä¸¤éƒ¨åˆ†çš„ç»“æœç›¸åŠ èåˆï¼Œå°±å¾—åˆ°äº†è¾“å‡ºçš„hidden\_stateã€‚
    â€ƒâ€ƒå¯¹äºå·¦å³ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå³ä¾§çœ‹èµ·æ¥åƒæ˜¯å·¦ä¾§åŸæœ‰çŸ©é˜µ W W Wçš„åˆ†è§£ï¼Œå°†å‚æ•°é‡ä» d Ã— d d\\times d dÃ—då˜æˆäº† d Ã— r + d Ã— r d\\times r +d\\times r dÃ—r+dÃ—rï¼Œåœ¨ r â‰ª d r\\ll d râ‰ªdçš„æƒ…å†µä¸‹ï¼Œå‚æ•°é‡å°±å¤§å¤§åœ°é™ä½äº†ã€‚ç†Ÿæ‚‰å„ç±»é¢„è®­ç»ƒæ¨¡å‹çš„åŒå­¦å¯èƒ½ä¼šå‘ç°ï¼Œè¿™ä¸ªæ€æƒ³å…¶å®ä¸Albertçš„æ€æƒ³æœ‰å¼‚æ›²åŒå·¥ä¹‹å¤„ï¼Œåœ¨Albertä¸­ï¼Œä½œè€…é€šè¿‡ä¸¤ä¸ªç­–ç•¥é™ä½äº†è®­ç»ƒçš„å‚æ•°é‡ï¼Œå…¶ä¸€æ˜¯EmbeddingçŸ©é˜µåˆ†è§£ï¼Œå…¶äºŒæ˜¯è·¨å±‚å‚æ•°å…±äº«ã€‚åœ¨Albertä¸­ï¼Œä½œè€…è€ƒè™‘åˆ°è¯è¡¨çš„ç»´åº¦å¾ˆå¤§ï¼Œæ‰€ä»¥å°†EmbeddingçŸ©é˜µåˆ†è§£æˆä¸¤ä¸ªç›¸å¯¹è¾ƒå°çš„çŸ©é˜µï¼Œç”¨æ¥æ¨¡æ‹ŸEmbeddingçŸ©é˜µçš„æ•ˆæœï¼Œè¿™æ ·ä¸€æ¥éœ€è¦è®­ç»ƒçš„å‚æ•°é‡å°±å‡å°‘äº†å¾ˆå¤šã€‚
    â€ƒâ€ƒLoRAä¹Ÿæ˜¯ç±»ä¼¼çš„æ€æƒ³ï¼Œå¹¶ä¸”å®ƒä¸å†å±€é™äºEmbeddingå±‚ï¼Œè€Œæ˜¯æ‰€æœ‰å‡ºç°å¤§çŸ©é˜µçš„åœ°æ–¹ï¼Œç†è®ºä¸Šéƒ½å¯ä»¥ç”¨åˆ°è¿™æ ·çš„åˆ†è§£ã€‚ä½†æ˜¯ä¸Albertä¸åŒçš„æ˜¯ï¼ŒAlbertç›´æ¥ç”¨ä¸¤ä¸ªå°çŸ©é˜µæ›¿æ¢äº†åŸæ¥çš„å¤§çŸ©é˜µï¼Œè€ŒLoRAä¿ç•™äº†åŸæ¥çš„çŸ©é˜µ W W Wï¼Œä½†æ˜¯ä¸è®© W W Wå‚ä¸è®­ç»ƒï¼ˆFine-Tuningæ˜¯æ›´æ–°æƒé‡çŸ©é˜µ W W Wï¼ŒLoRAä¸­çš„ W = W 0 + B A W=W\_{0}+BA W\=W0â€‹+BAï¼Œä½†æ˜¯ W 0 W\_{0} W0â€‹ä¸å‚ä¸æ›´æ–°ï¼Œåªæ›´æ–° A A Aå’Œ B B Bï¼‰ï¼Œæ‰€ä»¥éœ€è¦è®¡ç®—æ¢¯åº¦çš„éƒ¨åˆ†å°±åªå‰©ä¸‹æ—æ”¯çš„ A A Aå’Œ B B Bä¸¤ä¸ªå°çŸ©é˜µã€‚ç”¨éšæœºé«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–Aï¼Œç”¨0çŸ©é˜µåˆå§‹åŒ–Bï¼Œä¿è¯è®­ç»ƒçš„å¼€å§‹æ­¤æ—è·¯çŸ©é˜µæ˜¯0çŸ©é˜µï¼Œä½¿å¾—æ¨¡å‹ä¿ç•™åŸæœ‰çŸ¥è¯†ï¼Œåœ¨è®­ç»ƒçš„åˆå§‹é˜¶æ®µä»ç„¶è¡¨ç°è‰¯å¥½ã€‚AçŸ©é˜µä¸é‡‡ç”¨0åˆå§‹åŒ–ä¸»è¦æ˜¯å› ä¸ºå¦‚æœçŸ©é˜µAä¹Ÿç”¨0åˆå§‹åŒ–ï¼Œé‚£ä¹ˆçŸ©é˜µBæ¢¯åº¦å°±å§‹ç»ˆä¸º0ï¼ˆå¯¹Bæ±‚æ¢¯åº¦ï¼Œç»“æœå¸¦æœ‰AçŸ©é˜µï¼ŒAçŸ©é˜µå…¨0ï¼ŒBçš„æ¢¯åº¦ç»“æœå¿…ç„¶æ˜¯0ï¼‰ï¼Œæ— æ³•æ›´æ–°å‚æ•°ã€‚
    â€ƒâ€ƒä»è®ºæ–‡ä¸­çš„å…¬å¼æ¥çœ‹ï¼Œåœ¨åŠ å…¥LoRAä¹‹å‰ï¼Œæ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–è¡¨ç¤ºä¸º:
    m a x Î¦ âˆ‘ ( x , y âˆˆ Z ) âˆ‘ t = 1 âˆ£ y âˆ£ l o g ( P Î¦ ( y t âˆ£ x , y < t ) ) max\_{\\Phi} \\sum\_{(x,y \\in Z)}\\sum\_{t=1}^{|y|}log(P\_{\\Phi}(y\_{t}|x,y\_{<t})) maxÎ¦â€‹(x,yâˆˆZ)âˆ‘â€‹t\=1âˆ‘âˆ£yâˆ£â€‹log(PÎ¦â€‹(ytâ€‹âˆ£x,y<tâ€‹))
    å…¶ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°ç”¨ Î¦ \\Phi Î¦è¡¨ç¤ºã€‚
    â€ƒâ€ƒè€ŒåŠ å…¥äº†LoRAä¹‹åï¼Œæ¨¡å‹çš„ä¼˜åŒ–è¡¨ç¤ºä¸º:
    m a x Î˜ âˆ‘ ( x , y âˆˆ Z ) âˆ‘ t = 1 âˆ£ y âˆ£ l o g ( P Î¦ 0 + Î” Î¦ ( Î˜ ) ( y t âˆ£ x , y < t ) ) max\_{\\Theta} \\sum\_{(x,y \\in Z)}\\sum\_{t=1}^{|y|}log(P\_{\\Phi\_{0}+\\Delta\\Phi(\\Theta)}(y\_{t}|x,y\_{<t})) maxÎ˜â€‹(x,yâˆˆZ)âˆ‘â€‹t\=1âˆ‘âˆ£yâˆ£â€‹log(PÎ¦0â€‹+Î”Î¦(Î˜)â€‹(ytâ€‹âˆ£x,y<tâ€‹))
    å…¶ä¸­ï¼Œæ¨¡å‹åŸæœ‰çš„å‚æ•°æ˜¯ Î¦ 0 \\Phi\_{0} Î¦0â€‹ï¼ŒLoRAæ–°å¢çš„å‚æ•°æ˜¯ Î” Î¦ ( Î˜ ) \\Delta\\Phi(\\Theta) Î”Î¦(Î˜)ã€‚
    â€ƒâ€ƒä»ç¬¬äºŒä¸ªå¼å­å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡å‚æ•°çœ‹èµ·æ¥å¢åŠ äº† Î” Î¦ ( Î˜ ) \\Delta\\Phi(\\Theta) Î”Î¦(Î˜)ï¼Œä½†æ˜¯ä»å‰é¢çš„maxçš„ç›®æ ‡æ¥çœ‹ï¼Œéœ€è¦ä¼˜åŒ–çš„å‚æ•°åªæœ‰ Î˜ \\Theta Î˜ï¼Œè€Œæ ¹ âˆ£ Î˜ âˆ£ â‰ª âˆ£ Î¦ 0 âˆ£ |\\Theta|\\ll |\\Phi\_{0}| âˆ£Î˜âˆ£â‰ªâˆ£Î¦0â€‹âˆ£ï¼Œè¿™å°±ä½¿å¾—è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦è®¡ç®—é‡å°‘äº†å¾ˆå¤šï¼Œæ‰€ä»¥å°±åœ¨ä½èµ„æºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥åªæ¶ˆè€— Î˜ \\Theta Î˜è¿™éƒ¨åˆ†çš„èµ„æºï¼Œè¿™æ ·ä¸€æ¥å°±å¯ä»¥åœ¨å•å¡ä½æ˜¾å­˜çš„æƒ…å†µä¸‹è®­ç»ƒå¤§æ¨¡å‹äº†ã€‚è¿™é‡Œå†å¤šè¯´ä¸€ç‚¹ï¼Œé€šå¸¸åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä¸€èˆ¬LoRAä½œç”¨çš„çŸ©é˜µæ˜¯æ³¨æ„åŠ›æœºåˆ¶éƒ¨åˆ†çš„ W Q W\_{Q} WQâ€‹ã€ W K W\_{K} WKâ€‹ã€ W V W\_{V} WVâ€‹çŸ©é˜µï¼ˆå³ä¸è¾“å…¥ç›¸ä¹˜è·å– Q Q Qã€ K K Kã€ V V Vçš„æƒé‡çŸ©é˜µã€‚è¿™ä¸‰ä¸ªæƒé‡çŸ©é˜µçš„æ•°é‡æ­£å¸¸æ¥è¯´ï¼Œåˆ†åˆ«å’Œheadsçš„æ•°é‡ç›¸ç­‰ï¼Œä½†åœ¨å®é™…è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œæ˜¯å°†å¤šä¸ªå¤´çš„è¿™ä¸‰ä¸ªæƒé‡çŸ©é˜µåˆ†åˆ«è¿›è¡Œäº†åˆå¹¶ï¼Œå› æ­¤æ¯ä¸€ä¸ªtransformerå±‚éƒ½åªæœ‰ä¸€ä¸ª W Q W\_{Q} WQâ€‹ã€ W K W\_{K} WKâ€‹ã€ W V W\_{V} WVâ€‹çŸ©é˜µï¼‰ã€‚ä¸‹é¢ä»‹ç»ä¸‹LoRAæ¶æ„çš„ä¼˜ç‚¹:
    - **å…¨é‡å¾®è°ƒçš„ä¸€èˆ¬åŒ–**: LoRA ä¸è¦æ±‚æƒé‡çŸ©é˜µçš„ç´¯ç§¯æ¢¯åº¦æ›´æ–°åœ¨é€‚é…è¿‡ç¨‹ä¸­å…·æœ‰æ»¡ç§©ã€‚å½“å¯¹æ‰€æœ‰æƒé‡çŸ©é˜µåº”ç”¨ LoRA å¹¶è®­ç»ƒæ‰€æœ‰åå·®æ—¶ï¼Œå°† LoRA çš„ç§© r r rè®¾ç½®ä¸ºé¢„è®­ç»ƒæƒé‡çŸ©é˜µçš„ç§©ï¼Œå°±èƒ½å¤§è‡´æ¢å¤äº†å…¨é‡å¾®è°ƒçš„è¡¨ç°åŠ›ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œéšç€å¢åŠ å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œè®­ç»ƒ LoRA å¤§è‡´æ”¶æ•›äºè®­ç»ƒåŸå§‹æ¨¡å‹ï¼›
    - **æ²¡æœ‰é¢å¤–çš„æ¨ç†å»¶æ—¶**: åœ¨ç”Ÿäº§éƒ¨ç½²æ—¶ï¼Œå¯ä»¥æ˜ç¡®åœ°è®¡ç®—å’Œå­˜å‚¨ W = W 0 + B A W=W\_{0}+BA W\=W0â€‹+BAï¼Œå¹¶æ­£å¸¸æ‰§è¡Œæ¨ç†ã€‚å½“éœ€è¦åˆ‡æ¢åˆ°å¦ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå¯ä»¥é€šè¿‡å‡å» B A BA BAæ¥æ¢å¤ W 0 W\_{0} W0â€‹ï¼Œç„¶åå¢åŠ ä¸€ä¸ªä¸åŒçš„ B â€² A â€² B^{'}A^{'} Bâ€²Aâ€²ï¼Œè¿™æ˜¯ä¸€ä¸ªåªéœ€è¦å¾ˆå°‘å†…å­˜å¼€é”€çš„å¿«é€Ÿè¿ç®—ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¸Fine-Tuningçš„æ¨¡å‹ç›¸æ¯”ï¼ŒLoRA æ¨ç†è¿‡ç¨‹ä¸­æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„å»¶è¿Ÿï¼ˆå°† B A BA BAåŠ åˆ°åŸå‚æ•° W 0 W\_{0} W0â€‹ä¸Šåï¼Œè®¡ç®—é‡æ˜¯ä¸€è‡´çš„ï¼‰ï¼›
    - **å‡å°‘å†…å­˜å’Œå­˜å‚¨èµ„æºæ¶ˆè€—**: å¯¹äºç”¨Adamè®­ç»ƒçš„å¤§å‹Transformerï¼Œè‹¥ r â‰ª d m o d e l r\\ll d\_{model} râ‰ªdmodelâ€‹ï¼ŒLoRA å‡å°‘2/3çš„æ˜¾å­˜ç”¨é‡ï¼ˆè®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ¨¡å‹å‚æ•°å¾€å¾€éƒ½ä¼šå­˜å‚¨åœ¨æ˜¾å­˜ä¸­ï¼‰ï¼Œå› ä¸ºä¸éœ€è¦å­˜å‚¨å·²å›ºå®šçš„é¢„è®­ç»ƒå‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¯ä»¥ç”¨æ›´å°‘çš„GPUè¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒã€‚åœ¨175Bçš„GPT-3ä¸Šï¼Œè®­ç»ƒæœŸé—´çš„æ˜¾å­˜æ¶ˆè€—ä»1.2TBå‡å°‘åˆ°350GBã€‚åœ¨æœ‰ä¸”åªæœ‰queryå’ŒvalueçŸ©é˜µè¢«è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œcheckpointçš„å¤§å°å¤§çº¦å‡å°‘äº†10000å€ï¼ˆä»350GBåˆ°35MBï¼‰ã€‚å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œå¯ä»¥åœ¨éƒ¨ç½²æ—¶ä»¥æ›´ä½çš„æˆæœ¬åˆ‡æ¢ä»»åŠ¡ï¼Œåªéœ€æ›´æ¢ LoRA çš„æƒé‡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰çš„å‚æ•°ã€‚å¯ä»¥åˆ›å»ºè®¸å¤šå®šåˆ¶çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨å°†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡å­˜å‚¨åœ¨æ˜¾å­˜ä¸­çš„æœºå™¨ä¸Šè¿›è¡Œå®æ—¶åˆ‡æ¢ã€‚åœ¨175Bçš„GPT-3ä¸Šè®­ç»ƒæ—¶ï¼Œä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†25%ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ä¸ºç»å¤§å¤šæ•°çš„å‚æ•°è®¡ç®—æ¢¯åº¦ï¼›
    - **æ›´é•¿çš„è¾“å…¥**: ç›¸è¾ƒP-Tuningç­‰soft-promptæ–¹æ³•ï¼ŒLoRAæœ€æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œå°±æ˜¯ä¸ä¼šå ç”¨è¾“å…¥tokençš„é•¿åº¦ã€‚
- **AdaLoRA**: AdaLoRAæ˜¯å‘è¡¨äº2023å¹´3æœˆ[ã€ŠADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNINGã€‹](https://arxiv.org/pdf/2303.10512.pdf)ï¼Œè®ºæ–‡å¹¶æœªä»”ç»†é˜…è¯»ï¼Œç®€å•æ¥è¯´ï¼Œè®ºæ–‡ä¸­å‘ç°å¯¹ä¸åŒç±»å‹æƒé‡çŸ©é˜µæˆ–è€…ä¸åŒå±‚çš„æƒé‡çŸ©é˜µåº”ç”¨LoRAæ–¹æ³•ï¼Œäº§ç”Ÿçš„æ•ˆæœæ˜¯ä¸åŒçš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/f8722ab2b3d84dceb9e428a1354c8a65.png#pic_center)
    â€ƒâ€ƒåœ¨å‚æ•°é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚é™å®šæ¨¡å‹å¯å¾®è°ƒå‚æ•°çš„æ•°é‡ï¼‰ï¼Œå¦‚ä½•æ™ºèƒ½çš„é€‰å–æ›´é‡è¦çš„å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œæ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚è®ºæ–‡ä¸­æå‡ºçš„è§£å†³åŠæ³•ï¼Œæ˜¯å…ˆå¯¹LoRAå¯¹åº”çš„æƒé‡çŸ©é˜µè¿›è¡ŒSVDåˆ†è§£ï¼Œå³:
    W = W 0 + Î” = W 0 + B A = W 0 + P Î› Q Â  W=W\_{0}+\\Delta=W\_{0}+BA=W\_{0}+P\\Lambda Q\\ W\=W0â€‹+Î”\=W0â€‹+BA\=W0â€‹+PÎ›QÂ 
    å…¶ä¸­:  Î” \\Delta Î”ç§°ä¸ºå¢é‡çŸ©é˜µï¼Œ W âˆˆ R d 1 Ã— d 2 W\\in R^{d1 \\times d2} WâˆˆRd1Ã—d2ï¼Œ P âˆˆ R d 1 Ã— r P\\in R^{d1 \\times r} PâˆˆRd1Ã—rï¼Œ Q âˆˆ R r Ã— d 2 Q\\in R^{r \\times d2} QâˆˆRrÃ—d2ï¼Œ Î› âˆˆ R r Ã— r \\Lambda\\in R^{r \\times r} Î›âˆˆRrÃ—rï¼Œ r â‰ª m i n ( d 1 , d 2 ) r\\ll min(d1,d2) râ‰ªmin(d1,d2)ã€‚å†æ ¹æ®é‡è¦æ€§æŒ‡æ ‡åŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªå¢é‡çŸ©é˜µä¸­å¥‡å¼‚å€¼çš„å¤§å°ã€‚è¿™æ ·å¯ä»¥ä½¿å¾—åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åªæ›´æ–°é‚£äº›å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®è¾ƒå¤§æˆ–å¿…è¦çš„å‚æ•°ï¼Œä»è€Œæé«˜äº†æ¨¡å‹æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ã€‚å…·ä½“å¯å‚è€ƒè®ºæ–‡ç®€ä»‹[ADAPTIVE BUDGET ALLOCATION FOR PARAMETER- EFFICIENT FINE-TUNING](https://zhuanlan.zhihu.com/p/628259936) ã€‚
- **BitFit**: BitFitï¼ˆBias-term Fine-tuningï¼‰å‘è¡¨äº2022å¹´[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/pdf/2106.10199.pdf)çš„æ€æƒ³æ›´ç®€å•ï¼Œå…¶ä¸éœ€è¦å¯¹é¢„è®­ç»ƒæ¨¡å‹åšä»»ä½•æ”¹åŠ¨ï¼Œåªéœ€è¦æŒ‡å®šç¥ç»ç½‘ç»œä¸­çš„åç½®ï¼ˆBiasï¼‰ä¸ºå¯è®­ç»ƒå‚æ•°å³å¯ï¼ŒBitFitçš„å‚æ•°é‡åªæœ‰ä¸åˆ°2%ï¼Œä½†æ˜¯å®éªŒæ•ˆæœå¯ä»¥æ¥è¿‘å…¨é‡å‚æ•°ã€‚

#### 5.2ã€PEFTå®è·µ

â€ƒâ€ƒ**å®éªŒç¯å¢ƒ**: 2å¼ A30å¡ï¼ˆå•å¡æ˜¾å­˜24Gï¼‰ï¼ŒCentOS7ã€‚
â€ƒâ€ƒ**æ˜¾å­˜å ç”¨**: å¦‚ä¸‹è¡¨ã€‚

æ¨¡å‹æ–¹æ¡ˆ

è®­ç»ƒæ–¹æ¡ˆ

æ˜¾å­˜å ç”¨

ChatGLM-6B+P-Tuning v2

å•å¡è®­ç»ƒ

8Gå·¦å³

ChatGLM2-6B+P-Tuning v2

å•å¡è®­ç»ƒ

8Gå·¦å³

ChatGLM-6B+LoRA

ä¸¤å¡DDP

å•å¡13Gå·¦å³

ChatGLM2-6B+LoRA

ä¸¤å¡DDP

å•å¡13Gå·¦å³

ChatGLM-6B+LoRA+int8é‡åŒ–

ä¸¤å¡æµæ°´çº¿å¹¶è¡Œ

ä¸¤å¡13Gå·¦å³

ChatGLM2-6B+LoRA+int8é‡åŒ–

ä¸¤å¡æµæ°´çº¿å¹¶è¡Œ

ä¸¤å¡27Gå·¦å³

ChatGLM-6B+LoRA

ä¸¤å¡Deepspeed

å•å¡11Gå·¦å³

- **ChatGLM-6Bå¾®è°ƒå®è·µ**:

    - **ChatGLM-6B + P-Tuning v2 â‡’ \\Rightarrow â‡’å®˜æ–¹ä»»åŠ¡å®è·µ**: [ã€å®˜æ–¹æ•™ç¨‹ã€‘ChatGLM-6B å¾®è°ƒ](https://www.bilibili.com/video/BV1fd4y1Z7Y5/?spm_id_from=333.999.0.0&vd_source=25d0b87065d3da39fe110c6e0b4906e1)ã€‚

        - **æ¨¡å‹ä¸‹è½½**: ä¸‹è½½[ChatGLM-6B](https://www.huggingface.co/THUDM/chatglm-6b/tree/main)æ¨¡å‹çš„æ–¹æ³•å¾ˆå¤šï¼Œè¿™é‡Œä»‹ç»å®˜æ–¹ç»™å‡ºçš„æœ€å¿«ä¸‹è½½æ–¹å¼ã€‚
            - **ä¸‹è½½æ¨¡å‹å®ç°**:  ç”±äºä¸‹è½½æ•´ä½“æ¨¡å‹è¾ƒæ…¢ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆä¸‹è½½æ¨¡å‹å®ç°ï¼Œå†æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶ã€‚ä¸‹è½½æ¨¡å‹å®ç°å‰ï¼Œéœ€å…ˆ[å®‰è£…Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage?platform=mac)ï¼Œå®‰è£…å¥½ä¹‹åå†ä¸‹è½½æ¨¡å‹å®ç°ã€‚

                    GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b


            - **æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶**:

                - **è„šæœ¬æ–¹å¼ï¼ˆæ¨èï¼‰**:

                        git clone git@github.com:chenyifanthu/THU-Cloud-Downloader.git

                        cd THU-Cloud-Downloader

                        pip install argparse requests tqdm

                        python main.py --link https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/ --save ../chatglm-6b


                - **ç›´æ¥ä¸‹è½½**: ä»[ChatGLM-6B](https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/)ä¸­å°†æ‰€æœ‰æ–‡ä»¶ä¸‹è½½ä¸‹æ¥ï¼Œæ›¿æ¢æ¨¡å‹å®ç°æ­¥éª¤ä¸‹è½½çš„æ–‡ä»¶å¤¹`./chatglm-6b`ä¸­çš„æ–‡ä»¶ã€‚

                - **ç™¾åº¦ç½‘ç›˜ä¸‹è½½**: ä¸ºäº†é˜²æ­¢å®˜æ–¹å¾®è°ƒæ¨¡å‹ï¼Œå¯¼è‡´æ¨¡å‹ä¸è®­ç»ƒä»£ç ä¸é€‚é…ï¼Œåœ¨ç™¾åº¦ç½‘ç›˜ä¿å­˜äº†ä¸€ä»½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œä¼˜å…ˆçº§è¾ƒä½ï¼Œå¤§å®¶æŒ‰éœ€æå–ã€‚é“¾æ¥: [ChatGLM-6B](https://pan.baidu.com/s/1A5zVKtQYfML0omsMYPnWfg)ï¼Œæå–ç : 0314ã€‚

            - **ä¸‹è½½è®­ç»ƒä»£ç **: [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ã€‚

                    git clone git@github.com:THUDM/ChatGLM-6B.git


                åŒä¸Šæ–‡æ¨¡å‹ä¸‹è½½ä¸€è‡´ï¼Œå®˜ç½‘ä»£ç å­˜åœ¨æ›´æ–°çš„å¯èƒ½ï¼Œè‹¥æƒ³é¡ºåˆ©è¿è¡Œæœ¬é¡¹ç›®ï¼Œå¯ä»ç™¾åº¦ç½‘ç›˜ä¸‹è½½ä»£ç ã€‚é“¾æ¥: [ChatGLM-6B](https://pan.baidu.com/s/1bZWPdaayh2-FotCJdigqQw)ï¼Œ æå–ç : 0314ã€‚

            - **è¯•ç”¨åŸå§‹æ¨¡å‹**:

                - **å®‰è£…åŒ…**:

                        pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

                        # å…·ä½“å®‰è£…åŒ…
                        protobuf
                        transformers==4.27.1
                        cpm_kernels
                        torch>=1.10
                        gradio
                        mdtex2html
                        sentencepiece
                        accelerate


                - **æ¨¡å‹è¯•ç”¨**: è¿›è¡Œç®€å•è¯•ç”¨çš„å¯åŠ¨å‘½ä»¤ï¼Œä¸ä½¿ç”¨é‡åŒ–ï¼Œå•å¡æ˜¾å­˜13Gå·¦å³ï¼Œä½¿ç”¨8bité‡åŒ–ï¼Œå•å¡æ˜¾å­˜8Gå·¦å³ã€‚

                        CUDA_VISIBLE_DEVICES=1 python cli_demo.py


                - **æ³¨æ„**:
                    - **æ¨¡å‹è·¯å¾„**: å› ä¸ºå‰æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä¸‹è½½äº†chatglm-6Bæ¨¡å‹ï¼Œå› æ­¤ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œè¯•ç”¨æ—¶ï¼Œéœ€è¦ä¿®æ”¹æ¨¡å‹ä¸‹è½½è·¯å¾„ï¼Œå³å°†`cli_demo.py`å’Œ`web_demo.py`ä¸­çš„`tokenizer`å’Œ`model`åŠ è½½è·¯å¾„ï¼Œ`THUDM/chatglm-6b`ä¿®æ”¹ä¸ºæœ¬åœ°è·¯å¾„ã€‚åé¢åŒ…æ‹¬è®­ç»ƒåœ¨å†…çš„æ‰€æœ‰è¿‡ç¨‹ï¼Œéƒ½è¦æ³¨æ„è¿™ä¸€ç‚¹ï¼Œå°±ä¸é‡å¤èµ˜è¿°ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/cc620f27024341b8bd1690eb5dda2fdd.png#pic_center)
            - **é‡åŒ–ç»†èŠ‚**: å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œé‡åŒ–çš„å¤„ç†æ–¹å¼ä¹Ÿè¿›è¡Œäº†æ ‡è®°ã€‚é‡åŒ–æ“ä½œä¸€èˆ¬ç”¨äºæ¨ç†ï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸€èˆ¬ä¸é‡‡ç”¨æ­¤æ“ä½œã€‚åŒæ—¶ï¼Œé‡åŒ–æ“ä½œæ˜¯ä½œç”¨äºéƒ¨åˆ†å‚æ•°ï¼Œå°†è¿™éƒ¨åˆ†å‚æ•°è½¬æ¢ä¸º8ä½æ•´æ•°è¡¨ç¤ºï¼ŒåŒæ—¶å°†`requires_grad`å±æ€§ç½®ä¸º`False`ã€‚

            - **è®­ç»ƒå‰å®‰è£…åŒ…**:

                    pip install rouge_chinese nltk jieba datasets


            - **æ•°æ®é›†ä¸‹è½½**: [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1)ã€‚ä¸‹è½½è‡³ç›®å½•`./ptuning`ï¼ŒADGENæ•°æ®é›†ä»»åŠ¡ä¸ºæ ¹æ®è¾“å…¥ï¼ˆcontentï¼‰ç”Ÿæˆä¸€æ®µå¹¿å‘Šè¯ï¼ˆsummaryï¼‰ã€‚

                    {
                        "content": "ç±»å‹#ä¸Šè¡£*ç‰ˆå‹#å®½æ¾*ç‰ˆå‹#æ˜¾ç˜¦*å›¾æ¡ˆ#çº¿æ¡*è¡£æ ·å¼#è¡¬è¡«*è¡£è¢–å‹#æ³¡æ³¡è¢–*è¡£æ¬¾å¼#æŠ½ç»³",
                        "summary": "è¿™ä»¶è¡¬è¡«çš„æ¬¾å¼éå¸¸çš„å®½æ¾ï¼Œåˆ©è½çš„çº¿æ¡å¯ä»¥å¾ˆå¥½çš„éšè—èº«æä¸Šçš„å°ç¼ºç‚¹ï¼Œç©¿åœ¨èº«ä¸Šæœ‰ç€å¾ˆå¥½çš„æ˜¾ç˜¦æ•ˆæœã€‚é¢†å£è£…é¥°äº†ä¸€ä¸ªå¯çˆ±çš„æŠ½ç»³ï¼Œæ¼‚äº®çš„ç»³ç»“å±•ç°å‡ºäº†åè¶³çš„ä¸ªæ€§ï¼Œé…åˆæ—¶å°šçš„æ³¡æ³¡è¢–å‹ï¼Œå°½æ˜¾å¥³æ€§ç”œç¾å¯çˆ±çš„æ°”æ¯ã€‚"
                    }


            - **å¯åŠ¨è®­ç»ƒ**:

                    cd ./ptuning
                    sh train.sh


                - **æ³¨æ„**: è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‡ºç°é”™è¯¯[init\_process\_group error](https://github.com/THUDM/ChatGLM-6B/issues/1169)ï¼Œå¯æŒ‰ç…§[fix pturning init\_process\_group error](https://github.com/THUDM/ChatGLM-6B/pull/1173/files)è¿›è¡Œè§£å†³ã€‚
            - **æ¨¡å‹æ¨ç†**:

                    #!/usr/bin/env python3
                    # -*- coding: UTF-8 -*-
                    ################################################################################
                    #
                    # Copyright (c) 2023 Baidu.com, Inc. All Rights Reserved
                    #
                    ################################################################################
                    """
                    File    :   predict.py
                    brief   :   brief
                    Date    :   2023/07/03 08:00:52
                    Author  :   zhangce06
                    Contact :   zhangce06@baidu.com
                    """


                    from transformers import AutoConfig, AutoModel, AutoTokenizer
                    import torch
                    import os
                    import platform
                    import signal
                    import readline

                    # pre_seq_len = 128

                    # è½½å…¥Tokenizer
                    tokenizer = AutoTokenizer.from_pretrained("../../chatglm-6b-model", trust_remote_code=True)
                    config = AutoConfig.from_pretrained("../../chatglm-6b-model", trust_remote_code=True, pre_seq_len=128)
                    # config.pre_seq_len = pre_seq_len
                    model = AutoModel.from_pretrained("../../chatglm-6b-model", config=config, trust_remote_code=True)

                    CHECKPOINT_PATH = "output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-3000"
                    prefix_state_dict = torch.load(os.path.join(CHECKPOINT_PATH, "pytorch_model.bin"))
                    new_prefix_state_dict = {}
                    for k, v in prefix_state_dict.items():
                        if k.startswith("transformer.prefix_encoder."):
                            new_prefix_state_dict[k[len("transformer.prefix_encoder."):]] = v
                    model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)

                    # ä¹‹åæ ¹æ®éœ€æ±‚å¯ä»¥è¿›è¡Œé‡åŒ–
                    # Comment out the following line if you don't use quantization
                    model = model.quantize(4)
                    model = model.half().cuda()
                    model.transformer.prefix_encoder.float()
                    model = model.eval()

                    os_name = platform.system()
                    clear_command = 'cls' if os_name == 'Windows' else 'clear'
                    stop_stream = False

                    def build_prompt(history):
                        prompt = "æ¬¢è¿ä½¿ç”¨ ChatGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº"
                        for query, response in history:
                            prompt += f"\n\nç”¨æˆ·: {query}"
                            prompt += f"\n\nChatGLM-6B: {response}"
                        return prompt

                    def signal_handler(signal, frame):
                        global stop_stream
                        stop_stream = True

                    def main():
                        history = []
                        global stop_stream
                        print("æ¬¢è¿ä½¿ç”¨ ChatGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº")
                        while True:
                            query = input("\nç”¨æˆ·: ")
                            if query.strip() == "stop":
                                break
                            if query.strip() == "clear":
                                history = []
                                os.system(clear_command)
                                print("æ¬¢è¿ä½¿ç”¨ ChatGLM-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº")
                                continue
                            count = 0
                            for response, history in model.stream_chat(tokenizer, query, history=history):
                                if stop_stream:
                                    stop_stream = False
                                    break
                                else:
                                    count += 1
                                    if count % 8 == 0:
                                        os.system(clear_command)
                                        print(build_prompt(history), flush=True)
                                        signal.signal(signal.SIGINT, signal_handler)
                            os.system(clear_command)
                            print(build_prompt(history), flush=True)

                    if __name__ == "__main__":
                        main()


            - **ç¾éš¾æ€§é—å¿˜é—®é¢˜**: åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œä¼šå‡ºç°ç¾éš¾æ€§é—å¿˜çš„æƒ…å†µï¼Œåœ¨æ•°æ®é›†æœ‰é™çš„æƒ…å†µä¸‹ï¼Œç›®å‰é€šè¿‡å®è·µæ€»ç»“å‡ºä¸‹é¢ä¸‰ç§åšæ³•ï¼Œå¯åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£ç¾éš¾æ€§é—å¿˜

                - **å­¦ä¹ ç‡è°ƒæ•´**: é€šè¿‡è°ƒæ•´å­¦ä¹ ç‡è¿›è¡Œè§£å†³çš„[ç¾éš¾æ€§é—å¿˜é—®é¢˜](https://github.com/THUDM/ChatGLM-6B/issues/1148)ï¼›
                - **é‡‡ç”¨LoRAæ–¹æ³•**: å‚è§ã€Œ**ChatGLM-6B + LoRA â‡’ \\Rightarrow â‡’çœŸå®ä»»åŠ¡å®è·µ**ã€ï¼›
                - **é‡‡ç”¨ChatGLM2-6B**: ChatGLM2-6Bç¡®å®æ¯”ChatGLM-6Bå¼ºã€‚ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼ŒChatGLM2-6Båœ¨ä¸Šè¿°çš„å¹¿å‘Šæ•°æ®é›†ä¸Šå¾®è°ƒåï¼Œç¡®å®æ²¡æœ‰å‡ºç°ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚ä¸è¿‡ä»ç„¶å­˜åœ¨å…¶ä»–é—®é¢˜ï¼Œå¤§å®¶è‡ªè¡Œä½“éªŒã€‚ä¸‹é¢ç®€å•ä»‹ç»ä¸‹ï¼Œä½¿ç”¨ChatGLM2-6Bå¤ç”¨ChatGLM-6Bè¿›è¡ŒP-Tuning v2æµç¨‹éœ€è¦æ³¨æ„çš„ç‚¹ã€‚
                    - **æ¨¡å‹ä¸‹è½½**: æ¨¡å‹ä¸‹è½½æ–¹å¼åŒChatGLM-6Bç›¸åŒï¼Œå…ˆä¸‹è½½æ¨¡å‹å®ç°[ChatGLM2-6B](https://huggingface.co/THUDM/chatglm2-6b/tree/main)ï¼Œå†ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶[ChatGLM2-6B](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/?p=/chatglm2-6b&mode=list)ï¼Œæ³¨æ„è¿™é‡Œåšä¸»æ˜¯ç›´æ¥æ‰‹åŠ¨ä¸‹è½½çš„ï¼Œè„šæœ¬ä¸‹è½½æ–¹å¼æ²¡æœ‰å°è¯•æˆåŠŸï¼Œå¤§å®¶å¯ä»¥è¯•ä¸€è¯•ã€‚
                        - **ç™¾åº¦ç½‘ç›˜ä¸‹è½½**: åŒæ ·åœ¨ç™¾åº¦ç½‘ç›˜ä¿å­˜äº†ä¸€ä»½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œä¼˜å…ˆçº§è¾ƒä½ï¼Œå¤§å®¶æŒ‰éœ€æå–ã€‚é“¾æ¥: [ChatGLM2-6B](https://pan.baidu.com/s/1VsVY1di492WSRt1GsY8uGg)ï¼Œæå–ç : 0625ã€‚
                    - **ä¸‹è½½è®­ç»ƒä»£ç **: ChatGLM2-6Bå®˜æ–¹æ²¡æœ‰å¾®è°ƒä»£ç ï¼Œå› æ­¤å¾®è°ƒä»£ç åšä¸»è¿˜æ˜¯é‡‡ç”¨çš„ChatGLM-6Bçš„ä»£ç [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ï¼Œä¸‹è½½æ–¹å¼ä¸å˜ã€‚å¦‚æœåªæ˜¯è¯•ç”¨ChatGLM2-6Bï¼Œåˆ™å¯ä»¥ä¸‹è½½ChatGLM2-6Bçš„å®˜æ–¹ä»£ç [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)ï¼ˆç™¾åº¦ç½‘ç›˜ä¸‹è½½æ–¹å¼ï¼Œé“¾æ¥: [ChatGLM2-6B](https://pan.baidu.com/s/1OemV9rXON92HybmMWm_AeA)ï¼Œæå–ç : 0625ï¼‰ï¼Œè¯•ç”¨æ–¹å¼ä¹ŸåŒChatGLM-6Bä¸€è‡´ã€‚ä¸è®ºæ˜¯å¾®è°ƒè¿˜æ˜¯è¯•ç”¨ï¼Œè®°å¾—æ›´æ¢æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚
                        - **è¯•ç”¨ç»†èŠ‚**: ChatGLM-6Bè¯•ç”¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨åŠç²¾åº¦FP16åŠ è½½æ¨¡å‹ï¼Œå‘½ä»¤æ˜¯`model.half()`ï¼ŒChatGLM2-6Båˆ™ä¸ç”¨ï¼Œå› ä¸ºå…¶æœ¬èº«å°±æ˜¯åŠç²¾åº¦çŠ¶æ€ã€‚å¯é€šè¿‡å¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹æ¨¡å‹å‚æ•°çš„ç²¾åº¦æ„æˆï¼Œå¯ä»¥å‘ç°ï¼Œæœªä½¿ç”¨FP16åŠ è½½æ¨¡å‹å‰ï¼ŒChatGLM-6Bçš„æ¨¡å‹å‚æ•°ç²¾åº¦æ˜¯FP16å’ŒFP32æ··åˆçš„ï¼ŒChatGLM2-6Båˆ™åªæœ‰FP16ç²¾åº¦çš„å‚æ•°ã€‚

                                model = AutoModel.from_pretrained("../../chatglm-6b-model", trust_remote_code=True)
                                for name, param in model.named_parameters():
                                	if param.requires_grad == True:
                                	    print(f"{name},------------,{param.dtype}")


                    - **å®‰è£…åŒ…**: ChatGLM2-6Béœ€è¦é€‚é…æ›´é«˜ç‰ˆæœ¬çš„transformerså’Œpytorchï¼Œæ‰èƒ½å‘æŒ¥æ¨ç†æ€§èƒ½çš„ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œè¯•ç”¨ChatGLM2-6Bæ—¶ï¼Œå®‰è£…åŒ…å¦‚ä¸‹:

                            # å…·ä½“å®‰è£…åŒ…
                            protobuf
                            transformers==4.30.2
                            cpm_kernels
                            torch>=2.0
                            gradio
                            mdtex2html
                            sentencepiece
                            accelerate


                        å¦‚æœéœ€è¦å¾®è°ƒChatGLM2-6Bï¼Œåˆ™åŒChatGLM-6Bä¸€è‡´ï¼Œå®‰è£…å¦‚ä¸‹pythonåŒ…:

                            pip install rouge_chinese nltk jieba datasets


                    - **æ•°æ®é›†ä¸‹è½½**: æ— å˜åŒ–ï¼ŒåŒChatGLM-6Bä¸€è‡´ã€‚
                    - **å¯åŠ¨è®­ç»ƒ**: åŸºæœ¬æ— å˜åŒ–ï¼Œå¤§ä½“æµç¨‹åŒChatGLM-6Bä¸€è‡´ã€‚æœ‰ä¸¤ä¸ªåœ°æ–¹éœ€è¦æ³¨æ„ï¼Œä¸€ä¸ªæ˜¯è„šæœ¬`./ptuning/train.sh`ä¸­çš„å„ç§æ–‡ä»¶è·¯å¾„æŒ‰éœ€è°ƒæ•´ï¼›å¦ä¸€ä¸ªæ˜¯`./ptuning/main.py`æ–‡ä»¶`line 220`å·¦å³è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹:

                            # é€‚é…ChatGLM1
                            # context_length = input_ids.index(tokenizer.bos_token_id)
                            # mask_position = context_length - 1
                            # labels = [-100] * context_length + input_ids[mask_position+1:]

                            # é€‚é…ChatGLM2
                            context_length = len(input_ids) - len(b_ids)
                            mask_position = context_length
                            labels = [-100] * context_length + input_ids[mask_position:]```


                    - **æ¨¡å‹æ¨ç†**: åŸºæœ¬æ— å˜åŒ–ï¼ŒåŒæ ·æ³¨æ„ä¿®æ”¹æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚
    - **ChatGLM-6B + LoRA â‡’ \\Rightarrow â‡’å®˜æ–¹ä»»åŠ¡å®è·µ**: å‚è€ƒä»£ç [ChatGLM\_Tuning](https://github.com/zejunwang1/chatglm_tuning/blob/main/README.md)ï¼Œå®ç°äº†ChatGLM-6BåŸºäºLoRAçš„å¾®è°ƒæµç¨‹ã€‚å…·ä½“ä»£ç è§[LLMå¾®è°ƒå®è·µ](https://github.com/DankoZhang/LLM/blob/main/README.md)ã€‚æ¨¡å‹æ–‡ä»¶åŒæ ·å¯æ ¹æ®å‰æ–‡çš„æ–¹æ³•è¿›è¡Œè·å–ï¼Œå…¶ä¸­å®˜æ–¹çš„æ¨¡å‹å¯èƒ½å­˜åœ¨æ›´æ–°ï¼Œå¦‚æœæƒ³é¡ºåˆ©å¤ç°è®­ç»ƒè¿‡ç¨‹ï¼Œå»ºè®®ä»ç½‘ç›˜è¿›è¡Œä¸‹è½½ã€‚

        - **LoRAé…ç½®å‚æ•°**:

                r: loraçŸ©é˜µçš„ç§©ï¼ŒçŸ©é˜µAå’ŒçŸ©é˜µBç›¸è¿æ¥çš„å®½åº¦ï¼Œr<<dï¼Œä»¥ int è¡¨ç¤ºã€‚è¾ƒä½çš„ç§©ä¼šå¯¼è‡´è¾ƒå°çš„æ›´æ–°çŸ©é˜µå’Œè¾ƒå°‘çš„å¯è®­ç»ƒå‚æ•°

                target_modules: æ¨¡å‹ä¸­ä½¿ç”¨LoRAæ›´æ–°çŸ©é˜µçš„æ¨¡å—ï¼Œæ¨¡å‹ä¸­å¸¸è§çš„æ˜¯ï¼Œæ›´æ–°æ³¨æ„åŠ›æ¨¡å—

                lora_alpha : LoRAç¼©æ”¾å› å­

                bias : æŒ‡å®šæ˜¯å¦åº”è®­ç»ƒbias å‚æ•°ã€‚"none": å‡ä¸å¯ï¼›"all": å‡å¯ï¼›"lora_only": åªæœ‰loraéƒ¨åˆ†çš„biaså¯è®­ç»ƒ

                lora_dropout: loraå±‚çš„dropoutæ¯”ç‡

                task_type: æ¨¡å‹ä»»åŠ¡ç±»å‹ï¼Œä¾‹å¦‚CAUSAL_LMä»»åŠ¡


            - **æ³¨æ„**:
                - **å‚æ•°æ›´æ–°**: æ¨¡å‹ç»è¿‡LoRAé…ç½®åŠ è½½åï¼Œå¯æ›´æ–°æ¨¡å‹å‚æ•°åªæœ‰LoRAéƒ¨åˆ†ï¼Œä¸”å‚æ•°ç²¾åº¦è¢«é‡ç½®ä¸ºFP32ï¼›
                - **é‡åŒ–æ–¹å¼**: `load_in_8bit=True`å’Œ`quantize(8)`åŒºåˆ«ï¼ŒLoRAå¾®è°ƒæ—¶åªèƒ½ç”¨å‰è€…ï¼Œç”±bitsandbytesåº“æä¾›ï¼›P-Tuning v2å¯ä»¥é‡‡ç”¨åè€…ï¼Œå‚è€ƒ[é‡åŒ–æ–¹å¼åŒºåˆ«](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/69)ã€‚
        - **è®­ç»ƒå¯åŠ¨æ–¹å¼**:
            - **æ•°æ®å¹¶è¡Œ**:

                    # åˆ‡æ¢è·¯å¾„
                    cd chatglm-ft-lora/

                    # å¯åŠ¨è®­ç»ƒ
                    CUDA_VISIBLE_DEVICES=1,2 torchrun --nproc_per_node=2 train.py --train_args_file ./conf/chatglm2_6b_lora.json --model_name_or_path ../../chatglm2-6b-model/ --data_path ./data/AdvertiseGen/train.jsonl --max_input_length 128 --max_output_length 256


            - **æ¨¡å‹ï¼ˆæµæ°´çº¿ï¼‰å¹¶è¡Œ**:

                    # åˆ‡æ¢è·¯å¾„
                    cd ./chatglm-ft-lora/

                    # å¯åŠ¨è®­ç»ƒ
                    CUDA_VISIBLE_DEVICES=1,2 python train.py --train_args_file ./conf/chatglm_6b_lora.json --model_name_or_path ../../chatglm-6b-model/ --data_path ./data/AdvertiseGen/train.jsonl --max_input_length 128 --max_output_length 256 --int8


                - **æ³¨æ„**: è¿›è¡Œæ¨¡å‹å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œéœ€è¦æ³¨æ„ä¸€ä¸ªé—®é¢˜ï¼Œå³å®‰è£…åŒ…é—®é¢˜ã€‚
                    - **å®‰è£…åŒ…é—®é¢˜**: é‡‡ç”¨æ¨¡å‹å¹¶è¡Œæ—¶ï¼Œè¿˜éœ€å®‰è£…`accelerate`ã€`bitsandbytes`ã€`scipy`ã€`tensorboardX`å››ä¸ªå®‰è£…åŒ…ã€‚
    - **ChatGLM2-6B + LoRA â‡’ \\Rightarrow â‡’å®˜æ–¹ä»»åŠ¡å®è·µ**: å®ç°äº†ChatGLM2-6BåŸºäºLoRAçš„å¾®è°ƒæµç¨‹ã€‚å…·ä½“ä»£ç è§[LLMå¾®è°ƒå®è·µ](https://github.com/DankoZhang/LLM/blob/main/README.md)ã€‚æ¨¡å‹æ–‡ä»¶åŒæ ·å¯æ ¹æ®å‰æ–‡çš„æ–¹æ³•è¿›è¡Œè·å–ï¼Œå…¶ä¸­å®˜æ–¹çš„æ¨¡å‹å¯èƒ½å­˜åœ¨æ›´æ–°ï¼Œå¦‚æœæƒ³é¡ºåˆ©å¤ç°è®­ç»ƒè¿‡ç¨‹ï¼Œå»ºè®®ä»ç½‘ç›˜è¿›è¡Œä¸‹è½½ã€‚

        - **LoRAé…ç½®å‚æ•°**: åŒChatGLM-6Bï¼›
        - **è®­ç»ƒå¯åŠ¨æ–¹å¼**:
            - **æ•°æ®å¹¶è¡Œ**:

                    # åˆ‡æ¢è·¯å¾„
                    cd ./chatglm2-ft-lora/

                    # å¯åŠ¨è®­ç»ƒ
                    CUDA_VISIBLE_DEVICES=1,2 torchrun --nproc_per_node=2 train.py --train_args_file ./conf/chatglm2_6b_lora.json --model_name_or_path ../../chatglm2-6b-model/ --data_path ./data/AdvertiseGen/train.jsonl --max_input_length 128 --max_output_length 256


                - **æ³¨æ„**: ä½¿ç”¨ChatGLM2-6Bè¿›è¡Œæ•°æ®å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œéœ€è¦æ³¨æ„ä¸€ä¸ªé—®é¢˜ï¼Œå³å¹¶è¡Œé—®é¢˜ã€‚
                    - **å¹¶è¡Œé—®é¢˜**: å®é™…è¿è¡Œæ—¶ï¼Œå¦‚æœæŠ¥é”™å¦‚ä¸‹ï¼Œè¯´æ˜æ˜¾å­˜ä¸å¤Ÿäº†ï¼Œæˆ‘å½“æ—¶å› ä¸ºå¦ä¸€å¼ å¡å¹¶éå®Œå…¨ç©ºä½™ï¼Œå°±ä¿®æ”¹äº†å¹¶è¡Œç­–ç•¥ï¼Œåªé‡‡ç”¨äº†å•å¡è®­ç»ƒã€‚

                            # é”™è¯¯å†…å®¹
                            RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

                            # å•å¡è®­ç»ƒ
                            CUDA_VISIBLE_DEVICES=1 torchrun --nproc_per_node=1 train.py --train_args_file ./conf/chatglm2_6b_lora.json --model_name_or_path ../../chatglm2-6b-model/ --data_path ./data/AdvertiseGen/train.jsonl --max_input_length 128 --max_output_length 256		```


            - **æ¨¡å‹ï¼ˆæµæ°´çº¿ï¼‰å¹¶è¡Œ**:

                    # åˆ‡æ¢è·¯å¾„
                    cd chatglm2-ft-lora/

                    # å¯åŠ¨è®­ç»ƒ
                    CUDA_VISIBLE_DEVICES=1,2 python train.py --train_args_file ./conf/chatglm2_6b_lora.json --model_name_or_path ../../chatglm2-6b-model/ --data_path ./data/AdvertiseGen/train.jsonl --max_input_length 128 --max_output_length 256 --int8


                - **æ³¨æ„**: è¿›è¡Œæ¨¡å‹å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œéœ€è¦æ³¨æ„ä¸¤ä¸ªé—®é¢˜ï¼Œå³å®‰è£…åŒ…é—®é¢˜ã€æ¨¡å‹æºç ä¿®æ”¹é—®é¢˜ã€‚
                    - **å®‰è£…åŒ…é—®é¢˜**: é‡‡ç”¨æ¨¡å‹å¹¶è¡Œæ—¶ï¼Œè¿˜éœ€å®‰è£…`accelerate`ã€`bitsandbytes`ã€`scipy`ã€`tensorboardX`å››ä¸ªå®‰è£…åŒ…ï¼›
                    - **æ¨¡å‹æºç ä¿®æ”¹é—®é¢˜**: é‡‡ç”¨æ¨¡å‹å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œå¦‚æœæŠ¥é”™å¦‚ä¸‹`found at least two devices, cuda:1 and cuda:0!`ï¼Œæ˜¯æ¨¡å‹æºç é—®é¢˜ã€‚å¦‚æœé‡‡ç”¨å®˜æ–¹æ¨¡å‹ï¼Œå¯èƒ½è¿™ä¸ªbugå·²ç»è¢«ä¿®å¤ï¼Œä½†æ˜¯å¦‚æœé‡‡ç”¨çš„æ˜¯ç™¾åº¦ç½‘ç›˜ä¸‹è½½çš„æ¨¡å‹ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½ä¼šå‡ºç°ï¼Œå› æ­¤éœ€è¦è§£å†³æ‰ã€‚è§£å†³åŠæ³•å¯å‚è€ƒ[bugä¿®å¤](https://github.com/yuanzhoulvpi2017/zero_nlp/issues/139)ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹`modeling_chatglm.py`æ–‡ä»¶çš„`955`è¡Œä»£ç é™„è¿‘åšå¦‚ä¸‹ä¿®æ”¹ï¼ˆåªä¿®æ”¹ä¸€è¡Œï¼Œå…¶ä½™ä¸å˜ï¼‰:

                            # åŸä»£ç 
                            loss = None
                            if labels is not None:
                                lm_logits = lm_logits.to(torch.float32)

                                # Shift so that tokens < n predict n
                                shift_logits = lm_logits[..., :-1, :].contiguous()
                                shift_labels = labels[..., 1:].contiguous() #<<<------------------çœ‹è¿™é‡Œ
                                # Flatten the tokens
                                loss_fct = CrossEntropyLoss(ignore_index=-100)
                                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

                                lm_logits = lm_logits.to(hidden_states.dtype)
                                loss = loss.to(hidden_states.dtype)

                            if not return_dict:
                                output = (lm_logits,) + transformer_outputs[1:]
                                return ((loss,) + output) if loss is not None else output

                            return CausalLMOutputWithPast(
                                loss=loss,
                                logits=lm_logits,
                                past_key_values=transformer_outputs.past_key_values,
                                hidden_states=transformer_outputs.hidden_states,
                                attentions=transformer_outputs.attentions,
                            )

                            # ä¿®æ”¹ä¸º
                            loss = None
                            if labels is not None:
                                lm_logits = lm_logits.to(torch.float32)

                                # Shift so that tokens < n predict n
                                shift_logits = lm_logits[..., :-1, :].contiguous()
                                shift_labels = labels[..., 1:].contiguous().to(shift_logits.device) #<<<--------------------çœ‹è¿™é‡Œ
                                # Flatten the tokens
                                loss_fct = CrossEntropyLoss(ignore_index=-100)
                                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

                                lm_logits = lm_logits.to(hidden_states.dtype)
                                loss = loss.to(hidden_states.dtype)

                            if not return_dict:
                                output = (lm_logits,) + transformer_outputs[1:]
                                return ((loss,) + output) if loss is not None else output

                            return CausalLMOutputWithPast(
                                loss=loss,
                                logits=lm_logits,
                                past_key_values=transformer_outputs.past_key_values,
                                hidden_states=transformer_outputs.hidden_states,
                                attentions=transformer_outputs.attentions,
                            )


    - **ChatGLM-6B + LoRA + Accelerate + Deepspeed â‡’ \\Rightarrow â‡’å®˜æ–¹ä»»åŠ¡å®è·µ**: å‚è€ƒäº†ä»£ç [LLM-tuning](https://github.com/jiangxinyang227/LLM-tuning/blob/master/README.md)ï¼Œå®ç°äº†è¯¥æµç¨‹ï¼Œå…·ä½“ä»£ç è§[LLMå¾®è°ƒå®è·µ](https://github.com/DankoZhang/LLM/blob/main/README.md)ã€‚ChatGLM2-6Bå¯å‚è€ƒå‰æ–‡ä»£ç ï¼Œå¯¹tokensizeæ”¹å†™ï¼Œè¿›è¡Œé€‚é…è®­ç»ƒå³å¯ã€‚ç”±äºDeepspeedæ¡†æ¶å¯¹ç¯å¢ƒä¾èµ–æ€§å¾ˆé«˜ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨dockeræŠ€æœ¯ï¼Œæ„å»º**cuda11.7**+**torch2.0.0**+**python3.10**è™šæ‹Ÿç¯å¢ƒã€‚Dockeræ„å»ºçš„å…·ä½“æ–¹æ³•å‚è€ƒ[DockeråŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/131906881?csdn_share_tail=%7B%22type%22:%22blog%22,%22rType%22:%22article%22,%22rId%22:%22131906881%22,%22source%22:%22qq_39439006%22%7D)ï¼Œæ­¤å¤„ç®€è¦ä»‹ç»æ•´ä½“æµç¨‹ã€‚

        - **Dockerå®¹å™¨æ„å»º**:

                # è¿è¡Œå®¹å™¨
                docker run -itd -v å®¿ä¸»æœºè·¯å¾„:å®¹å™¨è·¯å¾„ --shm-size=8gb --rm --runtime=nvidia --gpus all --network host --name GPU-Docker nvidia/cuda:11.7.1-devel-ubi8 /bin/bash

                # è¿›å…¥å®¹å™¨
                docker exec -it GPU-Docker /bin/bash

                # æ³¨
                --shm-size=8gbå¿…é¡»åŠ ä¸Šï¼Œä¸ç„¶è¿è¡Œä»£ç ä¼šæŠ¥å­˜å‚¨é”™è¯¯


        - **Pythonç¯å¢ƒæ„å»º**:
            - **Pythonå®‰è£…**: è‡ªè¡Œä¸‹è½½Python3.10ç‰ˆæœ¬çš„[Miniconda](https://docs.conda.io/en/latest/miniconda.html#installing) ;
                - **æ³¨**: è®°å¾—åœ¨å®¹å™¨å†…è®¾å®šPythonç¯å¢ƒå˜é‡

                        vi ~/.bashrc
                        export PATH=/home/LLM/ChatGLM-FT/miniconda3/bin:$PATH
                        source ~/.bashrc


            - **è™šæ‹Ÿç¯å¢ƒæ„å»º**: å‚è€ƒ[PythonåŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/131925283?csdn_share_tail=%7B%22type%22:%22blog%22,%22rType%22:%22article%22,%22rId%22:%22131925283%22,%22source%22:%22qq_39439006%22%7D)ï¼›
            - **ä¾èµ–åŒ…å®‰è£…**: ä»¥ä¸‹æ‰€æœ‰å®‰è£…åŒ…çš„ç‰ˆæœ¬éƒ½æ˜¯æ¨èï¼Œå¯æŒ‰å®é™…æƒ…å†µè‡ªè¡Œè°ƒæ•´ã€‚

                    # torchå®‰è£…
                    pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117

                    # å…¶ä»–æ¨¡å—å®‰è£…
                    pip install transformers==4.31.0
                    pip install datasets==2.14.0
                    pip install peft==0.4.0
                    pip install accelerate==0.21.0
                    pip install deepspeed==0.10.0
                    pip install sentencepiece==0.1.99


            - **è®­ç»ƒå¯åŠ¨æ–¹å¼**:

                    # åˆ‡æ¢è·¯å¾„
                    cd ./chatglm-ft-lora-dp/

                    # å¯åŠ¨è®­ç»ƒ
                    accelerate launch --config_file ./conf/accelerate_config.yaml


                - **æ¨¡å‹åŠ è½½è¯´æ˜**:
                    - `empty_init=False`: ç›®å‰å¦‚æœä½¿ç”¨Deepspeedè¿›è¡Œè®­ç»ƒï¼Œåœ¨åŠ è½½ChatGLMæ¨¡å‹æ—¶ï¼Œå‚æ•°`empty_init`å¿…é¡»ç½®ä¸ºFalseï¼ˆå‚è€ƒ[empty\_inité—®é¢˜](https://github.com/THUDM/ChatGLM-6B/issues/530)ï¼‰ï¼Œåç»­å®˜æ–¹å¯èƒ½ä¼šæ›´æ–°æºç ï¼Œä¿®å¤è¯¥é—®é¢˜ï¼›
                    - `trust_remote_code=True`: åŠ è½½æ¨¡å‹ä»£ç æ—¶ï¼ŒåŠ ä¸Šæ­¤å‚æ•°ï¼Œé˜²æ­¢æŠ¥é”™ï¼›
                    - `torch_dtype=torch.float16`ï¼ŒFP16åŠ è½½æ¨¡å‹ï¼›
                    - `args.base_model`: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œæœ€åä¸€å®šæ˜¯ä»¥`/`ç»“å°¾ï¼Œå¦‚`./chatglm-6b-model/`ï¼Œ`./chatglm-6b-model`ä¼šæŠ¥é”™ã€‚

                            model = AutoModel.from_pretrained(
                                        args.base_model,
                                        empty_init=False,
                                        torch_dtype=torch.float16,
                                        trust_remote_code=True
                                    )


                - **æ³¨æ„**: æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚æœå‡ºç°å¦‚ä¸‹é”™è¯¯: `ValueError: max() arg is an empty sequence`ï¼Œéœ€è¦å¯¹deepspeedæºç è¿›è¡Œä¿®æ”¹ã€‚

                        # æºç è·¯å¾„
                        ./miniconda3/envs/zhangce-dp/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py

                        # åŸä»£ç 
                        largest_partitioned_param_numel = max([
                            max([max(tensor.numel(), tensor.ds_numel) for tensor in fp16_partitioned_group])
                            for fp16_partitioned_group in self.fp16_partitioned_groups
                        ])

                        # ä¿®æ”¹åä»£ç 
                        largest_partitioned_param_numel = max([
                            max([max(tensor.numel(), tensor.ds_numel) for tensor in fp16_partitioned_group])
                            for fp16_partitioned_group in self.fp16_partitioned_groups if len (fp16_partitioned_group) > 0
                        ])


- **ç›¸å…³å­¦ä¹ èµ„æº**:

    ç±»åˆ«

    ç®€ä»‹

    é“¾æ¥

    PEFTå·¥å…·

    PEFTçš„å®˜æ–¹ä»‹ç»

    [PEFT](https://github.com/huggingface/peft)

    PEFTå·¥å…·

    PEFTçš„ç®€å•ä½¿ç”¨

    [PEFT: åœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ](https://zhuanlan.zhihu.com/p/621740939)

    LLM-Tuning

    LLMåŸç†åŠå®æˆ˜ç»éªŒåˆ†äº«

    [LLM-å®æˆ˜ç»éªŒ](https://github.com/liguodongiot/llm-action/blob/main/README.md)

    LLM-Tuning

    ChatGLM-6Båœ¨çœŸå®ä»»åŠ¡ä¸Šçš„åº”ç”¨

    [ChatGLM-çœŸå®ä»»åŠ¡åº”ç”¨](https://github.com/liucongg/ChatGLM-Finetuning/blob/master/README.md)

    LLM-Tuning

    ChatGLM-6B/ChatGLM2-6Bç»“åˆQLoRAå®ç°LLM-Tuning

    [ChatGLM-6B+QLoRA](https://github.com/shuxueslpi/chatGLM-6B-QLoRA/blob/main/README.md)

    LLM-Tuning

    å…³äºLLMå¾®è°ƒçš„ä¸€äº›çŸ¥è¯†ç‚¹

    [NLPå¤§æ¨¡å‹å¾®è°ƒç­”ç–‘](https://blog.csdn.net/mingzai624/article/details/130735366)

    LLM-Tuning

    ä½œè€…å¯¹ä½¿ç”¨çš„ChatGLM+LoRAæ–¹æ¡ˆè¿›è¡Œäº†ä»£ç è§£æ

    [ChatGLM+LoRAä»£ç è§£æ](https://github.com/Pillars-Creation/ChatGLM-LoRA)

    LLM-Tuning

    å¾®è°ƒå·¥å…·transformers.Trainerçš„å‚æ•°è§£æ

    [Trainerå‚æ•°è§£æ](https://zhuanlan.zhihu.com/p/363670628)

    LLM-åŸºç¡€

    ä½œè€…é’ˆå¯¹LLMåŸç†è¿›è¡Œäº†çŸ¥è¯†æ€»ç»“

    [LLMåŸºç¡€çŸ¥è¯†åˆ†äº«](https://www.zhihu.com/people/suc16/posts)

    LLM-åŸºç¡€

    ä»‹ç»äº†LLMå¤šç§æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆçš„åŸç†

    [LLMæ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ](https://blog.csdn.net/qq_27590277/article/details/126635256)

    LLM-Pretrain

    ä»‹ç»åƒäº¿å‚æ•°å¼€æºå¤§æ¨¡å‹BLOOMèƒŒåçš„æŠ€æœ¯

    [BLOOMæŠ€æœ¯ä»‹ç»](https://zhuanlan.zhihu.com/p/641650843)

    ç³»ç»ŸçŸ¥è¯†

    å¯¹ç®—æ³•åŸºç¡€ã€ç®—æ³•åº”ç”¨è¿›è¡Œå…¨é¢æ€»ç»“

    [ç®—æ³•æ€»ç»“](https://www.huaxiaozhuan.com/)


#### 5.3ã€å¤§æ¨¡å‹Fine-Tuningä¹‹åˆ†å¸ƒå¼è®­ç»ƒ

â€ƒâ€ƒæŒ‰ç…§å¹¶è¡Œæ–¹å¼ï¼Œåˆ†å¸ƒå¼è®­ç»ƒä¸€èˆ¬åˆ†ä¸ºæ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œä¸¤ç§ï¼Œå½“ç„¶ä¹Ÿæœ‰æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„æ··åˆæ¨¡å¼ã€‚

- **æ¨¡å‹å¹¶è¡Œ**: åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ä¸åŒGPUè´Ÿè´£ç½‘ç»œæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸åŒç½‘ç»œå±‚è¢«åˆ†é…åˆ°ä¸åŒçš„GPUï¼ˆç§°ä½œ**pipelineå¹¶è¡Œ/æµæ°´çº¿å¹¶è¡Œ**ï¼‰ï¼Œæˆ–è€…åŒä¸€å±‚å†…éƒ¨çš„ä¸åŒå‚æ•°è¢«åˆ†é…åˆ°ä¸åŒGPUï¼ˆç§°ä½œ**tensorå¹¶è¡Œ/å¼ é‡å¹¶è¡Œ**ï¼‰ï¼›
- **æ•°æ®å¹¶è¡Œ**: ä¸åŒçš„GPUæœ‰åŒä¸€ä¸ªæ¨¡å‹çš„å¤šä¸ªå‰¯æœ¬ï¼Œæ¯ä¸ªGPUåˆ†é…åˆ°ä¸åŒçš„æ•°æ®ï¼Œç„¶åå°†æ‰€æœ‰GPUçš„è®¡ç®—ç»“æœæŒ‰ç…§æŸç§æ–¹å¼åˆå¹¶ã€‚

â€ƒâ€ƒä»¥PyTorchæ¡†æ¶ä¸ºä¾‹ï¼Œä»‹ç»å‡ ç§åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ã€‚

- **DataParallel(DP)**:
    - **ç®€ä»‹**: å•æœºå¤šå¡çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼›æ•°æ®å¹¶è¡Œæ¨¡å¼ã€‚

    - **åŸç†**: ç½‘ç»œåœ¨å‰å‘ä¼ æ’­çš„æ—¶å€™ä¼šå°†modelä»ä¸»å¡(é»˜è®¤æ˜¯é€»è¾‘0å¡)å¤åˆ¶ä¸€ä»½åˆ°æ‰€æœ‰çš„deviceä¸Šï¼Œinput\_dataä¼šåœ¨batchè¿™ä¸ªç»´åº¦è¢«åˆ†ç»„ååŠ è½½åˆ°ä¸åŒçš„deviceä¸Šè®¡ç®—ã€‚åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ªå¡ä¸Šçš„æ¢¯åº¦ä¼šæ±‡æ€»åˆ°ä¸»å¡ä¸Šï¼Œæ±‚å¾—æ¢¯åº¦çš„å‡å€¼åï¼Œå†ç”¨åå‘ä¼ æ’­æ›´æ–°å•ä¸ªGPUä¸Šçš„æ¨¡å‹å‚æ•°ï¼Œæœ€åå°†æ›´æ–°åçš„æ¨¡å‹å‚æ•°å¤åˆ¶åˆ°å‰©ä½™æŒ‡å®šçš„GPUä¸­è¿›è¡Œä¸‹ä¸€è½®çš„å‰å‘ä¼ æ’­ï¼Œä»¥æ­¤æ¥å®ç°å¹¶è¡Œã€‚

    - **å‚æ•°ç®€ä»‹**: `torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)`

        - **module**: æ˜¯è¦æ”¾åˆ°å¤šå¡è®­ç»ƒçš„æ¨¡å‹ï¼›
        - **device\_ids**: æ•°æ®ç±»å‹æ˜¯ä¸€ä¸ªåˆ—è¡¨, è¡¨ç¤ºå¯ç”¨çš„gpuå¡å·ï¼›
        - **output\_devices**: æ•°æ®ç±»å‹ä¹Ÿæ˜¯åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¨¡å‹è¾“å‡ºç»“æœå­˜æ”¾çš„å¡å·(å¦‚æœä¸æŒ‡å®šçš„è¯,é»˜è®¤æ”¾åœ¨0å¡ï¼Œå³device\_idsé¦–ä½ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¤šgpuè®­ç»ƒå¹¶ä¸æ˜¯è´Ÿè½½å‡è¡¡çš„ï¼Œä¸€èˆ¬0å¡ä¼šå ç”¨çš„å¤šï¼Œè¿™é‡Œè¿˜æ¶‰åŠåˆ°ä¸€ä¸ªå°çŸ¥è¯†ç‚¹: å¦‚æœä»£ç å¼€å§‹è®¾å®š`os.environ["CUDA_VISIBLE_DEVICES"] = "2, 3"`ï¼Œé‚£ä¹ˆ0å¡(é€»è¾‘å¡å·)æŒ‡çš„æ˜¯2å¡(ç‰©ç†å¡å·)ã€‚
    - **æ¨¡å‹å‚æ•°æ›´æ–°æ–¹å¼**:

        - DataLoaderæŠŠæ•°æ®é€šè¿‡å¤šä¸ªworkerè¯»åˆ°ä¸»è¿›ç¨‹çš„å†…å­˜ä¸­ï¼›
        - é€šè¿‡tensorçš„splitè¯­ä¹‰ï¼Œå°†ä¸€ä¸ªbatchçš„æ•°æ®åˆ‡åˆ†æˆå¤šä¸ªæ›´å°çš„batchï¼Œç„¶ååˆ†åˆ«é€å¾€ä¸åŒçš„cudaè®¾å¤‡ï¼›
        - åœ¨ä¸åŒçš„cudaè®¾å¤‡ä¸Šå®Œæˆå‰å‘è®¡ç®—ï¼Œç½‘ç»œçš„è¾“å‡ºè¢«gatheråˆ°ä¸»cudaè®¾å¤‡ä¸Šï¼ˆåˆå§‹åŒ–æ—¶ä½¿ç”¨çš„è®¾å¤‡ï¼‰ï¼Œlossè€Œååœ¨è¿™é‡Œè¢«è®¡ç®—å‡ºæ¥ï¼›
        - lossç„¶åè¢«scatteråˆ°æ¯ä¸ªcudaè®¾å¤‡ä¸Šï¼Œæ¯ä¸ªcudaè®¾å¤‡é€šè¿‡BPè®¡ç®—å¾—åˆ°æ¢¯åº¦ï¼›
        - ç„¶åæ¯ä¸ªcudaè®¾å¤‡ä¸Šçš„æ¢¯åº¦è¢«reduceåˆ°ä¸»cudaè®¾å¤‡ä¸Šï¼Œç„¶åæ¨¡å‹æƒé‡åœ¨ä¸»cudaè®¾å¤‡ä¸Šè·å¾—æ›´æ–°ï¼›
        - åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹å‰ï¼Œä¸»cudaè®¾å¤‡å°†æ¨¡å‹å‚æ•°broadcaståˆ°å…¶å®ƒcudaè®¾å¤‡ä¸Šï¼Œå®Œæˆæƒé‡å‚æ•°å€¼çš„åŒæ­¥ã€‚
    - **æœ¯è¯­ä»‹ç»**:

        - **broadcast**: æ˜¯ä¸»è¿›ç¨‹å°†ç›¸åŒçš„æ•°æ®åˆ†å‘ç»™ç»„é‡Œçš„æ¯ä¸€ä¸ªå…¶å®ƒè¿›ç¨‹ï¼›
        - **scatter**: æ˜¯ä¸»è¿›ç¨‹å°†æ•°æ®çš„æ¯ä¸€å°éƒ¨åˆ†ç»™ç»„é‡Œçš„å…¶å®ƒè¿›ç¨‹ï¼›
        - **gather**: æ˜¯å°†å…¶å®ƒè¿›ç¨‹çš„æ•°æ®æ”¶é›†è¿‡æ¥ï¼›
        - **reduce**: æ˜¯å°†å…¶å®ƒè¿›ç¨‹çš„æ•°æ®æ”¶é›†è¿‡æ¥å¹¶åº”ç”¨æŸç§æ“ä½œï¼ˆæ¯”å¦‚SUMï¼‰ï¼›
        - **è¡¥å……**: åœ¨gatherå’Œreduceæ¦‚å¿µå‰é¢è¿˜å¯ä»¥åŠ ä¸Šallï¼Œå¦‚all\_gatherï¼Œall\_reduceï¼Œé‚£å°±æ˜¯å¤šå¯¹å¤šçš„å…³ç³»äº†ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b198db115c8c4a7cacfe1db9cabf35c6.png#pic_center)
    - **ä½¿ç”¨ç¤ºä¾‹**: å‚è€ƒ[ä¸€æ–‡æå®šåˆ†å¸ƒå¼è®­ç»ƒ: dataparallelã€distributedã€deepspeedã€accelerateã€transformersã€horovod](https://zhuanlan.zhihu.com/p/628022953)

- **DistributedDataParallel(DDP)**:
    - **ç®€ä»‹**: æ—¢å¯å•æœºå¤šå¡åˆå¯å¤šæœºå¤šå¡çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼›æ•°æ®å¹¶è¡Œæ¨¡å¼ã€‚

    - **åŸç†**: DDPåœ¨å„è¿›ç¨‹æ¢¯åº¦è®¡ç®—å®Œæˆä¹‹åï¼Œå„è¿›ç¨‹éœ€è¦å°†æ¢¯åº¦è¿›è¡Œæ±‡æ€»å¹³å‡ï¼Œç„¶åå†ç”±rank=0çš„è¿›ç¨‹ï¼Œå°†å…¶broadcaståˆ°æ‰€æœ‰è¿›ç¨‹åï¼Œå„è¿›ç¨‹ç”¨è¯¥æ¢¯åº¦æ¥ç‹¬ç«‹çš„æ›´æ–°å‚æ•°ï¼Œè€ŒDPæ˜¯æ¢¯åº¦æ±‡æ€»åˆ°GPU0ï¼Œåå‘ä¼ æ’­æ›´æ–°å‚æ•°ï¼Œå†å¹¿æ’­å‚æ•°ç»™å…¶ä»–å‰©ä½™çš„GPUã€‚ç”±äºDDPå„è¿›ç¨‹ä¸­çš„æ¨¡å‹ï¼Œåˆå§‹å‚æ•°ä¸€è‡´ (åˆå§‹æ—¶åˆ»è¿›è¡Œä¸€æ¬¡broadcast)ï¼Œè€Œæ¯æ¬¡ç”¨äºæ›´æ–°å‚æ•°çš„æ¢¯åº¦ä¹Ÿä¸€è‡´ï¼Œå› æ­¤ï¼Œå„è¿›ç¨‹çš„æ¨¡å‹å‚æ•°å§‹ç»ˆä¿æŒä¸€è‡´ã€‚è€Œåœ¨DPä¸­ï¼Œå…¨ç¨‹ç»´æŠ¤ä¸€ä¸ªoptimizerï¼Œå¯¹å„ä¸ªGPUä¸Šæ¢¯åº¦è¿›è¡Œæ±‚å¹³å‡ï¼Œåœ¨ä¸»å¡è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä¹‹åå†å°†æ¨¡å‹å‚æ•°broadcaståˆ°å…¶ä»–GPUï¼Œç›¸è¾ƒäºDPï¼ŒDDPä¼ è¾“çš„æ•°æ®é‡æ›´å°‘ï¼Œå› æ­¤é€Ÿåº¦æ›´å¿«ï¼Œæ•ˆç‡æ›´é«˜ã€‚

    - **å‚æ•°ç®€ä»‹**: `torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False)`

        - **module**: æ˜¯è¦æ”¾åˆ°å¤šå¡è®­ç»ƒçš„æ¨¡å‹ï¼›
        - **device\_ids**: æ˜¯ä¸€ä¸ªåˆ—è¡¨, è¡¨ç¤ºå¯ç”¨çš„gpuå¡å·ï¼›
        - **output\_devices**: ä¹Ÿæ˜¯åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¨¡å‹è¾“å‡ºç»“æœå­˜æ”¾çš„å¡å·(å¦‚æœä¸æŒ‡å®šçš„è¯,é»˜è®¤æ”¾åœ¨0å¡ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¤šgpuè®­ç»ƒå¹¶ä¸æ˜¯è´Ÿè½½å‡è¡¡çš„,ä¸€èˆ¬0å¡ä¼šå ç”¨çš„å¤šï¼Œè¿™é‡Œè¿˜æ¶‰åŠåˆ°ä¸€ä¸ªå°çŸ¥è¯†ç‚¹: å¦‚æœç¨‹åºå¼€å§‹åŠ `os.environ["CUDA_VISIBLE_DEVICES"] = "2, 3"`ï¼Œé‚£ä¹ˆ0å¡(é€»è¾‘å¡å·)æŒ‡çš„æ˜¯2å¡(ç‰©ç†å¡å·))ï¼›
        - **dim**: æŒ‡æŒ‰å“ªä¸ªç»´åº¦è¿›è¡Œæ•°æ®çš„åˆ’åˆ†ï¼Œé»˜è®¤æ˜¯è¾“å…¥æ•°æ®çš„ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œå³æŒ‰batchsizeåˆ’åˆ†(è®¾æ•°æ®æ•°æ®çš„æ ¼å¼æ˜¯B, C, H, W)ã€‚
    - **æ¨¡å‹å‚æ•°æ›´æ–°æ–¹å¼**:

        - process groupï¼ˆè¿›ç¨‹ç»„ï¼‰ä¸­çš„è®­ç»ƒè¿›ç¨‹éƒ½èµ·æ¥åï¼Œrankä¸º0çš„è¿›ç¨‹ä¼šå°†ç½‘ç»œåˆå§‹åŒ–å‚æ•°broadcaståˆ°å…¶å®ƒæ¯ä¸ªè¿›ç¨‹ä¸­ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹ä¸­çš„ç½‘ç»œéƒ½æ˜¯ä¸€æ ·çš„åˆå§‹åŒ–çš„å€¼ï¼ˆé»˜è®¤è¡Œä¸ºï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡å‚æ•°ç¦æ­¢ï¼‰ï¼›
        - æ¯ä¸ªè¿›ç¨‹å„è‡ªè¯»å–å„è‡ªçš„è®­ç»ƒæ•°æ®ï¼ŒDistributedSamplerç¡®ä¿äº†è¿›ç¨‹ä¸¤ä¸¤ä¹‹é—´è¯»åˆ°çš„æ˜¯ä¸ä¸€æ ·çš„æ•°æ®ï¼›
        - å‰å‘å’Œlossçš„è®¡ç®—å¦‚ä»Šéƒ½æ˜¯åœ¨æ¯ä¸ªè¿›ç¨‹ä¸Šï¼ˆä¹Ÿå°±æ˜¯æ¯ä¸ªcudaè®¾å¤‡ä¸Šï¼‰ç‹¬ç«‹è®¡ç®—å®Œæˆçš„ï¼›ç½‘ç»œçš„è¾“å‡ºä¸å†éœ€è¦gatheråˆ°masterè¿›ç¨‹ä¸Šäº†ï¼Œè¿™å’ŒDPæ˜¾è‘—ä¸ä¸€æ ·ï¼›
        - åå‘é˜¶æ®µï¼Œæ¢¯åº¦ä¿¡æ¯é€šè¿‡all-reduceçš„MPIï¼ˆMessage Passing Interfaceï¼Œæ¶ˆæ¯ä¼ é€’æ¥å£ï¼‰åŸè¯­ï¼Œå°†æ¯ä¸ªè¿›ç¨‹ä¸­è®¡ç®—åˆ°çš„æ¢¯åº¦reduceåˆ°æ¯ä¸ªè¿›ç¨‹ï¼›ä¹Ÿå°±æ˜¯backwardè°ƒç”¨ç»“æŸåï¼Œæ¯ä¸ªè¿›ç¨‹ä¸­çš„param.gradéƒ½æ˜¯ä¸€æ ·çš„å€¼ï¼›æ³¨æ„ï¼Œä¸ºäº†æé«˜all-reduceçš„æ•ˆç‡ï¼Œæ¢¯åº¦ä¿¡æ¯è¢«åˆ’åˆ†æˆäº†å¤šä¸ªbucketsï¼›
        - æ›´æ–°æ¨¡å‹å‚æ•°é˜¶æ®µï¼Œå› ä¸ºåˆšå¼€å§‹æ¨¡å‹çš„å‚æ•°æ˜¯ä¸€æ ·çš„ï¼Œè€Œæ¢¯åº¦åˆæ˜¯all-reduceçš„ï¼Œè¿™æ ·æ›´æ–°å®Œæ¨¡å‹å‚æ•°åï¼Œæ¯ä¸ªè¿›ç¨‹/è®¾å¤‡ä¸Šçš„æƒé‡å‚æ•°ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚å› æ­¤ï¼Œå°±æ— éœ€DPé‚£æ ·æ¯æ¬¡è¿­ä»£åéœ€è¦åŒæ­¥ä¸€æ¬¡ç½‘ç»œå‚æ•°ï¼Œè¿™ä¸ªé˜¶æ®µçš„broadcastæ“ä½œå°±ä¸å­˜åœ¨äº†ã€‚æ³¨æ„ï¼ŒNetworkä¸­çš„Buffers (æ¯”å¦‚BatchNormæ•°æ®) éœ€è¦åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä»rankä¸º0çš„è¿›ç¨‹broadcaståˆ°è¿›ç¨‹ç»„çš„å…¶å®ƒè¿›ç¨‹ä¸Šã€‚
    - **åŸºæœ¬æ¦‚å¿µ**: å‡è®¾æˆ‘ä»¬æœ‰3å°æœºå­ï¼ˆèŠ‚ç‚¹ï¼‰ï¼Œæ¯å°æœºå­æœ‰4å—GPUã€‚æˆ‘ä»¬å¸Œæœ›è¾¾åˆ°12å¡å¹¶è¡Œçš„æ•ˆæœã€‚

        - **è¿›ç¨‹**: ç¨‹åºè¿è¡Œèµ·æ¥å°±æ˜¯è¿›ç¨‹ã€‚åœ¨DDPä¸­ï¼Œå¤§å®¶å¾€å¾€è®©ä¸€ä¸ªè¿›ç¨‹æ§åˆ¶ä¸€ä¸ªGPUï¼›åè¿‡æ¥è¯´ï¼Œæ¯ä¸ªGPUç”±ä¸€ä¸ªè¿›ç¨‹æ§åˆ¶ã€‚å› æ­¤12å¡å¹¶è¡Œå°±éœ€è¦åŒæ­¥è¿è¡Œçš„12ä¸ªè¿›ç¨‹ã€‚å› æ­¤åæ–‡ä¸­ï¼Œåªè¦æåˆ°è¿›ç¨‹ï¼ŒæŒ‡çš„å°±æ˜¯æŸå°æœºå­ä¸Šçš„æŸä¸ªGPUåœ¨è·‘çš„ç¨‹åºï¼›
        - **è¿›ç¨‹ç»„**: ä¸€ä¸ªåˆ†å¸ƒå¼ä»»åŠ¡å¯¹åº”äº†ä¸€ä¸ªè¿›ç¨‹ç»„ã€‚åªæœ‰ç”¨æˆ·éœ€è¦åˆ›ç«‹å¤šä¸ªè¿›ç¨‹ç»„æ—¶æ‰ä¼šç”¨åˆ°groupæ¥ç®¡ç†ï¼Œé»˜è®¤æƒ…å†µä¸‹åªæœ‰ä¸€ä¸ªgroupï¼›
        - **world size**: è¿›ç¨‹ç»„ä¸­è¿›ç¨‹ä¸ªæ•°ã€‚ä¹Ÿå«å…¨å±€å¹¶è¡Œæ•°ã€‚å°±æ˜¯æŒ‡æ€»å…±æƒ³è¦ç”¨çš„GPUçš„ä¸ªæ•°ã€‚è¿™é‡Œæˆ‘ä»¬çš„world sizeå°±æ˜¯12ï¼›
        - **rank**: å½“å‰è¿›ç¨‹åºå·ã€‚èŒƒå›´è¦†ç›–æ•´ä¸ªè¿›ç¨‹ç»„: 0 ~ world size-1ï¼Œæˆ‘ä»¬æœ‰12ä¸ªGPUï¼Œå„è‡ªè·‘1ä¸ªè¿›ç¨‹ï¼Œå„è‡ªçš„è¿›ç¨‹å·ä¸º0-11ã€‚è¿›ç¨‹å·ä¸º0çš„è¿›ç¨‹å«åšmasterï¼Œèº«ä»½æ¯”è¾ƒç‰¹åˆ«ï¼Œéœ€è¦ç•™æ„ï¼›
        - **local rank**: æ¯å°æœºå­ä¸Šè¿›ç¨‹çš„åºå·ï¼Œè¢«å„ä¸ªæœºå­ç”¨æ¥åŒºåˆ†è·‘åœ¨è‡ªå·±èº«ä¸Šçš„è¿›ç¨‹ã€‚èŒƒå›´æ˜¯0 ~ æŸæœºå­è¿›ç¨‹æ•°-1ã€‚æˆ‘ä»¬æ¯å°æœºå­æœ‰4ä¸ªGPUï¼Œå› æ­¤ä¸‰å°æœºå­ä¸Šçš„local rankéƒ½æ˜¯ä»0 ~ 3ã€‚åœ¨å•æœºå¤šå¡çš„æƒ…å†µä¸‹ï¼Œlocal rankä¸rankæ˜¯ç›¸åŒçš„ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/da2dd035f1e44a3d96f41768c1c64312.png#pic_center)
    - **ä½¿ç”¨ç¤ºä¾‹**: [åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ä»‹ç»](https://github.com/DankoZhang/Ner/blob/main/README.md)

    - **DP vs DDP**:

        - DDPé€šè¿‡å¤šè¿›ç¨‹å®ç°çš„ã€‚ä¹Ÿå°±æ˜¯è¯´æ“ä½œç³»ç»Ÿä¼šä¸ºæ¯ä¸ªGPUåˆ›å»ºä¸€ä¸ªè¿›ç¨‹ï¼Œä»è€Œé¿å…äº†Pythonè§£é‡Šå™¨GILå¸¦æ¥çš„æ€§èƒ½å¼€é”€ã€‚è€ŒDPæ˜¯é€šè¿‡å•è¿›ç¨‹æ§åˆ¶å¤šçº¿ç¨‹æ¥å®ç°çš„ã€‚è¿˜æœ‰ä¸€ç‚¹ï¼ŒDDPä¹Ÿä¸å­˜åœ¨å‰é¢DPæåˆ°çš„è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼›
        - å‚æ•°æ›´æ–°çš„æ–¹å¼ä¸åŒã€‚DDPåœ¨å„è¿›ç¨‹æ¢¯åº¦è®¡ç®—å®Œæˆä¹‹åï¼Œå„è¿›ç¨‹éœ€è¦å°†æ¢¯åº¦è¿›è¡Œæ±‡æ€»å¹³å‡ï¼Œç„¶åå†ç”±rank=0çš„è¿›ç¨‹ï¼Œå°†å…¶broadcaståˆ°æ‰€æœ‰è¿›ç¨‹åï¼Œå„è¿›ç¨‹ç”¨è¯¥æ¢¯åº¦æ¥ç‹¬ç«‹çš„æ›´æ–°å‚æ•°ï¼Œè€ŒDPæ˜¯æ¢¯åº¦æ±‡æ€»åˆ°GPU0ï¼Œåå‘ä¼ æ’­æ›´æ–°å‚æ•°ï¼Œå†å¹¿æ’­å‚æ•°ç»™å…¶ä»–å‰©ä½™çš„GPUã€‚ç”±äºDDPå„è¿›ç¨‹ä¸­çš„æ¨¡å‹ï¼Œåˆå§‹å‚æ•°ä¸€è‡´ (åˆå§‹æ—¶åˆ»è¿›è¡Œä¸€æ¬¡broadcast)ï¼Œè€Œæ¯æ¬¡ç”¨äºæ›´æ–°å‚æ•°çš„æ¢¯åº¦ä¹Ÿä¸€è‡´ï¼Œå› æ­¤ï¼Œå„è¿›ç¨‹çš„æ¨¡å‹å‚æ•°å§‹ç»ˆä¿æŒä¸€è‡´ã€‚è€Œåœ¨DPä¸­ï¼Œå…¨ç¨‹ç»´æŠ¤ä¸€ä¸ªoptimizerï¼Œå¯¹å„ä¸ªGPUä¸Šæ¢¯åº¦è¿›è¡Œæ±‚å¹³å‡ï¼Œåœ¨ä¸»å¡è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä¹‹åå†å°†æ¨¡å‹å‚æ•°broadcaståˆ°å…¶ä»–GPUï¼Œç›¸è¾ƒäºDPï¼ŒDDPä¼ è¾“çš„æ•°æ®é‡æ›´å°‘ï¼Œå› æ­¤é€Ÿåº¦æ›´å¿«ï¼Œæ•ˆç‡æ›´é«˜ï¼›
        - DDPæ”¯æŒall-reduce(æŒ‡æ±‡æ€»ä¸åŒGPUè®¡ç®—æ‰€å¾—çš„æ¢¯åº¦ï¼Œå¹¶åŒæ­¥è®¡ç®—ç»“æœ)ï¼Œbroadcastï¼Œsendå’Œreceiveç­‰ç­‰ã€‚é€šè¿‡MPIã€GLOOå®ç°CPUé€šä¿¡ï¼Œé€šè¿‡NCCLå®ç°GPUé€šä¿¡ï¼Œç¼“è§£äº†è¿›ç¨‹é—´é€šä¿¡å¼€é”€å¤§çš„é—®é¢˜ã€‚
- **è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰**: è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆautomatic mixed-precision trainingï¼‰å¹¶ä¸æ˜¯ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œé€šå¸¸å®ƒä¸å…¶ä»–åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ç›¸ç»“åˆï¼Œèƒ½è¿›ä¸€æ­¥æå‡è®­ç»ƒé€Ÿåº¦ã€‚ä¸‹é¢æˆ‘ä»¬ç®€å•ä»‹ç»ä¸‹AMPçš„åŸç†ï¼Œç„¶åä¸DDPç»“åˆï¼Œç»™å‡ºAMPçš„ä½¿ç”¨èŒƒä¾‹ã€‚å…·ä½“å‚è€ƒè®ºæ–‡[MIXED PRECISION TRAINING](https://arxiv.org/pdf/1710.03740.pdf)ã€‚
    - **ç®€ä»‹**: é»˜è®¤æƒ…å†µä¸‹ï¼Œå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½é‡‡ç”¨32ä½æµ®ç‚¹ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚2017å¹´ï¼ŒNVIDIAç ”ç©¶äº†ä¸€ç§ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒç½‘ç»œæ—¶å°†å•ç²¾åº¦ï¼ˆFP32ï¼Œä»¥32bitsè¡¨ç¤ºæ•°å­—ï¼Œå³4bytesï¼‰ä¸åŠç²¾åº¦(FP16ï¼Œä»¥16bitsè¡¨ç¤ºæ•°å­—ï¼Œå³2bytes)ç»“åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°å®ç°äº†ä¸FP32å‡ ä¹ç›¸åŒçš„æ•ˆæœã€‚ä»¥PyTorchä¸ºä¾‹ï¼Œå¯é€šè¿‡å¦‚ä¸‹å‘½ä»¤æŸ¥çœ‹æ¨¡å‹å‚æ•°ç²¾åº¦:

            for name, param in model.named_parameters():
                    print(name, param.dtype)


    - **å…³é”®è¯**: AMP(è‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰çš„å…³é”®è¯æœ‰ä¸¤ä¸ª: è‡ªåŠ¨ï¼Œæ··åˆç²¾åº¦ã€‚

        - **è‡ªåŠ¨**: Tensorçš„dtypeç±»å‹ä¼šè‡ªåŠ¨å˜åŒ–ï¼Œæ¡†æ¶æŒ‰éœ€è‡ªåŠ¨è°ƒæ•´tensorçš„dtypeï¼Œå½“ç„¶æœ‰äº›åœ°æ–¹è¿˜éœ€æ‰‹åŠ¨å¹²é¢„ï¼›
        - **æ··åˆç²¾åº¦**: é‡‡ç”¨ä¸æ­¢ä¸€ç§ç²¾åº¦çš„Tensorï¼Œtorch.FloatTensorå’Œtorch.HalfTensorã€‚
    - **é€‚ç”¨ç¡¬ä»¶**: Tensor Coreæ˜¯ä¸€ç§çŸ©é˜µä¹˜ç´¯åŠ çš„è®¡ç®—å•å…ƒï¼Œæ¯ä¸ªtensor coreæ—¶é’ˆæ‰§è¡Œ64ä¸ªæµ®ç‚¹æ··åˆç²¾åº¦æ“ä½œï¼ˆFP16çŸ©é˜µç›¸ä¹˜å’ŒFP32ç´¯åŠ ï¼‰ã€‚è‹±ä¼Ÿè¾¾å®£ç§°ä½¿ç”¨Tensor Coreè¿›è¡ŒçŸ©é˜µè¿ç®—å¯ä»¥è½»æ˜“çš„æé€Ÿï¼ŒåŒæ—¶é™ä½ä¸€åŠçš„æ˜¾å­˜è®¿é—®å’Œå­˜å‚¨ã€‚å› æ­¤ï¼Œåœ¨PyTorchä¸­ï¼Œå½“æåˆ°è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ŒæŒ‡çš„å°±æ˜¯åœ¨NVIDIAæ”¯æŒTensor Coreçš„CUDAè®¾å¤‡ä¸Šä½¿ç”¨ã€‚

    - **åŸç†**: å‰é¢å·²ä»‹ç»ï¼ŒAMPå…¶å®å°±æ˜¯Float32ä¸Float16çš„æ··åˆï¼Œé‚£ä¸ºä»€ä¹ˆä¸å•ç‹¬ä½¿ç”¨Float32æˆ–Float16ï¼Œè€Œæ˜¯ä¸¤ç§ç±»å‹æ··åˆå‘¢ï¼ŸåŸå› æ˜¯: åœ¨æŸäº›æƒ…å†µä¸‹Float32æœ‰ä¼˜åŠ¿ï¼Œè€Œåœ¨å¦å¤–ä¸€äº›æƒ…å†µä¸‹Float16æœ‰ä¼˜åŠ¿ã€‚è€Œç›¸æ¯”äºä¹‹å‰çš„é»˜è®¤çš„torch.FloatTensorï¼Œtorch.HalfTensorçš„åŠ£åŠ¿ä¸å¯å¿½è§†ã€‚è¿™é‡Œå…ˆä»‹ç»ä¸‹FP16ä¼˜åŠ£åŠ¿ã€‚
        â€ƒâ€ƒtorch.HalfTensorçš„ä¼˜åŠ¿å°±æ˜¯å­˜å‚¨å°ã€è®¡ç®—å¿«ã€æ›´å¥½çš„åˆ©ç”¨CUDAè®¾å¤‡çš„Tensor Coreã€‚å› æ­¤è®­ç»ƒçš„æ—¶å€™å¯ä»¥å‡å°‘æ˜¾å­˜çš„å ç”¨ï¼ˆå¯ä»¥å¢åŠ batchsizeäº†ï¼‰ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

        - **å‡å°‘æ˜¾å­˜å ç”¨**: ç°åœ¨æ¨¡å‹è¶Šæ¥è¶Šå¤§ï¼Œå½“ä½ ä½¿ç”¨Bertè¿™ä¸€ç±»çš„é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¾€å¾€æ¨¡å‹åŠæ¨¡å‹è®¡ç®—å°±å å»æ˜¾å­˜çš„å¤§åŠï¼Œå½“æƒ³è¦ä½¿ç”¨æ›´å¤§çš„batchsizeçš„æ—¶å€™ä¼šæ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚ç”±äºFP16çš„å†…å­˜å ç”¨åªæœ‰FP32çš„ä¸€åŠï¼Œè‡ªç„¶åœ°å°±å¯ä»¥å¸®åŠ©è®­ç»ƒè¿‡ç¨‹èŠ‚çœä¸€åŠçš„æ˜¾å­˜ç©ºé—´ï¼Œå¯ä»¥å¢åŠ batchsizeäº†ï¼›
        - **åŠ å¿«è®­ç»ƒå’Œæ¨æ–­çš„è®¡ç®—**: ä¸æ™®é€šçš„ç©ºé—´ä¸æ—¶é—´Trade-offçš„åŠ é€Ÿæ–¹æ³•ä¸åŒï¼ŒFP16é™¤äº†èƒ½èŠ‚çº¦å†…å­˜ï¼Œè¿˜èƒ½åŒæ—¶èŠ‚çœæ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚åœ¨å¤§éƒ¨åˆ†çš„æµ‹è¯•ä¸­ï¼ŒåŸºäºFP16çš„åŠ é€Ÿæ–¹æ³•èƒ½å¤Ÿç»™æ¨¡å‹è®­ç»ƒèƒ½å¸¦æ¥å¤šä¸€å€çš„åŠ é€Ÿä½“éªŒï¼›
        - **å¼ é‡æ ¸å¿ƒçš„æ™®åŠï¼ˆNVIDIA Tensor Coreï¼‰**: ä½ç²¾åº¦è®¡ç®—æ˜¯æœªæ¥æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªé‡è¦è¶‹åŠ¿ã€‚
            Â 

        â€ƒâ€ƒtorch.HalfTensorçš„åŠ£åŠ¿å°±æ˜¯: æº¢å‡ºé”™è¯¯ï¼Œæ•°å€¼èŒƒå›´å°ï¼ˆæ›´å®¹æ˜“Overflow / Underflowï¼‰ï¼›èˆå…¥è¯¯å·®ï¼ˆRounding Errorï¼‰ï¼Œå¯¼è‡´ä¸€äº›å¾®å°çš„æ¢¯åº¦ä¿¡æ¯è¾¾ä¸åˆ°16bitç²¾åº¦çš„æœ€ä½åˆ†è¾¨ç‡ï¼Œä»è€Œä¸¢å¤±ã€‚

        - **æº¢å‡ºé”™è¯¯**: ç”±äºFP16çš„åŠ¨æ€èŒƒå›´æ¯”FP32ä½çš„ç‹­çª„å¾ˆå¤šï¼Œå› æ­¤ï¼Œåœ¨è®¡ç®—è¿‡ç¨‹ä¸­å¾ˆå®¹æ˜“å‡ºç°ä¸Šæº¢å‡ºï¼ˆOverflowï¼‰å’Œä¸‹æº¢å‡ºï¼ˆUnderflowï¼‰ï¼Œæº¢å‡ºä¹‹åå°±ä¼šå‡ºç°"NaN"çš„é—®é¢˜ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œç”±äºæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦å¾€å¾€è¦æ¯”æƒé‡æ¢¯åº¦å°ï¼Œæ›´æ˜“å‡ºç°ä¸‹æº¢å‡ºçš„æƒ…å†µã€‚åœ¨è®­ç»ƒåæœŸï¼Œä¾‹å¦‚æ¿€æ´»å‡½æ•°çš„æ¢¯åº¦ä¼šéå¸¸å°ï¼Œ ç”šè‡³åœ¨æ¢¯åº¦ä¹˜ä»¥å­¦ä¹ ç‡åï¼Œå€¼ä¼šæ›´åŠ å°ï¼›
        - **èˆå…¥è¯¯å·®**: æŒ‡çš„æ˜¯å½“æ¢¯åº¦è¿‡å°æ—¶ï¼Œå°äºå½“å‰åŒºé—´å†…çš„æœ€å°é—´éš”æ—¶ï¼Œè¯¥æ¬¡æ¢¯åº¦æ›´æ–°å¯èƒ½ä¼šå¤±è´¥ã€‚å…·ä½“çš„ç»†èŠ‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç”±äºæ›´æ–°çš„æ¢¯åº¦å€¼è¶…å‡ºäº†FP16èƒ½å¤Ÿè¡¨ç¤ºçš„æœ€å°å€¼çš„èŒƒå›´ï¼Œå› æ­¤è¯¥æ•°å€¼å°†ä¼šè¢«èˆå¼ƒï¼Œè¿™ä¸ªæƒé‡å°†ä¸è¿›è¡Œæ›´æ–°ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/023a3d0259b2403ebda58d4ef481c261.png#pic_center)
            Â 

        â€ƒâ€ƒç»¼ä¸Šå¯çŸ¥ï¼Œtorch.HalfTensorå­˜åœ¨ä¸€å®šçš„åŠ£åŠ¿ã€‚å› æ­¤éœ€è¦é‡‡å–é€‚å½“çš„æ–¹æ³•ï¼Œä¸€æ–¹é¢å¯ä»¥åˆ©ç”¨torch.HalfTensorçš„ä¼˜åŠ¿ï¼Œå¦ä¸€æ–¹é¢éœ€è¦é¿å…torch.HalfTensorçš„åŠ£åŠ¿ã€‚AMPå³æ˜¯æœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆã€‚

        - **æ··åˆç²¾åº¦è®­ç»ƒ**: åœ¨æŸäº›æ¨¡å‹ä¸­ï¼ŒFP16çŸ©é˜µä¹˜æ³•çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åˆ©ç”¨FP32æ¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•ä¸­é—´çš„ç´¯åŠ (accumulated)ï¼Œç„¶åå†å°†FP32çš„å€¼è½¬åŒ–ä¸ºFP16è¿›è¡Œå­˜å‚¨ã€‚ æ¢å¥ä¸å¤ªä¸¥è°¨çš„è¯æ¥è¯´ï¼Œä¹Ÿå°±æ˜¯åœ¨å†…å­˜ä¸­ç”¨FP16åšå‚¨å­˜å’Œä¹˜æ³•ä»è€ŒåŠ é€Ÿè®¡ç®—ï¼Œè€Œç”¨FP32åšç´¯åŠ é¿å…èˆå…¥è¯¯å·®ã€‚æ··åˆç²¾åº¦è®­ç»ƒçš„ç­–ç•¥æœ‰æ•ˆåœ°ç¼“è§£äº†èˆå…¥è¯¯å·®çš„é—®é¢˜ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/7e3880e734634b529b349774d713a3dc.png#pic_center)
            â€ƒâ€ƒåœ¨è¿™é‡Œä¹Ÿå°±å¼•å‡ºäº†ï¼Œä¸ºä»€ä¹ˆç½‘ä¸Šå¤§å®¶éƒ½è¯´ï¼Œåªæœ‰Nvidia Voltaç»“æ„çš„æ‹¥æœ‰Tensor Coreçš„CPU(ä¾‹å¦‚V100)ï¼Œæ‰èƒ½åˆ©ç”¨FP16æ··åˆç²¾åº¦æ¥è¿›è¡ŒåŠ é€Ÿã€‚ é‚£æ˜¯å› ä¸ºTensor Coreèƒ½å¤Ÿä¿è¯FP16çš„çŸ©é˜µç›¸ä¹˜ï¼Œåˆ©ç”¨FP16 or FP32æ¥è¿›è¡Œç´¯åŠ ã€‚åœ¨ç´¯åŠ é˜¶æ®µèƒ½å¤Ÿä½¿ç”¨FP32å¤§å¹…å‡å°‘æ··åˆç²¾åº¦è®­ç»ƒçš„ç²¾åº¦æŸå¤±ã€‚è€Œå…¶ä»–çš„GPUåªèƒ½æ”¯æŒFP16çš„multiply-add operationã€‚è¿™é‡Œç›´æ¥è´´å‡ºåŸæ–‡å¥å­:

            > Whereas previous GPUs supported only FP16 multiply-add operation, NVIDIA Volta GPUs introduce Tensor Cores that multiply FP16 input matrices andaccumulate products into either FP16 or FP32 outputs

        - **FP32æƒé‡å¤‡ä»½**: è¿™ç§æ–¹æ³•ä¸»è¦æ˜¯ç”¨äºè§£å†³èˆå…¥è¯¯å·®çš„é—®é¢˜ã€‚å…¶ä¸»è¦æ€è·¯ï¼Œå¯ä»¥æ¦‚æ‹¬ä¸º: weightsï¼Œactivationsï¼Œgradientsç­‰æ•°æ®åœ¨è®­ç»ƒä¸­éƒ½åˆ©ç”¨FP16æ¥å­˜å‚¨ï¼ŒåŒæ—¶æ‹·è´ä¸€ä»½FP32çš„weightsï¼Œç”¨äºæ›´æ–°ã€‚å¦‚ä¸‹å›¾: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/00e19cb3f86b42b2afaa6a0c6c4357b9.jpeg#pic_center)
            Â 
            â€ƒâ€ƒå¯ä»¥çœ‹åˆ°ï¼Œå…¶ä»–æ‰€æœ‰å€¼ï¼ˆweightsï¼Œactivationsï¼Œ gradientsï¼‰å‡ä½¿ç”¨FP16æ¥å­˜å‚¨ï¼Œè€Œå”¯ç‹¬æƒé‡weightséœ€è¦ç”¨FP32çš„æ ¼å¼é¢å¤–å¤‡ä»½ä¸€æ¬¡ã€‚ è¿™ä¸»è¦æ˜¯å› ä¸ºï¼Œåœ¨æ›´æ–°æƒé‡çš„æ—¶å€™ï¼Œå¾€å¾€å…¬å¼: **æƒé‡ = æ—§æƒé‡ + lr \* æ¢¯åº¦**ï¼Œè€Œåœ¨æ·±åº¦æ¨¡å‹ä¸­ï¼Œ**lr \* æ¢¯åº¦**è¿™ä¸ªå€¼å¾€å¾€æ˜¯éå¸¸å°çš„ï¼Œå¦‚æœåˆ©ç”¨FP16æ¥è¿›è¡Œç›¸åŠ çš„è¯ï¼Œ åˆ™å¾ˆå¯èƒ½ä¼šå‡ºç°ä¸Šé¢æ‰€è¯´çš„ã€èˆå…¥è¯¯å·®ã€çš„è¿™ä¸ªé—®é¢˜ï¼Œå¯¼è‡´æ›´æ–°æ— æ•ˆã€‚å› æ­¤ä¸Šå›¾ä¸­ï¼Œé€šè¿‡å°†weightsæ‹·è´æˆFP32æ ¼å¼ï¼Œå¹¶ä¸”ç¡®ä¿æ•´ä¸ªæ›´æ–°ï¼ˆupdateï¼‰è¿‡ç¨‹æ˜¯åœ¨FP32æ ¼å¼ä¸‹è¿›è¡Œçš„ï¼Œå¦‚ä¸‹æ‰€ç¤º:
            w e i g h t 32 = w e i g h t 32 + Î· â‹… g r a d i e n t 32 weight\_{32}=weight\_{32}+\\eta \\cdot gradient\_{32} weight32â€‹\=weight32â€‹+Î·â‹…gradient32â€‹
            â€ƒâ€ƒçœ‹åˆ°è¿™é‡Œï¼Œå¯èƒ½æœ‰äººæå‡ºè¿™ç§FP32æ‹·è´weightsçš„æ–¹å¼ï¼Œé‚£å²‚ä¸æ˜¯ä½¿å¾—å†…å­˜å ç”¨åè€Œæ›´é«˜äº†å‘¢ï¼Ÿæ˜¯çš„ï¼ŒFP32é¢å¤–æ‹·è´ä¸€ä»½weightsçš„ç¡®æ–°å¢åŠ äº†è®­ç»ƒæ—¶å€™å­˜å‚¨çš„å ç”¨ã€‚ ä½†æ˜¯å®é™…ä¸Šï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå†…å­˜ä¸­å æ®å¤§éƒ¨åˆ†çš„åŸºæœ¬éƒ½æ˜¯activationsçš„å€¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç‰¹åˆ«æ˜¯åœ¨batchsizeå¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œ activationsæ›´æ˜¯ç‰¹åˆ«å æ®ç©ºé—´ã€‚ ä¿å­˜activiationsä¸»è¦æ˜¯ä¸ºäº†åœ¨backwardçš„æ—¶å€™è¿›è¡Œè®¡ç®—ã€‚å› æ­¤ï¼Œåªè¦activationsçš„å€¼åŸºæœ¬éƒ½æ˜¯ä½¿ç”¨FP16æ¥è¿›è¡Œå­˜å‚¨çš„è¯ï¼Œåˆ™æœ€ç»ˆæ¨¡å‹ä¸FP32ç›¸æ¯”èµ·æ¥ï¼Œ å†…å­˜å ç”¨ä¹ŸåŸºæœ¬èƒ½å¤Ÿå‡åŠã€‚ ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/09703dfad812470bbc47fc5a3f9989ac.png#pic_center)

        - **æŸå¤±æ”¾å¤§ï¼ˆLoss Scaleï¼‰**: å³ä½¿é‡‡ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¿˜æ˜¯å­˜åœ¨æ— æ³•æ”¶æ•›çš„æƒ…å†µï¼ŒåŸå› æ˜¯æ¿€æ´»æ¢¯åº¦çš„å€¼å¤ªå°ï¼Œé€ æˆäº†ä¸‹æº¢å‡ºï¼ˆUnderflowï¼‰ã€‚Loss Scaleä¸»è¦æ˜¯ä¸ºäº†è§£å†³FP16 underflowçš„é—®é¢˜ã€‚åˆšæ‰æåˆ°ï¼Œè®­ç»ƒåˆ°äº†åæœŸï¼Œæ¢¯åº¦ï¼ˆç‰¹åˆ«æ˜¯æ¿€æ´»å‡½æ•°å¹³æ»‘æ®µçš„æ¢¯åº¦ï¼‰ä¼šç‰¹åˆ«å°ï¼Œå¦‚æœç”¨FP16æ¥è¡¨ç¤ºï¼Œåˆ™è¿™äº›æ¢¯åº¦éƒ½ä¼šå˜æˆ0ï¼Œå› æ­¤å¯¼è‡´FP16è¡¨ç¤ºå®¹æ˜“äº§ç”Ÿunderflowç°è±¡ã€‚
            â€ƒâ€ƒä¸ºäº†è§£å†³æ¢¯åº¦è¿‡å°çš„é—®é¢˜ï¼Œè®ºæ–‡ä¸­å¯¹è®¡ç®—å‡ºæ¥çš„losså€¼è¿›è¡Œscaleï¼Œç”±äºé“¾å¼æ³•åˆ™çš„å­˜åœ¨ï¼Œlossä¸Šçš„scaleä¼šä½œç”¨åœ¨æ¢¯åº¦ä¸Šã€‚è¿™æ ·æ¯”èµ·å¯¹æ¯ä¸ªæ¢¯åº¦è¿›è¡Œscaleæ›´åŠ åˆ’ç®—ã€‚ scaledè¿‡åçš„æ¢¯åº¦ï¼Œå°±ä¼šå¹³ç§»åˆ°FP16æœ‰æ•ˆçš„å±•ç¤ºèŒƒå›´å†…ã€‚
            â€ƒâ€ƒè¿™æ ·ï¼Œscaled-gradientå°±å¯ä»¥ä¸€ç›´ä½¿ç”¨FP16è¿›è¡Œå­˜å‚¨äº†ã€‚åªæœ‰åœ¨è¿›è¡Œæ›´æ–°çš„æ—¶å€™ï¼Œæ‰ä¼šå°†scaled-gradientè½¬åŒ–ä¸ºFP32ï¼ŒåŒæ—¶å°†scaleæŠ¹å»ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œ scaleå¹¶éå¯¹äºæ‰€æœ‰ç½‘ç»œè€Œè¨€éƒ½æ˜¯å¿…é¡»çš„ã€‚è®ºæ–‡ç»™å‡ºscaleçš„å–å€¼åœ¨8 - 32kä¹‹é—´çš†å¯ã€‚
            â€ƒâ€ƒPytorchå¯ä»¥é€šè¿‡ä½¿ç”¨torch.cuda.amp.GradScalerï¼Œé€šè¿‡æ”¾å¤§lossçš„å€¼æ¥é˜²æ­¢æ¢¯åº¦çš„underflowï¼ˆåªåœ¨BPæ—¶ä¼ é€’æ¢¯åº¦ä¿¡æ¯ä½¿ç”¨ï¼ŒçœŸæ­£æ›´æ–°æƒé‡æ—¶è¿˜æ˜¯è¦æŠŠæ”¾å¤§çš„æ¢¯åº¦å†unscaleå›å»ï¼‰
            â€ƒâ€ƒç»¼ä¸Šï¼ŒæŸå¤±æ”¾å¤§çš„æ€è·¯æ˜¯:

            - åå‘ä¼ æ’­å‰ï¼Œå°†æŸå¤±å˜åŒ–æ‰‹åŠ¨å¢å¤§ 2 k 2^{k} 2kå€ï¼Œå› æ­¤åå‘ä¼ æ’­æ—¶å¾—åˆ°çš„ä¸­é—´å˜é‡ï¼ˆæ¿€æ´»å‡½æ•°æ¢¯åº¦ï¼‰åˆ™ä¸ä¼šæº¢å‡ºï¼›
            - åå‘ä¼ æ’­åï¼Œå°†æƒé‡æ¢¯åº¦ç¼©å° 2 k 2^{k} 2kå€ï¼Œæ¢å¤æ­£å¸¸å€¼ã€‚
    - **ä½¿ç”¨ç¤ºä¾‹**: [åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ä»‹ç»](https://github.com/DankoZhang/Ner/blob/main/README.md)

- **Accelerate**: DPç®€å•ä¸”å®¹æ˜“è°ƒè¯•ï¼ŒDDPå¿«ä½†æ˜¯éš¾debugï¼Œä¸”ä»£ç æ”¹åŠ¨ç¨å¤§ï¼Œä¾‹å¦‚è¦å¼€å¯åç«¯é€šè®¯ï¼Œæ•°æ®samplerçš„æ–¹å¼ä¹Ÿè¦æ”¹ã€‚æœ‰æ²¡æœ‰å·¥å…·ä¸ä»…ä»£ç æ”¹åŠ¨é‡å°‘ï¼Œæ–¹ä¾¿debugï¼Œè€Œä¸”è®­ç»ƒèµ·æ¥å¿«å‘¢ï¼Ÿå…¶ä¸­ä¸€ä¸ªç­”æ¡ˆå°±æ˜¯Accelerateåº“ï¼ŒAccelerateåº“æ˜¯å¤§åé¼é¼çš„huggingfaceå…¬å¸åœ¨2021å¹´åˆæ¨å‡ºçš„PyTorchåˆ†å¸ƒå¼è®­ç»ƒå·¥å…·åº“ï¼Œå®˜æ–¹é“¾æ¥æ˜¯ [Accelerate](https://github.com/huggingface/accelerate)ã€‚å¦å¤–æœ‰ç¯‡æ¯”è¾ƒå¥½çš„è¯´æ˜æ–‡æ¡£æ˜¯[Accelerate](https://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/huggingface_transformer/chapters/7_accelerate.html)ã€‚
    - **ç®€ä»‹**: Accelerateæ˜¯huggingfaceå¼€æºçš„ä¸€ä¸ªæ–¹ä¾¿å°†PyTorchæ¨¡å‹è¿ç§»åˆ°**multi-GPUs/TPU/FP16**æ¨¡å¼ä¸‹è®­ç»ƒçš„å°å·§å·¥å…·ã€‚å’Œæ ‡å‡†çš„PyTorchæ–¹æ³•ç›¸æ¯”ï¼Œä½¿ç”¨accelerateè¿›è¡Œ**multi-GPUs/TPU/FP16**æ¨¡å‹è®­ç»ƒå˜å¾—éå¸¸ç®€å•(åªéœ€è¦åœ¨æ ‡å‡†çš„PyTorchè®­ç»ƒä»£ç ä¸­æ”¹åŠ¨å‡ è¡Œä»£ç å°±å¯ä»¥é€‚åº”**multi-GPUs/TPU/FP16**ç­‰ä¸åŒçš„è®­ç»ƒç¯å¢ƒ)ï¼Œè€Œä¸”é€Ÿåº¦ä¸åŸç”ŸPyTorchç›¸æ¯”ï¼Œéå¸¸ä¹‹å¿«ã€‚
    - **ä½¿ç”¨ç¤ºä¾‹**: [åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ä»‹ç»](https://github.com/DankoZhang/Ner/blob/main/README.md)
        - **ä½¿ç”¨æŠ€å·§**: [HuggingFaceâ€”â€”Accelerateçš„ä½¿ç”¨](https://blog.csdn.net/c___c18/article/details/127616417)
            - accelerate config: é€šè¿‡åœ¨ç»ˆç«¯ä¸­å›ç­”ä¸€ç³»åˆ—é—®é¢˜ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼›

                    accelerate config --config_file ./accelerate_config.yaml


            - accelerate env: éªŒè¯é…ç½®æ–‡ä»¶çš„åˆæ³•æ€§ï¼›

                    accelerate env --config_file ./accelerate_config.yaml


            - accelerate launch: è¿è¡Œè‡ªå·±çš„pythonæ–‡ä»¶ï¼›

                    accelerate launch --config_file ./conf/accelerate_config.yaml train_accelerate.py


            - accelerate test: è¿è¡Œaccelerateé»˜è®¤çš„ç¥ç»ç½‘ç»œæ¨¡å‹æ¥æµ‹è¯•ç¯å¢ƒæ˜¯å¦å¯ä»¥ã€‚

                    accelerate test --config_file ./accelerate_config.yaml


- **Deepspeed**: Deepspeedæ˜¯Microsoftæä¾›çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼Œé€‚ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒï¼Œå®˜æ–¹é“¾æ¥æ˜¯[DeepSpeed](https://github.com/microsoft/DeepSpeed)ã€‚è¿™é‡Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»ä¸‹Deepspeedçš„åˆ†å¸ƒå¼åŸç†ï¼Œå…·ä½“çš„ä½¿ç”¨ç¤ºä¾‹å¯å‚è€ƒå‰æ–‡çš„**PEFTå®è·µ**éƒ¨åˆ†ã€‚
    - **ç®€ä»‹**: DeepSpeedæ˜¯ä¸€ä¸ªç”±å¾®è½¯å¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚DeepSpeedçš„æ ¸å¿ƒæŠ€æœ¯æ˜¯ZeROï¼ˆZero Redundancy Optimizerï¼Œé›¶å†—ä½™ä¼˜åŒ–ï¼‰ï¼Œé€šè¿‡ZeROæŠ€æœ¯å®ç°äº†æ•°æ®å¹¶è¡Œã€‚å¦å¤–ï¼ŒDeepSpeedä¹Ÿæ”¯æŒæ¨¡å‹å¹¶è¡Œï¼ˆå€Ÿç”¨è‹±ä¼Ÿè¾¾çš„Megatron-LMæ¥ä¸ºåŸºäºTransformerçš„è¯­è¨€æ¨¡å‹æä¾›å¼ é‡å¹¶è¡ŒåŠŸèƒ½ï¼Œå¼ é‡å¹¶è¡Œå‚è€ƒ[Megatron-LM](https://zhuanlan.zhihu.com/p/622212228)ï¼›é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¥å®ç°æµæ°´çº¿å¹¶è¡Œï¼Œæµæ°´çº¿å¹¶è¡Œå‚è€ƒ[Pipeline Parallelism](https://zhuanlan.zhihu.com/p/613196255)ï¼‰ã€‚

    - **åŸç†**: å…³äºæ¨¡å‹å¹¶è¡Œéƒ¨åˆ†å…·ä½“åŸç†ï¼Œå¤§å®¶è‡ªè¡ŒæŸ¥é˜…ç›¸å…³æ–‡æ¡£ï¼Œè¿™é‡Œä¸äºˆè¿‡å¤šä»‹ç»ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç€é‡ä»‹ç»ä¸‹DeepSpeedçš„æ ¸å¿ƒæŠ€æœ¯ZeRO: ZeRO-1ã€ZeRO-2ã€ZeRO-3ã€ZeRO-Offloadä¸ZeRO-Infinityï¼Œå…·ä½“å‚è€ƒ[ã€ŠZeRO: Memory Optimizations Toward Training Trillion Parameter Modelsã€‹](https://arxiv.org/pdf/1910.02054.pdf)ã€[ã€ŠZeRO-Offload: Democratizing Billion-Scale Model Trainingã€‹](https://arxiv.org/pdf/2101.06840.pdf)ã€[ã€ŠZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learningã€‹](https://arxiv.org/pdf/2104.07857.pdf)ã€[DeepSpeed ZeRO](https://zhuanlan.zhihu.com/p/618865052)ã€‚

        - **å­˜å‚¨åˆ†ç±»**: é¦–å…ˆï¼Œå¤§æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼ŒGPUéœ€è¦å­˜å‚¨çš„å†…å®¹åŒ…æ‹¬ä¸¤å¤§å—: Model Stateså’ŒResidual Statesã€‚

            - **Model State**: æŒ‡å’Œæ¨¡å‹æœ¬èº«æ¯æ¯ç›¸å…³çš„ï¼Œå¿…é¡»å­˜å‚¨çš„å†…å®¹ï¼Œå…·ä½“åŒ…æ‹¬:
                - **optimizer states**: Adamä¼˜åŒ–ç®—æ³•ä¸­çš„momentumå’Œvarianceï¼›
                - **gradients**: æ¨¡å‹æ¢¯åº¦Gï¼›
                - **parameters**: æ¨¡å‹å‚æ•°Wã€‚
            - **Residual States**: æŒ‡å¹¶éæ¨¡å‹å¿…é¡»çš„ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé¢å¤–äº§ç”Ÿçš„å†…å®¹ï¼Œå…·ä½“åŒ…æ‹¬:
                - **activations**: æ¿€æ´»å€¼ã€‚åœ¨backwardè¿‡ç¨‹ä¸­ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦æ—¶ä¼šç”¨åˆ°ã€‚æœ‰äº†å®ƒè®¡ç®—æ¢¯åº¦ä¼šæ›´å¿«ï¼Œä½†å®ƒä¸æ˜¯å¿…é¡»å­˜å‚¨çš„ï¼Œå› ä¸ºå¯ä»¥é€šè¿‡é‡æ–°åšforwardæ¥è®¡ç®—ç®—å®ƒã€‚å®é™…ä¸Šï¼Œactivationså°±æ˜¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸­é—´å€¼ï¼Œä¸¾ä¸ªä¾‹å­:  x 2 = w 1 âˆ— x ï¼Œ y = w 2 âˆ— x 2 x\_{2}=w\_{1} \* xï¼Œy=w\_{2} \* x\_{2} x2â€‹\=w1â€‹âˆ—xï¼Œy\=w2â€‹âˆ—x2â€‹ï¼Œå‡è®¾ä¸Šé¢çš„å‚æ•°ï¼ˆ w 1 w\_{1} w1â€‹ï¼Œ w 2 w\_{2} w2â€‹ï¼‰å’Œè¾“å…¥ x x xéƒ½æ˜¯æ ‡é‡ï¼Œåœ¨åå‘ä¼ æ’­é˜¶æ®µè¦è®¡ç®— y y yå¯¹ w 2 w\_{2} w2â€‹çš„æ¢¯åº¦ï¼Œå¾ˆæ˜æ˜¾æ˜¯ x 2 x\_{2} x2â€‹ï¼Œè¿™ä¸ª x 2 x\_{2} x2â€‹å°±å±äºactivationsï¼Œä¹Ÿå°±æ˜¯åœ¨å‰å‘é˜¶æ®µéœ€è¦ä¿å­˜çš„ä¸€ä¸ªä¸­é—´ç»“æœã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ä¿å­˜ï¼Œå½“åå‘é˜¶æ®µéœ€è¦ç”¨åˆ° x 2 x\_{2} x2â€‹æ—¶å†é‡æ–°é€šè¿‡forwardè¿‡ç¨‹ä¸´æ—¶è®¡ç®—ï¼›
                - **temporary buffers**: ä¸´æ—¶å­˜å‚¨ã€‚ä¾‹å¦‚æŠŠæ¢¯åº¦å‘é€åˆ°æŸå—GPUä¸ŠåšåŠ æ€»èšåˆæ—¶äº§ç”Ÿçš„å­˜å‚¨ã€‚
                - **unusable fragment memory**: ç¢ç‰‡åŒ–çš„å­˜å‚¨ç©ºé—´ã€‚è™½ç„¶æ€»å­˜å‚¨ç©ºé—´æ˜¯å¤Ÿçš„ï¼Œä½†æ˜¯å¦‚æœå–ä¸åˆ°è¿ç»­çš„å­˜å‚¨ç©ºé—´ï¼Œç›¸å…³çš„è¯·æ±‚ä¹Ÿä¼šè¢«failæ‰ã€‚å¯¹è¿™ç±»ç©ºé—´æµªè´¹å¯ä»¥é€šè¿‡å†…å­˜æ•´ç†æ¥è§£å†³ã€‚
        - **å­˜å‚¨å¤§å°**: äº†è§£äº†å­˜å‚¨åˆ†ç±»ï¼Œæ¥ä¸‹æ¥äº†è§£ä¸‹æ¯ç§å­˜å‚¨å ç”¨çš„å†…å­˜å¤§å°ã€‚é¦–å…ˆæˆ‘ä»¬å›å¿†ä¸‹æ··åˆç²¾åº¦è®­ç»ƒçš„è¿‡ç¨‹ï¼Œå¤§è‡´å¦‚ä¸‹å›¾æ‰€ç¤º: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/af39b25e42a945c2bb55d0e6c1cabc1d.png#pic_center)

            - **æ··åˆç²¾åº¦è®­ç»ƒ**: ç®€å•æ¥è¯´ï¼Œæ··åˆç²¾åº¦è®­ç»ƒçš„æµç¨‹æœ‰å¦‚ä¸‹å‡ æ­¥ã€‚
                - å­˜å‚¨ä¸€ä»½FP32çš„parameterï¼Œmomentumå’Œvarianceï¼ˆç»Ÿç§°model statesï¼‰ï¼›
                - åœ¨forwardå¼€å§‹ä¹‹å‰ï¼Œé¢å¤–å¼€è¾Ÿä¸€å—å­˜å‚¨ç©ºé—´ï¼Œå°†FP32çš„parameterå‡åŠåˆ°FP16 parameterï¼›
                - æ­£å¸¸åšforwardå’Œbackwardï¼Œåœ¨æ­¤ä¹‹é—´äº§ç”Ÿçš„activationså’Œgradientsï¼Œéƒ½ç”¨FP16è¿›è¡Œå­˜å‚¨ï¼›
                - å°†FP16çš„gradientsè½¬æ¢ä¸ºFP32çš„gradientsï¼Œç”¨FP32çš„gradientså»æ›´æ–°FP32ä¸‹çš„model statesã€‚ å½“æ¨¡å‹æ”¶æ•›åï¼ŒFP32çš„parameterå°±æ˜¯æœ€ç»ˆçš„å‚æ•°è¾“å‡ºã€‚
                    Â 

            â€ƒâ€ƒç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ¥è®¡ç®—æ¨¡å‹åœ¨è®­ç»ƒæ—¶éœ€è¦çš„å­˜å‚¨å¤§å°äº†ï¼Œå‡è®¾æ¨¡å‹çš„å‚æ•°Wå¤§å°æ˜¯ Î¦ \\Phi Î¦ ï¼ˆæ ¹æ®å‚æ•°é‡é¢„ä¼°æ˜¾å­˜å ç”¨çš„æ–¹æ³•å‚è§[å‚æ•°é‡ä¼°è®¡ä¸æ˜¾å­˜ä¼°è®¡](http://mingchao.wang/rJXF8VxX/)ï¼Œè¿™é‡Œç®€å•æä¸‹ï¼Œæ¯”å¦‚6Bçš„æ¨¡å‹ï¼Œä½¿ç”¨FP16æ–¹å¼è½½å…¥æ˜¾å­˜ï¼Œæ‰€éœ€æ˜¾å­˜å¤§å°: 6B âˆ— \\ast âˆ— 2 = 12Gï¼‰ï¼Œåˆ™è®­ç»ƒæ—¶å¯¹åº”çš„å­˜å‚¨å¦‚ä¸‹:
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/647ac6be79b741adb9025bd9b6a964cc.jpeg#pic_center)
            â€ƒâ€ƒå› ä¸ºé‡‡ç”¨äº†Adamä¼˜åŒ–ï¼Œæ‰€ä»¥æ‰ä¼šå‡ºç°momentumå’Œvarianceï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥é€‰æ‹©åˆ«çš„ä¼˜åŒ–åŠæ³•ï¼Œè¿™é‡Œä¸ºäº†é€šç”¨ï¼Œæ¨¡å‹å¿…å­˜çš„æ•°æ®å¤§å°ä¸º K Î¦ K\\Phi KÎ¦ï¼Œå› æ­¤æ€»çš„å­˜å‚¨å¤§å°ä¸º ï¼ˆ 2 + 2 + K ï¼‰ Î¦ ï¼ˆ2+2+Kï¼‰\\Phi ï¼ˆ2+2+Kï¼‰Î¦ã€‚å¦å¤–ï¼Œè¿™é‡Œæš‚ä¸å°†activationsçº³å…¥ç»Ÿè®¡èŒƒå›´ï¼ŒåŸå› æ˜¯:

            - activationsä¸ä»…ä¸æ¨¡å‹å‚æ•°ç›¸å…³ï¼Œè¿˜ä¸batchsizeç›¸å…³ï¼›
            - activationsçš„å­˜å‚¨ä¸æ˜¯å¿…é¡»çš„ã€‚å‰æ–‡å·²ç»æåˆ°ï¼Œå­˜å‚¨activationsåªæ˜¯ä¸ºäº†åœ¨ç”¨é“¾å¼æ³•åˆ™åšbackwardçš„è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—æ¢¯åº¦æ›´å¿«ä¸€äº›ã€‚ä½†ä½ æ°¸è¿œå¯ä»¥é€šè¿‡åªä¿ç•™æœ€åˆçš„è¾“å…¥Xï¼Œé‡æ–°åšforwardæ¥å¾—åˆ°æ¯ä¸€å±‚çš„activationsï¼ˆè™½ç„¶å®é™…ä¸­å¹¶ä¸ä¼šè¿™ä¹ˆæç«¯ï¼‰ï¼›
            - å› ä¸ºactivationsçš„è¿™ç§çµæ´»æ€§ï¼Œçº³å…¥å®ƒåä¸æ–¹ä¾¿è¡¡é‡ç³»ç»Ÿæ€§èƒ½éšæ¨¡å‹å¢å¤§çš„çœŸå®å˜åŠ¨æƒ…å†µã€‚å› æ­¤åœ¨è¿™é‡Œä¸è€ƒè™‘å®ƒã€‚
        - ZeRO-DP: äº†è§£äº†å­˜å‚¨ç§ç±»ä»¥åŠå®ƒä»¬æ‰€å çš„å­˜å‚¨å¤§å°ä¹‹åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä»‹ç»ä¸‹Deepspeedæ˜¯å¦‚ä½•ä¼˜åŒ–å­˜å‚¨çš„ã€‚è¿™é‡Œæå‰é€éœ²ä¸‹ï¼ŒZeROä¸‰é˜¶æ®µ: ZeRO-1ã€ZeRO-2ã€ZeRO-3çš„å®è´¨æ˜¯æ•°æ®å¹¶è¡Œï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿç§°ä¹‹ä¸ºZeRO-DPï¼Œåé¢ä¼šä»‹ç»å…·ä½“ç»†èŠ‚ã€‚é¦–å…ˆæˆ‘ä»¬åº”è¯¥æ¸…æ¥šï¼Œåœ¨æ•´ä¸ªè®­ç»ƒä¸­ï¼Œæœ‰å¾ˆå¤šstateså¹¶ä¸ä¼šæ¯æ—¶æ¯åˆ»éƒ½ç”¨åˆ°ï¼Œä¸¾ä¾‹æ¥è¯´ï¼›

            - Adamä¼˜åŒ–ä¸‹çš„optimizer statesåªåœ¨æœ€ç»ˆåšupdateæ—¶æ‰ç”¨åˆ°ï¼›
            - æ•°æ®å¹¶è¡Œä¸­ï¼Œgradientsåªåœ¨æœ€ååšall-reduceå’Œupdateæ—¶æ‰ç”¨åˆ°ï¼›
            - å‚æ•°Wåªåœ¨åšforwardå’Œbackwardçš„é‚£ä¸€åˆ»æ‰ç”¨åˆ°ã€‚
                Â 

            â€ƒâ€ƒè¯¸å¦‚æ­¤ç±»ï¼Œæ‰€ä»¥ï¼ŒZeRO-DPæƒ³äº†ä¸€ä¸ªç®€å•ç²—æš´çš„åŠæ³•: å¦‚æœæ•°æ®ç®—å®Œå³åºŸï¼Œç­‰éœ€è¦çš„æ—¶å€™ï¼Œæˆ‘å†æƒ³åŠæ³•ä»ä¸ªä»€ä¹ˆåœ°æ–¹æ‹¿å›æ¥ï¼Œé‚£ä¸å°±çœäº†ä¸€ç¬”å­˜å‚¨ç©ºé—´å—ï¼Ÿæ²¿ç€è¿™ä¸ªæ€è·¯ï¼Œæˆ‘ä»¬é€ä¸€æ¥çœ‹ZeROæ˜¯å¦‚ä½•é€’è¿›åšå­˜å‚¨ä¼˜åŒ–çš„ã€‚

            - **ZeRO-1**: å³ P o s P\_{os} Posâ€‹ï¼Œä¼˜åŒ–çŠ¶æ€åˆ†å‰²ã€‚é¦–å…ˆï¼Œä»optimizer stateså¼€å§‹ä¼˜åŒ–ã€‚å°†optimizer statesåˆ†æˆè‹¥å¹²ä»½ï¼Œæ¯å—GPUä¸Šå„è‡ªç»´æŠ¤ä¸€ä»½ã€‚è¿™æ ·å°±å‡å°‘äº†ç›¸å½“ä¸€éƒ¨åˆ†çš„æ˜¾å­˜å¼€é”€ã€‚å¦‚ä¸‹å›¾: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/94046d5dac01482180594cef742a0c4a.jpeg#pic_center)
                æ•´ä½“æ•°æ®å¹¶è¡Œçš„æµç¨‹å¦‚ä¸‹:
                - æ¯å—GPUä¸Šå­˜ä¸€ä»½å®Œæ•´çš„å‚æ•°Wã€‚å°†ä¸€ä¸ªbatchçš„æ•°æ®åˆ†æˆ3ä»½ï¼Œæ¯å—GPUå„åƒä¸€ä»½ï¼Œåšå®Œä¸€è½®forwardå’Œbackwardåï¼Œå„å¾—ä¸€ä»½æ¢¯åº¦ï¼›
                - å¯¹æ¢¯åº¦åšä¸€æ¬¡all-reduceï¼Œå¾—åˆ°å®Œæ•´çš„æ¢¯åº¦Gï¼Œäº§ç”Ÿå•å¡é€šè®¯é‡ 2 Î¦ 2\\Phi 2Î¦ã€‚å¯¹äºall-reduceï¼ˆreduce-scatter + all-gatherï¼‰çš„é€šè®¯é‡ï¼Œreduce-scatteræ“ä½œå‘é€å’Œæ¥æ”¶çš„é€šè®¯é‡ä¸º Î¦ \\Phi Î¦ï¼Œall-gatheræ“ä½œå‘é€å’Œæ¥æ”¶çš„é€šè®¯é‡ä¹Ÿä¸º Î¦ \\Phi Î¦ï¼Œå› æ­¤all-reduceçš„é€šè®¯å½•ä¸º 2 Î¦ 2\\Phi 2Î¦ã€‚æ³¨æ„ï¼Œæ­¤å¤„æˆ‘ä»¬ä¸å»æ¢å¯»å•æ¬¡å‘é€å’Œæ¥æ”¶çš„é€šè®¯é‡ä¸ºä»€ä¹ˆæ˜¯ Î¦ \\Phi Î¦ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯è‡ªè¡Œæ¢ç´¢[æ‰‹æŠŠæ‰‹æ¨å¯¼Ring All-reduceçš„æ•°å­¦æ€§è´¨](https://zhuanlan.zhihu.com/p/504957661)ï¼›
                - å¾—åˆ°å®Œæ•´æ¢¯åº¦Gï¼Œå°±å¯ä»¥å¯¹Wåšæ›´æ–°ã€‚æˆ‘ä»¬çŸ¥é“Wçš„æ›´æ–°ç”±optimizer stateså’Œæ¢¯åº¦å…±åŒå†³å®šã€‚ç”±äºæ¯å—GPUä¸Šåªä¿ç®¡éƒ¨åˆ†optimizer statesï¼Œå› æ­¤åªèƒ½å°†ç›¸åº”çš„Wï¼ˆè“è‰²éƒ¨åˆ†ï¼‰è¿›è¡Œæ›´æ–°ã€‚ä¸Šè¿°æ­¥éª¤å¯ä»¥ç”¨ä¸‹å›¾è¡¨ç¤º: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/ed410fbe73ea430cb032a20bdedaf2f6.png#pic_center)
                - æ­¤æ—¶ï¼Œæ¯å—GPUä¸Šéƒ½æœ‰éƒ¨åˆ†Wæ²¡æœ‰å®Œæˆæ›´æ–°ï¼ˆå›¾ä¸­ç™½è‰²éƒ¨åˆ†ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹Wåšä¸€æ¬¡all-gatherï¼Œä»åˆ«çš„GPUä¸ŠæŠŠæ›´æ–°å¥½çš„éƒ¨åˆ†Wå–å›æ¥ã€‚äº§ç”Ÿå•å¡é€šè®¯é‡ Î¦ \\Phi Î¦ã€‚
                    Â 

            â€ƒâ€ƒåšå®Œ P o s P\_{os} Posâ€‹åï¼Œè®¾GPUä¸ªæ•°ä¸º N d N\_{d} Ndâ€‹ï¼Œæ˜¾å­˜å’Œé€šè®¯é‡çš„æƒ…å†µå¦‚ä¸‹:

            å¹¶è¡ŒåŒ–æŠ€æœ¯

            æ˜¾å­˜

            æ˜¾å­˜ï¼ˆGBï¼‰ï¼Œ Î¦ = 7.5 B \\Phi=7.5B Î¦\=7.5Bï¼Œ N d = 64 N\_{d}=64 Ndâ€‹\=64ï¼Œ K = 12 K=12 K\=12

            å•å¡é€šè®¯é‡

            æœ´ç´ DP

            ï¼ˆ2+2+ K K Kï¼‰ Î¦ \\Phi Î¦

            120GB

            2 Î¦ \\Phi Î¦

            P o s P\_{os} Posâ€‹

            ï¼ˆ2+2+ K N d \\frac{K}{N\_{d}} Ndâ€‹Kâ€‹ï¼‰ Î¦ \\Phi Î¦

            31.4GB

            3 Î¦ \\Phi Î¦

            â€ƒâ€ƒ å¦‚å›¾æ‰€ç¤ºï¼Œ P o s P\_{os} Posâ€‹åœ¨å¢åŠ 1.5å€å•å¡é€šè®¯å¼€é”€çš„åŸºç¡€ä¸Šï¼Œå°†å•å¡å­˜å‚¨é™ä½äº†4å€ã€‚è¿™é‡Œéœ€è¦è¯´æ˜ä¸‹ï¼Œæœ‰å…¶ä»–ç›¸å…³æŠ€æœ¯åšå®¢ï¼Œç»™å‡ºçš„ P o s P\_{os} Posâ€‹å•å¡é€šè®¯é‡æ˜¯2 Î¦ \\Phi Î¦ã€‚å…¶å®è™½ç„¶æŒ‰ç…§è®ºæ–‡ä¸­å®šä¹‰ï¼Œè®¡ç®—çš„é€šè®¯é‡æ˜¯3 Î¦ \\Phi Î¦ï¼Œä½†åœ¨å®˜æ–¹ä»£ç çš„å…·ä½“å®ç°ä¸­ï¼Œé€šè®¯é‡åº”è¯¥æ˜¯2 Î¦ \\Phi Î¦ï¼Œè¿™æ˜¯å› ä¸ºåœ¨ç¬¬äºŒä¸ªæ­¥éª¤ä¸­ï¼Œç”±äºæ¯å—GPUä¸Šåªä¿ç®¡éƒ¨åˆ†optimizer statesï¼Œå› æ­¤æ ¹æœ¬ä¸éœ€è¦å¯¹æ¢¯åº¦åšall-gatheræ“ä½œã€‚å› ä¸ºå³ä½¿æ¯å—GPUä¸Šæœ‰å®Œæ•´çš„æ¢¯åº¦ï¼Œåœ¨å®é™…è®¡ç®—ä¸­æœ‰éƒ¨åˆ†æ¢¯åº¦ä¹Ÿç”¨ä¸ä¸Šã€‚è¿™æ · P o s P\_{os} Posâ€‹å•å¡é€šè®¯é‡å°±æ˜¯2 Î¦ \\Phi Î¦äº†ã€‚

            - **ZeRO-2**: å³ P o s + P g P\_{os}+P\_{g} Posâ€‹+Pgâ€‹ï¼Œä¼˜åŒ–çŠ¶æ€ä¸æ¢¯åº¦åˆ†å‰²ã€‚ç°åœ¨ï¼Œæ›´è¿‘ä¸€æ­¥ï¼Œæˆ‘ä»¬æŠŠæ¢¯åº¦ä¹Ÿæ‹†å¼€ï¼Œæ¯ä¸ªGPUæ ¼å­ç»´æŠ¤ä¸€å—æ¢¯åº¦ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/a18b35f0e7e544f192f9ecb8e30506a4.png#pic_center)
                æ­¤æ—¶ï¼Œæ•°æ®å¹¶è¡Œçš„æ•´ä½“æµç¨‹å¦‚ä¸‹:
                - æ¯å—GPUä¸Šå­˜ä¸€ä»½å®Œæ•´çš„å‚æ•°Wã€‚å°†ä¸€ä¸ªbatchçš„æ•°æ®åˆ†æˆ3ä»½ï¼Œæ¯å—GPUå„åƒä¸€ä»½ï¼Œåšå®Œä¸€è½®fowardå’Œbackwardåï¼Œç®—å¾—ä¸€ä»½å®Œæ•´çš„æ¢¯åº¦ï¼ˆä¸‹å›¾ä¸­ç»¿è‰²+ç™½è‰²ï¼‰ï¼›
                - å¯¹æ¢¯åº¦åšä¸€æ¬¡reduce-scatterï¼Œä¿è¯æ¯ä¸ªGPUä¸Šæ‰€ç»´æŒçš„é‚£å—æ¢¯åº¦æ˜¯èšåˆæ›´æ–°åçš„æ¢¯åº¦ã€‚ä¾‹å¦‚å¯¹GPU1ï¼Œå®ƒè´Ÿè´£ç»´æŠ¤G1ï¼Œå› æ­¤å…¶ä»–çš„GPUåªéœ€è¦æŠŠG1å¯¹åº”ä½ç½®çš„æ¢¯åº¦å‘ç»™GPU1åšåŠ æ€»å°±å¯ã€‚æ±‡æ€»å®Œæ¯•åï¼Œç™½è‰²å—å¯¹GPUæ— ç”¨ï¼Œå¯ä»¥ä»æ˜¾å­˜ä¸­ç§»é™¤ã€‚å•å¡é€šè®¯é‡ä¸º Î¦ \\Phi Î¦ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3e23d6c0685843018bcedfea73862647.png#pic_center)
                - æ¯å—GPUç”¨è‡ªå·±å¯¹åº”çš„Oå’ŒGå»æ›´æ–°ç›¸åº”çš„Wã€‚æ›´æ–°å®Œæ¯•åï¼Œæ¯å—GPUç»´æŒäº†ä¸€å—æ›´æ–°å®Œæ¯•çš„Wã€‚åŒç†ï¼Œå¯¹Wåšä¸€æ¬¡all-gatherï¼Œå°†åˆ«çš„GPUç®—å¥½çš„WåŒæ­¥åˆ°è‡ªå·±è¿™æ¥ã€‚å•å¡é€šè®¯é‡ Î¦ \\Phi Î¦ã€‚
                    Â 

            â€ƒâ€ƒåšå®Œ P o s + P g P\_{os}+P\_{g} Posâ€‹+Pgâ€‹åï¼Œè®¾GPUä¸ªæ•°ä¸º N d N\_{d} Ndâ€‹ï¼Œæ˜¾å­˜å’Œé€šè®¯é‡çš„æƒ…å†µå¦‚ä¸‹:

            å¹¶è¡ŒåŒ–æŠ€æœ¯

            æ˜¾å­˜

            æ˜¾å­˜ï¼ˆGBï¼‰ï¼Œ Î¦ = 7.5 B \\Phi=7.5B Î¦\=7.5Bï¼Œ N d = 64 N\_{d}=64 Ndâ€‹\=64ï¼Œ K = 12 K=12 K\=12

            å•å¡é€šè®¯é‡

            æœ´ç´ DP

            ï¼ˆ2+2+ K K Kï¼‰ Î¦ \\Phi Î¦

            120GB

            2 Î¦ \\Phi Î¦

            P o s P\_{os} Posâ€‹

            ï¼ˆ2+2+ K N d \\frac{K}{N\_{d}} Ndâ€‹Kâ€‹ï¼‰ Î¦ \\Phi Î¦

            31.4GB

            3 Î¦ \\Phi Î¦

            P o s + P g P\_{os}+P\_{g} Posâ€‹+Pgâ€‹

            ï¼ˆ2+ 2 + K N d \\frac{2+K}{N\_{d}} Ndâ€‹2+Kâ€‹ï¼‰ Î¦ \\Phi Î¦

            16.6GB

            2 Î¦ \\Phi Î¦

            â€ƒâ€ƒ å¦‚å›¾æ‰€ç¤ºï¼Œå’Œæœ´ç´ DPç›¸æ¯”ï¼Œå­˜å‚¨é™äº†8å€ï¼Œå•å¡é€šè®¯é‡æŒå¹³ã€‚

            - **ZeRO-3**: å³ P o s + P g + P p P\_{os}+P\_{g}+P\_{p} Posâ€‹+Pgâ€‹+Ppâ€‹ï¼Œä¼˜åŒ–çŠ¶æ€ã€æ¢¯åº¦ä¸å‚æ•°åˆ†å‰²ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æŠŠå‚æ•°ä¹Ÿåˆ‡å¼€ã€‚æ¯å—GPUç½®ç»´æŒå¯¹åº”çš„optimizer statesï¼Œgradientså’Œparametersï¼ˆå³Wï¼‰ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/f55d3e65be614701ac874871b309ae9f.png#pic_center)
                æ•°æ®å¹¶è¡Œçš„æµç¨‹å¦‚ä¸‹:
                - æ¯å—GPUä¸Šåªä¿å­˜éƒ¨åˆ†å‚æ•°Wã€‚å°†ä¸€ä¸ªbatchçš„æ•°æ®åˆ†æˆ3ä»½ï¼Œæ¯å—GPUå„åƒä¸€ä»½ï¼›
                - åšforwardæ—¶ï¼Œå¯¹Wåšä¸€æ¬¡all-gatherï¼Œå–å›åˆ†å¸ƒåœ¨åˆ«çš„GPUä¸Šçš„Wï¼Œå¾—åˆ°ä¸€ä»½å®Œæ•´çš„Wï¼Œå•å¡é€šè®¯é‡ Î¦ \\Phi Î¦ã€‚forwardåšå®Œï¼Œç«‹åˆ»æŠŠä¸æ˜¯è‡ªå·±ç»´æŠ¤çš„WæŠ›å¼ƒï¼›
                - åšbackwardæ—¶ï¼Œå¯¹Wåšä¸€æ¬¡all-gatherï¼Œå–å›å®Œæ•´çš„Wï¼Œå•å¡é€šè®¯é‡ Î¦ \\Phi Î¦ã€‚backwardåšå®Œï¼Œç«‹åˆ»æŠŠä¸æ˜¯è‡ªå·±ç»´æŠ¤çš„WæŠ›å¼ƒï¼›
                - åšå®Œbackwardï¼Œç®—å¾—ä¸€ä»½å®Œæ•´çš„æ¢¯åº¦Gï¼Œå¯¹Gåšä¸€æ¬¡reduce-scatterï¼Œä»åˆ«çš„GPUä¸Šèšåˆè‡ªå·±ç»´æŠ¤çš„é‚£éƒ¨åˆ†æ¢¯åº¦ï¼Œå•å¡é€šè®¯é‡ Î¦ \\Phi Î¦ã€‚èšåˆæ“ä½œç»“æŸåï¼Œç«‹åˆ»æŠŠä¸æ˜¯è‡ªå·±ç»´æŠ¤çš„GæŠ›å¼ƒã€‚
                - ç”¨è‡ªå·±ç»´æŠ¤çš„Oå’ŒGï¼Œæ›´æ–°Wã€‚ç”±äºåªç»´æŠ¤éƒ¨åˆ†Wï¼Œå› æ­¤æ— éœ€å†å¯¹Wåšä»»ä½•all-reduceæ“ä½œã€‚
                    Â 

            â€ƒâ€ƒåšå®Œ P o s + P g + P p P\_{os}+P\_{g}+P\_{p} Posâ€‹+Pgâ€‹+Ppâ€‹åï¼Œè®¾GPUä¸ªæ•°ä¸º N d N\_{d} Ndâ€‹ï¼Œæ˜¾å­˜å’Œé€šè®¯é‡çš„æƒ…å†µå¦‚ä¸‹:

            å¹¶è¡ŒåŒ–æŠ€æœ¯

            æ˜¾å­˜

            æ˜¾å­˜ï¼ˆGBï¼‰ï¼Œ Î¦ = 7.5 B \\Phi=7.5B Î¦\=7.5Bï¼Œ N d = 64 N\_{d}=64 Ndâ€‹\=64ï¼Œ K = 12 K=12 K\=12

            å•å¡é€šè®¯é‡

            æœ´ç´ DP

            ï¼ˆ2+2+ K K Kï¼‰ Î¦ \\Phi Î¦

            120GB

            2 Î¦ \\Phi Î¦

            P o s P\_{os} Posâ€‹

            ï¼ˆ2+2+ K N d \\frac{K}{N\_{d}} Ndâ€‹Kâ€‹ï¼‰ Î¦ \\Phi Î¦

            31.4GB

            3 Î¦ \\Phi Î¦

            P o s + P g P\_{os}+P\_{g} Posâ€‹+Pgâ€‹

            ï¼ˆ2+ 2 + K N d \\frac{2+K}{N\_{d}} Ndâ€‹2+Kâ€‹ï¼‰ Î¦ \\Phi Î¦

            16.6GB

            2 Î¦ \\Phi Î¦

            P o s + P g + P p P\_{os}+P\_{g}+P\_{p} Posâ€‹+Pgâ€‹+Ppâ€‹

            ï¼ˆ 2 + 2 + K N d \\frac{2+2+K}{N\_{d}} Ndâ€‹2+2+Kâ€‹ï¼‰ Î¦ \\Phi Î¦

            1.9GB

            3 Î¦ \\Phi Î¦

            â€ƒâ€ƒ å¦‚å›¾æ‰€ç¤ºï¼Œå’Œæœ´ç´ DPç›¸æ¯”ï¼Œç”¨1.5å€çš„é€šè®¯å¼€é”€ï¼Œæ¢å›è¿‘120å€çš„æ˜¾å­˜ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹ä¸‹è®ºæ–‡ä¸­çš„æ€»ä½“å¯¹æ¯”å›¾: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/1448e46ff6224bd89811ddd0f4c7ddf4.png#pic_center)

        - **ZeRO-DP VS æ¨¡å‹å¹¶è¡Œ**: é€šè¿‡ä¸Šè¿°çš„ä»‹ç»ï¼Œå¤§å®¶å¯èƒ½ä¼šæœ‰ç–‘é—®ï¼Œæ—¢ç„¶ZeROéƒ½æŠŠå‚æ•°Wç»™åˆ‡äº†ï¼Œé‚£å®ƒåº”è¯¥æ˜¯ä¸ªæ¨¡å‹å¹¶è¡Œï¼Œä¸ºä»€ä¹ˆå´å½’åˆ°æ•°æ®å¹¶è¡Œï¼Ÿå…¶å®ZeROæ˜¯æ¨¡å‹å¹¶è¡Œçš„å½¢å¼ï¼Œæ•°æ®å¹¶è¡Œçš„å®è´¨ã€‚

            - æ¨¡å‹å¹¶è¡Œï¼Œæ˜¯æŒ‡åœ¨forwardå’Œbackwardçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘åªéœ€è¦ç”¨è‡ªå·±ç»´æŠ¤çš„é‚£å—Wæ¥è®¡ç®—å°±è¡Œã€‚å³åŒæ ·çš„è¾“å…¥Xï¼Œæ¯å—GPUä¸Šå„ç®—æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œæœ€åé€šè¿‡æŸäº›æ–¹å¼èšåˆç»“æœï¼›
            - ä½†å¯¹ZeROæ¥è¯´ï¼Œå®ƒåšforwardå’Œbackwardçš„æ—¶å€™ï¼Œæ˜¯éœ€è¦æŠŠå„GPUä¸Šç»´æŠ¤çš„Wèšåˆèµ·æ¥çš„ï¼Œå³æœ¬è´¨ä¸Šè¿˜æ˜¯ç”¨å®Œæ•´çš„Wè¿›è¡Œè®¡ç®—ã€‚å®ƒæ˜¯ä¸åŒçš„è¾“å…¥Xï¼Œå®Œæ•´çš„å‚æ•°Wï¼Œæœ€ç»ˆå†åšèšåˆã€‚
        - **ZeRO-Offload**: ç®€å•ä»‹ç»ä¸€ä¸‹ZeRO-Offloadã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯: æ˜¾å­˜ä¸å¤Ÿï¼Œå†…å­˜æ¥å‡‘ã€‚å¦‚æœæŠŠè¦å­˜å‚¨çš„å¤§å¤´å¸è½½(offload)åˆ°CPUä¸Šï¼Œè€ŒæŠŠè®¡ç®—éƒ¨åˆ†æ”¾åˆ°GPUä¸Šï¼Œè¿™æ ·æ¯”èµ·è·¨æœºï¼Œæ—¢èƒ½é™ä½æ˜¾å­˜ä½¿ç”¨ï¼Œä¹Ÿèƒ½å‡å°‘ä¸€äº›é€šè®¯å‹åŠ›ã€‚ZeRO-Offloadçš„åšæ³•æ˜¯:

            - forwardå’Œbackwardè®¡ç®—é‡é«˜ï¼Œå› æ­¤å’Œå®ƒä»¬ç›¸å…³çš„éƒ¨åˆ†ï¼Œä¾‹å¦‚å‚æ•°Wï¼ˆFP16ï¼‰ã€activationsï¼Œå°±å…¨æ”¾å…¥GPUï¼›
            - updateçš„éƒ¨åˆ†è®¡ç®—é‡ä½ï¼Œå› æ­¤å’Œå®ƒç›¸å…³çš„éƒ¨åˆ†ï¼Œå…¨éƒ¨æ”¾å…¥CPUä¸­ã€‚ä¾‹å¦‚W(FP32)ã€optimizer statesï¼ˆFP32ï¼‰å’Œgradients(FP32)ç­‰ã€‚
                Â 

        â€ƒâ€ƒå…·ä½“åˆ‡åˆ†å¦‚ä¸‹å›¾: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/f0261c8558da46b0909e60603d47bd02.png#pic_center)

    - **Accelerate vs Deepspeed**:

        - Accelerateæ˜¯PyTorchå®˜æ–¹æä¾›çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼Œè€ŒDeepspeedæ˜¯ç”±Microsoftæä¾›çš„åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·ï¼›
        - æœ€ä¸»è¦çš„åŒºåˆ«åœ¨äºæ”¯æŒçš„æ¨¡å‹è§„æ¨¡ä¸åŒï¼ŒDeepspeedæ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼›
        - Deepspeedè¿˜æä¾›äº†æ›´å¤šçš„ä¼˜åŒ–ç­–ç•¥å’Œå·¥å…·ï¼Œä¾‹å¦‚ZeROå’ŒOffloadç­‰ï¼›
        - Accelerateæ›´åŠ ç¨³å®šå’Œæ˜“äºä½¿ç”¨ï¼Œé€‚åˆä¸­å°è§„æ¨¡çš„è®­ç»ƒä»»åŠ¡ï¼›
        - ç›®å‰Accelerateå·²ç»é›†æˆäº†DeepspeedåŠMegatronåˆ†å¸ƒå¼æŠ€æœ¯ï¼Œå…·ä½“å¯è¯¦è§å‰æ–‡çš„PEFTå®è·µéƒ¨åˆ†ã€‚
    - **èµ„æºåˆ†äº«**: [å¤§æ¨¡å‹è®­ç»ƒä¹‹å¾®è°ƒç¯‡](https://zhuanlan.zhihu.com/p/625896377)ã€[å¤§æ¨¡å‹è®­ç»ƒä¹‹æ¡†æ¶ç¯‡](https://zhuanlan.zhihu.com/p/625894118)ã€‚


#### 5.4ã€å¤§æ¨¡å‹çŸ¥è¯†é—®ç­”

- nBå¤§å°çš„æ¨¡å‹ï¼Œè®­ç»ƒå’Œæ¨ç†æ—¶ï¼Œæ˜¾å­˜å ç”¨æƒ…å†µï¼Ÿ
    - æ¨ç†æ—¶æ˜¾å­˜çš„ä¸‹é™æ˜¯2nGB ï¼Œè‡³å°‘è¦æŠŠæ¨¡å‹åŠ è½½å®Œå…¨ï¼›è®­ç»ƒæ—¶ï¼Œå¦‚æœç”¨Adamä¼˜åŒ–å™¨ï¼Œå‚è€ƒå‰æ–‡çš„2+2+12çš„å…¬å¼ï¼Œè®­ç»ƒæ—¶æ˜¾å­˜ä¸‹é™æ˜¯16nGBï¼Œéœ€è¦æŠŠæ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½è¿›æ¥ã€‚
- å¦‚æœæœ‰Nå¼ æ˜¾å­˜è¶³å¤Ÿå¤§çš„æ˜¾å¡ï¼Œæ€ä¹ˆåŠ é€Ÿè®­ç»ƒï¼Ÿ
    - æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ï¼Œå……åˆ†åˆ©ç”¨å¤šå¼ æ˜¾å¡çš„ç®—åŠ›ã€‚
- å¦‚æœæ˜¾å¡çš„æ˜¾å­˜ä¸å¤Ÿè£…ä¸‹ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å‘¢ï¼Ÿ
    - æœ€ç›´è§‚æƒ³æ³•ï¼Œéœ€è¦åˆ†å±‚åŠ è½½ï¼ŒæŠŠä¸åŒçš„å±‚åŠ è½½åˆ°ä¸åŒçš„GPUä¸Šï¼ˆaccelerateçš„device\_mapï¼‰ï¼Œä¹Ÿå°±æ˜¯å¸¸è§çš„PPï¼Œæµæ°´çº¿å¹¶è¡Œã€‚
- ä½†PPæ¨ç†èµ·æ¥ï¼Œæ˜¯ä¸€ä¸ªä¸²è¡Œçš„è¿‡ç¨‹ï¼Œ1ä¸ªGPUè®¡ç®—ï¼Œå…¶ä»–GPUç©ºé—²ï¼Œæœ‰æ²¡æœ‰å…¶ä»–æ–¹å¼ï¼Ÿ
    - æ¨ªå‘åˆ‡åˆ†ï¼Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ï¼Œä¹Ÿå°±æ˜¯åˆ†å±‚åŠ è½½åˆ°ä¸åŒçš„æ˜¾å¡ä¸Šï¼›
    - çºµå‘åˆ‡åˆ†ï¼Œå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ï¼Œä¹Ÿç§°ä½œæ¨¡å‹å¹¶è¡Œï¼ˆMPï¼‰ã€‚
- 3ç§å¹¶è¡Œæ–¹å¼å¯ä»¥å åŠ å—ï¼Ÿ
    - æ˜¯å¯ä»¥çš„ï¼ŒDP+PP+TPï¼Œè¿™å°±æ˜¯3Då¹¶è¡Œã€‚å¦‚æœçœŸæœ‰1ä¸ªè¶…å¤§æ¨¡å‹éœ€è¦é¢„è®­ç»ƒï¼Œ3Då¹¶è¡Œé‚£æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œå‚è€ƒBLOOMæ¨¡å‹çš„è®­ç»ƒï¼ŒDP+PPç”¨DeepSpeedï¼ŒTPç”¨Megatron-LMã€‚
- æœ€ä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Ÿ
    - ChatGLM-6Bï¼Œprefix LMï¼›
    - LLaMA-7Bï¼Œcausal LMã€‚
- prefix LMå’Œcausal LMçš„åŒºåˆ«ï¼Ÿ
    - Attention Maskä¸åŒï¼Œå‰è€…çš„prefixéƒ¨åˆ†çš„tokenäº’ç›¸èƒ½çœ‹åˆ°ï¼Œåè€…ä¸¥æ ¼éµå®ˆåªæœ‰åé¢çš„tokenæ‰èƒ½çœ‹åˆ°å‰é¢çš„tokençš„è§„åˆ™ã€‚
- å“ªç§æ¶æ„æ˜¯ä¸»æµï¼Ÿ
    - GPTç³»åˆ—å°±æ˜¯Causal LMï¼Œç›®å‰é™¤äº†T5å’ŒGLMï¼Œå…¶ä»–å¤§æ¨¡å‹åŸºæœ¬ä¸Šéƒ½æ˜¯Causal LMã€‚
- å¦‚ä½•ç»™LLMæ³¨å…¥é¢†åŸŸçŸ¥è¯†ï¼Ÿ
    - ç¬¬ä¸€ç§åŠæ³•ï¼Œæ£€ç´¢+LLMï¼Œå…ˆç”¨é—®é¢˜åœ¨é¢†åŸŸæ•°æ®åº“é‡Œæ£€ç´¢åˆ°å€™é€‰ç­”æ¡ˆï¼Œå†ç”¨LLMå¯¹ç­”æ¡ˆè¿›è¡ŒåŠ å·¥ï¼›
    - ç¬¬äºŒç§æ–¹æ³•ï¼ŒæŠŠé¢†åŸŸçŸ¥è¯†æ„å»ºæˆé—®ç­”æ•°æ®é›†ï¼Œç”¨SFTè®©LLMå­¦ä¹ è¿™éƒ¨åˆ†çŸ¥è¯†ã€‚

äºŒã€LLMç®€ä»‹
-------

â€ƒâ€ƒé€šè¿‡å‰æ–‡å¯¹TuningæŠ€æœ¯çš„ä»‹ç»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿäº†è§£åˆ°ï¼ŒTuningæŠ€æœ¯ä¾èµ–äºLLMçš„å‘å±•ï¼ŒåŒæ—¶ä¹Ÿåœ¨æ¨åŠ¨ç€LLMçš„å‘å±•ã€‚é€šå¸¸ï¼ŒLLMæŒ‡çš„æ˜¯åŒ…å«æ•°ç™¾äº¿ï¼ˆæˆ–æ›´å¤šï¼‰å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä»‹ç»å‡ ä¸ªè€³ç†Ÿèƒ½è¯¦çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ä»–LLMçš„ç›¸å…³å†…å®¹å¯å‚è€ƒ[LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)ã€[Open LLM Leaderboard
](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ã€[å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLM)æ±‡æ€»ï¼ˆæŒç»­æ›´æ–°ä¸­ï¼‰](https://blog.csdn.net/jarodyv/article/details/129992142)

### 1ã€GPTç³»åˆ—ï¼ˆOpenAIï¼‰

#### 1.1 GPT-1ã€GPT-2ã€GPT-3

â€ƒâ€ƒ2017å¹´ï¼ŒGoogleæ¨å‡ºTransformerï¼Œåˆ©ç”¨attentionå®Œå…¨æ›¿ä»£è¿‡å¾€æ·±åº¦å­¦ä¹ ä¸­çš„Recurrenceå’ŒConvolutionç»“æ„ï¼Œç›´ç™½åœ°å±•ç°å‡ºäº†â€œå¤§ä¸€ç»Ÿæ¨¡å‹â€çš„é‡å¿ƒï¼Œ"xxx is all you need"ä¹Ÿæˆäº†ä¸€ä¸ªç©ä¸çƒ‚çš„æ¢—ã€‚
â€ƒâ€ƒ2018å¹´6æœˆï¼ŒOpenAIæ¨å‡ºåŸºäºTransformer Decoderæ”¹é€ çš„ç¬¬ä¸€ä»£GPTï¼ˆGenerative Pre-Trainingï¼‰ï¼Œæœ‰æ•ˆè¯æ˜äº†åœ¨NLPé¢†åŸŸä¸Šä½¿ç”¨é¢„è®­ç»ƒ+å¾®è°ƒæ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚ç´§éšå…¶åï¼ŒåŒå¹´10æœˆGoogleæ¨å‡ºåŸºäºTransformer Encoderéƒ¨åˆ†çš„Bertï¼Œåœ¨åŒæ ·å‚æ•°å¤§å°çš„å‰æä¸‹ï¼Œå…¶æ•ˆæœé¢†è·‘äºGPT-1ï¼Œä¸€æ—¶æˆä¸ºNLPé¢†åŸŸçš„é¢†å¤´ç¾Šã€‚
â€ƒâ€ƒä¸ç”˜ç¤ºå¼±çš„OpenAIåœ¨4ä¸ªæœˆåï¼Œæ¨å‡ºæ›´å¤§çš„æ¨¡å‹GPT-2ï¼ˆGPT-1: 110Mï¼ŒBert: 340Mï¼ŒGPT-2: 1.5Bï¼‰ï¼ŒåŒæ—¶ï¼ŒOpenAIä¹ŸçŸ¥é“ï¼Œå…‰é å¢åŠ æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†æ¥è·å¾—ä¸€ä¸ªå’ŒBertå·®ä¸å¤šæ•ˆæœçš„æ¨¡å‹ï¼Œå…¶å®æ˜¯æ²¡æœ‰æŠ€æœ¯å«é‡çš„ã€‚äºæ˜¯ï¼Œåœ¨GPT-2é‡Œï¼ŒOpenAIå¼•å…¥zero-shotå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚
â€ƒâ€ƒæ­¤åï¼ŒOpenAIåœ¨LLMä¸Šä¹‰æ— åé¡¾åœ°èµ°äº†ä¸‹å»ï¼Œåœ¨2020å¹´6æœˆæ¨å‡ºå·¨äººGPT-3ï¼Œå‚æ•°é‡é«˜è¾¾175Bï¼Œå„ç±»å®éªŒæ•ˆæœè¾¾åˆ°é¡¶å³°ï¼Œæ®è¯´ä¸€æ¬¡è®­ç»ƒè´¹ç”¨ä¸º1200wç¾å…ƒï¼Œâ€œè´µâ€ä¹Ÿæˆäº†æ™®é€šå·¥ä¸šç•Œè¸è¶³GPTç³»åˆ—çš„å£å’ä¹‹ä¸€ã€‚
â€ƒâ€ƒåœ¨æ­£å¼ä»‹ç»GPTç³»åˆ—æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç»ä¸‹è¯­è¨€æ¨¡å‹çš„æ¦‚å¿µï¼Œè¯­è¨€æ¨¡å‹æ˜¯GPTç³»åˆ—æ¨¡å‹çš„åŸºåº§ã€‚ä»€ä¹ˆæ˜¯è¯­è¨€æ¨¡å‹ï¼Ÿç®€å•æ¥è¯´ï¼Œå°±æ˜¯çœ‹ä¸€ä¸ªå¥å­æ˜¯äººè¯çš„å¯èƒ½æ€§ã€‚ä¸“ä¸šä¸€ç‚¹æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œå…¶å­—ç¬¦æ˜¯ W = ( w 1 , w 2 , â‹¯ â€‰ , w L ) W=(w\_{1},w\_{2},\\cdots,w\_{L}) W\=(w1â€‹,w2â€‹,â‹¯,wLâ€‹)ï¼Œé‚£ä¹ˆï¼Œä»è¯­è¨€æ¨¡å‹æ¥çœ‹ï¼Œè¿™ä¸ªå¥å­æ˜¯äººè¯çš„å¯èƒ½æ€§å°±æ˜¯:
P ( W ) = P ( w 1 , w 2 , â‹¯ â€‰ , w L ) = P ( w 1 ) P ( w 2 âˆ£ w 1 ) P ( w 3 âˆ£ w 1 , w 2 ) â‹¯ P ( w L âˆ£ w 1 , w 2 , â‹¯ â€‰ , w L âˆ’ 1 ) \\begin{aligned} P(W)&=P(w\_{1},w\_{2},\\cdots,w\_{L})\\\\ &=P(w\_{1})P(w\_{2}|w\_{1})P(w\_{3}|w\_{1},w\_{2})\\cdots P(w\_{L}|w\_{1},w\_{2},\\cdots,w\_{L-1})\\\\ \\end{aligned} P(W)â€‹\=P(w1â€‹,w2â€‹,â‹¯,wLâ€‹)\=P(w1â€‹)P(w2â€‹âˆ£w1â€‹)P(w3â€‹âˆ£w1â€‹,w2â€‹)â‹¯P(wLâ€‹âˆ£w1â€‹,w2â€‹,â‹¯,wLâˆ’1â€‹)â€‹
ä½†æ˜¯ï¼Œ L L Lå¤ªé•¿å°±ä¼šå¾ˆç¨€ç–ï¼Œç›´æ¥ç®—è¿™ä¸ªæ¦‚ç‡ä¸å¥½è®¡ç®—ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¿‘ä¼¼è®¡ç®—:
P ( W ) = P ( w 1 , w 2 , â‹¯ â€‰ , w L ) = P ( w 1 ) P ( w 2 âˆ£ w 1 ) P ( w 3 âˆ£ w 1 , w 2 ) â‹¯ P ( w L âˆ£ w 1 , w 2 , â‹¯ â€‰ , w L âˆ’ 1 ) = P ( w 1 ) P ( w 2 âˆ£ w 1 ) â‹¯ P ( w L âˆ£ w L âˆ’ N , â‹¯ â€‰ , w L âˆ’ 1 ) \\begin{aligned} P(W)&=P(w\_{1},w\_{2},\\cdots,w\_{L})\\\\ &=P(w\_{1})P(w\_{2}|w\_{1})P(w\_{3}|w\_{1},w\_{2})\\cdots P(w\_{L}|w\_{1},w\_{2},\\cdots,w\_{L-1})\\\\ &=P(w\_{1})P(w\_{2}|w\_{1})\\cdots P(w\_{L}|w\_{L-N},\\cdots,w\_{L-1}) \\end{aligned} P(W)â€‹\=P(w1â€‹,w2â€‹,â‹¯,wLâ€‹)\=P(w1â€‹)P(w2â€‹âˆ£w1â€‹)P(w3â€‹âˆ£w1â€‹,w2â€‹)â‹¯P(wLâ€‹âˆ£w1â€‹,w2â€‹,â‹¯,wLâˆ’1â€‹)\=P(w1â€‹)P(w2â€‹âˆ£w1â€‹)â‹¯P(wLâ€‹âˆ£wLâˆ’Nâ€‹,â‹¯,wLâˆ’1â€‹)â€‹
è¿™å°±æ˜¯å¸¸è¯´çš„N-gramç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼ŒNé€šå¸¸æ˜¯2ï¼Œ3ï¼Œ4ã€‚ç‰¹åˆ«çš„ï¼Œå½“N=1æ—¶ï¼Œè¯­è¨€æ¨¡å‹å°±é€€åŒ–ä¸ºå„ä¸ªå­—ç¬¦å‡ºç°çš„æ¦‚ç‡ä¹‹ç§¯ã€‚å½“N=4æ—¶è¯­è¨€æ¨¡å‹å°±æ¯”è¾ƒå¤§äº†ï¼Œå®é™…åº”ç”¨ä¸­ä¸€èˆ¬æœ€å¤§ä¹Ÿå°±æ˜¯4äº†ã€‚æ ¹æ®æ¡ä»¶æ¦‚ç‡ P ( w L âˆ£ w L âˆ’ N , â‹¯ â€‰ , w L âˆ’ 1 ) P(w\_{L}|w\_{L-N},\\cdots,w\_{L-1}) P(wLâ€‹âˆ£wLâˆ’Nâ€‹,â‹¯,wLâˆ’1â€‹)ï¼Œæˆ‘ä»¬å°±èƒ½çŸ¥é“ç»™å®šå‰Nä¸ªå­—ï¼Œä¸‹ä¸€ä¸ªå­—æ˜¯ä»€ä¹ˆå­—çš„æ¦‚ç‡äº†ã€‚è¯­è¨€æ¨¡å‹çš„è¯„ä»·æŒ‡æ ‡å¯ä»¥é‡‡ç”¨PPLï¼ˆå›°æƒ‘åº¦ï¼ŒPerplexityï¼Œ[è¯­è¨€æ¨¡å‹](https://zhuanlan.zhihu.com/p/90741508)ï¼‰ã€‚
â€ƒâ€ƒæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ†åˆ«ä»‹ç»GPT-1ã€GPT-2ã€GPT-3çš„æ¨¡å‹åŸç†åŠé¢„è®­ç»ƒæ–¹æ³•ç­‰ç›¸å…³çŸ¥è¯†ã€‚

- **GPT-1**: GPT-1æ˜¯OpenAIåœ¨è®ºæ–‡[ã€ŠImproving Language Understanding by Generative Pre-Trainingã€‹](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)ä¸­æå‡ºçš„ç”Ÿæˆå¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³: é€šè¿‡äºŒæ®µå¼çš„è®­ç»ƒï¼Œç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼ˆæ— ç›‘ç£å½¢å¼ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡Fine-Tuningçš„æ¨¡å¼è§£å†³ä¸‹æ¸¸ä»»åŠ¡ï¼ˆç›‘ç£æ¨¡å¼ä¸‹ï¼‰ã€‚GPT-1å¯ä»¥å¾ˆå¥½åœ°å®Œæˆè‹¥å¹²ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€è‡ªç„¶è¯­è¨€æ¨ç†ã€é—®ç­”ã€è¯­ä¹‰ç›¸ä¼¼åº¦ç­‰ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¾®è°ƒåçš„GPT-1æ€§èƒ½å‡è¶…è¿‡äº†å½“æ—¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„SOTAæ¨¡å‹ã€‚

    - **è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNatural Language Inference æˆ–è€… Textual Entailmentï¼‰**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯åŒ…å«å…³ç³»ï¼ˆentailmentï¼‰ï¼ŒçŸ›ç›¾å…³ç³»ï¼ˆcontradictionï¼‰ï¼Œæˆ–è€…ä¸­ç«‹å…³ç³»ï¼ˆneutralï¼‰ï¼›

    - **é—®ç­”å’Œå¸¸è¯†æ¨ç†ï¼ˆQuestion answering and commonsense reasoningï¼‰**: ç±»ä¼¼äºå¤šé€‰é¢˜ï¼Œè¾“å…¥ä¸€ä¸ªæ–‡ç« ï¼Œä¸€ä¸ªé—®é¢˜ä»¥åŠè‹¥å¹²ä¸ªå€™é€‰ç­”æ¡ˆï¼Œè¾“å‡ºä¸ºæ¯ä¸ªç­”æ¡ˆçš„é¢„æµ‹æ¦‚ç‡ï¼›

    - **è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆSemantic Similarityï¼‰**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¯­ä¹‰ä¸Šæ˜¯ç›¸å…³çš„ï¼›

    - **åˆ†ç±»ï¼ˆClassificationï¼‰**: åˆ¤æ–­è¾“å…¥æ–‡æœ¬æ˜¯æŒ‡å®šçš„å“ªä¸ªç±»åˆ«ã€‚
        Â 
        ä¸‹é¢å…·ä½“ä»‹ç»ä¸‹GPT-1çš„æ¨¡å‹ç»“æ„åŠè®­ç»ƒæµç¨‹ã€‚

    - **æ¨¡å‹ç»“æ„**: GPT-1åŸºç¡€æ¶æ„æ˜¯åŸºäºTransformerçš„Decoderéƒ¨åˆ†ï¼ŒåŒæ—¶åˆ é™¤äº†Encoder-Decoder Attentionå±‚ï¼Œåªä¿ç•™äº†Masked Multi-Head Attentionå±‚å’ŒFeed Forwardå±‚ã€‚Transformerç»“æ„æå‡ºä¹‹å§‹ä¾¿ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œæœºå™¨ç¿»è¯‘æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œå› æ­¤Transformerè®¾è®¡äº†Encoderç”¨äºæå–æºç«¯è¯­è¨€çš„è¯­ä¹‰ç‰¹å¾ï¼Œè€Œç”¨Decoderæå–ç›®æ ‡ç«¯è¯­è¨€çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆç›¸å¯¹åº”çš„è¯‘æ–‡ã€‚GPT-1ç›®æ ‡æ˜¯æœåŠ¡äºå•åºåˆ—æ–‡æœ¬çš„ç”Ÿæˆå¼ä»»åŠ¡ï¼Œæ‰€ä»¥å«å¼ƒäº†å…³äºEncoderéƒ¨åˆ†ï¼ŒåŒ…æ‹¬Decoderçš„Encoder-Decoder Attentionå±‚ã€‚æ•´ä½“æ˜¯12å±‚çš„Transformer-Decoderå˜ä½“ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/7f79e1bd37914445a22c151d4d91bd12.jpeg#pic_center)
        é™¤æ­¤ä¹‹å¤–ï¼ŒGPT-1è¿˜å°†attentionçš„ç»´æ•°æ‰©å¤§åˆ°768ï¼ˆåŸæ¥ä¸º512ï¼‰ï¼Œå°†attentionçš„å¤´æ•°å¢åŠ åˆ°12ä¸ªï¼ˆåŸæ¥ä¸º8ä¸ªï¼‰ï¼Œå°†Feed Forwardå±‚çš„éšå±‚ç»´æ•°å¢åŠ åˆ°3072ï¼ˆåŸæ¥ä¸º2048ï¼‰ï¼Œæ€»å‚æ•°è¾¾åˆ°110Mã€‚GPT-1è¿˜ä¼˜åŒ–äº†å­¦ä¹ ç‡é¢„çƒ­ç®—æ³•ï¼Œä½¿ç”¨æ›´å¤§çš„BPEç è¡¨ï¼ˆè¯è¡¨å¤§å°ä¸º40478ï¼Œ478ä¸ªbase characters + 40000ä¸ªç»“åˆçš„å­—ç¬¦ï¼‰ï¼Œæ¿€æ´»å‡½æ•°ReLUæ”¹ä¸ºå¯¹æ¢¯åº¦æ›´æ–°æ›´å‹å¥½çš„é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒGeLUï¼Œå°†æ­£ä½™å¼¦æ„é€ çš„ä½ç½®ç¼–ç æ”¹ä¸ºäº†å¸¦å­¦ä¹ çš„ä½ç½®ç¼–ç ã€‚

    - **æ¨¡å‹è®­ç»ƒ**: ä¸Šæ–‡å·²ç»æåˆ°ï¼ŒGPT-1æ¨¡å‹è®­ç»ƒæ•´ä½“ä¸Šåˆ†ä¸ºä¸¤æ­¥: 1ï¼‰åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ åˆ°ä¸€ä¸ªé«˜å®¹é‡çš„è¯­è¨€æ¨¡å‹ï¼›2ï¼‰åœ¨æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚å…¶ä¸­ç¬¬äºŒæ­¥æ˜¯é’ˆå¯¹å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚

        - **æ— ç›‘ç£é¢„è®­ç»ƒ**: æ€»ä½“è®­ç»ƒä»»åŠ¡ç›®æ ‡æ˜¯æ ¹æ®å·²çŸ¥çš„è¯é¢„æµ‹æœªçŸ¥çš„è¯ã€‚åœ¨è¿™é‡Œè®¾å®šä¸€å®šçš„çª—å£å¤§å°ï¼Œå³æ ¹æ®æœ‰é™çš„è¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯: ç»™å®šä¸€ä¸ªè¯­æ–™çš„å¥å­åºåˆ— U = { u 1 , â‹¯ â€‰ , u n } \\mathcal{U}=\\{u\_{1},\\cdots,u\_{n}\\} U\={u1â€‹,â‹¯,unâ€‹}ï¼Œå·²çŸ¥å‰ k k kä¸ªè¯é¢„æµ‹å½“å‰è¯ u i u\_{i} uiâ€‹ï¼Œç”¨ä¸€ä¸ªæ ‡å‡†çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å»æå¤§åŒ–è¿™ä¸ªä¼¼ç„¶å‡½æ•°:
            L 1 ( U ) = âˆ‘ i l o g P ( u i âˆ£ u i âˆ’ k , â‹¯ â€‰ , u i âˆ’ 1 ; Î˜ ) Â  L\_{1}(\\mathcal{U})=\\sum\_{i} logP(u\_{i}|u\_{i-k},\\cdots, u\_{i-1};\\Theta)\\ L1â€‹(U)\=iâˆ‘â€‹logP(uiâ€‹âˆ£uiâˆ’kâ€‹,â‹¯,uiâˆ’1â€‹;Î˜)Â 
            å…¶ä¸­:  k k kæ˜¯æ»‘åŠ¨çª—å£å¤§å°ï¼Œ Î˜ \\Theta Î˜æ˜¯è¦ä¼˜åŒ–çš„å‚æ•°ã€‚ P ( u ) P(u) P(u)çš„è®¡ç®—æ–¹æ³•æ˜¯:
            h 0 = U W e + W p h i = t r a n s f o r m e r \_ b l o c k ( h i âˆ’ 1 ) ï¼Œ âˆ€ i âˆˆ \[ 1 , n \] P ( u ) = s o f t m a x ( h n W e T ) \\begin{aligned} h\_{0}&=UW\_{e}+W\_{p}\\\\ h\_{i}&=transformer\\\_block(h\_{i-1})ï¼Œ\\forall i\\in\[1,n\]\\\\ P(u)&=softmax(h\_{n}W\_{e}^{T}) \\end{aligned} h0â€‹hiâ€‹P(u)â€‹\=UWeâ€‹+Wpâ€‹\=transformer\_block(hiâˆ’1â€‹)ï¼Œâˆ€iâˆˆ\[1,n\]\=softmax(hnâ€‹WeTâ€‹)â€‹
            å…¶ä¸­:  W e W\_{e} Weâ€‹æ˜¯è¯å‘é‡çŸ©é˜µï¼ˆtoken embedding matrixï¼‰ï¼Œ W p W\_{p} Wpâ€‹æ˜¯ä½ç½®å‘é‡çŸ©é˜µï¼ˆposition embedding matrixï¼‰ï¼Œ U = u âˆ’ k , â‹¯ â€‰ , u âˆ’ 1 ) U=u\_{-k},\\cdots,u\_{-1}) U\=uâˆ’kâ€‹,â‹¯,uâˆ’1â€‹)æ˜¯tokensçš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆæºä»£ç ä¸­ï¼Œ u i u\_{i} uiâ€‹éƒ½æ˜¯one-hotç¼–ç å‘é‡ï¼Œç›¸å½“äºåšä¸€ä¸ªæŸ¥è¯¢æ“ä½œï¼Œ U U Uå­˜å‚¨ç´¢å¼•ï¼Œ W e W\_{e} Weâ€‹å­˜å‚¨ç€è¯å‘é‡å€¼ï¼‰ï¼Œ n n næ˜¯Decoderå±‚çš„æ•°é‡ã€‚
            â€ƒâ€ƒä¸Šé¢æ˜¯è®ºæ–‡ä¸­çš„æè¿°ï¼Œæˆ‘ä»¬ä¸¾ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œæ¥è¯´æ˜GPT-1å®é™…ä¸Šæ˜¯å¦‚ä½•è¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒçš„ã€‚ä¾‹å¦‚è¾“å…¥æ–‡æœ¬æ˜¯: ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¿™æ®µæ–‡æœ¬ç»è¿‡åˆ‡è¯è½¬æ¢ä¸ºä¸€ä¸ªä¸ªtokenåï¼Œè¾“å…¥GPT-1çš„transformer-decoderç»“æ„ï¼Œåœ¨æœ€åä¸€å±‚ï¼Œä¼šè¾“å‡ºæ¯ä¸ªtokenå¯¹åº”çš„è¡¨å¾å‘é‡ï¼Œå³ä¸Šæ–‡çš„ h n âˆˆ R m Ã— d h\_{n}\\in R^{m\\times d} hnâ€‹âˆˆRmÃ—dï¼Œå…¶ä¸­ m m mæ˜¯tokenæ•°é‡ï¼Œè¿™ä¸ªä¾‹å­ä¸­å°±æ˜¯5ï¼Œ d d dæ˜¯æ¨¡å‹ç»´åº¦ï¼ŒGPT-1ä¸­å°±æ˜¯768ï¼›æ¥ä¸‹æ¥ï¼Œ h n h\_{n} hnâ€‹å†ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œç”Ÿæˆ z n âˆˆ R m Ã— v z\_{n}\\in R^{m\\times v} znâ€‹âˆˆRmÃ—vï¼Œå…¶ä¸­ v v væ˜¯è¯è¡¨çš„å¤§å°ï¼›æœ€åï¼Œ z n z\_{n} znâ€‹ä¼šç»è¿‡softmaxæ“ä½œï¼Œç„¶åé€‰å–å®ƒæ¯ä¸€è¡Œä¸­æ•°å€¼æœ€å¤§çš„ç´¢å¼•åˆ°è¯è¡¨ä¸­æœç´¢å¯¹åº”çš„tokenï¼Œæœç´¢åˆ°çš„tokenæ€ä¹ˆç”¨å‘¢ï¼Ÿæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ï¼Œè¾“å…¥æ˜¯ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¾“å‡ºä¹Ÿæ˜¯5ä¸ªtokenï¼Œå› ä¸ºè¾“å…¥çš„ç¬¬ä¸€ä¸ªtokenæ˜¯ã€ä»Šã€‘ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„ç¬¬ä¸€ä¸ªtokenæ˜¯ã€å¤©ã€‘ï¼Œè¾“å…¥çš„ç¬¬äºŒä¸ªtokenæ˜¯ã€å¤©ã€‘ï¼Œåˆ™å¸Œæœ›è¾“å‡ºçš„ç¬¬äºŒä¸ªtokenæ˜¯ã€å¾ˆã€‘ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°æœ€åä¸€ä¸ªè¾“å…¥tokenã€å¿ƒã€‘ï¼Œä¸è¿‡å› ä¸ºå®ƒæ²¡æœ‰ä¸‹ä¸€ä¸ªè¯ï¼Œæ‰€ä»¥åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸åœ¨æˆ‘ä»¬çš„æŸå¤±è®¡ç®—èŒƒå›´å†…ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬ä¼šæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå°½å¯èƒ½çš„è®©æœ€ç»ˆçš„è¾“å‡ºtokençš„å‰å››ä¸ªå­—æ˜¯ã€å¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¿™å°±æ˜¯é¢„è®­ç»ƒä»»åŠ¡çš„æ•´ä½“æµç¨‹ã€‚å›è¿‡å¤´æ¥ï¼Œæˆ‘ä»¬ä¹Ÿç†è§£äº†ä¸ºä»€ä¹ˆé¢„è®­ç»ƒå«åšæ— ç›‘ç£è®­ç»ƒï¼Œå°±æ˜¯å› ä¸ºæˆ‘ä»¬å…¶å®æ²¡æœ‰æ ‡æ³¨æ ·æœ¬ï¼Œè€Œæ˜¯æ‹¿ä¸‹ä¸€ä¸ªè¯å½“åšæ ‡ç­¾è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¿™ç§æ–¹å¼ä¹Ÿè¢«ç§°ä½œè‡ªç›‘ç£è®­ç»ƒã€‚

        - **ç›‘ç£è®­ç»ƒ**: å½“å¾—åˆ°æ— ç›‘ç£çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬å°†å®ƒç›´æ¥åº”ç”¨åˆ°æœ‰ç›‘ç£ä»»åŠ¡ä¸­ç»§ç»­è®­ç»ƒã€‚å¯¹äºä¸€ä¸ªæœ‰æ ‡ç­¾çš„æ•°æ®é›† C \\mathcal{C} Cï¼Œæ¯ä¸ªå®ä¾‹æœ‰ m m mä¸ªè¾“å…¥token:  { x 1 , â‹¯ â€‰ , x m } \\{x^{1},\\cdots,x^{m}\\} {x1,â‹¯,xm}ï¼Œå®ƒå¯¹åº”çš„æ ‡ç­¾æ˜¯ y y yã€‚é¦–å…ˆå°†è¿™äº›tokenè¾“å…¥åˆ°è®­ç»ƒå¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè·å–æœ€åä¸€ä¸ªtransformer decoderçš„è¾“å‡ºï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾å‘é‡ h l m h\_{l}^{m} hlmâ€‹ã€‚ç„¶åå†é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å¾—åˆ°é¢„æµ‹ç»“æœ y y y:
            P ( y âˆ£ x 1 , â‹¯ â€‰ , x m ) = s o f t m a x ( h l m W y ) Â  P(y|x^{1},\\cdots,x^{m})=softmax(h\_{l}^{m}W\_{y})\\ P(yâˆ£x1,â‹¯,xm)\=softmax(hlmâ€‹Wyâ€‹)Â 
            å…¶ä¸­ W y W\_{y} Wyâ€‹ä¸ºå…¨è¿æ¥å±‚çš„å‚æ•°ã€‚æœ‰ç›‘ç£çš„ç›®æ ‡åˆ™æ˜¯æœ€å¤§åŒ–ä¸‹å¼çš„å€¼:
            L 2 ( C ) = âˆ‘ x , y P ( y âˆ£ x 1 , â‹¯ â€‰ , x m ) Â  L\_{2}(\\mathcal{C})=\\sum\_{x,y}P(y|x^{1},\\cdots,x^{m})\\ L2â€‹(C)\=x,yâˆ‘â€‹P(yâˆ£x1,â‹¯,xm)Â 
            æ³¨æ„: è¿™é‡Œçš„ h l m h^m\_l hlmâ€‹æ˜¯æ¯ä¸€ä¸ªè¯å¯¹åº”çš„Decoderè¾“å‡ºæ‹¼æ¥èµ·æ¥çš„ï¼Œ h l m = { h l < 1 > , â‹¯ â€‰ , h l < m > } h^m\_l=\\{h^{<1>}\_l,\\cdots,h^{<m>}\_l\\} hlmâ€‹\={hl<1\>â€‹,â‹¯,hl<m\>â€‹}ï¼Œ , h l < i > ,h^{<i>}\_l ,hl<i\>â€‹å¯¹åº” x i x^{i} xiçš„åµŒå…¥è¡¨ç¤ºã€‚
            Â 
            GPT-1çš„å®éªŒä¸­å‘ç°ï¼ŒåŠ å…¥è¯­è¨€æ¨¡å‹å­¦ä¹ ç›®æ ‡ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æŸå¤±å‡½æ•°ä¸­åŠ å…¥ L 1 L\_{1} L1â€‹èƒ½å¸¦æ¥ä¸¤ç‚¹å¥½å¤„: 1ï¼‰æå‡ç›‘ç£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›2ï¼‰åŠ å¿«æ”¶æ•›ï¼›å› æ­¤ï¼Œæœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼ˆ Î» \\lambda Î»ä¸€èˆ¬å–0.5ï¼‰:
            L 3 ( C ) = L 2 ( C ) + Î» L 1 ( C ) Â  L\_{3}(\\mathcal{C})=L\_{2}(\\mathcal{C})+\\lambda L\_{1}(\\mathcal{C})\\ L3â€‹(C)\=L2â€‹(C)+Î»L1â€‹(C)Â 

        - **ä¸‹æ¸¸ä»»åŠ¡**: GPT-1è®ºæ–‡ä¸­ç»™å‡ºäº†å››ä¸ªä¸‹æ¸¸é€‚é…ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯æ–‡æœ¬åˆ†ç±»ã€è‡ªç„¶è¯­è¨€æ¨ç†ã€é—®ç­”ã€è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ŒåŒæ—¶ç»™å‡ºäº†é’ˆå¯¹è¿™å››ä¸ªä»»åŠ¡ï¼Œå¦‚ä½•è¿›è¡Œé’ˆå¯¹æ€§çš„å¾®è°ƒã€‚è¿™å››ä¸ªä»»åŠ¡è™½ç„¶æœ¬è´¨ä¸Šéƒ½æ˜¯å±äºè‡ªç„¶è¯­è¨€ç†è§£çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä½†æ˜¯GPT-1çš„ç»“æ„æ˜¯å¾ˆé€‚é…åšè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡çš„ã€‚ä¸‹é¢æˆ‘ä»¬ä»‹ç»ä¸‹GPT-1å¦‚ä½•åœ¨ä¸Šè¿°å››ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

            - **åˆ†ç±»ä»»åŠ¡**: å°†èµ·å§‹å’Œç»ˆæ­¢tokenåŠ å…¥åˆ°åŸå§‹åºåˆ—ä¸¤ç«¯ï¼Œè¾“å…¥transformerä¸­å¾—åˆ°ç‰¹å¾å‘é‡ï¼Œæœ€åç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å¾—åˆ°é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼›
            - **è‡ªç„¶è¯­è¨€æ¨ç†**: å°†å‰æï¼ˆpremiseï¼‰å’Œå‡è®¾ï¼ˆhypothesisï¼‰é€šè¿‡åˆ†éš”ç¬¦ï¼ˆDelimiterï¼‰éš”å¼€ï¼Œä¸¤ç«¯åŠ ä¸Šèµ·å§‹å’Œç»ˆæ­¢tokenã€‚å†ä¾æ¬¡é€šè¿‡transformerå’Œå…¨è¿æ¥å¾—åˆ°é¢„æµ‹ç»“æœï¼›
            - **è¯­ä¹‰ç›¸ä¼¼åº¦**: è¾“å…¥çš„ä¸¤ä¸ªå¥å­ï¼Œæ­£å‘å’Œåå‘å„æ‹¼æ¥ä¸€æ¬¡ï¼ˆç”±äºç›¸ä¼¼æ€§è´¨æ˜¯å¯¹ç§°çš„ï¼Œä¸ºäº†æ¶ˆé™¤é¡ºåºçš„å½±å“ï¼‰ï¼Œç„¶ååˆ†åˆ«è¾“å…¥ç»™transformerï¼Œå¾—åˆ°çš„ç‰¹å¾å‘é‡æ‹¼æ¥åå†é€ç»™å…¨è¿æ¥å¾—åˆ°é¢„æµ‹ç»“æœï¼›
            - **é—®ç­”å’Œå¸¸è¯†æ¨ç†**: å°†ä¸ªé€‰é¡¹çš„é—®é¢˜æŠ½è±¡åŒ–ä¸ºä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œå³æ¯ä¸ªé€‰é¡¹åˆ†åˆ«å’Œå†…å®¹è¿›è¡Œæ‹¼æ¥ï¼Œç„¶åå„é€å…¥transformerå’Œå…¨è¿æ¥ä¸­ï¼Œæœ€åé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ä½œä¸ºé¢„æµ‹ç»“æœã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/0c96d17a280347a6b80e71a0a4da483a.png#pic_center)

            â€ƒâ€ƒè¿™é‡Œæˆ‘ä»¬åŒæ ·é€šè¿‡ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»çš„ä¾‹å­ï¼Œæ¥ä»‹ç»ä¸‹GPT-1åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¯å¦‚ä½•å¾®è°ƒçš„ã€‚ä¾‹å¦‚ä¸‹æ¸¸ä»»åŠ¡æ˜¯æƒ…æ„Ÿæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…æ‹¬å–œã€æ€’ã€å“€ã€æƒ§ã€å…¶ä»–äº”ä¸ªç±»åˆ«ï¼Œå…¶ä¸­ä¸€ä¸ªæ ·æœ¬æ˜¯ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼ŒçœŸå®æ ‡ç­¾æ˜¯ã€å–œã€‘ã€‚é€šè¿‡å‰é¢çš„ä»‹ç»ï¼Œæˆ‘ä»¬çŸ¥é“GPT-1åœ¨ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒæŸå¤±å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯ä¸é¢„è®­ç»ƒä¿æŒä¸€è‡´çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æŸå¤±ï¼Œè¿™éƒ¨åˆ†å°±ä¸ä»‹ç»äº†ã€‚å¦ä¸€éƒ¨åˆ†æ˜¯åˆ†ç±»æŸå¤±ï¼Œå¯¹äºåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œæˆ‘ä»¬æœ€ç»ˆä¹Ÿä¼šè·å–åˆ°GPT-1æœ€åä¸€å±‚çš„å‘é‡è¡¨å¾ h l âˆˆ R m Ã— d h\_{l}\\in R^{m\\times d} hlâ€‹âˆˆRmÃ—dï¼Œå…¶ä¸­ m m mæ˜¯tokenæ•°é‡ï¼Œè¿™ä¸ªä¾‹å­ä¸­å°±æ˜¯5ï¼Œ d d dæ˜¯æ¨¡å‹ç»´åº¦ï¼ŒGPT-1ä¸­å°±æ˜¯768ï¼Œ l l læ˜¯æ¨¡å‹å±‚æ•°ï¼›æ¥ä¸‹æ¥ï¼Œ h l h\_{l} hlâ€‹çš„æœ€åä¸€è¡Œå†ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆæ³¨æ„ï¼Œé¢„è®­ç»ƒä»»åŠ¡æ˜¯ h l h\_{l} hlâ€‹æ•´ä½“éƒ½è¦ç»è¿‡å…¨è¿æ¥å±‚ï¼Œæˆ‘ä»¬è¿™é‡Œåªéœ€ç”¨åˆ°æœ€åä¸€ä¸ªtokenï¼Œå³å›¾ç‰‡ä¸­çš„Extractå¯¹åº”çš„å‘é‡è¡¨å¾ï¼‰ï¼Œç”Ÿæˆ z l âˆˆ R c z\_{l}\\in R^{c} zlâ€‹âˆˆRcï¼Œå…¶ä¸­ c c cæ˜¯ç±»åˆ«æ•°ç›®ï¼›æœ€åï¼Œ z l z\_{l} zlâ€‹ä¼šç»è¿‡softmaxæ“ä½œï¼Œè·å–ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘è¿™æ®µæ–‡æœ¬å¯¹åº”çš„æ¯ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼ï¼Œæˆ‘ä»¬çš„æœŸæœ›æ˜¯ã€å–œã€‘çš„æ¦‚ç‡å€¼è¦å°½å¯èƒ½çš„å¤§ï¼Œä¹Ÿå°±æ˜¯ z l z\_{l} zlâ€‹çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„å€¼è¦å°½å¯èƒ½å¤§ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡ã€‚

    - **GPT-1ç‰¹ç‚¹**:

        - **ä¼˜ç‚¹**: ç‰¹å¾æŠ½å–å™¨ä½¿ç”¨äº†å¼ºå¤§çš„Transformerï¼Œèƒ½å¤Ÿæ•æ‰åˆ°æ›´é•¿çš„è®°å¿†ä¿¡æ¯ï¼Œä¸”è¾ƒä¼ ç»Ÿçš„RNNæ›´æ˜“äºå¹¶è¡ŒåŒ–ï¼›transformerçš„å¹¶è¡ŒåŒ–å¯ä»¥å‚è€ƒ[æµ…æTransformerè®­ç»ƒæ—¶å¹¶è¡Œé—®é¢˜](https://zhuanlan.zhihu.com/p/368592551)ï¼›
        - **ç¼ºç‚¹**: GPT-1æœ€å¤§çš„é—®é¢˜å°±æ˜¯ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹æ˜¯å•å‘çš„ã€‚
    - **GPT-1ä¸ELMoï¼ŒBertçš„åŒºåˆ«**:

        - **GPT-1ä¸ELMoçš„åŒºåˆ«**:
            - **æ¨¡å‹æ¶æ„ä¸åŒ**: ELMoæ˜¯æµ…å±‚çš„åŒå‘RNNï¼›GPT-1æ˜¯å¤šå±‚çš„Transformer decoderï¼›
            - **é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å¤„ç†ä¸åŒ**: ELMoå°†è¯åµŒå…¥æ·»åŠ åˆ°ç‰¹å®šä»»åŠ¡ä¸­ï¼Œä½œä¸ºé™„åŠ åŠŸèƒ½ï¼›GPTåˆ™é’ˆå¯¹æ‰€æœ‰ä»»åŠ¡å¾®è°ƒç›¸åŒçš„åŸºæœ¬æ¨¡å‹ã€‚
        - **GPT-1ä¸Bertçš„åŒºåˆ«**:
            - **é¢„è®­ç»ƒ**: GPT-1é¢„è®­ç»ƒçš„æ–¹å¼å’Œä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ä¸€æ ·ï¼Œé€šè¿‡ä¸Šæ–‡ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼›Bertä¼šåŒæ—¶åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼›
            - **æ¨¡å‹æ•ˆæœ**: GPT-1å› ä¸ºé‡‡ç”¨äº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹æ‰€ä»¥æ›´åŠ é€‚åˆç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆç±»çš„ä»»åŠ¡ (NLG)ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡é€šå¸¸æ˜¯æ ¹æ®å½“å‰ä¿¡æ¯ç”Ÿæˆä¸‹ä¸€åˆ»çš„ä¿¡æ¯ã€‚è€ŒBertæ›´é€‚åˆç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ (NLU)ã€‚å½“ç„¶è¿™æ˜¯ä¹‹å‰çš„è¯´æ³•ï¼Œç°åœ¨chatgptå‡ºæ¥ä»¥åå“ªä¸ªæ›´é€‚åˆNLUä»»åŠ¡è¿˜çœŸä¸ä¸€å®šã€‚GPT-1çš„æ¨¡å‹å‚æ•°ä¸ºL=12ï¼ŒH=768ï¼ŒA=12ï¼Œè¿™ä¸ªè®¾ç½®å’Œåæ¥Bert-baseä¸€æ¨¡ä¸€æ ·ï¼Œä½†åè€…çš„æ•ˆæœè¦å¥½ä¸Šå¾ˆå¤šã€‚åŸå› ä¹‹ä¸€æ˜¯ï¼ŒGPT-1é‡‡ç”¨Mask-Attentionç»“æ„ï¼Œå¯¹æ¨¡å‹å’Œè®­ç»ƒæ•°æ®çš„è¦æ±‚ä¼šæ›´é«˜ï¼Œå› ä¸ºæ¨¡å‹èƒ½è¯»åˆ°çš„ä¿¡æ¯åªæœ‰ä¸Šæ–‡ã€‚è€Œé‡‡ç”¨æ™®é€šattentionçš„Bertåœ¨è®­ç»ƒé˜¶æ®µå°±èƒ½åŒæ—¶è¯»åˆ°ä¸Šä¸‹æ–‡ã€‚è¿™ä¸ªæ€§è´¨å†³å®šäº†GPTæ¨¡å‹è¶Šæ¥è¶Šå¤§çš„è¶‹åŠ¿ã€‚ä½†æ˜¯ï¼Œé•¿è¿œæ¥çœ‹ï¼ŒMasked-Attentionæ˜¯æ¨åŠ¨æ¨¡å‹æ›´å¥½ç†è§£æ–‡å­—çš„é‡è¦æ‰‹æ®µï¼Œæ¯•ç«Ÿåœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›åŸ¹å…»æ¨¡å‹çŸ¥ä¸Šæ–‡è¡¥ä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å•çº¯åœ°åšå®Œå½¢å¡«ç©ºã€‚
            - **æ¨¡å‹ç»“æ„**:  GPT-1é‡‡ç”¨äº†Transformerçš„Decoderï¼Œè€ŒBerté‡‡ç”¨äº†Transformerçš„Encoderã€‚
    - **GPT-1çš„æ•°æ®é›†**: GPT-1ä½¿ç”¨äº†BooksCorpusæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†åŒ…å«7000æœ¬æ²¡æœ‰å‘å¸ƒçš„ä¹¦ç±ã€‚ä½œè€…é€‰è¿™ä¸ªæ•°æ®é›†çš„åŸå› æœ‰äºŒ: 1ï¼‰æ•°æ®é›†æ‹¥æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä½¿å¾—æ¨¡å‹èƒ½å­¦å¾—æ›´é•¿æœŸçš„ä¾èµ–å…³ç³»ï¼›2ï¼‰è¿™äº›ä¹¦ç±å› ä¸ºæ²¡æœ‰å‘å¸ƒï¼Œæ‰€ä»¥å¾ˆéš¾åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šè§åˆ°ï¼Œæ›´èƒ½éªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

- **GPT-2**: æˆ‘ä»¬çŸ¥é“ï¼ŒGPT-1å’ŒBertçš„è®­ç»ƒéƒ½æ˜¯åˆ†ä¸¤æ­¥èµ°: pre-training + supervised fine-tuningã€‚è¿™å¥—æ–¹æ³•çš„ç¼ºç‚¹:

    - è™½ç„¶å€ŸåŠ©é¢„è®­ç»ƒè¿™ä¸€æ­¥æå‡æ€§èƒ½ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šè¿˜æ˜¯éœ€è¦æœ‰ç›‘ç£çš„Fine-Tuningæ‰èƒ½ä½¿å¾—æ¨¡å‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼›
    - éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šé¢æœ‰æ ‡æ³¨çš„æ•°æ®ã€‚å½“æˆ‘ä»¬åªæœ‰å¾ˆå°‘é‡çš„å¯ç”¨æ•°æ® (å³zero-shotçš„æƒ…å†µä¸‹) æ—¶å°±å¾ˆéš¾æœ‰å¾ˆå¥½çš„æ•ˆæœã€‚
        Â 

    â€ƒâ€ƒå¦å¤–ï¼Œåœ¨Bertæ¨¡å‹æå‡ºä¹‹åï¼ŒEncoder vs Decoderï¼ŒBert vs GPT-1ï¼Œä¸¤ä¸¤ä¹‹é—´çš„æ¯”è¾ƒå°±å¼€å§‹äº†ï¼Œä½†æ˜¯æ­¤æ—¶GPT-1ä»å¤„åœ¨åŠ£åŠ¿ã€‚Bertæå‡ºä¹‹åï¼Œé™¤äº†ç”Ÿæˆä»»åŠ¡å¤–ï¼ŒNLPä»»åŠ¡çš„èŒƒå¼åŸºæœ¬å°±æ˜¯Bertçš„é¢„è®­ç»ƒ+Fine-Tuningäº†ã€‚OpenAIæ”¾å¼ƒäº†å—ï¼Ÿå¹¶æ²¡æœ‰ï¼æˆ‘ä»¬çŸ¥é“ï¼ŒåŸºäºDecoderçš„æ¨¡å‹ï¼Œæ¨¡å‹å’Œæ•°æ®é‡è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚ä½†OpenAIå¦‚æœåªåšåˆ°è¿™ä¸€ç‚¹ï¼Œä»æŠ€æœ¯ä¸Šæ¥è¯´åˆå¤ªé€Šè‰²äº†ï¼Œæ€§ä»·æ¯”ä¹Ÿä¸é«˜ã€‚å› æ­¤ï¼ŒOpenAIä»è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œå¼•å…¥äº†zero-shotè¿™ä¸€åˆ›æ–°ç‚¹ï¼ŒGPT-2å°±è¯ç”Ÿäº†[ã€ŠLanguage Models are Unsupervised Multitask Learnersã€‹](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)ã€‚
    â€ƒâ€ƒè®ºæ–‡ä¸­è®¤ä¸ºç°åœ¨çš„è®­ç»ƒæ–¹å¼è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹åªèƒ½ç®—æ˜¯ä¸€ä¸ªå°ä»»åŠ¡ä¸Šçš„ä¸“å®¶ç³»ç»Ÿï¼Œè€Œä¸”è¿˜éƒ½ä¸å¤Ÿé²æ£’ã€‚é€ æˆè¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯æ¨¡å‹éƒ½æ˜¯åœ¨å•ä¸€é¢†åŸŸå†…çš„å•ä¸€ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œç¼ºä¹æ³›åŒ–æ€§ã€‚è·Ÿäººä¸€æ ·ï¼Œè§è¯†å’ŒçŸ¥è¯†å¤ªå°‘æ—¶ï¼Œå°±å¾ˆéš¾å¯¹äº‹æƒ…æœ‰å…¨é¢çš„äº†è§£ã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªå¯è¡Œçš„æ€è·¯æ˜¯å¤šä»»åŠ¡å­¦ä¹ ï¼Œè€Œä¸”æ˜¯å¤§é‡ä¸åŒé¢†åŸŸçš„ä¸åŒä»»åŠ¡ã€‚ä½†æ˜¯ï¼Œè¿™æ ·çš„å¤šä»»åŠ¡å­¦ä¹ æ˜¯æœ‰ç›‘ç£çš„è®­ç»ƒï¼Œéœ€è¦å¤§é‡çš„æ•°æ®ï¼Œè¿™ä¸ªå°±æ¯”è¾ƒéš¾å®ç°äº†ã€‚
    â€ƒâ€ƒGPT-2åœ¨GPT-1çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†æ–°çš„å‘å±•æ€è·¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼ŒGPT-2çš„æ€è·¯å°±æ˜¯å……åˆ†ç›¸ä¿¡è¯­è¨€æ¨¡å‹ï¼Œä¸å†å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡ŒFine-Tuningæˆ–è€…å¢åŠ ä»»åŠ¡å¤´äº†ï¼Œå°±ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥è§£å†³æ‰€æœ‰ä»»åŠ¡ï¼Œç›´æ¥åšzero-shotçš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ä¸Šé«˜è´¨é‡çš„å¤§æ•°æ®ï¼Œå †å æ›´å¤šçš„å‚æ•°ï¼Œä¸åŒä»»åŠ¡æ”¹é€ æˆç”Ÿæˆä»»åŠ¡ã€‚
    â€ƒâ€ƒGPT-2æœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä½†æ˜¯ä¸ä¸€æ ·çš„æ˜¯ï¼Œå®ƒè¯æ˜äº†è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ zero-shot çš„æƒ…å†µä¸‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒGPT-2åœ¨åšä¸‹æ¸¸ä»»åŠ¡çš„æ—¶å€™å¯ä»¥æ— éœ€ä»»ä½•æ ‡æ³¨çš„ä¿¡æ¯ï¼Œä¹Ÿæ— éœ€ä»»ä½•å‚æ•°æˆ–æ¶æ„çš„ä¿®æ”¹ã€‚åæ¥çš„GPT-3ä¹Ÿæ˜¯æ²¿ç”¨äº†è¿™ä¸ªæ€è·¯ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œå·²ç»å¯ä»¥çœ‹å‡ºä¸€äº›ChatGPTçš„å½±å­äº†ã€‚

    - **æ¨¡å‹ç»“æ„**: GPT-2çš„æ¨¡å‹åœ¨GPT-1çš„åŸºç¡€ä¸Šåšäº†ä¸€äº›æ”¹è¿›ï¼Œå¦‚ä¸‹:

        - **ç»“æ„å˜åŒ–**: å¯¹äºæ¯ä¸ªsub-block: ç¬¬ä¸€ä¸ªlayer normå±‚ç§»åˆ°sub-blockçš„è¾“å…¥éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯attentionä¹‹å‰ï¼Œç¬¬äºŒä¸ªlayer normå±‚ç§»åˆ°feed forwardä¹‹å‰ï¼›å¯¹äºæ•´ä½“æ¨¡å‹æ¶æ„ï¼Œåœ¨æœ€åä¸€ä¸ªsub-blockåå†åŠ ä¸€ä¸ªlayer normå±‚ï¼›
        - **æƒé‡å˜åŒ–**: é‡‡ç”¨ä¸€ç§æ”¹è¿›çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†æ®‹å·®è·¯å¾„ä¸æ¨¡å‹æ·±åº¦çš„ç´¯ç§¯ã€‚åœ¨åˆå§‹åŒ–æ—¶å°†residual layersçš„æƒé‡æŒ‰ N \\sqrt{N} N â€‹çš„å› å­è¿›è¡Œç¼©æ”¾ï¼Œå…¶ä¸­ N N Næ˜¯residual layersçš„æ•°é‡ï¼›
            - å¤‡æ³¨: è¿™ä¸ªæ”¹åŠ¨å…¶å®æ²¡å¤ªçœ‹æ‡‚ï¼Œresidual layerså°±æ˜¯ä¸€ä¸ªç›¸åŠ æ“ä½œï¼Œæ€ä¹ˆä¼šæœ‰å‚æ•°å‘¢ï¼ŸæŸ¥é˜…äº†å¾ˆå¤šèµ„æ–™ï¼Œæºç ä¹Ÿçœ‹äº†[GPT-2](https://github.com/openai/gpt-2)ï¼Œæ²¡çœ‹åˆ°æƒé‡ç¼©æ”¾çš„æµç¨‹ã€‚åœ¨æ­¤ç»™ä¸€ä¸ªæœ¬äººçš„è§è§£: æ ¹æ®è¿™ä¸ªæ“ä½œçš„ç›®çš„å¯çŸ¥ï¼Œæ˜¯ä¸ºäº†é˜²æ­¢éšç€æ¨¡å‹æ·±åº¦çš„ç´¯ç§¯ï¼Œæ®‹å·®è¶ŠåŠ è¶Šå¤§ï¼Œå› æ­¤è®¤ä¸ºè¿™é‡Œçš„ç¼©æ”¾æŒ‡çš„æ˜¯æ¯æ¬¡è¿›è¡Œæ®‹å·®æ“ä½œä¹‹å‰ï¼ˆå³å°†è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œç›¸åŠ ä¹‹å‰ï¼‰ï¼Œå…ˆå°†è¾“å…¥è¿›è¡Œç¼©æ”¾ï¼Œç¼©æ”¾å› å­è·Ÿå½“å‰æ˜¯æ•´ä½“ç»“æ„çš„ç¬¬å‡ å±‚æœ‰å…³ï¼Œå±‚æ•°è¶Šå¤§ï¼Œç´¯ç§¯çš„è¶Šå¤§ï¼Œæ‰€ä»¥åº”è¯¥ç¼©æ”¾çš„è¶Šå¤šã€‚æ¯”å¦‚ç°åœ¨æ˜¯æ•´ä½“ç»“æ„çš„ç¬¬äº”å±‚ï¼Œé‚£ä¹ˆç¼©æ”¾å› å­ N N Nå°±æ˜¯5ã€‚ä¸Šè¿°å†…å®¹åªæ˜¯æœ¬äººçš„ä¸€ä¸ªæƒ³æ³•ï¼Œå¦‚æœ‰å¤§ç¥æ›´äº†è§£å…¶ä¸­çš„åŸç†ï¼Œæ¬¢è¿äº¤æµæŒ‡æ­£ã€‚ï¼‰ï¼›
        - **è¯è¡¨å˜åŒ–**: è¯è¡¨å¤§å°è®¾ç½®ä¸º50257ï¼›
        - **è¾“å…¥å˜åŒ–**: æ— ç›‘ç£é¢„è®­ç»ƒå¯çœ‹åˆ°çš„ä¸Šä¸‹æ–‡çš„contextç”±512æ‰©å±•ä¸º1024ï¼›
        - **æ‰¹æ¬¡å˜åŒ–**: è®­ç»ƒæ—¶ï¼Œbatchsizeå¤§å°ä»64è°ƒæ•´ä¸º512ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/707f0f02c6aa4dd1854bf28312013182.png#pic_center)

        â€ƒâ€ƒè®ºæ–‡ç»™äº†ä¸åŒå±‚æ•°çš„æ¨¡å‹ï¼Œæœ€å¤§çš„æ¨¡å‹ç§°ä¸ºGPT-2æ¨¡å‹ï¼Œå‚æ•°æœ‰1.5Bï¼›æœ€å°çš„å³æ˜¯GPT-1ï¼Œå¯¹æ ‡Bert-baseï¼›å€’æ•°ç¬¬äºŒå°çš„å¯¹æ ‡Bert-largeã€‚ä¸åŒæ¨¡å‹å¤§å°å¦‚ä¸‹: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/888b7f350a62461d84897fcd39e8b2a5.png#pic_center)

    - **æ¨¡å‹è®­ç»ƒ**: GPT-2åªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ã€‚

        - **æ— ç›‘ç£è®­ç»ƒ**: GPT-2çš„è®­ç»ƒæ–¹å¼å’ŒGPT-1çš„è®­ç»ƒæ–¹å¼ç›¸æ¯”ï¼Œä¸¤è€…éƒ½æ˜¯æœ‰é¢„è®­ç»ƒè¿‡ç¨‹çš„ï¼Œä¸è¿‡GPT-2åªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¸é‡‡ç”¨Fine-Tuningæ–¹æ³•ï¼Œè€Œæ˜¯é‡‡ç”¨è®ºæ–‡ä¸­æåˆ°çš„zero-shotæ–¹æ³•ã€‚GPT-2é‡‡ç”¨è¿™ç§æ¨¡å¼ï¼Œå½’å› äºGPT-2æå‡ºçš„æ ¸å¿ƒæ€æƒ³: å½“ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å®¹é‡è¶³å¤Ÿå¤§æ•°æ®é‡è¶³å¤Ÿä¸°å¯Œæ—¶ï¼Œå®ƒå°±è¶³ä»¥è¦†ç›–æ‰€æœ‰çš„æœ‰ç›‘ç£ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰€æœ‰çš„æœ‰ç›‘ç£å­¦ä¹ éƒ½æ˜¯æ— ç›‘ç£è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªå­é›†ï¼Œä»…ä»…é è®­ç»ƒè¯­è¨€æ¨¡å‹ä¾¿å¯ä»¥å®Œæˆå…¶ä»–æœ‰ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ã€‚
        - **ä¸‹æ¸¸ä»»åŠ¡**: GPT-2å¦‚ä½•è®©æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡å‘¢ï¼Ÿé¦–å…ˆå‰æ–‡æåˆ°ï¼ŒGPT-2åœ¨å¤§è§„æ¨¡æ— ç›‘ç£è®­ç»ƒè¿‡ç¨‹å­¦ä¹ åˆ°äº†ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚ä½œè€…æ˜¯è¿™ä¹ˆè®¤ä¸ºçš„: æ¯”å¦‚ä¸‹æ¸¸ä»»åŠ¡æ˜¯è‹±æ–‡ç¿»è¯‘æ³•æ–‡ï¼Œé‚£ä¹ˆå¦‚æœæ¨¡å‹åœ¨æ— ç›‘ç£é¢„è®­ç»ƒçš„è¿‡ç¨‹ä¸­çœ‹è¿‡ç›¸å…³çš„æ–‡å­— (ä¾‹å¦‚"Mentez mentez, il en restera toujours quelque chose," which translates as, "Lie lie and something will always remain."è¿™å¥è¯æ˜¯è®­ç»ƒçš„è¯­æ–™)ï¼Œé‚£ä¹ˆæ¨¡å‹å°±èƒ½å¤Ÿå­¦ä¼š (translate to french, english text, french text) è¿™æ ·çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸåˆ™ä¸Šï¼Œé€šè¿‡å¤§é‡çš„è¯­æ–™è®­ç»ƒï¼Œè¯­è¨€å»ºæ¨¡èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„ç›‘ç£ä¿¡æ¯ã€‚ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆè®²å‘¢ï¼Ÿå› ä¸ºä½œè€…è®¤ä¸º: ä¸‹æ¸¸ä»»åŠ¡ (æœ‰ç›‘ç£è®­ç»ƒ) å¯ä»¥è§†ä¸ºé¢„è®­ç»ƒè¿‡ç¨‹ (æ— ç›‘ç£è®­ç»ƒ) çš„ä¸€ä¸ªå­é›†ã€‚æ— ç›‘ç£ç›®æ ‡çš„å…¨å±€æœ€ä¼˜è§£ä¹Ÿæ˜¯æœ‰ç›‘ç£è®­ç»ƒçš„å…¨å±€æœ€ä¼˜è§£ã€‚å½“é¢„è®­ç»ƒè§„æ¨¡è¶³å¤Ÿå¤§æ—¶ï¼ŒæŠŠæ— ç›‘ç£çš„ä»»åŠ¡è®­ç»ƒå¥½äº†ï¼Œæœ‰ç›‘ç£çš„ä¸‹æ¸¸ä»»åŠ¡å³ä¸å†éœ€è¦é¢å¤–è®­ç»ƒï¼Œå°±æ˜¯æ‰€è°“çš„zero-shotã€‚æ‰€ä»¥ä¸‹é¢çš„é—®é¢˜å°±å˜æˆäº†: åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¦‚ä½•èƒ½å¤Ÿä¼˜åŒ–æ— ç›‘ç£é¢„è®­ç»ƒè¿‡ç¨‹ä»¥è¾¾åˆ°æ”¶æ•›ã€‚åˆæ­¥å®éªŒè¯å®ï¼Œè¶³å¤Ÿå¤§çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ— ç›‘ç£çš„é¢„è®­ç»ƒè¿‡ç¨‹ä¹‹ååšä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†å­¦ä¹ é€Ÿåº¦æ¯”æ˜¾å¼ç›‘ç£æ–¹æ³•æ…¢å¾—å¤šã€‚é‚£ä¹ˆæœ€åä¸€ä¸ªé—®é¢˜å°±æ˜¯å…·ä½“æ€ä¹ˆå»åšä¸‹æ¸¸ä»»åŠ¡å‘¢ï¼Ÿä»¥è‹±æ–‡ç¿»è¯‘æ³•æ–‡ä¸ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡æ—¶é¢„å…ˆå‘Šè¯‰æ¨¡å‹ â€œtranslate English to Frenchâ€ï¼Œå³ç»™æ¨¡å‹ä¸€ä¸ªæç¤º (Prompt)ã€‚
    - **GPT-2çš„æ•°æ®é›†**: è®¸å¤šä¹‹å‰çš„å·¥ä½œæ˜¯åœ¨å•ä¸ªæ–‡æœ¬åŸŸä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚æ–°é—»æ–‡ç« ï¼Œç»´åŸºç™¾ç§‘æˆ–å°è¯´ç­‰ç­‰ã€‚GPT-2åˆ™æ˜¯å¸Œæœ›ä½¿å¾—è®­ç»ƒæ•°æ®é›†çš„é¢†åŸŸå’Œä¸Šä¸‹æ–‡æ›´å¤šä¸€ç‚¹ã€‚åœ¨ç½‘ç«™ä¸Šçˆ¬å–æ–‡æœ¬æ˜¯ä¸€ä¸ªæ–¹æ¡ˆï¼Œæ¯”å¦‚è¯´Common Crawlç½‘ç«™ã€‚è™½ç„¶è¿™äº›ç½‘ç«™æ‰‹æœºçš„æ•°æ®é›†åœ¨é‡çº§ä¸Šå¾ˆå¤§ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¥é‡çš„æ•°æ®è´¨é‡é—®é¢˜ï¼Œè¿™ä¸Šé¢çš„å†…å®¹æœ‰å¾ˆå¤šæ˜¯ä¿¡å™ªæ¯”å¾ˆä½çš„ï¼Œéš¾ä»¥ç†è§£çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³æ•°æ®é›†è´¨é‡çš„é—®é¢˜ï¼ŒGPT-2åªçˆ¬å–äººç±»è¿‡æ»¤ä¹‹åçš„ç½‘é¡µã€‚ä½†æ˜¯ï¼Œæ‰‹åŠ¨è¿‡æ»¤çš„ç½‘ç»œçˆ¬å–å¾ˆæ˜‚è´µï¼Œæ‰€ä»¥GPT-2ä»ç¤¾äº¤åª’ä½“å¹³å°Reddit ä¸ŠæŠ“å–äº†è‡³å°‘æ”¶åˆ°äº†3ä¸ªkarmaçš„é“¾æ¥ã€‚karmaå¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§å¯å‘å¼æŒ‡æ ‡ï¼Œç”¨äºåˆ¤æ–­å…¶ä»–ç”¨æˆ·æ˜¯å¦è®¤ä¸ºè¯¥é“¾æ¥æœ‰è¶£ã€æœ‰æ•™è‚²æ„ä¹‰æˆ–åªæ˜¯æœ‰è¶£ã€‚å¾—åˆ°çš„è¿™ä¸ªæ•°æ®é›†ç§°ä¹‹ä¸ºWebTextï¼Œæ˜¯ä¸€ä¸ªåŒ…å«äº†4500ä¸‡ä¸ªé“¾æ¥çš„æ–‡æœ¬æ•°æ®é›†ã€‚ç»è¿‡é‡å¤æ•°æ®åˆ é™¤å’Œä¸€äº›åŸºäºå¯å‘å¼çš„æ¸…ç†åï¼Œå®ƒåŒ…å«ç•¥å¤šäº800ä¸‡ä¸ªæ–‡æ¡£ï¼Œæ€»æ–‡æœ¬å®¹é‡ä¸º40GBã€‚ä½œè€…ä»WebTextä¸­åˆ é™¤äº†æ‰€æœ‰ç»´åŸºç™¾ç§‘æ–‡æ¡£ï¼Œå› ä¸ºå®ƒå¯èƒ½æ¶‰åŠåˆ° test evaluation tasksã€‚ç›®å‰å…¨é‡çš„æ•°æ®æ˜¯æ²¡æœ‰å¼€æ”¾ä¸‹è½½çš„ï¼Œå¯é€šè¿‡[GPT-2è®­ç»ƒæ•°æ®é›†](https://link.zhihu.com/?target=https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/webtext.train.jsonl)ä¸‹è½½éƒ¨åˆ†è®­ç»ƒæ•°æ®ã€‚

    - **GPT-2ç‰¹ç‚¹**:

        - **ä¼˜ç‚¹**: GPT-2ç›¸å¯¹GPT-1æ¨¡å‹çš„äº®ç‚¹æ˜¯æ”¯æŒzero-shotçš„è®¾ç½®ï¼ŒåŒæ—¶åœ¨zero-shotçš„å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­å±•ç¤ºå‡ºä¸é”™çš„æ€§èƒ½ã€‚GPT-2é¦–å…ˆæ„é€ äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†: WebTextï¼Œå®ƒæ˜¯ä¸€ä¸ªæœ‰ç™¾ä¸‡çº§åˆ«æ–‡æœ¬çš„æ•°æ®é›†ã€‚GPT-2è‡ªå·±æ˜¯ä¸€ä¸ªæœ‰ç€1.5Bå‚æ•°é‡çš„æ¨¡å‹ï¼›GPT-2æå‡ºäº†æ–°çš„NLPèŒƒå¼ï¼Œå¼ºè°ƒé€šè¿‡æ›´å¤šçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®è®­ç»ƒé«˜å®¹é‡è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ— ç›‘ç£å®Œæˆä¸‹æ¸¸å¤šä»»åŠ¡ã€‚å°è¯•ä»¥ä¸€ç§é€šç”¨çš„è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå»è§£å†³ç°æœ‰çš„å¤§éƒ¨åˆ†NLPä»»åŠ¡ï¼›
        - **ç¼ºç‚¹**: GPT-2åœ¨æ¨¡å‹æœ¬èº«ä¸Šæ²¡å•¥å¤§çš„å˜åŒ–å’Œåˆ›æ–°ã€‚
- **GPT-3**: GPT-2çš„æœ€å¤§è´¡çŒ®æ˜¯éªŒè¯äº†é€šè¿‡æµ·é‡æ•°æ®å’Œå¤§é‡å‚æ•°è®­ç»ƒå‡ºæ¥çš„è¯å‘é‡æ¨¡å‹æœ‰è¿ç§»åˆ°å…¶å®ƒç±»åˆ«ä»»åŠ¡ä¸­è€Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚ä½†æ˜¯å¾ˆå¤šå®éªŒä¹Ÿè¡¨æ˜ï¼ŒGPT-2çš„æ— ç›‘ç£å­¦ä¹ çš„èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œç”šè‡³åœ¨æœ‰äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ¯”éšæœºçš„å¥½ã€‚å°½ç®¡åœ¨æœ‰äº›zero-shotçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸é”™ï¼Œä½†æ˜¯æˆ‘ä»¬ä»ä¸æ¸…æ¥šGPT-2çš„è¿™ç§ç­–ç•¥ç©¶ç«Ÿèƒ½åšæˆä»€ä¹ˆæ ·å­ã€‚GPT-2è¡¨æ˜éšç€æ¨¡å‹å®¹é‡å’Œæ•°æ®é‡çš„å¢å¤§ï¼Œå…¶æ½œèƒ½è¿˜æœ‰è¿›ä¸€æ­¥å¼€å‘çš„ç©ºé—´ï¼ŒåŸºäºè¿™ä¸ªæ€æƒ³ï¼Œè¯ç”Ÿäº†æˆ‘ä»¬ä¸‹é¢è¦ä»‹ç»çš„GPT-3[ã€ŠLanguage Models are Few-Shot Learnersã€‹](https://arxiv.org/pdf/2005.14165.pdf)ã€‚
    â€ƒâ€ƒGPT-2åœ¨GPT-1çš„åŸºç¡€ä¸Šå¾€å‰èµ°äº†ä¸€å¤§æ­¥: å®Œå…¨æŠ›å¼ƒäº†å¾®è°ƒï¼Œå¹¶é‡‡ç”¨äº†zero-shotçš„æ–¹å¼ã€‚Zero-shotçš„æ–¹å¼è¢«GPT-2è®¤è¯å¯è¡Œåï¼ŒOpenAIå°±ä¸å¾—ä¸å¼€å§‹è€ƒè™‘æ¨¡å‹æ˜¯å¦èƒ½çœŸæ­£åšåˆ°å¼ºå¤§äº†ï¼Œæ¯•ç«Ÿç°åœ¨åªæ˜¯å’ŒBertæŒå¹³è€Œå·²ã€‚è¿™ä¸€åˆ»OpenAIå¼€å§‹æ‚Ÿè¿‡æ¥ï¼Œæ—¢ç„¶LLMè¦ä¸€è·¯èµ°åˆ°åº•ï¼Œæ—¢ç„¶æ¨¡å‹å˜å¤§é¿å…ä¸äº†ï¼Œé‚£ä¸å¦‚æ¥å¾—æ›´å½»åº•ä¸€äº›ã€‚GPT-3æ²¿ç”¨äº†å»é™¤Fine-Tuningï¼Œåªåšé€šç”¨è¯­è¨€æ¨¡å‹çš„æ€è·¯ï¼ŒåŒæ—¶æŠ€æœ¯ä¸Šå°åšæ›¿æ¢ï¼ˆsparse Transformerï¼‰ï¼›å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨ä¸åšå¾®è°ƒçš„å‰æä¸‹é‡‡ç”¨äº†few-shotçš„æ–¹å¼ï¼ˆæ¯•ç«Ÿå®Œå…¨ä¸ç»™æ¨¡å‹ä»»ä½•æ˜¾æ€§æç¤ºï¼Œæ•ˆæœç¡®å®æ²¡è¾¾åˆ°é¢„æœŸï¼‰ã€‚æœ€ç»ˆç”Ÿæˆäº†ä¸€ä¸ªå¤§å°é«˜è¾¾175Bçš„å¤§æ¨¡å‹ï¼Œå½“ç„¶æ•ˆæœä¹Ÿæ˜¯ä¸€éª‘ç»å°˜çš„ã€‚

    - **æ¨¡å‹ç»“æ„**: GPT-3çš„æ¨¡å‹ä¸GPT-2çš„æ¨¡å‹åŸºæœ¬ä¸€è‡´ï¼Œä¸»è¦æ”¹è¿›åªæœ‰ä¸€ç‚¹:

        - **Sparse Attention**: åœ¨æ¨¡å‹ç»“æ„ä¸­çš„æ³¨æ„åŠ›å±‚ï¼ŒGPT-3é‡‡ç”¨Sparse Transformerä¸­çš„Sparse Attentionæ–¹æ¡ˆï¼Œsparse attentionä¸ä¼ ç»Ÿself-attentionï¼ˆç§°ä¸ºdense attentionï¼‰çš„åŒºåˆ«åœ¨äº:

            - **dense attention**: æ¯ä¸ª token ä¹‹é—´ä¸¤ä¸¤è®¡ç®—attentionï¼Œå¤æ‚åº¦ O ( n 2 ) O(n^{2}) O(n2)ï¼›
            - **sparse attention**: æ¯ä¸ªtokenåªä¸å…¶ä»–tokençš„ä¸€ä¸ªå­é›†è®¡ç®—attentionï¼Œå¤æ‚åº¦ O ( n âˆ— l o g n ) O(n\*logn) O(nâˆ—logn)ã€‚
                Â 

            å…·ä½“æ¥è¯´ï¼Œsparse attentioné™¤äº†ç›¸å¯¹è·ç¦»ä¸è¶…è¿‡ k k kä»¥åŠç›¸å¯¹è·ç¦»ä¸º k ï¼Œ 2 k ï¼Œ 3 k ï¼Œ â‹¯ kï¼Œ2kï¼Œ3kï¼Œ\\cdots kï¼Œ2kï¼Œ3kï¼Œâ‹¯çš„tokenï¼Œå…¶ä»–æ‰€æœ‰tokençš„æ³¨æ„åŠ›éƒ½è®¾ä¸º0ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2190cc14dc354960b919117f658590c9.jpeg#pic_center)
            æˆ‘ä»¬æ¥å…·ä½“è§‚å¯Ÿä¸€ä¸‹ï¼Œå®é™…ä¸Šå›¾ä¸­çš„ç¬¬äºŒè¡Œå°±æ˜¯æ¶‰åŠåˆ°çš„attentionçš„tokenå†…å®¹ï¼Œå¯ä»¥çœ‹å‡ºé¦–å…ˆå…³æ³¨äº†é™„è¿‘å››ä¸ªtokenï¼Œå…¶æ¬¡æ˜¯ 2 k , 3 k 2k,3k 2k,3kè·ç¦»çš„tokenï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿™ä¹ˆåšå‘¢ï¼Ÿä½¿ç”¨ sparse attentionçš„å¥½å¤„ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç‚¹:

            - å‡å°‘æ³¨æ„åŠ›å±‚çš„è®¡ç®—å¤æ‚åº¦ï¼ŒèŠ‚çº¦æ˜¾å­˜å’Œè€—æ—¶ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ï¼›
            - å…·æœ‰â€œå±€éƒ¨ç´§å¯†ç›¸å…³å’Œè¿œç¨‹ç¨€ç–ç›¸å…³â€çš„ç‰¹æ€§ï¼Œå¯¹äºè·ç¦»è¾ƒè¿‘çš„ä¸Šä¸‹æ–‡å…³æ³¨æ›´å¤šï¼Œå¯¹äºè·ç¦»è¾ƒè¿œçš„ä¸Šä¸‹æ–‡å…³æ³¨è¾ƒå°‘ã€‚
                Â 
                å…³äºSparse Transformerçš„è¯¦ç»†ä»‹ç»å¯ä»¥å‚è§OpenAIäº2019å¹´å‘è¡¨çš„è®ºæ–‡[ã€ŠGenerating Long Sequences with Sparse Transformersã€‹](https://arxiv.org/pdf/1904.10509.pdf)ã€‚
                Â 

            è®ºæ–‡ä¸­ä¾›è®­ç»ƒäº†8ä¸ªä¸é€šè§„æ¨¡çš„æ¨¡å‹ï¼Œæœ€å¤§çš„ä¸€ä¸ªç§°ä½œä¸ºGPT-3: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/49511955866740cab0d31d50b68cd8a0.png#pic_center)

    - **æ¨¡å‹è®­ç»ƒ**: GPT-3ä¹Ÿåªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ã€‚

        - **æ— ç›‘ç£è®­ç»ƒ**: GPT-3ä»é‡‡ç”¨GPT-2æå‡ºçš„ä»…åšé¢„è®­ç»ƒã€ä¸åšå¾®è°ƒçš„æ€è·¯ã€‚GPT-3é‡‡ç”¨äº†In-context learningã€‚å€Ÿç”¨meta-learningï¼ˆå…ƒå­¦ä¹ ï¼‰çš„æ€æƒ³ï¼Œåœ¨pre-trainingæœŸé—´è®©æ¨¡å‹å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½å’Œæ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œè€Œåœ¨æ¨ç†æœŸé—´åˆ©ç”¨è¿™äº›æŠ€èƒ½å’Œèƒ½åŠ›è¿…é€Ÿé€‚é…åˆ°æœŸæœ›çš„ä»»åŠ¡ä¸Šã€‚åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»è¿‡In-context learningï¼Œä¸‹é¢ç®€å•ä»‹ç»ä¸‹GPT-3ä¸­çš„In-context learningã€‚
            â€ƒâ€ƒIn-context learningæ˜¯è¿™ç¯‡è®ºæ–‡ä¸­ä»‹ç»çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œè¦ç†è§£In-context learningï¼Œæˆ‘ä»¬éœ€è¦å…ˆç†è§£meta-learningï¼ˆå…ƒå­¦ä¹ ï¼‰ã€‚å¯¹äºä¸€ä¸ªå°‘æ ·æœ¬çš„ä»»åŠ¡æ¥è¯´ï¼Œæ¨¡å‹çš„åˆå§‹åŒ–å€¼éå¸¸é‡è¦ï¼Œä»ä¸€ä¸ªå¥½çš„åˆå§‹åŒ–å€¼ä½œä¸ºèµ·ç‚¹ï¼Œæ¨¡å‹èƒ½å¤Ÿå°½å¿«æ”¶æ•›ï¼Œä½¿å¾—åˆ°çš„ç»“æœéå¸¸å¿«çš„é€¼è¿‘å…¨å±€æœ€ä¼˜è§£ã€‚å…ƒå­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³åœ¨äºé€šè¿‡å°‘é‡çš„æ•°æ®å¯»æ‰¾ä¸€ä¸ªåˆé€‚çš„åˆå§‹åŒ–èŒƒå›´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šå¿«é€Ÿæ‹Ÿåˆï¼Œå¹¶è·å¾—ä¸é”™çš„æ•ˆæœã€‚
            â€ƒâ€ƒè¿™é‡Œçš„ä»‹ç»ä½¿ç”¨çš„æ˜¯MAMLï¼ˆModel-Agnostic Meta-Learningï¼‰ç®—æ³•ï¼Œæ­£å¸¸çš„ç›‘ç£å­¦ä¹ æ˜¯å°†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ‰“åŒ…æˆä¸€ä¸ªbatchè¿›è¡Œå­¦ä¹ ã€‚ä½†æ˜¯å…ƒå­¦ä¹ æ˜¯å°†ä¸€ä¸ªä¸ªä»»åŠ¡æ‰“åŒ…æˆbatchï¼Œæ¯ä¸ªbatchåˆ†ä¸ºæ”¯æŒé›†ï¼ˆsupport setï¼‰å’Œè´¨è¯¢é›†ï¼ˆquery setï¼‰ï¼Œç±»ä¼¼äºå­¦ä¹ ä»»åŠ¡ä¸­çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
            â€ƒâ€ƒå¯¹ä¸€ä¸ªç½‘ç»œæ¨¡å‹ f f fï¼Œå…¶å‚æ•°è¡¨ç¤ºä¸º Î¸ \\theta Î¸ï¼Œå®ƒçš„åˆå§‹åŒ–å€¼è¢«å«åšmeta-initializationã€‚MAMLçš„ç›®æ ‡åˆ™æ˜¯å­¦ä¹ ä¸€ç»„meta-initializationï¼Œèƒ½å¤Ÿå¿«é€Ÿåº”ç”¨åˆ°å…¶å®ƒä»»åŠ¡ä¸­ã€‚MAMLçš„è¿­ä»£æ¶‰åŠä¸¤æ¬¡å‚æ•°æ›´æ–°ï¼Œåˆ†åˆ«æ˜¯å†…å¾ªç¯ï¼ˆinner loopï¼‰å’Œå¤–å¾ªç¯ï¼ˆouter loopï¼‰ã€‚å†…å¾ªç¯æ˜¯æ ¹æ®ä»»åŠ¡æ ‡ç­¾å¿«é€Ÿçš„å¯¹å…·ä½“çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ å’Œé€‚åº”ï¼Œè€Œå¤–å­¦ä¹ åˆ™æ˜¯å¯¹meta-initializationè¿›è¡Œæ›´æ–°ã€‚ç›´è§‚çš„ç†è§£ï¼Œæˆ‘ç”¨ä¸€ç»„meta-initializationå»å­¦ä¹ å¤šä¸ªä»»åŠ¡ï¼Œå¦‚æœæ¯ä¸ªä»»åŠ¡éƒ½å­¦å¾—æ¯”è¾ƒå¥½ï¼Œåˆ™è¯´æ˜è¿™ç»„meta-initializationæ˜¯ä¸€ä¸ªä¸é”™çš„åˆå§‹åŒ–å€¼ï¼Œå¦åˆ™æˆ‘ä»¬å°±å»å¯¹è¿™ç»„å€¼è¿›è¡Œæ›´æ–°ã€‚
            â€ƒâ€ƒGPT-3ä¸­ä»‹ç»çš„In-context learningåˆ™æ˜¯å…ƒå­¦ä¹ çš„å†…å¾ªç¯ï¼ŒåŸºäºè¯­è¨€æ¨¡å‹çš„SGDåˆ™æ˜¯å¤–å¾ªç¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/49030ab15e4b4045bc65c9b8b66ada22.png#pic_center)

        - **ä¸‹æ¸¸ä»»åŠ¡**: åœ¨è®­ç»ƒé˜¶æ®µï¼Œé¢„è®­ç»ƒé€šç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…·å¤‡è¯†åˆ«ä¸åŒNLPä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ­¤æ—¶æ¨¡å‹å…·å¤‡äº†ä¸€å®šçš„ICLèƒ½åŠ›ã€‚è€Œåœ¨æ¨ç†é˜¶æ®µï¼Œä¾èµ–äºæ¨¡å‹çš„ICLèƒ½åŠ›ï¼Œé’ˆå¯¹å„NLPä»»åŠ¡ï¼Œå‘æ¨¡å‹ä¸­è¾“å…¥ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œä¸Šä¸‹æ–‡åŒ…æ‹¬ä»»åŠ¡æè¿°ã€è‹¥å¹²ä¸ªä»»åŠ¡æ ·æœ¬å’Œä»»åŠ¡æç¤ºï¼Œæ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ç»™å‡ºä»»åŠ¡è¾“å‡ºã€‚æ ¹æ®ä¸Šä¸‹æ–‡åŒ…å«çš„ä»»åŠ¡æ ·æœ¬æ•°é‡å¯è¿›ä¸€æ­¥å°†ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†ä¸ºZero-Shotï¼ˆæ— ä»»åŠ¡æ ·æœ¬ï¼‰ã€One-Shotï¼ˆä»…ä¸€ä¸ªä»»åŠ¡æ ·æœ¬ï¼‰å’ŒFew-Shotï¼ˆå¤šä¸ªä»»åŠ¡æ ·æœ¬ï¼‰ä¸‰ç±»ã€‚

            - **Fine-Tunningï¼ˆFTï¼‰**: FTåˆ©ç”¨æˆåƒä¸Šä¸‡çš„ä¸‹æ¸¸ä»»åŠ¡æ ‡æ³¨æ•°æ®æ¥æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æƒé‡ä»¥è·å¾—å¼ºå¤§çš„æ€§èƒ½ã€‚ä½†æ˜¯ï¼Œè¯¥æ–¹æ³•ä¸ä»…å¯¼è‡´æ¯ä¸ªæ–°çš„ä¸‹æ¸¸ä»»åŠ¡éƒ½éœ€è¦å¤§é‡çš„æ ‡æ³¨è¯­æ–™ï¼Œè¿˜å¯¼è‡´æ¨¡å‹åœ¨æ ·æœ¬å¤–é¢„æµ‹çš„èƒ½åŠ›å¾ˆå¼±ã€‚è™½ç„¶GPT-3ä»ç†è®ºä¸Šæ”¯æŒFTï¼Œä½†è®ºæ–‡ä¸­æ²¡è¿™ä¹ˆåšï¼›
            - **Few-Shotï¼ˆFSï¼‰**: æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µå¯ä»¥å¾—åˆ°å°‘é‡çš„ä¸‹æ¸¸ä»»åŠ¡ç¤ºä¾‹ä½œä¸ºé™åˆ¶æ¡ä»¶ï¼Œä½†æ˜¯ä¸å…è®¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æƒé‡ã€‚FSè¿‡ç¨‹çš„ç¤ºä¾‹å¯ä»¥çœ‹ä¸‹å›¾ä¸­æ•´ç†çš„æ¡ˆä¾‹ã€‚FSçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å¹¶ä¸éœ€è¦å¤§é‡çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ï¼ŒåŒæ—¶ä¹Ÿé˜²æ­¢äº†æ¨¡å‹åœ¨Fine-Tuningé˜¶æ®µçš„è¿‡æ‹Ÿåˆã€‚FSçš„ä¸»è¦ç¼ºç‚¹æ˜¯ä¸ä»…ä¸Fine-Tuningçš„SOTAæ¨¡å‹æ€§èƒ½å·®è·è¾ƒå¤§ä¸”ä»éœ€è¦å°‘é‡çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ï¼›
            - **One-Shotï¼ˆ1Sï¼‰**: æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä»…å¾—åˆ°1ä¸ªä¸‹æ¸¸ä»»åŠ¡ç¤ºä¾‹ã€‚æŠŠ1Sç‹¬ç«‹äºFew-Shotå’ŒZero-Shotè®¨è®ºæ˜¯å› ä¸ºè¿™ç§æ–¹å¼ä¸äººç±»æ²Ÿé€šçš„æ–¹å¼æœ€ç›¸ä¼¼ï¼›
            - **Zero-Shotï¼ˆ0Sï¼‰**: æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä»…å¾—åˆ°ä¸€æ®µä»¥è‡ªç„¶è¯­è¨€æè¿°çš„ä¸‹æ¸¸ä»»åŠ¡è¯´æ˜ã€‚0Sçš„ä¼˜ç‚¹æ˜¯æä¾›äº†æœ€å¤§ç¨‹åº¦çš„æ–¹ä¾¿æ€§ã€å°½å¯èƒ½å¤§çš„é²æ£’æ€§å¹¶å°½å¯èƒ½é¿å…äº†ä¼ªç›¸å…³æ€§ã€‚0Sçš„æ–¹å¼æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜çš„ï¼Œå³ä½¿æ˜¯äººç±»æœ‰æ—¶å€™ä¹Ÿéš¾ä»¥ä»…ä¾èµ–ä»»åŠ¡æè¿°è€Œæ²¡æœ‰ç¤ºä¾‹çš„æƒ…å†µä¸‹ç†è§£ä¸€ä¸ªä»»åŠ¡ã€‚ä½†æ¯«æ— ç–‘é—®ï¼Œ0Sè®¾ç½®ä¸‹çš„æ€§èƒ½æ˜¯æœ€ä¸äººç±»çš„æ°´å¹³å…·æœ‰å¯æ¯”æ€§çš„ã€‚
                ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3fc26c6dcbfb4fff8d7fa67ad2a5c9ff.png#pic_center)
    - **GPT-3çš„æ•°æ®é›†**: GPT-3çš„è®­ç»ƒæ•°æ®åŒ…æ‹¬ä½è´¨é‡çš„Common Crawlï¼Œé«˜è´¨é‡çš„WebText2ã€Books1ã€Books2å’ŒWikipediaã€‚GPT-3æ ¹æ®æ•°æ®é›†çš„ä¸åŒè´¨é‡èµ‹äºˆäº†ä¸åŒçš„æƒå€¼ï¼Œæƒå€¼è¶Šé«˜çš„åœ¨è®­ç»ƒçš„æ—¶å€™è¶Šå®¹æ˜“æŠ½æ ·åˆ°ï¼ˆè§ä¸‹å›¾ï¼‰ã€‚ä¸ºäº†æ¸…ç†è„æ•°æ®ï¼ŒOpenAIåšäº†ä»¥ä¸‹çš„æ•°æ®å¤„ç†:

        - ä½¿ç”¨é«˜è´¨é‡æ•°æ®ä½œä¸ºæ­£ä¾‹ï¼Œè®­ç»ƒLRåˆ†ç±»ç®—æ³•ï¼Œå¯¹ CommonCrawl çš„æ‰€æœ‰æ–‡æ¡£åšåˆæ­¥è¿‡æ»¤ï¼›
        - åˆ©ç”¨å…¬å¼€çš„ç®—æ³•åšæ–‡æ¡£å»é‡ï¼Œå‡å°‘å†—ä½™æ•°æ®ï¼›
        - åŠ å…¥å·²çŸ¥çš„é«˜è´¨é‡æ•°æ®é›†ï¼›

        æœ€ç»ˆå¤„ç†å®Œæˆåä½¿ç”¨çš„æ•°æ®è§„æ¨¡çº¦570Gã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b28940a5d66449cf9e9e415560b2bfe8.png#pic_center)
        å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåœ¨å®é™…å®éªŒè¿‡ç¨‹ä¸­ï¼Œå¯¹ä¸åŒæ•°æ®é›†æŒ‰ç…§ä¸€å®šçš„æ¯”ä¾‹è¿›è¡Œé‡‡æ ·ï¼Œè¿™ä¸ªæ¯”ä¾‹ä¸æ˜¯æŒ‰ç…§åŸå§‹æ•°æ®é‡å¤šå°‘æ¥åˆ’åˆ†çš„ï¼Œä¸ç„¶è¿™é‡ŒåŸºæœ¬é‡‡æ ·åˆ°çš„å°±éƒ½æ˜¯Common Crawlçš„æ•°æ®äº†ï¼Œå¯ä»¥çœ‹åˆ°è¿™é‡ŒCommon Crawlçš„æ•°æ®é‡æ¯”å…¶ä»–å‡ ä¸ªå¤šå¾ˆå¤šã€‚è¿›è¡Œé‡‡æ ·çš„åŸå› ä¸»è¦è€ƒè™‘åˆ°ï¼Œå°±ç®—åšäº†ä¸€äº›æ•°æ®æ¸…æ´—è¿˜æ˜¯è§‰å¾—Common Crawlçš„æ•°æ®è´¨é‡ä¸å¦‚å…¶ä»–å‡ ä¸ªã€‚æœ€ç»ˆé‡‡æ ·çš„æ—¶å€™ï¼Œè™½ç„¶Common Crawlçš„æ•°æ®é‡æ˜¯å…¶ä»–å‡ ä¸ªæ•°æ®é›†çš„ä¸Šç™¾å€ï¼Œä½†æ˜¯å®é™…å æ¯”æ˜¯60%ï¼Œæœ‰40%çš„æ•°æ®æ˜¯èƒ½å¤Ÿä¿è¯è´¨é‡çš„ã€‚

    - **GPT-3çš„ç‰¹ç‚¹**:

        - **ä¼˜ç‚¹**: GPT-3çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸éœ€è¦å¾®è°ƒï¼Œåªéœ€è¦åœ¨è¾“å…¥åºåˆ—é‡Œç”¨è‡ªç„¶è¯­è¨€è¡¨è¿°ä»»åŠ¡è¦æ±‚ï¼Œå°±å¯ä»¥è®©æ¨¡å‹æ‰§è¡Œä¸åŒçš„å­ä»»åŠ¡ã€‚GPT-3åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†SOTAï¼Œå¹¶ä¸”éªŒè¯äº†æ¨¡å‹è§„æ¨¡è¶Šå¤§ã€ä»»åŠ¡æ•ˆæœè¶Šå¥½ï¼Œä¸”å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼ŒGPT-3çš„Few-Shotä¼˜äºOne-Shotå’ŒZero-Shotã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/da96b6d40b2c4523b21c63d0b87d1fdd.jpeg#pic_center)

        - **ç¼ºç‚¹**:

            - ç”Ÿæˆçš„å†…å®¹å­˜åœ¨é‡å¤æˆ–ä¸åˆç†çš„å¥å­ã€æ®µè½ï¼Œç¼ºä¹å¸¸è¯†ï¼Œåœ¨ä¸€äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸€èˆ¬ï¼Œç”šè‡³å’Œéšæœºåˆ¤æ–­å·®ä¸å¤šï¼›
            - æ¨¡å‹ç»“æ„ä½¿ç”¨çš„Transformerè§£ç å™¨æ˜¯ä¸€ä¸ªå•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥åœ¨ä¸€äº›éœ€è¦åŒå‘ç†è§£çš„NLPä»»åŠ¡ï¼ˆæ¯”å¦‚æ–‡æœ¬è•´å«ï¼‰ä¸Šè¡¨ç°ä¸ä½³ï¼›
            - è¯­è¨€æ¨¡å‹åº•å±‚åŸç†è¿˜æ˜¯æ ¹æ®å‰åºè¯å…ƒé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒï¼Œæ²¡æœ‰è€ƒè™‘ä¸åŒè¯å…ƒçš„æƒé‡ï¼›
            - æ¨¡å‹è§„æ¨¡å¤ªå¤§ï¼Œè®¡ç®—èµ„æºæˆæœ¬è¾ƒé«˜ï¼Œåç»­çš„ä¸€ä¸ªæ–¹å‘æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼›
            - å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¼ºï¼›
            - æ­¤å¤–ï¼Œä½œè€…è¿˜å±•æœ›äº†ä¸€ä¸‹GPT-3å¯èƒ½å¸¦æ¥çš„ç¤¾ä¼šå½±å“ã€‚æ¯”å¦‚å®ƒå¯èƒ½è¢«æ‹¿æ¥ç”Ÿæˆå‡æ–°é—»ã€åƒåœ¾é‚®ä»¶ï¼Œä»¥åŠè®ºæ–‡é€ å‡ã€‚ç”±äºGPT-3 çš„è®­ç»ƒæ•°æ®æ¥è‡ªç½‘ç»œï¼Œå…¶ä¸­åŒ…å«äº†ä¸€äº›æ€§åˆ«ã€å®—æ•™ã€ç§æ—æ­§è§†çš„ä¿¡æ¯ï¼Œå¯¼è‡´GPT-3ç”Ÿæˆçš„æ–‡æœ¬ä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ã€‚

#### 1.2 InstructGPT

â€ƒâ€ƒGPT-3è™½ç„¶åœ¨å„å¤§NLPä»»åŠ¡ä»¥åŠæ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ä¸Šä»¤äººæƒŠè‰³ï¼Œä½†æ˜¯ä»–ä»ç„¶è¿˜æ˜¯ä¼šç”Ÿæˆä¸€äº›å¸¦æœ‰åè§çš„ï¼Œä¸çœŸå®çš„ï¼Œæœ‰å®³çš„é€ æˆè´Ÿé¢ç¤¾ä¼šå½±å“çš„ä¿¡æ¯ï¼Œè€Œä¸”å¾ˆå¤šæ—¶å€™ï¼Œä»–å¹¶ä¸æŒ‰äººç±»å–œæ¬¢çš„è¡¨è¾¾æ–¹å¼å»è¯´è¯ã€‚åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼ŒOpenAIæå‡ºäº†ä¸€ä¸ªæ¦‚å¿µâ€œAlignmentâ€ï¼Œæ„æ€æ˜¯æ¨¡å‹è¾“å‡ºä¸äººç±»çœŸå®æ„å›¾å¯¹é½ï¼Œç¬¦åˆäººç±»åå¥½ã€‚å› æ­¤ï¼Œä¸ºäº†è®©æ¨¡å‹è¾“å‡ºä¸ç”¨æˆ·æ„å›¾æ›´åŠ å¯¹é½ï¼Œå°±æœ‰äº†InstructGPTè¿™ä¸ªå·¥ä½œ[ã€ŠTraining language models to follow instructionswith human feedbackã€‹](https://arxiv.org/pdf/2203.02155.pdf)ã€‚InstructGPTæå‡ºäº†ä¸€ä¸ªç†æƒ³åŒ–è¯­è¨€æ¨¡å‹çš„ä¸‰å¤§ç›®æ ‡: **helpful**ï¼ˆèƒ½å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ï¼‰ã€**honest**ï¼ˆä¸èƒ½æé€ äº‹å®ï¼Œä¸èƒ½è¯¯å¯¼ç”¨æˆ·ï¼‰ã€**harmless**ï¼ˆä¸èƒ½å¯¹ç”¨æˆ·æˆ–ç¯å¢ƒé€ æˆç‰©ç†ã€ç²¾ç¥ã€ç¤¾ä¼šå±‚é¢çš„ä¼¤å®³ï¼‰ã€‚
â€ƒâ€ƒä¸ºäº†å®ç°ä¸Šè¿°çš„ç›®æ ‡ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºäººç±»åé¦ˆæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªç”¨æˆ·çš„æŒ‡ç¤ºï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„è´¨é‡å’Œå¯ä¿¡åº¦ã€‚åŸºæœ¬æµç¨‹åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤:

- **æ­¥éª¤ä¸€**: ä»OpenAI APIä¸­è·å–ç”¨æˆ·æäº¤çš„æŒ‡ä»¤promptï¼ˆåé¢æåˆ°çš„æŒ‡ä»¤promptéƒ½å¯ç†è§£ä¸ºé—®é¢˜ï¼‰å’Œæ ‡æ³¨äººå‘˜ç¼–å†™çš„æŒ‡ä»¤promptä¸­æ”¶é›†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä»æ”¶é›†åˆ°çš„æŒ‡ä»¤promptæ•°æ®é›†ä¸­å–å‡ºä¸€äº›æŒ‡ä»¤promptï¼Œç„¶åè®©æ ‡æ³¨äººå‘˜æ ‡æ³¨å¯¹åº”çš„ç­”æ¡ˆï¼Œå†ç”¨è¿™äº›æ•°æ®å¾®è°ƒGPT-3å¾—åˆ°SFTæ¨¡å‹ï¼›
- **æ­¥éª¤äºŒ**: è¾“å…¥æŒ‡ä»¤promptï¼Œè®©æ¨¡å‹è¾“å‡ºå‡ ä¸ªç­”æ¡ˆï¼Œç„¶åè®©æ ‡æ³¨äººå‘˜å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œç”¨è¿™äº›æ’åºæ•°æ®è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹RMï¼Œèƒ½å¤Ÿå¯¹ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ï¼Œæ‰“åˆ†çš„å¤§å°é¡ºåºæ»¡è¶³è®­ç»ƒä½¿ç”¨çš„è¿™äº›ç­”æ¡ˆçš„é¡ºåºï¼›
- **æ­¥éª¤ä¸‰**: å†è¾“å…¥ä¸€äº›æŒ‡ä»¤promptè®©STFå»ç”Ÿæˆä¸€äº›ç­”æ¡ˆï¼ŒæŠŠç­”æ¡ˆæ”¾åˆ°RMé‡Œé¢å»æ‰“åˆ†ï¼Œç„¶åç”¨PPOç®—æ³•å»ä¼˜åŒ–SFTçš„å‚æ•°ä½¿å¾—å®ƒç”Ÿæˆæ›´é«˜çš„åˆ†æ•°ï¼Œæœ€åå¾—åˆ°InstrctGPTã€‚

â€ƒâ€ƒæœ€ç»ˆå¾—åˆ°çš„InstrctGPTç›¸è¾ƒäºGPT-3:

- å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·æŒ‡ç¤ºä¸­éšå«æˆ–æ˜¾å¼åœ°è¡¨è¾¾å‡ºæ¥çš„ç›®æ ‡ã€çº¦æŸå’Œåå¥½ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›å’Œéœ€æ±‚çš„è¾“å‡ºï¼›
- å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºä¸­æä¾›çš„ä¿¡æ¯æˆ–ç»“æ„ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œåˆç†æ¨æ–­æˆ–åˆ›é€ ã€‚
- å¯ä»¥æ›´ç¨³å®šåœ°ä¿æŒè¾“å‡ºè´¨é‡ï¼Œå¹¶å‡å°‘é”™è¯¯æˆ–å¤±è´¥ç‡ï¼›

â€ƒâ€ƒä¸‹é¢è¯¦ç»†ä»‹ç»ä¸‹InstructGPTæ•°æ®é›†æ„å»ºä»¥åŠè®­ç»ƒæµç¨‹ã€‚

- **InstructGPTæ•°æ®é›†æ„å»º**: InstructGPTæ•°æ®é›†æ„å»ºå¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯ä¸ºäº†æ„å»ºåˆå§‹çš„æŒ‡ä»¤promptæ•°æ®é›†ï¼Œå…·ä½“åšæ³•æ˜¯è®©æ ‡æ³¨äººå‘˜æ„å»ºä¸‹é¢ä¸‰ç§prompt:

    - **Plain**: åªè¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸€ä¸ªä»»æ„çš„ä»»åŠ¡ï¼ˆä¹Ÿå°±æ˜¯æŒ‡ä»¤promotï¼‰ï¼Œå¹¶ä¿è¯ä»»åŠ¡æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼›
    - **Few-shot**: è¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸€ä¸ªæŒ‡ä»¤promptï¼Œå¹¶ç»™å‡ºå¤šä¸ªç¬¦åˆè¯¥æŒ‡ä»¤çš„query/responseç»„åˆï¼›
    - **User-based**: åŸºäºç”¨æˆ·æœŸæœ›OpenAI APIä¿±å¤‡çš„èƒ½åŠ›æ‰€æå‡ºçš„ä¸€äº›ç”¨ä¾‹ï¼Œè¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸è¿™äº›ç”¨ä¾‹ç›¸å…³çš„æŒ‡ä»¤promptã€‚
        Â 

    â€ƒâ€ƒåŸºäºä¸Šé¢ä¸‰ç§æŒ‡ä»¤promptï¼ŒOpenAIå›¢é˜Ÿè®­ç»ƒäº†åˆå§‹ç‰ˆæœ¬çš„InstructGPTæ¨¡å‹ï¼Œç„¶åå°†è¿™ä¸ªInstructGPTæ¨¡å‹æ”¾åˆ°Playgroundï¼ˆPlaygroundå¯ç†è§£ä¸ºæµ‹è¯•APIï¼Œéç”Ÿäº§APIï¼‰é‡Œä¾›ç”¨æˆ·ä½¿ç”¨ï¼Œè¿™å°±å¼•ç”³è‡³InstructGPTæ•°æ®é›†æ„å»ºçš„ç¬¬äºŒä¸ªé˜¶æ®µ: ç”¨æˆ·åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œä¼šç»§ç»­é—®ä¸€äº›é—®é¢˜ï¼ŒOpenAIå›¢é˜Ÿå°†è¿™äº›é—®é¢˜æ”¶é›†å›æ¥ï¼Œå¹¶è¿›è¡Œè¿‡æ»¤ç­‰æ“ä½œï¼Œå…·ä½“æ¥è¯´ï¼Œå°†æ¯ä¸ªç”¨æˆ·IDçš„å¯¹åº”çš„æŒ‡ä»¤promptæ•°é‡é™åˆ¶ä¸º200ä¸ªï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸ªäººä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ·IDæ‹†åˆ†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ï¼ˆåŒä¸€ä¸ªç”¨æˆ·é—®é¢˜ä¼šæ¯”è¾ƒç±»ä¼¼ï¼Œä¸é€‚åˆåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ï¼‰ã€‚å¯ä»¥çœ‹å‡ºï¼Œç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µæ˜¯ä¸€ä¸ªå¾ªç¯è¿‡ç¨‹: å…ˆæ‹¿éƒ¨åˆ†æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç„¶åé€šè¿‡æ¨¡å‹è·å–æ–°æ•°æ®ï¼Œå†ç”¨æ–°æ•°æ®ç»§ç»­ä¼˜åŒ–æ¨¡å‹ï¼Œè¿™ç§æ€è·¯ä¹Ÿå¾ˆé€‚åˆæˆ‘ä»¬ä»¥åçš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚
    â€ƒâ€ƒè‡³æ­¤ï¼Œé€šè¿‡ä¸Šè¿°ä¸¤é˜¶æ®µçš„å¤„ç†ï¼ŒOpenAIå›¢é˜Ÿå·²ç»è·å–äº†ä¸€å®šé‡çš„æŒ‡ä»¤promptï¼ˆåŒ…æ‹¬æ ‡æ³¨äººå‘˜æ„å»ºçš„promptä»¥åŠä»ç”¨æˆ·ä¾§æ”¶é›†çš„promptï¼‰ï¼Œæ¥ä¸‹æ¥å³æ˜¯é’ˆå¯¹ä¸åŒè®­ç»ƒä»»åŠ¡æ„å»ºä¸åŒçš„æ•°æ®é›†ï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸‰é˜¶æ®µã€‚åŸºäºç¬¬äºŒé˜¶æ®µè·å–çš„æŒ‡ä»¤promptï¼Œæ„å»ºä¸‰ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºåç»­çš„ä¸‰ä¸ªè®­ç»ƒä»»åŠ¡SFTã€RMã€RLã€‚

    - **SFT Dataset**: æ ‡æ³¨äººå‘˜æ ¹æ®æŒ‡ä»¤promptæ„é€ ç­”æ¡ˆï¼Œå°†promptå’Œç­”æ¡ˆæ‹¼åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€æ®µå¯¹è¯ï¼ˆpromptï¼Œanswerï¼‰ï¼Œç”¨äºè®­ç»ƒSFTæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦13kï¼ŒåŒ…æ‹¬äººå·¥æ ‡è®°prompt+ç”¨æˆ·æ”¶é›†promptã€‚
    - **RM Dataset**: å…ˆå°†promptè¾“å…¥SFTæ¨¡å‹ï¼Œæ ‡æ³¨äººå‘˜å†å¯¹SFTæ¨¡å‹è¾“å‡ºçš„ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œç„¶åç”¨è¿™äº›æ’åºæ•°æ®ï¼ˆpromptï¼ŒRankï¼‰è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹RMï¼Œèƒ½å¤Ÿå¯¹ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ã€‚æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦33kï¼ŒåŒ…æ‹¬äººå·¥æ ‡è®°prompt+ç”¨æˆ·æ”¶é›†promptã€‚
    - **RL Dataset**: æ­¤éƒ¨åˆ†æ•°æ®é›†ä¸éœ€è¦æ ‡æ³¨ï¼Œåªéœ€è¦ä»æŒ‡ä»¤promptæ•°æ®é›†é‡Œé¢è·å–éƒ¨åˆ†æŒ‡ä»¤promptï¼Œç„¶åä½¿ç”¨SFTå’ŒRMæ¨¡å‹åˆ†åˆ«å¾—åˆ°answerå’ŒRMç»™å‡ºçš„åˆ†æ•°ï¼Œæ„æˆä¸‰å…ƒç»„ï¼ˆpromptï¼Œanswerï¼ŒRMç»™å‡ºçš„åˆ†æ•°ï¼‰ï¼Œç”¨äºè¿›ä¸€æ­¥é‡‡ç”¨PPOç®—æ³•å¾®è°ƒSFTæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦31kï¼ŒåªåŒ…æ‹¬ç”¨æˆ·æ”¶é›†promptã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/dd5b7ba2c2714e91a38f4ef6f9ccc705.png#pic_center)

    â€ƒâ€ƒSFT Datasetå’ŒRM Datasetéƒ½éœ€è¦äººå·¥æ ‡æ³¨ï¼ŒåŒºåˆ«åœ¨äºå‰è€…çš„ç”Ÿæˆå¼çš„æ ‡æ³¨è¦æ¯”åè€…çš„åˆ¤åˆ«å¼çš„æ ‡æ³¨è´µå¾ˆå¤šï¼ŒåŒæ ·çš„æ ‡æ³¨æ—¶é—´å’Œæˆæœ¬ï¼Œè”åˆå‰è€…å’Œåè€…å¾—åˆ°çš„æ•°æ®è¦æ¯”åªç”¨å‰è€…å¾—åˆ°çš„æ•°æ®å¤šå¾ˆå¤šï¼Œåœ¨è¿™ä¸Šé¢è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ€§èƒ½å¯èƒ½ä¼šå¥½ä¸€äº›ã€‚

- **InstructGPTè®­ç»ƒæµç¨‹**: ä¸Šæ–‡å·²ç»ä»‹ç»ï¼Œå…³äºInstructGPTçš„è®­ç»ƒæµç¨‹ï¼Œè®ºæ–‡ä¸­åˆ†ä¸ºäº†ä¸‰ä¸ªæ­¥éª¤: æœ‰ç›‘ç£å¾®è°ƒï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å®é™…ä¸Šå¯ä»¥æŠŠå®ƒæ‹†åˆ†æˆä¸¤ç§æŠ€æœ¯æ–¹æ¡ˆï¼Œä¸€ä¸ªæ˜¯æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä¸€ä¸ªæ˜¯åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œä¸‹é¢æˆ‘ä»¬ç®€å•ä»‹ç»è¿™ä¸¤ç§æŠ€æœ¯æ–¹æ¡ˆã€‚
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/8d822267b9b04a08bb20b999216c284f.png#pic_center)

    - **æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**:  ä»¥GPT-3æ¨¡å‹ä¸ºåº•åº§ï¼Œåœ¨æ ‡æ³¨å¥½çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼ˆé—®é¢˜+ç­”æ¡ˆï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œè¿­ä»£è½®æ•°ä½¿ç”¨16ä¸ªepochï¼Œå­¦ä¹ ç‡ä½¿ç”¨ä½™å¼¦è¡°å‡ï¼Œæ¨¡å‹æ®‹å·®è¿æ¥dropoutç‡ä¸º0.2ã€‚ç”±äºåªæœ‰13000ä¸ªæ•°æ®ï¼Œ1ä¸ªepochå°±è¿‡æ‹Ÿåˆï¼Œä¸è¿‡è®ºæ–‡ä¸­è¯æ˜äº†è¿™ä¸ªæ¨¡å‹è¿‡æ‹Ÿåˆä¹Ÿæ²¡ä»€ä¹ˆå…³ç³»ï¼Œç”šè‡³è®­ç»ƒæ›´å¤šçš„epochå¯¹åç»­æ˜¯æœ‰å¸®åŠ©çš„ï¼Œæœ€ç»ˆè®­ç»ƒäº†16ä¸ªepochã€‚
    - **åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**: æ­¤éƒ¨åˆ†åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯RMæ¨¡å‹è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨PPOç®—æ³•ç»§ç»­å¾®è°ƒSFTæ¨¡å‹ï¼Œä¸¤é˜¶æ®µç›¸ç»“åˆï¼Œå³æ˜¯RLHFè¿‡ç¨‹ã€‚
        - **å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰**: æ¨¡å‹ç»“æ„æ˜¯æŠŠSFTæ¨¡å‹æœ€åçš„unembeddingå±‚ï¼ˆå°±æ˜¯å°†æ¨¡å‹è¾“å‡ºçš„token embeddingè½¬æ¢ä¸ºlogitsçš„é‚£ä¸€å±‚ï¼‰å»æ‰ï¼Œå³æœ€åä¸€å±‚ä¸ç”¨softmaxï¼Œæ”¹æˆä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¿™æ ·è®­ç»ƒå¥½çš„RMæ¨¡å‹å°±å¯ä»¥åšåˆ°è¾“å…¥é—®é¢˜+ç­”æ¡ˆï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡çš„åˆ†æ•°ã€‚RMæ¨¡å‹ä½¿ç”¨6Bï¼Œè€Œä¸æ˜¯175Bï¼Œä¸»è¦åŸå› æ˜¯:

            - èŠ‚çœè®¡ç®—ï¼Œæ›´ä¾¿å®œï¼›
            - å¤§æ¨¡å‹175B-RMä¸ç¨³å®šï¼ˆå¤§æ¨¡å‹çš„é€šç—…ï¼Œæ¨¡å‹å‚æ•°å¾ˆå¤šï¼Œå¾ˆéš¾æ”¶æ•›ï¼‰ï¼Œå› æ­¤ä¸å¤ªé€‚åˆåœ¨RLæœŸé—´ç”¨ä½œå€¼å‡½æ•°ã€‚
                Â 

            å‰æ–‡å·²ç»ä»‹ç»ï¼ŒRMæ•°æ®é›†åœ¨æ ‡æ³¨é˜¶æ®µï¼Œæ ‡æ³¨äººå‘˜è¢«è¦æ±‚å¯¹æ¯ä¸€ä¸ªpromptä¸‹çš„ä¸åŒå›ç­”è¿›è¡Œæ’åºã€‚å¦‚ä¸‹å›¾ï¼ŒæŸä¸ªpromptä¸‹æœ‰Aã€Bã€Cä¸‰ä¸ªå›ç­”ï¼Œæ ‡æ³¨äººå‘˜è®¤ä¸ºA>B>Cã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œå‡è®¾ä¸€ä¸ªpromptä¸‹æœ‰Kä¸ªå›ç­”ï¼Œåˆ™ä¸¤ä¸¤å›ç­”ä¸€ç»„ï¼Œç»„æˆä¸€æ¡è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚ï¼ˆprompt, A, Bï¼‰ï¼Œåˆ™ä¸€å…±æœ‰ C k 2 C\_{k}^{2} Ck2â€‹æ¡è®­ç»ƒæ•°æ®ã€‚è¿™äº›è®­ç»ƒæ•°æ®å°†ç»„æˆä¸€ä¸ªbatchï¼Œé€šè¿‡æ„é€ å¹¶æœ€å°åŒ–Pairwise Ranking Lossçš„æ–¹æ³•ï¼Œæ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæ•´ä½“è¿‡ç¨‹å¦‚ä¸‹: å…ˆä»¥RM Datasetä¸­çš„æŒ‡ä»¤promptä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ç¬¬ä¸€é˜¶æ®µå¾®è°ƒå¥½çš„SFTæ¨¡å‹ï¼Œç”ŸæˆKä¸ªä¸åŒçš„å›ç­”ï¼Œå½¢æˆ<prompt,answer1>,<prompt,answer2>â€¦.<prompt,answerK>æ•°æ®ã€‚ç„¶åï¼Œæ ‡æ³¨äººå‘˜æ ¹æ®ç›¸å…³æ€§ã€ä¿¡æ¯æ€§å’Œæœ‰å®³ä¿¡æ¯ç­‰æ ‡å‡†ï¼Œå¯¹Kä¸ªç»“æœè¿›è¡Œæ’åºï¼Œç”Ÿæˆæ’åºç»“æœæ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨è¿™ä¸ªæ’åºç»“æœæ•°æ®è¿›è¡Œpair-wise learning to rankæ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒRMæ¨¡å‹ã€‚RMæ¨¡å‹æ¥å—ä¸€ä¸ªè¾“å…¥<prompt,answer>ï¼Œç»™å‡ºè¯„ä»·å›ç­”è´¨é‡é«˜ä½çš„å¥–åŠ±åˆ†æ•°scoreã€‚å¯¹äºä¸€å¯¹è®­ç»ƒæ•°æ®<answer1,answer2>ï¼Œå‡è®¾äººå·¥æ’åºä¸­answer1æ’åœ¨answer2å‰é¢ï¼Œé‚£ä¹ˆlosså‡½æ•°åˆ™é¼“åŠ±RMæ¨¡å‹å¯¹<prompt,answer1>çš„æ‰“åˆ†è¦æ¯”<prompt,answer2>çš„æ‰“åˆ†è¦é«˜ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/4bf95ac06149479b8bb5189c7f38cc48.jpeg#pic_center)
            æ¥ä¸‹æ¥æˆ‘ä»¬æ ¹æ®æ¨¡å‹çš„æŸå¤±å‡½æ•°Pairwise Ranking Lossï¼Œè¯¦ç»†äº†è§£ä¸‹RMæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚Pairwise Ranking Lossè¡¨è¾¾å¼å¦‚ä¸‹æ‰€ç¤º:
            l o s s ( Î¸ ) = âˆ’ 1 C k 2 E ( x , y w , y l ) âˆ¼ D \[ l o g ( Ïƒ ( r Î¸ ( x , y w ) âˆ’ r Î¸ ( x , y l ) ) ) \] Â  loss(\\theta)=-\\frac{1}{C\_{k}^{2}}E\_{(x,y\_{w},y\_{l})\\sim D}\[log(\\sigma(r\_{\\theta}(x,y\_{w})-r\_{\\theta}(x,y\_{l})))\]\\ loss(Î¸)\=âˆ’Ck2â€‹1â€‹E(x,ywâ€‹,ylâ€‹)âˆ¼Dâ€‹\[log(Ïƒ(rÎ¸â€‹(x,ywâ€‹)âˆ’rÎ¸â€‹(x,ylâ€‹)))\]Â 
            å…¶ä¸­ï¼Œ

            - x x xè¡¨ç¤ºæŸä¸ªpromptï¼›
            - y w y\_{w} ywâ€‹å’Œ y l y\_{l} ylâ€‹åˆ†åˆ«è¡¨ç¤ºè¯¥promptä¸‹çš„ä»»æ„ä¸€å¯¹å›ç­”ï¼Œå¹¶ä¸”å‡è®¾æ ‡æ³¨ä¸­ y w y\_{w} ywâ€‹çš„æ’åºæ˜¯é«˜äº y l y\_{l} ylâ€‹çš„ï¼›
            - D D Dè¡¨ç¤ºè¯¥promptä¸‹äººç±»æ ‡æ³¨æ’åºçš„æ‰€æœ‰ä¸¤ä¸¤å›ç­”ç»„åˆï¼›
            - r Î¸ r\_{\\theta} rÎ¸â€‹è¡¨ç¤ºå¥–åŠ±æ¨¡å‹ï¼›
            - Ïƒ \\sigma Ïƒè¡¨ç¤º s i g m o i d sigmoid sigmoidå‡½æ•°ã€‚
                Â 

            ä¹Ÿå¯ä»¥å‚è€ƒä¸‹å›¾ï¼ˆæœ‰ä¸ªå°é”™è¯¯ï¼Œå°±æ˜¯ s i g m o i d sigmoid sigmoidå‡½æ•°æ˜¯å°†å€¼æ˜ å°„è‡³ ( 0 ï¼Œ 1 ) (0ï¼Œ1) (0ï¼Œ1)ï¼Œè€Œä¸æ˜¯ ( âˆ’ 1 ï¼Œ 1 ) (-1ï¼Œ1) (âˆ’1ï¼Œ1)ï¼Œä¸è¿‡æ— ä¼¤å¤§é›…ï¼‰ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b35eac803dbf4336b3a40eadd7881bee.png#pic_center)
            Â 
            è®ºæ–‡ä¸­æœŸæœ›å½“å›ç­” y y yçš„æ’åºç›¸å¯¹è¾ƒé«˜æ—¶ï¼Œ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)çš„å¾—åˆ†ä¹Ÿèƒ½è¶Šé«˜ã€‚ä¸ºäº†ä¸è®©Kçš„ä¸ªæ•°å½±å“è®­ç»ƒæ¨¡å‹ï¼Œè®ºæ–‡ä¸­åœ¨å‰é¢ä¹˜ä¸Š 1 C k 2 \\frac{1}{C\_{k}^{2}} Ck2â€‹1â€‹ï¼Œå°†losså¹³å‡åˆ°æ¯ä¸€ä¸ªç­”æ¡ˆç»„åˆä¸Šã€‚é™¤æ­¤ä¹‹å‰ï¼Œè¿˜æœ‰å‡ ç‚¹éœ€è¦æˆ‘ä»¬æ³¨æ„:

            - **Kå€¼çš„é€‰æ‹©**: è®ºæ–‡ä¸­é‡‡ç”¨äº†K=9ï¼Œè€Œä¸æ˜¯æ›´å°çš„å€¼ï¼Œæ¯”å¦‚4ã€‚åŸå› åœ¨äº:
                - è¿›è¡Œæ ‡æ³¨çš„æ—¶å€™ï¼Œéœ€è¦èŠ±å¾ˆå¤šæ—¶é—´å»ç†è§£é—®é¢˜ï¼Œä½†ç­”æ¡ˆå’Œç­”æ¡ˆæ¯”è¾ƒç›¸è¿‘ï¼Œå¯¹9ä¸ªç­”æ¡ˆåšæ’åºç›¸è¾ƒäºå¯¹4ä¸ªç­”æ¡ˆåšæ’åºå¤šèŠ±çš„æ—¶é—´ä¸åˆ°ä¸€å€ã€‚åŒæ—¶K=9ç”Ÿæˆçš„é—®ç­”å¯¹æ˜¯K=4çš„6å€ï¼ˆ C 9 2 {C\_{9}^{2}} C92â€‹\=36ï¼Œ C 4 2 {C\_{4}^{2}} C42â€‹\=6ï¼‰ï¼Œéå¸¸åˆ’ç®—ï¼›
                - K=9æ—¶ï¼Œæ¯æ¬¡è®¡ç®—RMæ¨¡å‹çš„lossæ—¶éœ€è¦éƒ½æœ‰36é¡¹ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)è¦è®¡ç®—ï¼Œè¿™ä¸ªè®¡ç®—æ¯”è¾ƒè´µï¼Œä½†å¯ä»¥é€šè¿‡é‡å¤åˆ©ç”¨ä¹‹å‰ç®—è¿‡çš„å€¼ï¼Œä½¿å¾—åªè¦è®¡ç®—9æ¬¡å°±è¡Œï¼Œä¹Ÿå°±æ˜¯è¯´å°†9ä¸ªç­”æ¡ˆå¯¹åº”çš„ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)è®¡ç®—å‡ºæ¥ä¹‹åï¼Œåé¢è®¡ç®—æŸå¤±æ—¶ï¼Œä¸¤ä¸¤ç»„åˆå°±å¯ä»¥äº†ï¼Œè¿™æ ·å°±å¯ä»¥çœä¸‹å¾ˆå¤šæ—¶é—´ã€‚
            - **è®­ç»ƒæ•°æ®è¾“å…¥æ¨¡å¼é€‰æ‹©**: è®ºæ–‡ä¸­å°† ( x , y w , y l ) âˆ¼ D (x,y\_{w},y\_{l})\\sim D (x,ywâ€‹,ylâ€‹)âˆ¼Då½“æˆä¸€ä¸ªbatchåŒæ—¶é€å…¥æ¨¡å‹ï¼Œè€Œä¸æ˜¯å°†å•æ¡ ( x , y w , y l ) (x,y\_{w},y\_{l}) (x,ywâ€‹,ylâ€‹)æ•°æ®åˆ†åˆ«é€å…¥æ¨¡å‹ï¼ŒåŸå› åœ¨äº:
                - ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆã€‚å¯¹äºæŸä¸€å¯¹ ( x , y w , y l ) (x,y\_{w},y\_{l}) (x,ywâ€‹,ylâ€‹)ä¸­çš„ä¸€ä¸ªæ ·æœ¬ ( x , y ) (x,y) (x,y)ï¼Œç”¨batchæ–¹å¼æ—¶ï¼Œå®ƒåªå‚ä¸ä¸€æ¬¡æ¢¯åº¦è®¡ç®—ï¼›ç”¨å•æ¡æ–¹å¼æ—¶ï¼Œå®ƒéœ€è¦å‚ä¸K-1æ¬¡æ¢¯åº¦è®¡ç®—ã€‚æ¨¡å‹è¶…è¿‡ä¸€ä¸ªepochåä¼šè¿‡æ‹Ÿåˆï¼Œåœ¨ä¸€ä¸ªepochä¸­åå¤ä½¿ç”¨æ•°æ®æ›´ä¼šè¿‡æ‹Ÿåˆäº†ï¼›
                - ä¸ºäº†æå‡è®¡ç®—æ•ˆç‡ã€‚åœ¨æ¨¡å‹forwardçš„è¿‡ç¨‹ä¸­ï¼Œæœ€è€—æ—¶çš„æ­¥éª¤æ˜¯è®¡ç®— r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)ã€‚ç”¨batchæ–¹å¼æ—¶ï¼Œè¯¥è®¡ç®—åªéœ€æ‰§è¡ŒKæ¬¡ï¼ˆå› ä¸ºæ¨¡å‹å‚æ•°æ²¡æœ‰æ›´æ–°ï¼Œç›¸åŒçš„(x, y)å¯ä»¥é‡å¤ä½¿ç”¨ï¼‰ï¼›é‡‡ç”¨å•æ¡æ–¹å¼æ—¶ï¼Œéœ€è¦è®¡ç®—K(K-1)æ¬¡ï¼ˆå› ä¸ºä¸€æ¡è®¡ç®—æ›´æ–°ä¸€æ¬¡æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°ï¼Œç›¸åŒçš„(x,y)éœ€è¦é‡æ–°è®¡ç®— r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)ï¼‰ã€‚å› æ­¤ï¼ŒKè¶Šå¤§æ—¶ï¼Œé‡‡ç”¨batchçš„æ–¹å¼è¶Šåˆ’ç®—ï¼Œå®ƒåœ¨ä¿è¯ç›¸å¯¹æ’åºä¿¡æ¯ä¸°å¯Œçš„åŒæ—¶ï¼ŒåˆèŠ‚çœäº†è®¡ç®—æ•ˆç‡ã€‚
            - **è®­ç»ƒepoché€‰æ‹©**: æ¨¡å‹è®­ç»ƒè¶…è¿‡ä¸€ä¸ªepochåä¼šè¿‡æ‹Ÿåˆï¼Œæ•…åªè®­ç»ƒä¸€ä¸ªepochã€‚
        - **å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼ˆRLï¼‰**: è¿™ä¸ªé˜¶æ®µå…ˆå°†RLæ¨¡å‹çš„æƒé‡åˆå§‹åŒ–ä¸ºSFTæ¨¡å‹çš„æƒé‡ï¼Œç„¶åé€šè¿‡æ”¹è‰¯åçš„PPOç®—æ³•ï¼ˆPPO-ptxç®—æ³•ï¼‰ç»§ç»­å¯¹RLæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå¾—åˆ°InstructGPTã€‚å¼ºåŒ–å­¦ä¹ çš„å¤§è‡´æµç¨‹å¯ä»¥æ€»ç»“ä¸º: æ¨¡å‹åœ¨åšå‡ºè¡ŒåŠ¨åï¼Œéœ€è¦äººæ¥å¯¹æ¨¡å‹è¿›è¡Œåé¦ˆï¼Œç„¶åæ¨¡å‹åšå‡ºå¯¹åº”çš„æ›´æ–°ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­è®­ç»ƒRMå°±æ˜¯ä¸ºäº†å­¦ä¹ äººæ¥å¯¹æ¨¡å‹è¿›è¡Œåé¦ˆï¼ŒSFTæ¨¡å‹åœ¨æ‹¿åˆ°promptå¹¶ç”Ÿæˆå¯¹åº”çš„ç­”æ¡ˆåï¼Œç”±RMè¿›è¡Œæ‰“åˆ†ï¼Œå†æ ¹æ®è¿™ä¸ªæ‰“åˆ†å»æ›´æ–°æ¨¡å‹ï¼Œç„¶åç”¨æ›´æ–°çš„æ¨¡å‹ç”Ÿæˆæ–°çš„ç­”æ¡ˆï¼Œå¹¶è¿›è¡Œä¸‹ä¸€æ­¥å­¦ä¹ ï¼Œè¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å‡½æ•° o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)å¦‚ä¸‹æ‰€ç¤ºï¼ŒRLæ¨¡å‹æœ€ç»ˆçš„è®­ç»ƒç›®æ ‡æ˜¯è®© o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)è¶Šå¤§è¶Šå¥½ã€‚
            o b j e c t i v e ( Ï• ) = E ( x , y ) âˆ¼ D Ï€ Ï• R L \[ r Î¸ ( x , y ) âˆ’ Î² l o g ( Ï€ Ï• R L ( y âˆ£ x ) / Ï€ S F T ( y âˆ£ x ) ) \] + Î³ E x âˆ¼ D p r e t r a i n \[ l o g ( Ï€ Ï• R L ( x ) ) \] \\begin{aligned} objective(\\phi)=&E\_{(x,y)\\sim D\_{\\pi\_{\\phi}^{RL}}}\[r\_{\\theta}(x,y)-\\beta log(\\pi\_{\\phi}^{RL}(y|x)/\\pi^{SFT}(y|x))\]+\\\\ &\\gamma E\_{x\\sim D\_{pretrain}}\[log(\\pi\_{\\phi}^{RL}(x))\] \\end{aligned} objective(Ï•)\=â€‹E(x,y)âˆ¼DÏ€Ï•RLâ€‹â€‹â€‹\[rÎ¸â€‹(x,y)âˆ’Î²log(Ï€Ï•RLâ€‹(yâˆ£x)/Ï€SFT(yâˆ£x))\]+Î³Exâˆ¼Dpretrainâ€‹â€‹\[log(Ï€Ï•RLâ€‹(x))\]â€‹
            å…¶ä¸­:

            - Ï€ S F T \\pi^{SFT} Ï€SFT: å³ç¬¬ä¸€é˜¶æ®µï¼Œç»è¿‡supervised fine-tuningçš„GPT-3æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯SFTæ¨¡å‹ï¼›
            - Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹: å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¨¡å‹ç§°åšpolicyï¼Œ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å°±æ˜¯éœ€è¦å­¦ä¹ çš„æ¨¡å‹ï¼Œå³æœ€ç»ˆçš„æ¨¡å‹ã€‚åˆå§‹æ—¶:  Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹\= Ï€ S F T \\pi^{SFT} Ï€SFTï¼›
            - r Î¸ r\_{\\theta} rÎ¸â€‹: å³ç¬¬äºŒé˜¶æ®µè®­ç»ƒçš„RMæ¨¡å‹ã€‚
                Â 

            æ•´ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸Šè¿°çš„ç›®æ ‡å‡½æ•°ï¼Œç°åœ¨åˆ†åˆ«ä»‹ç»ä¸‹ç›®æ ‡å‡½æ•°çš„æ¯ä¸€é¡¹ï¼Œä¹Ÿå¯ä»¥å‚è€ƒä¸‹é¢çš„å›¾ç‰‡:

            - ( x , y ) âˆ¼ D Ï€ Ï• R L (x,y)\\sim D\_{\\pi\_{\\phi}^{RL}} (x,y)âˆ¼DÏ€Ï•RLâ€‹â€‹:  x x xæ˜¯ç¬¬RL Datasetæ•°æ®é›†ä¸­çš„é—®é¢˜ï¼ˆæŒ‡ä»¤promptï¼‰ï¼Œ y y yæ˜¯ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹æ¨¡å‹å¾—åˆ°çš„ç­”æ¡ˆ;
            - r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y): å¯¹é—®é¢˜ x x x+ç­”æ¡ˆ y y yï¼Œè¾“å…¥RMæ¨¡å‹è¿›è¡Œæ‰“åˆ†ï¼Œç›®æ ‡æ˜¯å¸Œæœ›è¿™ä¸ªåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼›
            - Ï€ Ï• R L ( y âˆ£ x ) \\pi\_{\\phi}^{RL}(y|x) Ï€Ï•RLâ€‹(yâˆ£x): é—®é¢˜ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å¾—åˆ°ç­”æ¡ˆ y y yçš„æ¦‚ç‡ï¼Œå…·ä½“æ¥è¯´ Ï€ ( y âˆ£ x ) \\pi(y|x) Ï€(yâˆ£x)æ˜¯æŠŠæ¨¡å‹è¾“å‡º y y yçš„æ¯ä¸€ä¸ªtokenå¯¹åº”çš„softmaxæ¦‚ç‡ç›¸ä¹˜å¾—åˆ°çš„ç»“æœï¼Œä¸‹åŒï¼›
            - Ï€ S F T ( y âˆ£ x ) \\pi^{SFT}(y|x) Ï€SFT(yâˆ£x): é—®é¢˜ x x xé€šè¿‡ Ï€ S F T \\pi^{SFT} Ï€SFTå¾—åˆ°ç­”æ¡ˆ y y yçš„æ¦‚ç‡ï¼›
            - l o g ( Ï€ Ï• R L ( y âˆ£ x ) / Ï€ S F T ( y âˆ£ x ) ) log(\\pi\_{\\phi}^{RL}(y|x)/\\pi^{SFT}(y|x)) log(Ï€Ï•RLâ€‹(yâˆ£x)/Ï€SFT(yâˆ£x)): KLæ•£åº¦ï¼Œå–å€¼èŒƒå›´>=0ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ˜¯å¦ç›¸ä¼¼ï¼ŒKLå€¼è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šä¸ç›¸ä¼¼ï¼Œåˆ†å¸ƒç›¸åŒæ—¶KL=0ã€‚åœ¨æœ¬é˜¶æ®µï¼Œè®ºæ–‡ä¸­å¸Œæœ›å¼ºåŒ–å­¦ä¹ åå¾—åˆ°çš„æ¨¡å‹ï¼Œåœ¨èƒ½å¤Ÿç†è§£äººç±»æ„å›¾çš„åŸºç¡€ä¸Šï¼Œåˆä¸è¦å’Œæœ€åŸå§‹çš„æ¨¡å‹è¾“å‡ºç›¸å·®å¤ªè¿œï¼ˆåœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åï¼Œ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ä¼šå‘ç”Ÿå˜åŒ–ï¼Œ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ y y yä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œè€Œ r Î¸ r\_{\\theta} rÎ¸â€‹æ‰“åˆ†æ¨¡å‹æ˜¯æ ¹æ® Ï€ S F T \\pi^{SFT} Ï€SFTæ¨¡å‹çš„æ•°æ®è®­ç»ƒè€Œæ¥ï¼Œå¦‚æœ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å’Œ Ï€ S F T \\pi^{SFT} Ï€SFTå·®çš„å¤ªå¤šï¼Œåˆ™ä¼šå¯¼è‡´ r Î¸ r\_{\\theta} rÎ¸â€‹çš„åˆ†æ•°è¾“å‡ºä¸å‡†ç¡®ã€‚å› æ­¤éœ€è¦é€šè¿‡KLæ•£åº¦æ¥è®¡ç®— Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒå’Œ Ï€ S F T \\pi^{SFT} Ï€SFTç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¸è¦å·®çš„å¤ªè¿œã€‚ï¼‰ã€‚å‚æ•° Î² \\beta Î²åˆ™è¡¨ç¤ºå¯¹è¿™ç§åå·®çš„å®¹å¿ç¨‹åº¦ã€‚åç¦»è¶Šè¿œï¼Œå°±è¦ä»å¥–åŠ±æ¨¡å‹çš„åŸºç¡€ä¸Šå¾—åˆ°è¶Šå¤šçš„æƒ©ç½šï¼›
            - x âˆ¼ D p r e t r a i n x\\sim D\_{pretrain} xâˆ¼Dpretrainâ€‹:  x x xæ˜¯æ¥è‡ªGPT-3é¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®ï¼›
            - l o g ( Ï€ Ï• R L ( x ) ) log(\\pi\_{\\phi}^{RL}(x)) log(Ï€Ï•RLâ€‹(x)): è¡¨ç¤ºå°†æ¥è‡ªåˆå§‹GPT-3ä¸­çš„æ•°æ®é€å…¥å½“å‰å¼ºåŒ–æ¨¡å‹ä¸‹ï¼ŒåŒæ ·ï¼Œè®ºæ–‡ä¸­å¸Œæœ›åœ¨è®­ç»ƒå¾—åˆ°æ–°æ¨¡å‹ä¹‹åï¼Œä¸èƒ½é™ä½åœ¨åŸå§‹ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå³ä¸èƒ½å¤ªåç¦»åŸå§‹ä»»åŠ¡ï¼Œä¿è¯æ–°æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚ Î³ \\gamma Î³åˆ™æ˜¯å¯¹è¿™ç§åç¦»çš„æƒ©ç½šç¨‹åº¦ã€‚
                ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/ec1c753c065148ecbe75efbbf240120f.png#pic_center)
                Â 

            â€ƒâ€ƒæœ€åå†ç»™å‡ºå¯¹ç›®æ ‡å‡½æ•°çš„ç†è§£ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯ä½¿å¾—ä¸Šè¿°ç›®æ ‡å‡½æ•°è¶Šå¤§è¶Šå¥½ï¼Œé€šè¿‡ä¸Šè¿°ä»‹ç»ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œ o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)å¯åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼ŒRMæ‰“åˆ†éƒ¨åˆ†+KLæ•£åº¦éƒ¨åˆ†+GPT-3é¢„è®­ç»ƒéƒ¨åˆ†:

            - å°†RL Datasetæ•°æ®é›†ä¸­çš„é—®é¢˜ x x xï¼Œé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹æ¨¡å‹å¾—åˆ°ç­”æ¡ˆ y y yï¼›
            - æŠŠä¸€å¯¹ ( x , y ) (x,y) (x,y)é€è¿›RMæ¨¡å‹è¿›è¡Œæ‰“åˆ†ï¼Œå¾—åˆ° r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)ï¼Œå³ç¬¬ä¸€éƒ¨åˆ†æ‰“åˆ†éƒ¨åˆ†ï¼Œè¿™ä¸ªåˆ†æ•°è¶Šé«˜å°±ä»£è¡¨æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆè¶Šå¥½ï¼›
            - åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åï¼Œ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ä¼šå‘ç”Ÿå˜åŒ–ï¼Œ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ y y yä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œè€Œ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)æ‰“åˆ†æ¨¡å‹æ˜¯æ ¹æ® Ï€ S F T \\pi^{SFT} Ï€SFTæ¨¡å‹çš„æ•°æ®è®­ç»ƒè€Œæ¥ï¼Œå¦‚æœ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å’Œ Ï€ S F T \\pi^{SFT} Ï€SFTå·®çš„å¤ªå¤šï¼Œåˆ™ä¼šå¯¼è‡´ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)çš„åˆ†æ•°ä¼°ç®—ä¸å‡†ç¡®ã€‚å› æ­¤éœ€è¦é€šè¿‡KLæ•£åº¦æ¥è®¡ç®— Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒå’Œ Ï€ S F T \\pi^{SFT} Ï€SFTç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¸è¦å·®çš„å¤ªè¿œã€‚æˆ‘ä»¬å¸Œæœ›ä¸¤ä¸ªæ¨¡å‹çš„å·®è·è¶Šå°è¶Šå¥½ï¼Œå³KLæ•£åº¦è¶Šå°è¶Šå¥½ï¼Œå‰é¢éœ€è¦åŠ ä¸€ä¸ªè´Ÿå·ï¼Œä½¿å¾— o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)è¶Šå¤§è¶Šå¥½ã€‚è¿™ä¸ªå°±æ˜¯KLæ•£åº¦éƒ¨åˆ†ï¼›
            - å¦‚æœæ²¡æœ‰ç¬¬ä¸‰éƒ¨åˆ†ï¼Œé‚£ä¹ˆæ¨¡å‹æœ€ç»ˆå¯èƒ½åªå¯¹è¿™ä¸€ä¸ªä»»åŠ¡èƒ½å¤Ÿåšå¥½ï¼Œåœ¨åˆ«çš„ä»»åŠ¡ä¸Šä¼šå‘ç”Ÿæ€§èƒ½ä¸‹é™ã€‚æ‰€ä»¥ç¬¬ä¸‰éƒ¨åˆ†å°±æŠŠåŸå§‹çš„GPT-3ç›®æ ‡å‡½æ•°åŠ äº†ä¸Šå»ï¼Œä½¿å¾—å‰é¢ä¸¤ä¸ªéƒ¨åˆ†åœ¨æ–°çš„æ•°æ®é›†ä¸Šåšæ‹Ÿåˆï¼ŒåŒæ—¶ä¿è¯åŸå§‹çš„æ•°æ®ä¹Ÿä¸è¦ä¸¢ï¼Œè¿™ä¸ªå°±æ˜¯ç¬¬ä¸‰éƒ¨åˆ†GPT-3é¢„è®­ç»ƒéƒ¨åˆ†ï¼›
            - å½“ Î³ \\gamma Î³\=0æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åšPPOï¼Œå½“ Î³ \\gamma Î³ä¸ä¸º0æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åšPPO-ptxã€‚InstructGPTæ›´åå‘äºä½¿ç”¨PPO-ptxï¼›
            - æœ€ç»ˆä¼˜åŒ–åçš„ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹æ¨¡å‹å°±æ˜¯InstructGPTçš„æ¨¡å‹ã€‚
                Â 

    â€ƒâ€ƒå›é¡¾ä¸‹InstructGPTçš„è®­ç»ƒæµç¨‹ï¼Œå…±åŒ…å«ä¸¤æ¬¡å¯¹æ¨¡å‹çš„å¾®è°ƒ: GPT-3æ¨¡å‹ â‡’ \\Rightarrow â‡’SFTæ¨¡å‹ â‡’ \\Rightarrow â‡’RLæ¨¡å‹ï¼Œå…¶å®è¿™é‡Œå§‹ç»ˆéƒ½æ˜¯åŒä¸€ä¸ªæ¨¡å‹ï¼Œåªæ˜¯ä¸åŒè¿‡ç¨‹ä¸­åç§°ä¸ä¸€æ ·ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨SFTæ¨¡å‹ â‡’ \\Rightarrow â‡’RLæ¨¡å‹é˜¶æ®µï¼Œè¿˜ä¼šä¾èµ–äºå¦ä¸€ä¸ªåœ¨SFTæ¨¡å‹åŸºç¡€ä¸Šè®­ç»ƒçš„RMæ¨¡å‹ã€‚InstructGPTè®­ç»ƒSFTã€RMã€RLä¸‰ä¸ªæ¨¡å‹çš„åŸå› ä¸º:

    - **éœ€è¦SFTæ¨¡å‹çš„åŸå› **: GPT-3æ¨¡å‹ä¸ä¸€å®šèƒ½å¤Ÿä¿è¯æ ¹æ®äººçš„æŒ‡ç¤ºã€æœ‰å¸®åŠ©çš„ã€å®‰å…¨çš„ç”Ÿæˆç­”æ¡ˆï¼Œéœ€è¦äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼›
    - **éœ€è¦RMæ¨¡å‹çš„åŸå› **: æ ‡æ³¨æ’åºçš„åˆ¤åˆ«å¼æ ‡æ³¨ï¼Œæˆæœ¬è¿œè¿œä½äºç”Ÿæˆç­”æ¡ˆçš„ç”Ÿæˆå¼æ ‡æ³¨ï¼›
    - **éœ€è¦RLæ¨¡å‹çš„åŸå› **: è®©æ¨¡å‹å€ŸåŠ©å¼ºåŒ–å­¦ä¹ çš„èƒ½åŠ›ï¼Œæ›´å¥½çš„ç†è§£äººç±»çš„æ„å›¾ã€‚
        Â 

    â€ƒâ€ƒæœ€åï¼Œæˆ‘ä»¬å±•ç¤ºä¸‹è®ºæ–‡ä¸­æåˆ°çš„InstructGPTæ€§èƒ½å¯¹æ¯”ç»“æœï¼Œå¯ä»¥å‘ç°ï¼Œå‚æ•°é‡ä¸º13Bçš„InstructGPTæ¨¡å‹ï¼Œæ€§èƒ½éƒ½è¦è¿œè¿œå¥½äºå‚æ•°é‡ä¸º175Bçš„GPT-3æ¨¡å‹: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b1e168ac492f4c478d978d06548a1018.png#pic_center)


#### 1.3 ChatGPT

â€ƒâ€ƒInstructGPTæ˜¯åœ¨GPT-3çš„åŸºç¡€ä¸Šé€šè¿‡SFT+RLHFä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå®Œæˆï¼›ChatGPTåˆ™æ˜¯åœ¨GPT-3.5çš„åŸºç¡€ä¸Šé€šè¿‡SFT+RLHFä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå®Œæˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ã€‚SFå’ŒRLHFä¸¤ä¸ªé˜¶æ®µï¼Œåœ¨InstructGPTç« èŠ‚ä¸­æˆ‘ä»¬å·²ç»åšäº†è¯¦ç»†ä»‹ç»ï¼Œè¿™é‡Œä¸åšè¿‡å¤šèµ˜è¿°ã€‚å…³äºGPT-3å’ŒGPT-3.5ï¼Œå®ƒä»¬å…¶å®æ˜¯ä¸¤ä¸ªæ¨¡å‹ç³»åˆ—ï¼Œåˆ†åˆ«ç§°ä¸ºGPT-3ç³»åˆ—å’ŒGPT-3.5ç³»åˆ—ï¼Œä¸‹é¢æˆ‘ä»¬å‚è€ƒç»¼è¿°[æ‹†è§£è¿½æº¯ GPT-3.5 å„é¡¹èƒ½åŠ›çš„èµ·æº](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)ï¼Œç®€å•å±•ç¤ºä¸‹OpenAIå›¢é˜Ÿæ‰€æ„å»ºçš„GPT-3ç³»åˆ—å’ŒGPT-3.5ç³»åˆ—æ˜¯å¦‚ä½•è¿›åŒ–çš„ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/44f3c3bb1edb409682f8b2103ebd4ec6.jpeg#pic_center)

#### 1.4 GPT-4

2022å¹´3æœˆï¼ŒOpenAIå›¢é˜Ÿåˆæ”¾å¤§æ‹›ï¼Œå‘å¸ƒäº†æ›´å¼ºçš„LLM: GPT-4ã€‚è™½ç„¶æ— ä»å¾—çŸ¥GPT-4çš„è®­ç»ƒç»†èŠ‚ï¼Œä½†æ˜¯å¯ä»¥è‚¯å®šçš„æ˜¯ï¼ŒGPT-4é‡‡ç”¨äº†æ›´å¤§çš„æ¨¡å‹ç»“æ„ï¼Œå¢åŠ äº†æ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å®˜æ–¹åšå®¢[GPT-4](https://openai.com/research/gpt-4)äº†è§£ä¸‹GPT-4çš„å¼ºå¤§èƒ½åŠ›ã€‚ç›®å‰GPT-4çš„ä¸»è¦èƒ½åŠ›ç‚¹å¦‚ä¸‹:

- GPT-4æ˜¯å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå¯æ”¯æŒå›¾ç‰‡æˆ–æ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºæ˜¯æ–‡æœ¬ï¼›
- GPT-4çš„è¾“å…¥å¯æ¥å—8192ä¸ªtokenã€‚å¦å­˜åœ¨å˜ä½“æ¨¡å‹ï¼Œå¯æ¥å—è¾“å…¥32768ä¸ªtokenï¼›
- GPT-4ç›¸è¾ƒäºChatGPTï¼Œå…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œé™¤äº†èŠå¤©æœºå™¨äººä¹‹å¤–ï¼Œè¿˜åŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰å¤šä¸ªé¢†åŸŸã€‚è€ŒChatGPTä¸»è¦é’ˆå¯¹èŠå¤©æœºå™¨äººé¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ—¥å¸¸å¯¹è¯ã€é—®é¢˜è§£ç­”ã€ä¿¡æ¯æŸ¥è¯¢ç­‰æœåŠ¡ã€‚

### 2ã€å…¶ä»–å¤§æ¨¡å‹

â€ƒâ€ƒæ¯«ä¸å¤¸å¼ çš„è¯´ï¼Œå°½ç®¡éœ€è¦è€—è´¹å·¨å¤§çš„èµ„æºï¼Œä½†æ˜¯ç›®å‰å›½å†…å¤–å„å¤§å…¬å¸éƒ½åœ¨æˆ–å¤šæˆ–å°‘çš„å‚ä¸ç€LLMçš„å†›å¤‡ç«èµ›ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿ƒè¿›ç€NLPæŠ€æœ¯çš„å‘å±•ã€‚å½’å› äºæ­¤ï¼Œç›®å‰å·²ç»æœ‰ä¸€ç³»åˆ—LLMé™†é™†ç»­ç»­é—®ä¸–äº†ã€‚æˆ‘ä»¬æ— æ³•å¯¹è¿™äº›LLMè¿›è¡Œä¸€ä¸€ä»‹ç»ï¼Œè¿™é‡ŒæŒ‘é€‰ä¸€äº›æˆ‘ä»¬åœ¨å…¶åŸºç¡€ä¸Šåšè¿‡å¾®è°ƒæˆ–æœ‰ä½¿ç”¨ç»éªŒçš„æ¨¡å‹ï¼Œå¯¹å®ƒä»¬è¿›è¡Œç®€å•ä»‹ç»ã€‚

å¤§æ¨¡å‹

å›¢é˜Ÿ

å‘å¸ƒæ—¶é—´

æ¨¡å‹è§„æ¨¡

æ˜¯å¦å¼€æº

Hugging Face

Github

ChatGLM-6B

æ¸…åå¤§å­¦

2023

6B

å·²å¼€æºï¼Œä¸å¯å•†ç”¨ï¼ˆè·å–è®¸å¯è¯ï¼‰

[ChatGLM-6B](https://www.huggingface.co/THUDM/chatglm-6b)

[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)

ChatGLM2-6B

æ¸…åå¤§å­¦

2023

6B

å·²å¼€æºï¼Œä¸å¯å•†ç”¨ï¼ˆè·å–è®¸å¯è¯ï¼‰

[ChatGLM2-6B](https://www.huggingface.co/THUDM/chatglm2-6b)

[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)

LLaMA2-7B

Meta

2023

7B

å·²å¼€æºï¼Œå¯å•†ç”¨

[LLaMA2-7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)

[LLaMA](https://github.com/facebookresearch/llama)ã€[Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)

baichuan-7B

ç™¾å·æ™ºèƒ½

2023

7B

å·²å¼€æºï¼Œå¯å•†ç”¨

[baichuan-7B](https://huggingface.co/baichuan-inc/baichuan-7B)

[baichuan-7B](https://github.com/baichuan-inc/baichuan-7B)

æ–‡å¿ƒä¸€è¨€

ç™¾åº¦

2023

åƒäº¿

æœªå¼€æºï¼Œä¸å¯å•†ç”¨

æš‚æ— 

æš‚æ— 

#### 2.1 ChatGLM

â€ƒâ€ƒChatGLMæ˜¯ä¸€ä¸ªåŸºäºåƒäº¿åŸºåº§æ¨¡å‹GLM-130Bå¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰é—®ç­”ã€å¤šè½®å¯¹è¯å’Œä»£ç ç”ŸæˆåŠŸèƒ½ã€‚ç›®å‰ï¼ŒChatGLMæœ‰ä¸¤ä¸ªç‰ˆæœ¬: åƒäº¿å‚æ•°çš„ChatGLM-130Bï¼ˆå†…æµ‹ç‰ˆï¼‰å’Œ62äº¿å‚æ•°çš„ChatGLM-6Bï¼ˆå¼€æºç‰ˆï¼Œå®˜æ–¹Githubæ˜¯[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ï¼‰ã€‚ChatGLM-6Bæ˜¯åœ¨2023å¹´3æœˆ14æ—¥æ­£å¼å¼€æºçš„ï¼Œç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€6GBæ˜¾å­˜ï¼‰ã€‚ChatGLMçš„æŠ€æœ¯åŸºç¡€æ˜¯GLM-130Bï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç›®æ ‡å‡½æ•°çš„è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–åƒäº¿è§„æ¨¡çš„æ¨¡å‹ã€‚
â€ƒâ€ƒChatGLMçš„æ€§èƒ½è¡¨ç°ä¹Ÿååˆ†å‡ºè‰²ã€‚ç»è¿‡çº¦1Tæ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åé¦ˆè‡ªåŠ©ï¼ˆRWï¼‰ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯ï¼Œ62 äº¿å‚æ•°çš„ChatGLM-6Bå·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚è€Œåƒäº¿å‚æ•°çš„ChatGLMåˆ™æ›´è¿›ä¸€æ­¥ï¼Œåœ¨é—®ç­”å’Œå¯¹è¯æ–¹é¢å…·æœ‰æ›´å¼ºå¤§çš„èƒ½åŠ›ã€‚
â€ƒâ€ƒ2023å¹´6æœˆ25æ—¥ï¼Œæ¸…åå¤§å­¦å‘å¸ƒäº†ChatGLM2-6Bï¼ŒChatGLM-6Bçš„å‡çº§ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§:

- **æ›´å¼ºå¤§çš„æ€§èƒ½**: åŸºäºChatGLMåˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œå…¨é¢å‡çº§äº†ChatGLM2-6Bçš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6Bä½¿ç”¨äº†GLMçš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº†1.4Tä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6Båœ¨MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ï¼›
- **æ›´é•¿çš„ä¸Šä¸‹æ–‡**: åŸºäºFlashAttentionæŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”±ChatGLM-6Bçš„2Kæ‰©å±•åˆ°äº†32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨8Kçš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒï¼Œå…è®¸æ›´å¤šè½®æ¬¡çš„å¯¹è¯ã€‚ä½†å½“å‰ç‰ˆæœ¬çš„ ChatGLM2-6Bå¯¹å•è½®è¶…é•¿æ–‡æ¡£çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­è¿­ä»£å‡çº§ä¸­ç€é‡è¿›è¡Œä¼˜åŒ–ï¼›
- **æ›´é«˜æ•ˆçš„æ¨ç†**: åŸºäºMulti Query AttentionæŠ€æœ¯ï¼ŒChatGLM2-6Bæœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨: åœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº†42%ï¼ŒINT4é‡åŒ–ä¸‹ï¼Œ6Gæ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”±1Kæå‡åˆ°äº†8Kã€‚

â€ƒâ€ƒæœ‰å…³ChatGLM2-6Bæ›´å¤šçš„ç»†èŠ‚ï¼Œå¤§å®¶å¯å‚è€ƒå®˜æ–¹Githubï¼Œ[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å…ˆä»‹ç»ä¸‹åŸå§‹GLMçš„æ¨¡å‹ç»“æ„åŠé¢„è®­ç»ƒåŸç†ï¼Œå†ä»‹ç»ä¸‹GhatGLMç³»åˆ—çš„åŸºåº§æ¨¡å‹: GLM-130Bï¼Œå¦‚ä½•åœ¨GLMåŸºç¡€ä¸Šè¿›è¡Œçš„ä¼˜åŒ–è°ƒæ•´ã€‚
â€ƒâ€ƒGLMï¼ˆGeneral Language Modelï¼‰æ˜¯æ¸…åå¤§å­¦åœ¨2022å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ä¸­[ã€ŠGLM: General Language Model Pretraining with Autoregressive Blank Infillingã€‹](https://aclanthology.org/2022.acl-long.26.pdf)æå‡ºçš„æ¨¡å‹ã€‚GLMæ¨¡å‹è¢«æå‡ºä¹‹å‰ï¼ŒNLPé¢†åŸŸä¸»æµçš„é¢„è®­ç»ƒæ¡†æ¶å¯ä»¥åˆ†ä¸ºä¸‰ç§:

- **autoregressiveè‡ªå›å½’æ¨¡å‹ï¼ˆARæ¨¡å‹ï¼‰**: ä»£è¡¨æ˜¯GPTï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä»å·¦åˆ°å³çš„è¯­è¨€æ¨¡å‹ï¼Œå¸¸ç”¨äºæ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼ˆunconditional generationï¼‰ï¼Œåœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œæ¯”å¦‚è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰é¢†åŸŸçš„ä»»åŠ¡ã€‚å½“æ‰©å±•åˆ°åäº¿çº§åˆ«å‚æ•°æ—¶ï¼Œè¡¨ç°å‡ºäº†å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç¼ºç‚¹æ˜¯å•å‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨NLUä»»åŠ¡ä¸­ï¼Œæ— æ³•å®Œå…¨æ•æ‰ä¸Šä¸‹æ–‡çš„ä¾èµ–å…³ç³»ï¼›
- **autoencodingè‡ªç¼–ç æ¨¡å‹ï¼ˆAEæ¨¡å‹ï¼‰**: ä»£è¡¨æ˜¯Bertï¼Œæ˜¯é€šè¿‡æŸä¸ªé™å™ªç›®æ ‡ï¼ˆå¦‚æ©ç è¯­è¨€æ¨¡å‹ï¼‰è®­ç»ƒçš„è¯­è¨€ç¼–ç å™¨ã€‚è‡ªç¼–ç æ¨¡å‹æ“…é•¿è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ï¼Œå¸¸è¢«ç”¨æ¥ç”Ÿæˆå¥å­çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä½†ä¸èƒ½ç›´æ¥åº”ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼›
- **encoder-decoderï¼ˆSeq2Seqæ¨¡å‹ï¼‰**: ä»£è¡¨ä½œT5ï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„Transformerç»“æ„ï¼ŒåŒ…å«ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚é‡‡ç”¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šå¸¸ç”¨äºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼ˆconditional generationï¼‰ï¼Œæ¯”å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚2019ï¼‰ã€‚å®ƒä»¬é€šå¸¸è¢«éƒ¨ç½²åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦å’Œå›åº”ç”Ÿæˆã€‚T5é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç»Ÿä¸€äº†NLUå’Œæœ‰æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œä½†éœ€è¦æ›´å¤šçš„å‚æ•°æ¥åŒ¹é…åŸºäºBRETçš„æ¨¡å‹çš„æ€§èƒ½ã€‚

â€ƒâ€ƒä¸Šè¿°ä¸‰ç§é¢„è®­ç»ƒæ¶æ„çš„è®­ç»ƒç›®æ ‡ä¹Ÿç•¥æœ‰ä¸åŒ:

- GPTçš„è®­ç»ƒç›®æ ‡æ˜¯ä»å·¦åˆ°å³çš„æ–‡æœ¬ç”Ÿæˆï¼›
- Bertçš„è®­ç»ƒç›®æ ‡æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œéšæœºæ©ç ï¼Œç„¶åé¢„æµ‹è¢«æ©ç çš„è¯ï¼›
- T5åˆ™æ˜¯æ¥å—ä¸€æ®µæ–‡æœ¬ï¼Œä»å·¦åˆ°å³çš„ç”Ÿæˆå¦ä¸€æ®µæ–‡æœ¬ã€‚

â€ƒâ€ƒä¸‰ç§é¢„è®­ç»ƒæ¡†æ¶å„æœ‰åˆ©å¼Šï¼Œæ²¡æœ‰ä¸€ç§æ¡†æ¶åœ¨ä»¥ä¸‹ä¸‰ç§é¢†åŸŸçš„è¡¨ç°æœ€ä½³: è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ã€æ— æ¡ä»¶ç”Ÿæˆä»¥åŠæ¡ä»¶ç”Ÿæˆã€‚GLMåŸºäºä»¥ä¸ŠèƒŒæ™¯è¯ç”Ÿäº†ã€‚GLMæ¨¡å‹æ ¸å¿ƒæ˜¯Autoregressive Blank Infillingï¼Œç»“åˆäº†ä¸Šè¿°ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹çš„æ€æƒ³ã€‚

- **é¢„è®­ç»ƒç›®æ ‡**:
    - **Autoregressive Blank Infillingï¼ˆè‡ªå›å½’çš„ç©ºç™½å¡«å……ï¼‰**: GLMæ˜¯é€šè¿‡ä¼˜åŒ–è‡ªå›å½’ç©ºç™½å¡«å……ç›®æ ‡æ¥è®­ç»ƒçš„ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥æ–‡æœ¬ x = \[ x 1 , â‹¯ â€‰ , x n \] x = \[x\_{1}, \\cdots, x\_{n}\] x\=\[x1â€‹,â‹¯,xnâ€‹\]ï¼Œå¤šä¸ªæ–‡æœ¬è·¨åº¦ï¼ˆæ–‡æœ¬ç‰‡æ®µï¼‰ { s 1 , â‹¯ â€‰ , s m } \\{s\_{1},\\cdots, s\_{m}\\} {s1â€‹,â‹¯,smâ€‹}è¢«é‡‡æ ·ï¼Œå…¶ä¸­æ¯ä¸ªè·¨åº¦ s i s\_{i} siâ€‹å¯¹åº”äº x x xä¸­ä¸€ç³»åˆ—è¿ç»­çš„token:  \[ s i , 1 , â‹¯ â€‰ , s i , l i \] \[s\_{i,1}, \\cdots, s\_{i,l\_{i}}\] \[si,1â€‹,â‹¯,si,liâ€‹â€‹\]ï¼Œå…¶ä¸­ l i l\_{i} liâ€‹ä»£è¡¨è·¨åº¦ s i s\_{i} siâ€‹çš„é•¿åº¦ã€‚ x x xä¸­çš„æ¯ä¸€ä¸ªè·¨åº¦éƒ½ä¼šè¢«ä¸€ä¸ª`[Mask]`æ›¿æ¢æ‰ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªè¢«ç ´åçš„ x c o r r u p t x\_{corrupt} xcorruptâ€‹ã€‚GLMæ¨¡å‹ä»¥è‡ªå›å½’çš„æ–¹å¼é¢„æµ‹è¢«ç ´åçš„æ–‡æœ¬ä¸­ç¼ºå°‘çš„tokenï¼Œè¿™æ„å‘³ç€å½“é¢„æµ‹ä¸€ä¸ªè·¨åº¦ä¸­ç¼ºå°‘çš„tokenæ—¶ï¼ŒGLMæ—¢å¯ä»¥è®¿é—®è¢«ç ´åçš„æ–‡æœ¬ x c o r r u p t x\_{corrupt} xcorruptâ€‹ï¼Œåˆå¯ä»¥è®¿é—®è·¨åº¦ä¸­ä¹‹å‰å·²ç»è¢«é¢„æµ‹çš„tokenã€‚ä¸ºäº†å……åˆ†æ•æ‰ä¸åŒè·¨åº¦ä¹‹é—´çš„ç›¸äº’ä¾å­˜å…³ç³»ï¼ŒGLMéšæœºæ’åˆ—è·¨åº¦çš„é¡ºåºã€‚å½¢å¼ä¸Šï¼Œè®© Z m Z\_{m} Zmâ€‹è¡¨ç¤ºé•¿åº¦ä¸º m m mçš„ç´¢å¼•åºåˆ— \[ 1 , 2 , â‹¯ â€‰ , m \] \[1, 2, \\cdots, m\] \[1,2,â‹¯,m\]æ‰€æœ‰å¯èƒ½æ’åˆ—çš„é›†åˆï¼Œ s z < i s\_{z\_{<i}} sz<iâ€‹â€‹ä»£è¡¨ \[ s z 1 , â‹¯ â€‰ , s z i âˆ’ 1 \] \[s\_{z\_{1}}, \\cdots, s\_{z\_{i-1}}\] \[sz1â€‹â€‹,â‹¯,sziâˆ’1â€‹â€‹\]ï¼Œæ­¤æ—¶ï¼Œå¯å®šä¹‰é¢„è®­ç»ƒç›®æ ‡ä¸º:
        max â¡ Î¸ E z âˆ¼ Z m \[ âˆ‘ i = 1 m l o g Â  p Î¸ ( s z i âˆ£ x c o r r u p t , s z < i ) \] \\max\_{\\theta}E\_{z\\sim Z\_{m}}\[\\sum\_{i=1}^{m}log\\ p\_{\\theta}(s\_{z\_{i}}|x\_{corrupt}, s\_{z\_{<i}})\] Î¸maxâ€‹Ezâˆ¼Zmâ€‹â€‹\[i\=1âˆ‘mâ€‹logÂ pÎ¸â€‹(sziâ€‹â€‹âˆ£xcorruptâ€‹,sz<iâ€‹â€‹)\]
        å…¶ä¸­ï¼Œ z z zä»£è¡¨ Z m Z\_{m} Zmâ€‹ä¸­ä»»æ„ä¸€ä¸ªæ’åˆ—ï¼Œä¹Ÿå°±æ˜¯ç´¢å¼•é›†åˆï¼› { z 1 , â‹¯ â€‰ , z m } \\{z\_{1},\\cdots,z\_{m}\\} {z1â€‹,â‹¯,zmâ€‹}ä»£è¡¨ z z zä¸­çš„ç´¢å¼•å…ƒç´ ï¼› s z i s\_{z\_{i}} sziâ€‹â€‹ä»£è¡¨ { s 1 , â‹¯ â€‰ , s m } \\{s\_{1},\\cdots, s\_{m}\\} {s1â€‹,â‹¯,smâ€‹}ä¸­ç¬¬ z i z\_{i} ziâ€‹ä¸ªè·¨åº¦ã€‚ä¸Šè¿°å…¬å¼çš„å«ä¹‰å°±æ˜¯: ç”¨è¢«ç ´åçš„ x c o r r u p t x\_{corrupt} xcorruptâ€‹ï¼Œä¸ s z i s\_{z\_{i}} sziâ€‹â€‹ä¹‹å‰çš„è·¨åº¦ \[ s z 1 , â‹¯ â€‰ , s z i âˆ’ 1 \] \[s\_{z\_{1}}, \\cdots, s\_{z\_{i-1}}\] \[sz1â€‹â€‹,â‹¯,sziâˆ’1â€‹â€‹\]è¿›è¡Œæ‹¼æ¥ï¼Œé¢„æµ‹ç”Ÿæˆçš„æ–‡æœ¬æ˜¯è·¨åº¦ s z i s\_{z\_{i}} sziâ€‹â€‹çš„æ¦‚ç‡è¶Šå¤§è¶Šå¥½ï¼Œè¿™ä¹Ÿæ˜¯å…¸å‹çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å‡½æ•°ã€‚
        â€ƒâ€ƒå¦å¤–è®ºæ–‡ä¸­æåˆ°ï¼Œç”Ÿæˆä»»åŠ¡éƒ½æ˜¯æŒ‰ç…§ä»å·¦åˆ°å³çš„é¡ºåºç”Ÿæˆæ¯ä¸ªç©ºç™½å¤„çš„æ ‡è®°ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç”Ÿæˆè·¨åº¦ s i s\_{i} siâ€‹çš„æ¦‚ç‡è¢«åˆ†è§£ä¸º:
        p Î¸ ( s z i âˆ£ x c o r r u p t , s z < i ) = âˆ j = 1 l i p ( s i , j âˆ£ x c o r r u p t , s z < i , s i < j ) p\_{\\theta}(s\_{z\_{i}}|x\_{corrupt}, s\_{z\_{<i}})=\\prod\_{j=1}^{l\_{i}}p(s\_{i,j}|x\_{corrupt}, s\_{z\_{<i}},s\_{i<j}) pÎ¸â€‹(sziâ€‹â€‹âˆ£xcorruptâ€‹,sz<iâ€‹â€‹)\=j\=1âˆliâ€‹â€‹p(si,jâ€‹âˆ£xcorruptâ€‹,sz<iâ€‹â€‹,si<jâ€‹)
        â€ƒâ€ƒåœ¨æ„å»ºå¥½ä¼˜åŒ–ç›®æ ‡åï¼Œè®ºæ–‡ä¸­é€šè¿‡ä»¥ä¸‹æŠ€æœ¯å®ç°è¯¥ç›®æ ‡ï¼Œå³ä¸Šè¿°çš„è‡ªå›å½’ç©ºç™½å¡«è¡¥ç›®æ ‡ã€‚è¾“å…¥çš„ x x xè¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚Part Aæ˜¯è¢«ç ´åçš„æ–‡æœ¬ x c o r r u p t x\_{corrupt} xcorruptâ€‹ï¼ŒPart Bç”±è¢«maskçš„è·¨åº¦ç»„æˆã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾åŸå§‹çš„æ–‡æœ¬åºåˆ—ä¸º x = \[ x 1 , x 2 , x 3 , x 4 , x 5 , x 6 \] x = \[x\_{1}, x\_{2}, x\_{3}, x\_{4}, x\_{5}, x\_{6}\] x\=\[x1â€‹,x2â€‹,x3â€‹,x4â€‹,x5â€‹,x6â€‹\]ï¼Œé‡‡æ ·çš„ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µä¸º \[ x 3 \] \[x\_{3}\] \[x3â€‹\]å’Œ \[ x 5 , x 6 \] \[x\_{5}, x\_{6}\] \[x5â€‹,x6â€‹\]ï¼Œé‚£ä¹ˆæ©ç åçš„æ–‡æœ¬åºåˆ— x c o r r u p t x\_{corrupt} xcorruptâ€‹ä¸º \[ x 1 , x 2 , \[ M \] , x 4 , \[ M \] \] \[x\_{1}, x\_{2}, \[M\], x\_{4}, \[M\]\] \[x1â€‹,x2â€‹,\[M\],x4â€‹,\[M\]\]ï¼Œä¹Ÿå°±æ˜¯Part Aã€‚æ–‡æœ¬ç‰‡æ®µ \[ x 3 \] \[x\_{3}\] \[x3â€‹\]å’Œ \[ x 5 , x 6 \] \[x\_{5}, x\_{6}\] \[x5â€‹,x6â€‹\]ç”¨äºç»„æˆPart Bï¼ŒåŒæ—¶éœ€è¦å¯¹Part Bçš„ç‰‡æ®µè¿›è¡Œshuffleï¼Œä¹Ÿå°±æ˜¯æ‰“ä¹±æ–‡æœ¬ç‰‡æ®µçš„é¡ºåºï¼ˆæ³¨æ„ä¸æ˜¯æ–‡æœ¬ç‰‡æ®µçš„å†…éƒ¨é¡ºåºï¼Œè€Œæ˜¯æ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„é¡ºåºï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªç‰‡æ®µä½¿ç”¨ \[ S \] \[S\] \[S\]å¡«å……åœ¨å¼€å¤´ä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨ \[ E \] \[E\] \[E\]å¡«å……åœ¨æœ«å°¾ä½œä¸ºè¾“å‡ºã€‚æœ€åï¼Œä»å¼€å§‹æ ‡è®° \[ S \] \[S\] \[S\]å¼€å§‹ä¾æ¬¡è§£ç å‡ºè¢«æ©ç çš„æ–‡æœ¬ç‰‡æ®µï¼Œç›´è‡³ç»“æŸæ ‡è®° \[ E \] \[E\] \[E\]ã€‚
        â€ƒâ€ƒä»¥ä¸Šæ˜¯å®ç°è‡ªå›å½’ç©ºç™½å¡«è¡¥ç›®æ ‡çš„å¤§ä½“æµç¨‹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰æœ‰ä¸¤ç‚¹éœ€è¦æ³¨æ„ï¼Œä¸€ä¸ªæ˜¯self-attention maskçš„è®¾è®¡ï¼Œä¸€ä¸ªæ˜¯`[Mask]`æ–‡æœ¬ç‰‡æ®µçš„é‡‡æ ·è®¾è®¡ã€‚

        - **self-attention mask**:
            - Part Aä¸­çš„è¯å½¼æ­¤å¯è§ï¼Œä½†ä¸å¯è§Part Bä¸­çš„è¯ï¼ˆä¸‹å›¾(d)ä¸­è“è‰²æ¡†ä¸­çš„åŒºåŸŸï¼‰ï¼›
            - Part Bä¸­çš„è¯å•å‘å¯è§ï¼ˆä¸‹å›¾(d)é»„è‰²å’Œç»¿è‰²çš„åŒºåŸŸã€‚é»„è‰²å’Œç»¿è‰²åˆ†åˆ«å¯¹åº” \[ x 3 \] \[x\_{3}\] \[x3â€‹\]å’Œ \[ x 5 , x 6 \] \[x\_{5}, x\_{6}\] \[x5â€‹,x6â€‹\]ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œä¸‹åŒï¼‰ï¼›
            - Part Bå¯è§Part Aï¼ˆä¸‹å›¾(d)ä¸­é»„è‰²å’Œç»¿è‰²çš„åŒºåŸŸï¼‰ï¼›
            - å…¶ä½™ä¸å¯è§ï¼ˆä¸‹å›¾(d)ä¸­ç°è‰²çš„åŒºåŸŸï¼‰
        - **`[Mask]`æ–‡æœ¬ç‰‡æ®µé‡‡æ ·**: è®ºæ–‡ä¸­éšæœºå¯¹è·¨åº¦çš„é•¿åº¦é‡‡æ ·ï¼Œé‡‡æ ·åˆ†å¸ƒå±äºæ³Šæ¾åˆ†å¸ƒï¼Œå…¶ä¸­ Î» = 3 \\lambda=3 Î»\=3ï¼Œç›´åˆ°è‡³å°‘15%çš„åŸå§‹tokenè¢«maskã€‚æ ¹æ®ç»éªŒï¼Œè®ºæ–‡ä¸­å‘ç°ï¼Œ15%çš„æ¯”ä¾‹å¯¹äºä¸‹æ¸¸NLUä»»åŠ¡çš„è‰¯å¥½è¡¨ç°è‡³å…³é‡è¦ã€‚
            Â 

        â€ƒâ€ƒæœ€ç»ˆé€šè¿‡ä»¥ä¸Šæ–¹å¼ï¼ŒGLMè‡ªåŠ¨å­¦ä¹ ä¸€ä¸ªåŒå‘ç¼–ç å™¨ï¼ˆPart Aï¼‰å’Œä¸€ä¸ªå•å‘è§£ç å™¨ï¼ˆPart Bï¼‰ç»Ÿä¸€çš„æ¨¡å‹ã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/71aa1a29c9804f36b06c7b4a82a96235.png#pic_center)

    - **Multi-Task Pretrainingï¼ˆå¤šä»»åŠ¡é¢„è®­ç»ƒï¼‰**: ä¸Šè¿°ä¾‹å­ä¸­ï¼ŒGLMæ©ç›–äº†çŸ­è·¨åº¦ï¼Œé€‚ç”¨äºNLUä»»åŠ¡ã€‚è€Œè®ºæ–‡çš„å…³æ³¨ç‚¹æ˜¯é¢„è®­ç»ƒä¸€ä¸ªèƒ½åŒæ—¶å¤„ç†NLUå’Œæ–‡æœ¬ç”Ÿæˆçš„æ¨¡å‹ã€‚å› æ­¤ï¼Œè®ºæ–‡ç ”ç©¶äº†ä¸€ä¸ªå¤šä»»åŠ¡é¢„è®­ç»ƒçš„è®¾ç½®ã€‚åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œå¢åŠ ä¸€ä¸ªç”Ÿæˆè¾ƒé•¿æ–‡æœ¬çš„ç›®æ ‡ï¼Œä¸ç©ºç™½å¡«å……ç›®æ ‡å…±åŒä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­è€ƒè™‘ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡ã€‚

        - **æ–‡æ¡£çº§**: ä»æ–‡æ¡£ä¸­é‡‡æ ·ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µè¿›è¡Œmaskï¼Œä¸”ç‰‡æ®µé•¿åº¦ä¸ºæ–‡æ¡£é•¿åº¦çš„50%ï½100%ã€‚è¯¥ç›®æ ‡æ—¨åœ¨ç”Ÿæˆé•¿æ–‡æœ¬ï¼›
        - **å¥å­çº§**: é™åˆ¶è¢«maskçš„æ–‡æœ¬ç‰‡æ®µå¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ã€‚å¤šä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆå¥å­ï¼‰è¢«å–æ ·ï¼Œä»¥è¦†ç›–15%çš„åŸå§‹tokenã€‚è¿™ä¸€ç›®æ ‡æ˜¯é’ˆå¯¹seq2seqä»»åŠ¡ï¼Œå…¶é¢„æµ‹å¾€å¾€æ˜¯å®Œæ•´çš„å¥å­æˆ–æ®µè½ã€‚
            Â 

        â€ƒâ€ƒè¿™ä¸¤ä¸ªæ–°ç›®æ ‡çš„å®šä¹‰ä¸åŸç›®æ ‡ç›¸åŒã€‚å”¯ä¸€ä¸åŒçš„æ˜¯çš„è·¨åº¦æ•°é‡å’Œè·¨åº¦é•¿åº¦ã€‚

- **æ¨¡å‹ç»“æ„**: GLMä½¿ç”¨Transformeræ¶æ„ï¼Œå¹¶å¯¹æ¶æ„è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ã€‚å…¶ä¸­ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œæ˜¯äºŒç»´ä½ç½®ç¼–ç ã€‚
    - **Layer Normalization**: é‡æ–°è°ƒæ•´äº†LayerNormå’Œæ®‹å·®è¿æ¥çš„é¡ºåºï¼ˆå…ˆè¿›è¡ŒLayerNormï¼Œå†è¿›è¡Œæ®‹å·®è¿æ¥ï¼Œç±»ä¼¼äºPre-LNï¼Œä¸è¿‡GLM-130Bè®­ç»ƒæ—¶åˆè°ƒæ•´ä¸ºDeepNormäº†ï¼‰ï¼›
    - **è¾“å‡ºå±‚**: ä½¿ç”¨å•ä¸ªçº¿æ€§å±‚è¿›è¡Œè¾“å‡ºtokené¢„æµ‹ï¼›
    - **æ¿€æ´»å‡½æ•°**: ä½¿ç”¨GeLUæ›¿æ¢ReLUæ¿€æ´»å‡½æ•°ï¼›
    - **äºŒç»´ä½ç½®ç¼–ç **: è‡ªå›å½’ç©ºç™½å¡«å……ä»»åŠ¡çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å¦‚ä½•å¯¹ä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚Transformerä¾é ä½ç½®ç¼–ç æ¥æ³¨å…¥tokençš„ç»å¯¹å’Œç›¸å¯¹ä½ç½®ã€‚è®ºæ–‡ä¸­æå‡ºäº†äºŒç»´ä½ç½®ç¼–ç æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å¦‚ä¸Šé¢å›¾ç‰‡æ‰€ç¤ºï¼Œå…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªtokenéƒ½æœ‰ä¸¤ä¸ªä½ç½®æ ‡è¯†ç¼–ç ã€‚ç¬¬ä¸€ä¸ªä½ç½®æ ‡è¯†ä»£è¡¨åœ¨è¢«ç ´åæ–‡æœ¬ x c o r r u p t x\_{corrupt} xcorruptâ€‹ä¸­çš„ä½ç½®ã€‚å¯¹äºè¢«maskçš„è·¨åº¦ï¼Œå®ƒæ˜¯ç›¸åº”çš„`[Mask]`çš„ä½ç½®ã€‚ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†ä»£è¡¨è·¨åº¦å†…çš„ä½ç½®ã€‚å¯¹äºAéƒ¨åˆ†çš„tokenï¼Œå®ƒä»¬çš„ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†æ˜¯0ï¼›å¯¹äºBéƒ¨åˆ†çš„tokenï¼Œå®ƒä»¬çš„ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†æ˜¯ä»1åˆ°è·¨åº¦çš„é•¿åº¦ã€‚è¿™ä¸¤ä¸ªä½ç½®æ ‡è¯†é€šè¿‡å¯å­¦ä¹ çš„åµŒå…¥è¡¨æ˜ å°„ä¸ºä¸¤ä¸ªå‘é‡ï¼Œè¿™ä¸¤ä¸ªå‘é‡éƒ½è¢«æ·»åŠ åˆ°è¾“å…¥tokençš„embeddingè¡¨è¾¾ä¸­ã€‚

â€ƒâ€ƒè‡³æ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸå§‹GLMçš„é¢„è®­ç»ƒåŸç†åŠæ¨¡å‹ç»“æ„ã€‚æ›´å¤šç»†èŠ‚å¤§å®¶å¯å‚è€ƒ[GLM: è‡ªå›å½’ç©ºç™½å¡«å……çš„é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ](https://zhuanlan.zhihu.com/p/560559133)ã€[ChatGLMå®˜æ–¹åšå®¢](https://chatglm.cn/blog)ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç®€å•ä»‹ç»ä¸‹ChatGLMçš„åŸºåº§æ¨¡å‹: GLM-130Bã€‚

- **ç›¸å¯¹åŸå§‹GLMçš„ä¼˜åŒ–è°ƒæ•´**:
    - **Layer Normalization**: ä½¿ç”¨DeepNormï¼ˆPost-LNçš„å‡çº§ç‰ˆï¼‰æ¥æä¾›æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ä¸‹é¢æ˜¯ä¸‰ç§LNæ¨¡å¼ï¼Œå…¶ä¸­ f f fä»£è¡¨FFNæˆ–Attentionå±‚ã€‚
        - **Post-LN**: åŸå§‹Bertã€GPT-1é‡‡ç”¨çš„Layer Normalizationå½¢å¼ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œä½†æ•ˆæœè¾ƒå¥½ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹:
            x = L a y e r N o r m ( x + f ( x ) ) x=LayerNorm(x+f(x)) x\=LayerNorm(x+f(x))
        - **Pre-LN**: GPT-2ã€GPT-3ä»¥åŠLLaMAé‡‡ç”¨çš„Layer Normalizationå½¢å¼éƒ½è¿‘ä¼¼äºPre-LNï¼Œæ•ˆæœä¸å¦‚Post-LNï¼Œä½†ç¨³å®šæ€§è¾ƒå¥½ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹:
            x = x + L a y e r N o r m ( f ( x ) ) x=x+LayerNorm(f(x)) x\=x+LayerNorm(f(x))
        - **DeepNorm**: é›†æˆPost-LNå’ŒPre-LNçš„ä¼˜ç‚¹ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹:
            x = L a y e r N o r m ( Î± x + f ( x ) ) , ( Î± > 1 ) x=LayerNorm(\\alpha x+f(x)), (\\alpha>1) x\=LayerNorm(Î±x+f(x)),(Î±\>1)
    - **Position Embedding**: ä½¿ç”¨RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æ›¿æ¢2D Position Embeddingï¼›
    - **Feed Forward Networkï¼ˆFFNï¼‰**: ä½¿ç”¨GeGLUæ›¿æ¢GeLU ã€‚
- **GLM-130Bé¢„è®­ç»ƒé…ç½®**:
    - **è‡ªç›‘ç£ç©ºç™½å¡«å……ï¼ˆ95% tokensï¼‰**: é€šè¿‡ä¸åŒçš„Maskç­–ç•¥ï¼Œæ¥ä½¿æ¨¡å‹è·å¾—è‡ªç¼–ç å’Œè‡ªå›å½’çš„èƒ½åŠ›ï¼Œå…·ä½“æ¥è¯´:
        - **è¯Mask**: 30%çš„è®­ç»ƒtokenè¿›è¡Œè¯çº§åˆ«çš„Maskï¼ŒMaskæ–¹å¼å‚è€ƒå‰æ–‡çš„è·¨åº¦é‡‡æ ·æ–¹æ³•: è·¨åº¦é•¿åº¦éµå¾ªæ³Šæ¾åˆ†å¸ƒï¼ˆÎ»=3ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬çš„è·¨åº¦é•¿åº¦åŠ èµ·æ¥æœ€å¤šä¸ºè¯¥æ ·æœ¬é•¿åº¦çš„15%ï¼›
        - **å¥å­åŠæ–‡æ¡£Mask**: å‰©ä¸‹70%çš„tokenè¿›è¡Œå¥å­æˆ–æ–‡æ¡£çº§åˆ«çš„Maskã€‚
    - **å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆMIPï¼Œ5%tokensï¼‰**: T5å’ŒExT5ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒä¸­çš„å¤šä»»åŠ¡å­¦ä¹ æ¯”å¾®è°ƒæ›´æœ‰å¸®åŠ©ã€‚å› æ­¤ï¼ŒGLM-130Båœ¨é¢„è®­ç»ƒä¸­åŒ…å«å„é¡¹æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¯­æ„ç†è§£ã€ç”Ÿæˆå’Œä¿¡æ¯æŠ½å–ã€‚ä¸ºäº†ä¿è¯æ¨¡å‹çš„å…¶ä»–ç”Ÿæˆèƒ½åŠ›ä¸å—å½±å“ï¼Œç”¨äºMIPè®­ç»ƒçš„æ•°æ®é›†åªå äº†5%ã€‚

#### 2.2 LLaMA

â€ƒâ€ƒ2023å¹´7æœˆï¼ŒMetaæ¨å‡ºäº†å®Œå…¨å¯å•†ç”¨çš„å¼€æºå¤§æ¨¡å‹LLaMA2ã€‚è¿™é‡Œç®€å•ä»‹ç»ä¸‹ä¸¤ä»£LLaMAçš„å…±æœ‰ç»“æ„ä»¥åŠLLaMA2ç›¸è¾ƒäºåˆä»£LLaMAçš„ä¼˜åŒ–ç‚¹ã€‚

- **é€šç”¨ç»“æ„**: LLaMAçš„å…·ä½“ç»“æ„è§ä¸‹å›¾ã€‚
    - **Layer Normalization**: ä½¿ç”¨å‰ç½®çš„RMSNormï¼›åœ¨BERTã€GPTç­‰æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„LayerNormæ˜¯å¦‚ä¸‹å½¢å¼:
        y = W âˆ— x âˆ’ M e a n ( x ) V a r ( x ) + Ïµ + b y=W\*\\frac{x-Mean(x)}{\\sqrt{Var(x)+\\epsilon}}+b y\=Wâˆ—Var(x)+Ïµ â€‹xâˆ’Mean(x)â€‹+b
        RMSNorm(root mean square)å‘ç°LayerNormçš„ä¸­å¿ƒåç§»æ²¡ä»€ä¹ˆç”¨(å‡å»å‡å€¼ç­‰æ“ä½œ)ã€‚å°†å…¶å»æ‰ä¹‹åï¼Œæ•ˆæœå‡ ä¹ä¸å˜ï¼Œä½†æ˜¯é€Ÿåº¦æå‡äº†40%ã€‚RMSNormå…¬å¼ä¸º:
        y = W âˆ— x M e a n ( x 2 ) + Ïµ y=W\*\\frac{x}{\\sqrt{Mean(x^{2})+\\epsilon}} y\=Wâˆ—Mean(x2)+Ïµ â€‹xâ€‹
        æ³¨æ„é™¤äº†æ²¡æœ‰å‡å‡å€¼ï¼ŒåŠ åç½®ä»¥å¤–ï¼Œåˆ†æ¯ä¸Šæ±‚çš„æ˜¯RMSè€Œä¸æ˜¯æ–¹å·®ã€‚å¦å¤–LLaMAåœ¨Attention Layerå’ŒMLPçš„è¾“å…¥ä¸Šä½¿ç”¨äº†RMSNormï¼Œç›¸æ¯”åœ¨è¾“å‡ºä¸Šä½¿ç”¨ï¼Œè®­ç»ƒä¼šæ›´åŠ ç¨³å®šï¼Œç±»ä¼¼äºPre-LNæ–¹å¼ã€‚
    - **ä½ç½®ç¼–ç **: åœ¨Qã€Kä¸Šä½¿ç”¨RoPEæ—‹è½¬å¼ä½ç½®ç¼–ç ï¼›
    - **Causal Mask**: ä½¿ç”¨causal maskä¿è¯æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å‰é¢çš„tokensï¼›
    - **æ¿€æ´»å‡½æ•°**: ä½¿ç”¨SwiGLUæ›¿ä»£ReLUã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3351f0c56f4f48c29d9ca152aee90f9f.png#pic_center)
- **LLaMA2ä¼˜åŒ–ç‚¹**: å‚è€ƒ[LLaMA2 vs LLaMA](https://zhuanlan.zhihu.com/p/636784644)ã€[LLaMA2ä»‹ç»](https://zhuanlan.zhihu.com/p/647862867)ã€‚
    - **æ›´å¤šçš„è®­ç»ƒè¯­æ–™**: é¢„è®­ç»ƒè¯­æ–™ä»1ä¸‡äº¿å¢åŠ åˆ°2ä¸‡äº¿tokensï¼›
    - **æ›´é•¿çš„ä¸Šä¸‹æ–‡**: ä¸Šä¸‹æ–‡é•¿åº¦ä»2048å¢åŠ åˆ°4096ï¼›
    - **æ–°å¢SFTè¿‡ç¨‹**: æ”¶é›†äº†10ä¸‡äººç±»æ ‡æ³¨æ•°æ®è¿›è¡ŒSFTï¼›
    - **æ–°å¢RLHFè¿‡ç¨‹**: æ”¶é›†äº†100ä¸‡äººç±»åå¥½æ•°æ®è¿›è¡ŒRLHFï¼›
    - **è°ƒæ•´Attentionæœºåˆ¶**: å’ŒFalconä¸€æ ·ï¼Œä½¿ç”¨äº†Group Query Attentionï¼ŒèŠ‚çœæ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶æå‡è®¡ç®—é€Ÿåº¦ã€‚
        - **Multi Head Attentionï¼ˆMHAï¼‰**: åŸå§‹å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€æœ‰å¤´å„è‡ªä¿å­˜ç‹¬ç«‹çš„Qã€Kã€VçŸ©é˜µï¼›
        - **Multi Query Attentionï¼ˆMQAï¼‰**: æ‰€æœ‰çš„å¤´ä¹‹é—´å…±äº«åŒä¸€ä»½Kå’ŒVçŸ©é˜µï¼Œæ¯ä¸ªå¤´åªå•ç‹¬ä¿ç•™äº†ä¸€ä»½QçŸ©é˜µå‚æ•°ï¼Œä»è€Œå¤§å¤§å‡å°‘Kå’ŒVçŸ©é˜µçš„å‚æ•°é‡ï¼›
        - **Group Query Attentionï¼ˆGQAï¼‰**: æ²¡æœ‰åƒMQAä¸€æ ·æç«¯ï¼Œè€Œæ˜¯å°†Qåˆ†ç»„ï¼Œç»„å†…å…±äº«Kã€VçŸ©é˜µã€‚å½“group=1æ—¶ï¼ŒGQAç­‰ä»·äºMQAã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/206eaadf3abe41c4a911e0e46e6b9583.png#pic_center)

ä¸‰ã€è¡¥å……çŸ¥è¯†
------

### 1ã€LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoder onlyæ¶æ„ï¼Ÿ

â€ƒâ€ƒæ€»çš„æ¥è¯´ï¼ŒLLMä¹‹æ‰€ä»¥ä¸»è¦éƒ½ç”¨Decoder-onlyæ¶æ„ï¼Œé™¤äº†è®­ç»ƒæ•ˆç‡å’Œå·¥ç¨‹å®ç°ä¸Šçš„ä¼˜åŠ¿å¤–ï¼Œä¸€æ–¹é¢ï¼Œåœ¨ç†è®ºä¸Šæ˜¯å› ä¸ºEncoderçš„åŒå‘æ³¨æ„åŠ›ä¼šå­˜åœ¨ä½ç§©é—®é¢˜ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼›å¦ä¸€æ–¹é¢ï¼Œå°±ç”Ÿæˆä»»åŠ¡è€Œè¨€ï¼Œå¼•å…¥åŒå‘æ³¨æ„åŠ›å¹¶æ— å®è´¨å¥½å¤„ã€‚è€ŒEncoder-Decoderæ¶æ„ä¹‹æ‰€ä»¥èƒ½å¤Ÿåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ï¼Œå¤§æ¦‚åªæ˜¯å› ä¸ºå®ƒå¤šäº†ä¸€å€å‚æ•°ã€‚æ‰€ä»¥ï¼Œåœ¨åŒç­‰å‚æ•°é‡ã€åŒç­‰æ¨ç†æˆæœ¬ä¸‹ï¼ŒDecoder-onlyæ¶æ„å°±æ˜¯æœ€ä¼˜é€‰æ‹©äº†ã€‚å…·ä½“å‚è€ƒ[LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoder onlyæ¶æ„ï¼Ÿ](https://www.zhihu.com/question/588325646?utm_division=hot_list_page)ã€‚

- **è®­ç»ƒæ•ˆç‡**: Decoder-onlyæ¶æ„åªéœ€è¦è¿›è¡Œå•å‘çš„è‡ªå›å½’é¢„æµ‹ï¼Œè€ŒEncoder-Decoderæ¶æ„éœ€è¦è¿›è¡ŒåŒå‘çš„è‡ªç¼–ç é¢„æµ‹å’Œå•å‘çš„è‡ªå›å½’é¢„æµ‹ï¼Œè®¡ç®—é‡æ›´å¤§ï¼›
- **å·¥ç¨‹å®ç°**: Decoder-onlyæ¶æ„åªéœ€è¦ä¸€ä¸ªæ¨¡å—ï¼Œè€ŒEncoder-Decoderæ¶æ„éœ€è¦ä¸¤ä¸ªæ¨¡å—ï¼Œå¹¶ä¸”éœ€è¦å¤„ç†ä¸¤è€…ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’å’Œå¯¹é½ï¼Œå®ç°èµ·æ¥æ›´å¤æ‚ï¼›
- **ç†è®ºåˆ†æ**: Encoderçš„åŒå‘æ³¨æ„åŠ›ä¼šå­˜åœ¨ä½ç§©é—®é¢˜ï¼Œå³æ³¨æ„åŠ›çŸ©é˜µçš„ç§©éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ è€Œé™ä½ï¼ˆæ¥è‡ªè®ºæ–‡: [Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)ï¼Œç»“è®ºæ˜¯å¦‚æœæ²¡æœ‰æ®‹å·®è¿æ¥å’ŒMLPå…œç€ï¼Œæ³¨æ„åŠ›çŸ©é˜µä¼šæœç§©ä¸º1çš„çŸ©é˜µæ”¶æ•›ï¼Œæœ€åæ¯ä¸ªtokençš„è¡¨ç¤ºéƒ½ä¸€æ ·äº†ï¼Œç½‘ç»œå°±åºŸäº†ï¼‰ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è€ŒDecoderçš„å•å‘æ³¨æ„åŠ›åˆ™ä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜ï¼›
- **ç”Ÿæˆä»»åŠ¡**: å¯¹äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒEncoderçš„åŒå‘æ³¨æ„åŠ›å¹¶æ— å®è´¨å¥½å¤„ï¼Œå› ä¸ºå®ƒä¼šå¼•å…¥å³ä¾§çš„ä¿¡æ¯ï¼Œç ´åäº†è‡ªå›å½’çš„å‡è®¾ã€‚è€ŒDecoderçš„å•å‘æ³¨æ„åŠ›åˆ™å¯ä»¥ä¿æŒè‡ªå›å½’çš„ä¸€è‡´æ€§ã€‚

### 2ã€NLPå°çŸ¥è¯†ç‚¹

- **Bert**ï¼›è‡ªç¼–ç æ¨¡å‹ï¼Œé€‚ç”¨äºNLUï¼ˆé¢„è®­ç»ƒä»»åŠ¡ä¸»è¦æŒ–æ˜å¥å­ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼‰ï¼Œæœ‰å…³Bertçš„ç†è®ºçŸ¥è¯†ï¼Œå¯å‚è€ƒ[â€”æ­¥æ­¥èµ°è¿›Bert](https://zhuanlan.zhihu.com/p/519432336)ï¼›
- **GPT**ï¼›è‡ªå›å½’æ¨¡å‹ï¼Œé€‚ç”¨äºNLGï¼ˆé¢„è®­ç»ƒä»»åŠ¡ä¸»è¦ç”¨äºç”Ÿæˆä¸‹æ–‡ï¼‰ï¼Œæœ‰å…³GPTç³»åˆ—çš„ç†è®ºçŸ¥è¯†ï¼Œå¯å‚è€ƒæœ¬æ–‡çš„GPTç³»åˆ—ç« èŠ‚ï¼›
- **OOV**: OOV é—®é¢˜æ˜¯NLPä¸­å¸¸è§çš„ä¸€ä¸ªé—®é¢˜ï¼Œå…¶å…¨ç§°æ˜¯Out Of Vocabularyï¼Œä¸‹é¢ç®€è¦çš„è¯´äº†ä¸€ä¸‹OOVã€‚
    - **å®šä¹‰**: åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸€ä¸ªå­—è¯åº“ï¼ˆvocabularyï¼‰ã€‚è¿™ä¸ªvocabularyæˆ–è€…æ˜¯æå‰åŠ è½½çš„ï¼Œæˆ–è€…æ˜¯è‡ªå·±å®šä¹‰çš„ï¼Œæˆ–è€…æ˜¯ä»å½“å‰æ•°æ®é›†æå–çš„ã€‚å‡è®¾é€šè¿‡ä¸Šè¿°æ–¹æ³•å·²ç»è·å–åˆ°ä¸€ä¸ªvocabularyï¼Œä½†åœ¨å¤„ç†å…¶ä»–æ•°æ®é›†æ—¶ï¼Œå‘ç°è¿™ä¸ªæ•°æ®é›†ä¸­æœ‰ä¸€äº›è¯å¹¶ä¸åœ¨ç°æœ‰çš„vocabularyä¸­ï¼Œè¿™æ—¶ç§°è¿™äº›è¯æ˜¯Out-Of-Vocabularyï¼Œå³OOVï¼›
    - **è§£å†³æ–¹æ³•**: Bertä¸­è§£å†³OOVé—®é¢˜ã€‚å¦‚æœä¸€ä¸ªå•è¯ä¸åœ¨è¯è¡¨ä¸­ï¼Œåˆ™æŒ‰ç…§subwordçš„æ–¹å¼é€ä¸ªæ‹†åˆ†tokenï¼Œå¦‚æœè¿é€ä¸ªtokenéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™ç›´æ¥åˆ†é…ä¸º\[unknown\]ã€‚
- **å¯¹æ¯”å­¦ä¹ **: è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)å¯ä»¥é¿å…å¯¹æ•°æ®é›†è¿›è¡Œå¤§é‡çš„æ ‡ç­¾æ ‡æ³¨ã€‚æŠŠè‡ªå·±å®šä¹‰çš„ä¼ªæ ‡ç­¾å½“ä½œè®­ç»ƒçš„ä¿¡å·ï¼Œç„¶åæŠŠå­¦ä¹ åˆ°çš„è¡¨ç¤º(representation)ç”¨ä½œä¸‹æ¸¸ä»»åŠ¡é‡Œã€‚æœ€è¿‘ï¼Œå¯¹æ¯”å­¦ä¹ è¢«å½“ä½œè‡ªç›‘ç£å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œè¢«å¹¿æ³›è¿ç”¨åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚å®ƒçš„ç›®æ ‡æ˜¯: å°†ä¸€ä¸ªæ ·æœ¬çš„ä¸åŒçš„ã€å¢å¼ºè¿‡çš„æ–°æ ·æœ¬ä»¬åœ¨åµŒå…¥ç©ºé—´ä¸­å°½å¯èƒ½åœ°è¿‘ï¼Œç„¶åè®©ä¸åŒçš„æ ·æœ¬ä¹‹é—´å°½å¯èƒ½åœ°è¿œã€‚SimCSE[ã€ŠSimCSE: Simple Contrastive Learning of Sentence Embeddingsã€‹](https://aclanthology.org/2021.emnlp-main.552.pdf)æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå³é‡‡ç”¨å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œè·å–æ›´å¥½çš„æ–‡æœ¬è¡¨å¾ã€‚SimCSEç»†èŠ‚å¯å‚è€ƒ[è®ºæ–‡ç²¾è¯»-SimCSE](https://www.bilibili.com/video/BV12D4y1p788/?spm_id_from=333.337.search-card.all.click&vd_source=25d0b87065d3da39fe110c6e0b4906e1)ã€‚

### 3ã€åè¯è§£é‡Š

- **LLM**: Large Language Modelï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- **PLM**: Pretrain Language Modelï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
- **RL**: Reinforcement Learningï¼Œå¼ºåŒ–å­¦ä¹ ã€‚
- **SFT**: Supervised Fine-Tuningï¼Œæœ‰ç›‘ç£å¾®è°ƒã€‚
- **ICL**: In-Context Learningï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚
- **Fine-Tuning** : å¾®è°ƒã€‚
- **Prompt-Tuning**: æç¤ºå¾®è°ƒã€‚
- **Instruction-Tuning**: æŒ‡ç¤º/æŒ‡ä»¤å¾®è°ƒã€‚
- **NLU**: Natural Language Understandingï¼Œè‡ªç„¶è¯­è¨€ç†è§£ã€‚
- **NLG**: Natural Language Generationï¼Œè‡ªç„¶è¯­è¨€ç”Ÿæˆã€‚
- **CoT**: Chain-of-Thoughtï¼Œæ€ç»´é“¾ã€‚
- **OOV**: out of vocabularyï¼Œè¶…å‡ºè¯è¡¨å¤–çš„è¯ã€‚
- **shifted right**: æŒ‡çš„æ˜¯Transformer Decoderç»“æ„ä¸­ï¼Œdecoderåœ¨ä¹‹å‰æ—¶åˆ»çš„ä¸€äº›è¾“å‡ºï¼Œä½œä¸ºæ­¤æ—¶çš„è¾“å…¥ï¼Œä¸€ä¸ªä¸€ä¸ªå¾€å³ç§»ã€‚
- **é‡å‚æ•°åŒ–**: å¸¸è§„æ€æƒ³: å¯¹äºç½‘ç»œå±‚éœ€è¦çš„å‚æ•°æ˜¯ Î¦ \\Phi Î¦ï¼Œè®­ç»ƒå‡ºæ¥çš„å‚æ•°å°±æ˜¯ Î¦ \\Phi Î¦ã€‚é‡å‚æ•°åŒ–æ–¹æ³•: è®­ç»ƒæ—¶ç”¨çš„æ˜¯å¦ä¸€å¥—ä¸åŒäº Î¦ \\Phi Î¦çš„å‚æ•°ï¼Œè®­ç»ƒå®Œåç­‰ä»·è½¬æ¢ä¸º Î¦ \\Phi Î¦ç”¨äºæ¨ç†ã€‚
- **PPL**: å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼Œç”¨äºè¯„ä»·è¯­è¨€æ¨¡å‹çš„å¥½åã€‚
- **FCNN**: Fully connected neural networkï¼Œå…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚
- **FNN**: Feedforward neural networkï¼Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚
- **DNN**: Deep neural networkï¼Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚
- **MLP**: Multi-layer perceptron neural networksï¼Œå¤šå±‚æ„ŸçŸ¥æœºã€‚
- **RM**: Reward Modelï¼Œå¥–åŠ±æ¨¡å‹ã€‚
- **PPO**ï¼ŒProximal Policy Optimizationï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¯¹ç›®æ ‡å‡½æ•°é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ã€‚
- **Emergent Ability**: å¾ˆå¤šèƒ½åŠ›å°æ¨¡å‹æ²¡æœ‰ï¼Œåªæœ‰å½“æ¨¡å‹å¤§åˆ°ä¸€å®šçš„é‡çº§ä¹‹åæ‰ä¼šå‡ºç°ã€‚è¿™æ ·çš„èƒ½åŠ›ç§°ä¸ºæ¶Œç°èƒ½åŠ›ã€‚
- **AutoRegression Language Model**: è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚
- **Autoencoder Language Model**: è‡ªç¼–ç è¯­è¨€æ¨¡å‹ã€‚
- **CLM**: Causal language modelingï¼Œå› æœè¯­è¨€å»ºæ¨¡ï¼Œç­‰ä»·äºAutoRegression Language Modelã€‚
- **AIGC**: Artificial Intelligence Generated Contentï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚
- **AGI**: Artificial General Intelligenceï¼Œé€šç”¨äººå·¥æ™ºèƒ½ã€‚

### 4ã€å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„ä¸€äº›ä¸ªäººæ„Ÿæƒ³

â€ƒâ€ƒå°±ç›®å‰çš„å‘å±•è¶‹åŠ¿è€Œè¨€ï¼Œå¤§æ¨¡å‹çš„è®­ç»ƒåŸºæœ¬å°±æ˜¯æŒ‰ç…§**Pretrain**ã€**Instruction-Tuning**ã€**RLHF**ä¸‰æ­¥èµ°æ¨¡å¼è¿›è¡Œï¼Œå› æ­¤æŠ€æœ¯ä¸ŠåŸºæœ¬æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä¸»è¦ç“¶é¢ˆå­˜åœ¨äºç®—åŠ›ï¼Œå½“ç„¶å¯¹äºä¸­æ–‡å¤§æ¨¡å‹æ¥è¯´ï¼Œè·å–é«˜è´¨é‡ä¸­æ–‡æ•°æ®ä¹Ÿæ˜¯ä¸ªé—®é¢˜ã€‚å›½å†…èƒ½è€—è´¹å¤§è§„æ¨¡æˆæœ¬è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒçš„å‚å•†å±ˆæŒ‡å¯æ•°ï¼Œå› æ­¤ä¸ªäººè®¤ä¸ºï¼Œæœªæ¥çš„å‘å±•è¶‹åŠ¿ï¼Œå¯èƒ½æœ‰ä»¥ä¸‹å››ä¸ªæ–¹å‘:

- **ç»Ÿä¸€å¤§æ¨¡å‹**: å¤´éƒ¨ä¼ä¸šé€æ­¥è¿­ä»£å‡ºå¯ç”¨æ€§å¾ˆå¼ºçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆåƒäº¿çº§åˆ«ï¼‰ï¼Œå¼€æ”¾APIæˆ–åœ¨å…¬æœ‰äº‘ä¸Šä¾›å¤§å®¶ä½¿ç”¨ï¼›
- **å‚ç›´é¢†åŸŸæ¨¡å‹**: éƒ¨åˆ†ä¼ä¸šé€æ­¥è¿­ä»£å‡ºåœ¨ç›¸å…³å‚ç›´é¢†åŸŸå¯ç”¨æ€§å¾ˆå¼ºçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆç™¾äº¿çº§åˆ«ï¼‰ï¼Œè‡ªç”¨æˆ–ç§æœ‰åŒ–æä¾›å®¢æˆ·ä½¿ç”¨ï¼›
- **å¹¶è¡Œè®­ç»ƒæŠ€æœ¯**: å…¨æ–°æˆ–æ›´å…·å¯ç”¨æ€§ã€ç”Ÿæ€æ›´å®Œæ•´çš„å¹¶è¡Œè®­ç»ƒæŠ€æœ¯/æ¡†æ¶å¼€æºï¼Œæ»¡è¶³å¤§éƒ¨åˆ†æœ‰è®­ç»ƒéœ€æ±‚çš„ä¼ä¸š/ä¸ªäººä½¿ç”¨ï¼Œé€æ­¥å®ç°äººäººéƒ½èƒ½è®­ç»ƒå¤§æ¨¡å‹ï¼›
- **é¢ è¦†**: æˆ–è®¸æŸä¸€å¤©ï¼Œæ¨ªç©ºå‡ºä¸–çš„è®ºæ–‡æ¨ç¿»äº†ç›®å‰çš„å¤§æ¨¡å‹å‘å±•è·¯çº¿ï¼Œè½¬è€Œè¯æ˜äº†äººä»¬æ›´éœ€è¦çš„æ˜¯å¦ä¸€ç§å¤§æ¨¡å‹æŠ€æœ¯ï¼Œäº‹æƒ…å°±ä¼šå˜å¾—æœ‰æ„æ€äº†ã€‚

æ€»ç»“
--

â€ƒâ€ƒäº†è§£LLMï¼Œçœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†ï¼ï¼ï¼



$(function() { setTimeout(function () { var mathcodeList = document.querySelectorAll('.htmledit\_views img.mathcode'); if (mathcodeList.length > 0) { for (let i = 0; i < mathcodeList.length; i++) { if (mathcodeList\[i\].naturalWidth === 0 || mathcodeList\[i\].naturalHeight === 0) { var alt = mathcodeList\[i\].alt; alt = '\\\\(' + alt + '\\\\)'; var curSpan = $('<span class="img-codecogs"></span>'); curSpan.text(alt); $(mathcodeList\[i\]).before(curSpan); $(mathcodeList\[i\]).remove(); } } MathJax.Hub.Queue(\["Typeset",MathJax.Hub\]); } }, 1000) });

![](https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close.png)

ä¼˜æƒ åŠµ

[](https://mall.csdn.net/vip)

 [![](https://profile-avatar.csdnimg.cn/default.jpg!1) é•¿ç«¹Danko](https://blog.csdn.net/qq_39439006)

[å…³æ³¨](javascript:;) å…³æ³¨

-    ![](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarThumbUpactive.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2021Active.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2021Black.png) 8

    ç‚¹èµ

-  ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newUnHeart2021Active.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newUnHeart2021Black.png)

    è¸©

-    [![](https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollectionActive.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newCollectBlack.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newCollectActive.png) 26](javascript:;)

    æ”¶è—

    è§‰å¾—è¿˜ä¸é”™? ä¸€é”®æ”¶è— ![](https://csdnimg.cn/release/blogv2/dist/pc/img/collectionCloseWhite.png)

- [![æ‰“èµ](https://csdnimg.cn/release/blogv2/dist/pc/img/newRewardBlack.png)](javascript:;)

    æ‰“èµ

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward01.png) çŸ¥é“äº†

     [![](https://csdnimg.cn/release/blogv2/dist/pc/img/newComment2021Black.png) 4](#commentBox)

    è¯„è®º


- [![](https://csdnimg.cn/release/blogv2/dist/pc/img/newShareBlack.png)](javascript:;)

    ![](https://profile-avatar.csdnimg.cn/default.jpg!1)

    é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸Šç¯‡ï¼‰

    2022å¹´å¹´åº•OpenAIå‘å¸ƒChatGPTï¼Œå°†LLMï¼ˆLarge Language Modelï¼‰å¸¦å‘äº†ä¸€ä¸ªæ–°çš„é«˜åº¦ï¼Œè€Œ2023å¹´OpenAIç»§ç»­æ”¾å‡ºå¤§æ‹›: æ›´å¼ºå¤§çš„GPT-4é—®ä¸–ï¼Œå¼•çˆ†äº†æ•´ä¸ªäº’è”ç½‘åœˆã€‚åœ¨è¿™ä¸ªå¤§æ¨¡å‹æ—¶ä»£ï¼Œä½œä¸ºä¸€åNLPerï¼ŒæŒç»­å¸æ”¶ç€å±‚å‡ºä¸ç©·çš„æ–°æŠ€æœ¯ï¼Œç¡®å®æœ‰äº›åƒä¸æ¶ˆã€‚ä¿—è¯è¯´ï¼Œå¥½è®°æ€§ä¸å¦‚çƒ‚ç¬”å¤´ï¼Œåœ¨æ­¤è®°å½•ä¸‹LLMç›¸å…³æŠ€æœ¯åŠè¿›å±•ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œä½ å¯ä»¥è¯´å®ƒä¸å…¨é¢ï¼Œä½†ä¸èƒ½è¯´å®ƒä¸é€šä¿—æ˜“æ‡‚ã€‚GPTç³»åˆ—æ¨¡å‹è¯¦è§£ã€LLMç³»åˆ—-01ã€‘Language Models are Few-Shot Learnersã€‚

    å¤åˆ¶é“¾æ¥

    æ‰«ä¸€æ‰«


[

ã€_äººå·¥æ™ºèƒ½_ã€‘_LLM_å¤§æ¨¡å‹ä¸­çš„è¯åµŒå…¥å’Œä¸Šä¸‹æ–‡ç†è§£æŠ€æœ¯å®ä¾‹è®²è§£ï¼Œé™„å…·ä½“çš„ä»£ç ä¾‹å­

](https://dreamit.blog.csdn.net/article/details/130758378)

[ç¦…ä¸è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯](https://blog.csdn.net/universsky2015)

05-19 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 1ä¸‡+

[

è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰å¯ä»¥å°†é«˜ç»´çš„æ–‡æœ¬æ•°æ®è½¬æ¢æˆä½ç»´çš„ç¨ å¯†å‘é‡è¡¨ç¤ºï¼Œåœ¨è¿›è¡Œ_è‡ªç„¶è¯­è¨€å¤„ç†_ä»»åŠ¡æ—¶ï¼Œè¿™æ ·çš„è¡¨ç¤ºæ–¹å¼å¯ä»¥å¸®åŠ©ç®—æ³•ç†è§£è¯è¯­ä¹‹é—´çš„ç›¸ä¼¼æ€§ä»¥åŠä¸Šä¸‹æ–‡å…³ç³»ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ TensorFlow å’Œ Keras å®ç° Word2Vec è¯åµŒå…¥æ¨¡å‹çš„ä»£ç å®ä¾‹ã€‚

](https://dreamit.blog.csdn.net/article/details/130758378)

[

å¤§æ¨¡å‹è¯¥è¢«çŸ¥é“çš„æŠ€æœ¯å®ç°-_LLM_

](https://download.csdn.net/download/weixin_44077556/88488049)

10-31

[

_LLM_+æŠ€æœ¯å‘å±•+æŠ€æœ¯å®ç°+xmindæ¸…æ™°æ€è·¯ã€‚å¸®åŠ©å¿«é€Ÿç†æ¸…å¤§æ¨¡å‹å®ç°éœ€è¦çš„æŠ€æœ¯ï¼Œå¿«é€Ÿå®šä½è‡ªèº«å®ç°å¤§æ¨¡å‹éœ€è¦çš„åŸºç¡€èƒ½åŠ›åŠå¦‚ä½•å¾®è°ƒï¼Œå¦‚ä½•è¿›è¡Œæ€ç»´é“¾æç¤ºï¼Œå³å¦‚ä½•è¿›è¡ŒITã€COTç­‰å·¥ä½œã€‚

](https://download.csdn.net/download/weixin_44077556/88488049)

4Â æ¡è¯„è®º æ‚¨è¿˜æœªç™»å½•ï¼Œè¯·å…ˆ ç™»å½• åå‘è¡¨æˆ–æŸ¥çœ‹è¯„è®º

[

_é€šä¿—æ˜“æ‡‚_çš„_LLM_ï¼ˆä¸‹ç¯‡ï¼‰

](https://blog.csdn.net/qq_39439006/article/details/132388975)

[qq\_39439006çš„åšå®¢](https://blog.csdn.net/qq_39439006)

08-20 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 236

[

ä¹¦æ¥ä¸Šæ–‡ã€‚äº†è§£_LLM_ï¼Œå†çœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†ï¼ï¼ï¼

](https://blog.csdn.net/qq_39439006/article/details/132388975)

[

ä¸ºä»€ä¹ˆç°åœ¨çš„_LLM_éƒ½æ˜¯Decoder onlyçš„æ¶æ„ï¼Ÿ

](https://blog.csdn.net/TFATS/article/details/133100383)

[TFATSçš„åšå®¢](https://blog.csdn.net/TFATS)

09-21 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 345

[

æ€»è€Œè¨€ä¹‹ï¼Œdecoder-only åœ¨å‚æ•°é‡ä¸å¤ªå¤§æ—¶å°±æ›´å…·æœ‰æ›´å¼ºçš„zero-shotæ€§èƒ½ã€æ›´åŒ¹é…ä¸»æµçš„è‡ªç›‘ç£è®­ç»ƒèŒƒå¼ï¼›è€Œåœ¨å¤§å‚æ•°é‡çš„åŠ æŒä¸‹ï¼Œå…·æœ‰äº†æ¶Œç°èƒ½åŠ›åã€å¯ä»¥åŒ¹æ•Œencoder-decoderåšfinetuningçš„æ•ˆæœï¼›åœ¨In Contextçš„ç¯å¢ƒä¸‹ã€åˆèƒ½æ›´å¥½åœ°åšfew-shotä»»åŠ¡ã€‚decoder-only æ¶æ„ç¬¦åˆä¼ ç»Ÿçš„ Language Model çš„æ¨¡å¼ã€‚

](https://blog.csdn.net/TFATS/article/details/133100383)

[

_LLM_+èµ‹èƒ½ç ”å‘æ•ˆèƒ½æå‡

](https://download.csdn.net/download/mzh200801/88310617)

09-07

[

æ¢ç´¢è½¯ä»¶å¼€å‘æ–°å·¥åº: _LLM_+èµ‹èƒ½ç ”å‘æ•ˆèƒ½æå‡

](https://download.csdn.net/download/mzh200801/88310617)

[

_LLM_\-Custome.zip

](https://download.csdn.net/download/wchg21131/88225770)

08-17

[

ä¸­æ–‡å¼€æºå¤§æ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œå®šåˆ¶åŒ–çš„å¾®è°ƒï¼Œåœ¨å°ç®—åŠ›æ¡ä»¶ä¸‹ä¹Ÿèƒ½è¿è¡Œè‡ªå·±ä¸“å±çš„è¯­è¨€æ¨¡å‹ã€‚

](https://download.csdn.net/download/wchg21131/88225770)

[

_LLM_ç¬”è®°

](https://blog.csdn.net/zh515858237/article/details/128628862)

[zh515858237çš„ä¸“æ ](https://blog.csdn.net/zh515858237)

01-10 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 4774

[

_LLM_å…¶å®å°±æ˜¯large language modelï¼Œå¤§è¯­è¨€æ¨¡å‹ã€‚å¦‚æœå¯¹â€œæœ€ç»ˆä»»åŠ¡â€è¿›ä¸€æ­¥è¿›è¡Œåˆ†ç±»ï¼Œåˆå¤§è‡´å¯ä»¥åˆ†ä¸ºä¸¤å¤§ä¸åŒç±»å‹çš„ä»»åŠ¡: è‡ªç„¶è¯­è¨€ç†è§£ç±»ä»»åŠ¡å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆç±»ä»»åŠ¡ã€‚å¦‚æœæ’é™¤æ‰â€œä¸­é—´ä»»åŠ¡â€çš„è¯ï¼Œå…¸å‹çš„è‡ªç„¶è¯­è¨€ç†è§£ç±»ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€å¥å­å…³ç³»åˆ¤æ–­ã€æƒ…æ„Ÿå€¾å‘åˆ¤æ–­ç­‰ï¼Œè¿™ç§ä»»åŠ¡æœ¬è´¨ä¸Šéƒ½æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œå°±æ˜¯è¯´è¾“å…¥ä¸€ä¸ªå¥å­ï¼ˆæ–‡ç« ï¼‰ï¼Œæˆ–è€…ä¸¤ä¸ªå¥å­ï¼Œæ¨¡å‹å‚è€ƒæ‰€æœ‰è¾“å…¥å†…å®¹ï¼Œæœ€åç»™å‡ºå±äºå“ªä¸ªç±»åˆ«çš„åˆ¤æ–­ã€‚è‡ªç„¶è¯­è¨€ç”Ÿæˆä¹ŸåŒ…å«å¾ˆå¤šNLPç ”ç©¶å­æ–¹å‘ï¼Œæ¯”å¦‚èŠå¤©æœºå™¨äººã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚

](https://blog.csdn.net/zh515858237/article/details/128628862)

[

é€šå‘ AGI ä¹‹è·¯: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆ_LLM_ï¼‰æŠ€æœ¯ç²¾è¦

](https://devpress.csdn.net/v1/article/detail/128656824)

[äººå·¥æ™ºèƒ½å­¦å®¶](https://blog.csdn.net/cf2suds8x8f0v)

01-10 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 1ä¸‡+

[

æ¥æº: AIç§‘æŠ€è¯„è®ºä½œè€…: å¼ ä¿Šæ—æœ¬æ–‡ç»æˆæƒè½¬è½½è‡ªçŸ¥ä¹: https://zhuanlan.zhihu.com/p/597586623?utm\_campaign=shareopn&utm\_medium=social&utm\_oi=37478916423680&utm\_psn=1595705313004679168&utm\_source=wechat\_timeline&am...

](https://devpress.csdn.net/v1/article/detail/128656824)

[

ä¸ºä»€ä¹ˆç°åœ¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆ_LLM_ï¼‰éƒ½æ˜¯Decoder-onlyçš„æ¶æ„ï¼Ÿ

](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/129630659)

[Paper weekly](https://blog.csdn.net/c9Yv2cf9I06K2A9E)

03-17 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 1269

[

Â©PaperWeekly åŸåˆ› Â·ä½œè€… |è‹å‰‘æ—å•ä½ |è¿½ä¸€ç§‘æŠ€ç ”ç©¶æ–¹å‘ |NLPã€ç¥ç»ç½‘ç»œ_LLM_ æ˜¯â€œLarge Language Modelâ€çš„ç®€å†™ï¼Œç›®å‰ä¸€èˆ¬æŒ‡ç™¾äº¿å‚æ•°ä»¥ä¸Šçš„è¯­è¨€æ¨¡å‹ï¼Œä¸»è¦é¢å‘æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚è·Ÿå°å°ºåº¦æ¨¡å‹ï¼ˆ10 äº¿æˆ–ä»¥å†…é‡çº§ï¼‰çš„â€œç™¾èŠ±é½æ”¾â€ä¸åŒï¼Œç›®å‰ _LLM_ çš„ä¸€ä¸ªç°çŠ¶æ˜¯ Decoder-only æ¶æ„çš„ç ”ç©¶å±…å¤šï¼Œåƒ OpenAI ä¸€ç›´åšæŒ Decoder-only çš„ ...

](https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/129630659)

[

GPTèƒŒåçš„æ³•æœ¯â€”â€”_LLM_å…¨é¢è§£æ„

](https://swiftui.blog.csdn.net/article/details/130034948)

[iCloudEndçš„åšå®¢](https://blog.csdn.net/iCloudEnd)

04-08 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 810

[

é™¤äº†ä¸¤ä¸ªå˜åŒ–å¤–ï¼Œè§£ç å™¨ç«¯ä¸ç¼–ç å™¨ç«¯çš„æ¶æ„å‡ ä¹ç›¸åŒã€‚è’™é¢æ³¨æ„å±‚ã€‚ä»ç¼–ç å™¨ç«¯å¯¼å…¥é”®å’Œå€¼å‘é‡ã€‚è®©æˆ‘ä»¬é¦–å…ˆäº†è§£è§£ç å™¨çš„è¾“å…¥å’Œè¾“å‡ºã€‚è§£ç å™¨çš„è¾“å…¥: ç›®æ ‡åºåˆ—ï¼ˆç§»ä½ï¼‰: åœ¨è®­ç»ƒæœŸé—´ï¼Œç›®æ ‡åºåˆ—ï¼ˆä¾‹å¦‚ï¼Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„ç¿»è¯‘å¥å­ï¼‰ä½œä¸ºè¾“å…¥æä¾›ç»™è§£ç å™¨ï¼Œä½†å®ƒè¢«ç§»ä½äº†ä¸€ä¸ªä½ç½®ã€‚è¿™æ„å‘³ç€è§£ç å™¨æ¥æ”¶åˆ°ç›®æ ‡åºåˆ—ä¸­é™¤æœ€åä¸€ä¸ªä¹‹å¤–çš„æ‰€æœ‰æ ‡è®°ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†ç¡®ä¿æ¨¡å‹å­¦ä¼šåœ¨ç»™å®šå…ˆå‰æ ‡è®°çš„æƒ…å†µä¸‹é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚ç¼–ç å™¨è¾“å‡º: ç¼–ç å™¨äº§ç”Ÿçš„éšè—çŠ¶æ€ï¼Œä»£è¡¨è¾“å…¥åºåˆ—æˆ–æ¢å¥è¯è¯´ï¼ŒK å’Œ V å‘é‡ã€‚

](https://swiftui.blog.csdn.net/article/details/130034948)

[

_LLM_.zip\__LLM_\_mushroom\_è˜‘è‡\_è˜‘è‡ åˆ†ç±»\_è˜‘è‡ç®—æ³•

](https://download.csdn.net/download/weixin_42651281/86200169)

07-15

[

åŸºäºçº¿æ€§å­¦ä¹ æœºçš„è˜‘è‡åˆ†ç±»ç®—æ³•æºä»£ç ï¼ŒåŒ…å«ç»“æœ

](https://download.csdn.net/download/weixin_42651281/86200169)

[

_LLM_èƒŒæ™¯ä¸Šçš„åŠç»å…¸ï¼ˆ2ï¼‰å­—ç¬¦ä¸²

](https://download.csdn.net/download/weixin_38522795/12298824)

04-03

[

å¯¹äºé€šç”¨_LLM_å‡ ä½•ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å­˜åœ¨äº_LLM_å¹³é¢ç™½è‰²åŒºåŸŸä¸Šçš„åŠç»å…¸å­—ç¬¦ä¸²è§£å†³æ–¹æ¡ˆã€‚ è¿™äº›ä¸²æ¿€åŠ±ç”±å®ˆæ’ç”µè·Eï¼ŒJå’ŒSæ ‡è®°ï¼Œå› æ­¤åœ¨å…¨æ¯å›¾ä¸Šå¯¹N = 4è¶…çº§Yangâ€“Millsçš„SLï¼ˆ2ï¼‰åŒºæ®µä¸­çš„ç®—ç¬¦æ˜¯åŒé‡çš„ï¼Œå®ƒç”±ä½œç”¨äºå¤æ‚æ ‡é‡åœºZçš„åå˜å¯¼æ•°ç»„æˆã€‚ å¦ä¸€æ–¹é¢ï¼Œ_LLM_å‡ ä½•æœ¬èº«å¯¹ç”±Oï¼ˆN2ï¼‰Zåœºç»„æˆçš„ç®—å­æ˜¯å¯¹å¶çš„ï¼Œå› æ­¤å¯¹æˆ‘ä»¬çš„è§£ï¼ˆåŒ…æ‹¬ä¸¥æ ¼æ¿€åŠ±å’ŒèƒŒæ™¯ï¼‰å¯¹å¶çš„ç®—å­æ˜¯éå¹³é¢çš„ã€‚ åœ¨ä¸€ä¸ªé€‚å½“çš„çŸ­å­—ç¬¦ä¸²é™åˆ¶ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºï¼Œæ‰€æ‰¾åˆ°çš„å­—ç¬¦ä¸²è§£å†³æ–¹æ¡ˆåº”è¯¥ä¸è§„èŒƒç†è®ºè¯­è¨€ä¸­çš„å±€éƒ¨SLï¼ˆ2ï¼‰è¿ç®—ç¬¦å¯¹å¶ã€‚ è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹æœ€è¿‘çš„æè®®è¿›è¡Œä¸€æ¬¡ç®€å•çš„æ£€æŸ¥ï¼Œå³ç›´åˆ°å¯¹'t Hooftè€¦åˆè¿›è¡Œé‡æ–°ç¼©æ”¾ï¼Œå±€éƒ¨åŒ–ç®—å­çš„åŠ¨åŠ›å­¦åº”è¯¥ä¸AdS5Ã—S5èƒŒæ™¯ä¸‹é‚£äº›ç›¸åŒæ¿€å‘çš„åŠ¨åŠ›å­¦ç›¸åŒã€‚

](https://download.csdn.net/download/weixin_38522795/12298824)

[

_llm_.rar\_Freescale\__LLM_\_ç‚¹ç«\_è„‰å†²ç‚¹ç«\_è„‰å†²ç‚¹ç«å™¨

](https://download.csdn.net/download/weixin_42660494/86550500)

09-19

[

è„‰å†²ç‚¹ç«å™¨é¢‘ç‡æµ‹è¯•å™¨ freescaleèŠ¯ç‰‡ å¸¦åŸç†å›¾

](https://download.csdn.net/download/weixin_42660494/86550500)

[

_LLM_.pdf

æœ€æ–°å‘å¸ƒ

](https://download.csdn.net/download/m0_69474989/88491171)

11-01

[

_LLM_.pdf

](https://download.csdn.net/download/m0_69474989/88491171)

[

_LLM_åŸç†ä¸ChatPDFå®ç°.pdf

](https://download.csdn.net/download/goodxianping/88382424)

09-28

[

_LLM_åŸç†ä¸ChatPDFå®ç°.pdf

](https://download.csdn.net/download/goodxianping/88382424)

[

ä»€ä¹ˆæ˜¯è®©ChatGPTçˆ†ç«çš„å¤§è¯­è¨€æ¨¡å‹(_LLM_)

](https://devpress.csdn.net/v1/article/detail/129293381)

[ä¸“æ³¨äºäººå·¥æ™ºèƒ½é¢†åŸŸçš„å°ä½•å°š](https://blog.csdn.net/kunhe0512)

03-02 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 1ä¸‡+

[

AI åº”ç”¨ç¨‹åºæ­£åœ¨æ€»ç»“æ–‡ç« ã€æ’°å†™æ•…äº‹å’Œè¿›è¡Œé•¿æ—¶é—´å¯¹è¯â€”â€”è€Œå¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨æ‰¿æ‹…ç¹é‡çš„å·¥ä½œã€‚ å¤§å‹è¯­è¨€æ¨¡å‹æˆ– _LLM_ æ˜¯ä¸€ç§_æ·±åº¦å­¦ä¹ _ç®—æ³•ï¼Œå¯ä»¥æ ¹æ®ä»æµ·é‡æ•°æ®é›†ä¸­è·å¾—çš„çŸ¥è¯†æ¥è¯†åˆ«ã€æ€»ç»“ã€ç¿»è¯‘ã€é¢„æµ‹å’Œç”Ÿæˆæ–‡æœ¬å’Œå…¶ä»–å†…å®¹ã€‚ å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ Transformer æ¨¡å‹æœ€æˆåŠŸçš„åº”ç”¨ä¹‹ä¸€ã€‚ å®ƒä»¬ä¸ä»…ç”¨äºæ•™æˆ AI äººç±»è¯­è¨€ï¼Œè¿˜ç”¨äºç†è§£è›‹ç™½è´¨ã€ç¼–å†™è½¯ä»¶ä»£ç ç­‰ç­‰ã€‚

](https://devpress.csdn.net/v1/article/detail/129293381)

[

å¤§è¯­è¨€æ¨¡å‹_LLM_çš„åŸç†ï¼Œbertã€GPTã€GLMä¸‰è€…çš„å¯¹æ¯”

](https://blog.csdn.net/qq_43199049/article/details/132629534)

[qq\_43199049çš„åšå®¢](https://blog.csdn.net/qq_43199049)

09-01 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 682

[

å¤§è¯­è¨€æ¨¡å‹_LLM_çš„åŸç†ï¼Œbertã€GPTã€GLMä¸‰è€…çš„å¯¹æ¯”

](https://blog.csdn.net/qq_43199049/article/details/132629534)

[

æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹(_LLM_): è®©ChatGPTç«çˆ†çš„èƒŒå

çƒ­é—¨æ¨è

](https://chenshuyu.blog.csdn.net/article/details/129319800)

[é™ˆä¹¦äºˆ](https://blog.csdn.net/weixin_46780832)

03-03 ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 3ä¸‡+

[

ChatGPTä½œä¸ºä¸€æ¬¾å¤§å‹çš„è¯­è¨€æ¨¡å‹ï¼Œå…¶å¤šå±‚æ¬¡ã€å¤šç²’åº¦çš„è®¾è®¡å’Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ¨¡å‹è®­ç»ƒæ–¹å¼ä½¿å¾—å…¶åœ¨_è‡ªç„¶è¯­è¨€å¤„ç†_é¢†åŸŸå…·æœ‰éå¸¸é«˜çš„åº”ç”¨ä»·å€¼å’Œå‰æ™¯ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼ŒChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å°†ä¸æ–­æ‰©å¤§å’Œæ™®åŠã€‚

](https://chenshuyu.blog.csdn.net/article/details/129319800)

[

_LLM_ Transformer

](https://wenku.csdn.net/answer/20rwa0g59j)

07-13

[

_LLM_ï¼ˆLarge Language Modelï¼‰æ˜¯æŒ‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ¨¡å‹ï¼Œç”¨äºå¤„ç†_è‡ªç„¶è¯­è¨€å¤„ç†_ï¼ˆNLPï¼‰ä»»åŠ¡ã€‚_LLM_ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡è§„å¾‹ï¼Œä»¥ä¾¿èƒ½å¤Ÿç”Ÿæˆã€é¢„æµ‹æˆ–ç†è§£è‡ªç„¶è¯­è¨€ã€‚ Transformeræ˜¯ä¸€ç§ç”¨äºæ„å»º_LLM_çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚å®ƒåœ¨NLPé¢†åŸŸæœ‰ç€é‡è¦çš„åœ°ä½ï¼Œå¹¶åœ¨è®¸å¤šä»»åŠ¡ä¸­å–å¾—äº†ä¼˜ç§€çš„è¡¨ç°ã€‚Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆself-attentionï¼‰æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ï¼Œå¹¶é€šè¿‡å¤šå±‚å †å çš„æ–¹å¼æ¥å¤„ç†é•¿æ–‡æœ¬ã€‚ _LLM_ TransformeræŒ‡çš„æ˜¯åŸºäºTransformeræ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡é¢„è®­ç»ƒå¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ï¼Œå­¦ä¹ åˆ°äº†ä¸°å¯Œçš„è¯­è¨€çŸ¥è¯†å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥ç”¨äºç”Ÿæˆæ–‡æœ¬ã€å›ç­”é—®é¢˜ã€å¯¹è¯ç­‰ä»»åŠ¡ã€‚ CçŸ¥é“å’ŒChatGPTéƒ½æ˜¯åŸºäº_LLM_ Transformerçš„æ¨¡å‹ï¼Œä½†æ˜¯å®ƒä»¬å¯èƒ½åœ¨è®­ç»ƒæ•°æ®ã€é¢„è®­ç»ƒä»»åŠ¡ã€å¾®è°ƒæ–¹å¼ç­‰æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œå› æ­¤å¯èƒ½åœ¨å…·ä½“åº”ç”¨åœºæ™¯å’Œæ€§èƒ½ä¸Šæœ‰æ‰€ä¸åŒã€‚

](https://wenku.csdn.net/answer/20rwa0g59j)

### â€œç›¸å…³æ¨èâ€å¯¹ä½ æœ‰å¸®åŠ©ä¹ˆï¼Ÿ

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel1.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey1.png)

    éå¸¸æ²¡å¸®åŠ©

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel2.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey2.png)

    æ²¡å¸®åŠ©

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel3.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey3.png)

    ä¸€èˆ¬

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel4.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey4.png)

    æœ‰å¸®åŠ©

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel5.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey5.png)

    éå¸¸æœ‰å¸®åŠ©


 æäº¤

window.csdn.csdnFooter.options = { el: '.blog-footer-bottom', type: 2 }

[![](https://profile-avatar.csdnimg.cn/default.jpg!1)](https://blog.csdn.net/qq_39439006)

[é•¿ç«¹Danko](https://blog.csdn.net/qq_39439006 "é•¿ç«¹Danko") CSDNè®¤è¯åšå®¢ä¸“å®¶ CSDNè®¤è¯ä¼ä¸šåšå®¢

ç é¾„6å¹´  [![](https://csdnimg.cn/identity/nocErtification.png) æš‚æ— è®¤è¯](https://i.csdn.net/#/uc/profile?utm_source=14998968 "æš‚æ— è®¤è¯")

[

10

åŸåˆ›

](https://blog.csdn.net/qq_39439006)

[

102ä¸‡+

å‘¨æ’å

](https://blog.csdn.net/rank/list/weekly)

[

7ä¸‡+

æ€»æ’å

](https://blog.csdn.net/rank/list/total)

8ä¸‡+

è®¿é—®

[![](https://csdnimg.cn/identity/blog2.png)](https://blog.csdn.net/blogdevteam/article/details/103478461)

ç­‰çº§

270

ç§¯åˆ†

170

ç²‰ä¸

149

è·èµ

19

è¯„è®º

767

æ”¶è—

![æ–°ç§€å‹‹ç« ](https://csdnimg.cn/medal/xinxiu@240.png)

![åˆ›ä½œèƒ½æ‰‹](https://csdnimg.cn/medal/qixiebiaobing4@240.png)

[ç§ä¿¡](https://im.csdn.net/chat/qq_39439006)

å…³æ³¨

[![å†™æ–‡ç« ](https://img-home.csdnimg.cn/images/20230817060240.png)](https://mp.csdn.net/edit)

 ![](//csdnimg.cn/cdn/content-toolbar/csdn-sou.png?v=1587021042)

(adsbygoogle = window.adsbygoogle || \[\]).push({});

### çƒ­é—¨æ–‡ç« 

- [é€šä¿—æ˜“æ‡‚çš„RNN ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 59579](https://blog.csdn.net/qq_39439006/article/details/121554808)
- [é€šä¿—æ˜“æ‡‚çš„TextCNN ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 16255](https://blog.csdn.net/qq_39439006/article/details/126760701)
- [é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸Šç¯‡ï¼‰ ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 2602](https://blog.csdn.net/qq_39439006/article/details/130796416)
- [é€šä¿—æ˜“æ‡‚çš„LSTM ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 2444](https://blog.csdn.net/qq_39439006/article/details/121572945)
- [ä¸€äº›å¥‡å¥‡æ€ªæ€ªçš„çŸ¥è¯†ç‚¹ ![](https://csdnimg.cn/release/blogv2/dist/pc/img/readCountWhite.png) 732](https://blog.csdn.net/qq_39439006/article/details/121611352)

### æœ€æ–°è¯„è®º

- [é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸Šç¯‡ï¼‰](https://blog.csdn.net/qq_39439006/article/details/130796416#comments_28451799)

    [æ¶¤é£:](https://blog.csdn.net/m0_55334148) æ˜¯çš„ï¼Œç¡®å®çœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†ã€‚æˆ‘çœ‹ä¹‹å‰å®Œå…¨é›¶åŸºç¡€ï¼Œçœ‹å®Œå·²ç»æœ‰è¾ƒè¯¦ç»†çš„äº†è§£äº†ã€‚è°¢è°¢åšä¸»

- [é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸Šç¯‡ï¼‰](https://blog.csdn.net/qq_39439006/article/details/130796416#comments_28292654)

    [ä¼šé£çš„ç¥è¡Œå¤ªä¿:](https://blog.csdn.net/qq_27493615) å†™çš„å¤ªå…¨é¢äº†ï¼æ„Ÿè°¢åšä¸»ï¼![è¡¨æƒ…åŒ…](https://g.csdnimg.cn/static/face/emoji/003.png)

- [é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸‹ç¯‡ï¼‰](https://blog.csdn.net/qq_39439006/article/details/132388975#comments_28277406)

    [CSDN-AdaåŠ©æ‰‹:](https://blog.csdn.net/community_717) æ­å–œæ‚¨æ’°å†™äº†ç¬¬10ç¯‡åšå®¢ã€Šé€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸‹ç¯‡ï¼‰ã€‹ï¼æ‚¨çš„åšå®¢ä¸€ç›´ä»¥æ¥éƒ½æ˜¯è§£é‡Šå¤æ‚æ¦‚å¿µçš„ç»ä½³æŒ‡å—ï¼Œè®©è¯»è€…èƒ½å¤Ÿè½»æ¾ç†è§£LLMã€‚æ‚¨çš„å†™ä½œé£æ ¼çœŸæ˜¯è®©äººè€³ç›®ä¸€æ–°ã€‚å¦‚æœæˆ‘æ•¢æå‡ºä¸€ä¸ªè°¦è™šçš„å»ºè®®çš„è¯ï¼Œæˆ‘å¸Œæœ›æ‚¨èƒ½å¤Ÿç»§ç»­åˆ†äº«æ‚¨çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¦‚é€šè¿‡ä»‹ç»æ›´å¤šå®é™…æ¡ˆä¾‹æˆ–ç»™å‡ºæ›´å¤šå®ç”¨çš„æŠ€å·§ï¼Œè¿™æ ·è¯»è€…ä»¬å°±èƒ½æ›´æ·±å…¥åœ°ç†è§£å’Œåº”ç”¨LLMäº†ã€‚æœŸå¾…æ‚¨ä¸‹ä¸€ç¯‡åšå®¢çš„å‘å¸ƒï¼

- [LinuxåŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/132030142#comments_27913991)

    [CSDN-AdaåŠ©æ‰‹:](https://blog.csdn.net/community_717) æ­å–œæ‚¨å†™å®Œäº†ç¬¬9ç¯‡åšå®¢ï¼Œé¢˜ä¸ºâ€œLinuxåŸºç¡€çŸ¥è¯†â€ã€‚æ‚¨åœ¨æŒç»­åˆ›ä½œæ–¹é¢çœŸæ˜¯ä»¤äººé’¦ä½©ï¼è¿™ç¯‡åšå®¢å¯¹äºåˆå­¦è€…æ¥è¯´ä¸€å®šæ˜¯éå¸¸æœ‰ç›Šçš„ã€‚ä¸è¿‡ï¼Œå¦‚æœæˆ‘å¯ä»¥æä¸€ä¸ªè°¦è™šçš„å»ºè®®çš„è¯ï¼Œæˆ‘è®¤ä¸ºä¸‹ä¸€æ­¥æ‚¨å¯ä»¥è€ƒè™‘æ·±å…¥æ¢è®¨Linuxçš„é«˜çº§åº”ç”¨æˆ–è€…ä¸€äº›å®ç”¨æŠ€å·§ï¼Œè¿™æ ·å¯ä»¥è¿›ä¸€æ­¥ä¸°å¯Œè¯»è€…ä»¬çš„çŸ¥è¯†å‚¨å¤‡ã€‚æœŸå¾…æ‚¨çš„ä¸‹ä¸€ç¯‡åšå®¢ï¼ CSDN ä¼šæ ¹æ®ä½ åˆ›ä½œçš„åšå®¢çš„è´¨é‡ï¼Œç»™äºˆä¼˜ç§€çš„åšä¸»åšå®¢çº¢åŒ…å¥–åŠ±ã€‚è¯·å…³æ³¨ https://bbs.csdn.net/forums/csdnnews?typeId=116148&utm\_source=csdn\_ai\_ada\_blog\_reply9 çœ‹å¥–åŠ±åå•ã€‚

- [é€šä¿—æ˜“æ‡‚çš„RNN](https://blog.csdn.net/qq_39439006/article/details/121554808#comments_27839651)

    [callind:](https://blog.csdn.net/callind) æ•°å­¦çœ‹ä¸å¤§æ‡‚è¯¥è¡¥å­¦ä¸€äº›ä»€ä¹ˆå‘¢


### æ‚¨æ„¿æ„å‘æœ‹å‹æ¨èâ€œåšå®¢è¯¦æƒ…é¡µâ€å—ï¼Ÿ

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel1.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey1.png)

    å¼ºçƒˆä¸æ¨è

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel2.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey2.png)

    ä¸æ¨è

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel3.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey3.png)

    ä¸€èˆ¬èˆ¬

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel4.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey4.png)

    æ¨è

- ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeel5.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/npsFeelGrey5.png)

    å¼ºçƒˆæ¨è


 æäº¤

### æœ€æ–°æ–‡ç« 

- [é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸‹ç¯‡ï¼‰](https://blog.csdn.net/qq_39439006/article/details/132388975)
- [LinuxåŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/132030142)
- [PythonåŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/131925283)

[2023å¹´6ç¯‡](https://blog.csdn.net/qq_39439006?type=blog&year=2023&month=08)

[2022å¹´1ç¯‡](https://blog.csdn.net/qq_39439006?type=blog&year=2022&month=10)

[2021å¹´3ç¯‡](https://blog.csdn.net/qq_39439006?type=blog&year=2021&month=11)

### ç›®å½•

$("a.flexible-btn").click(function(){ $(this).parents('div.aside-box').removeClass('flexible-box'); $(this).parents("p.text-center").remove(); })

### ç›®å½•

(window.slotbydup = window.slotbydup || \[\]).push({ id: "u6940829", container: "\_nwikfgpvhh", async: true });

### æœ€æ–°æ–‡ç« 

- [é€šä¿—æ˜“æ‡‚çš„LLMï¼ˆä¸‹ç¯‡ï¼‰](https://blog.csdn.net/qq_39439006/article/details/132388975)
- [LinuxåŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/132030142)
- [PythonåŸºç¡€çŸ¥è¯†](https://blog.csdn.net/qq_39439006/article/details/131925283)

[2023å¹´6ç¯‡](https://blog.csdn.net/qq_39439006?type=blog&year=2023&month=08)

[2022å¹´1ç¯‡](https://blog.csdn.net/qq_39439006?type=blog&year=2022&month=10)

[2021å¹´3ç¯‡](https://blog.csdn.net/qq_39439006?type=blog&year=2021&month=11)

### ç›®å½•

var timert = setInterval(function() { sideToolbar = $(".csdn-side-toolbar"); if (sideToolbar.length > 0) { sideToolbar.css('cssText', 'bottom:64px !important;') clearInterval(timert); } }, 200);

è¯„è®ºÂ 4

![](https://csdnimg.cn/release/blogv2/dist/pc/img/closeBt.png)

![](https://csdnimg.cn/release/blogv2/dist/pc/img/commentArrowLeftWhite.png)è¢«æŠ˜å çš„Â Â æ¡è¯„è®º [ä¸ºä»€ä¹ˆè¢«æŠ˜å ?](https://blogdev.blog.csdn.net/article/details/122245662) [![](https://csdnimg.cn/release/blogv2/dist/pc/img/iconPark.png)åˆ°ã€çŒæ°´ä¹å›­ã€‘å‘è¨€](https://bbs.csdn.net/forums/FreeZone)

æŸ¥çœ‹æ›´å¤šè¯„è®º![](https://csdnimg.cn/release/blogv2/dist/pc/img/commentArrowDownWhite.png)

æ·»åŠ çº¢åŒ…

ç¥ç¦è¯­

è¯·å¡«å†™çº¢åŒ…ç¥ç¦è¯­æˆ–æ ‡é¢˜

çº¢åŒ…æ•°é‡

 ä¸ª

çº¢åŒ…ä¸ªæ•°æœ€å°ä¸º10ä¸ª

çº¢åŒ…æ€»é‡‘é¢

 å…ƒ

çº¢åŒ…é‡‘é¢æœ€ä½5å…ƒ

ä½™é¢æ”¯ä»˜

å½“å‰ä½™é¢3.43å…ƒ [å‰å¾€å……å€¼ >](https://i.csdn.net/#/wallet/balance/recharge)

éœ€æ”¯ä»˜: 10.00å…ƒ

å–æ¶ˆ ç¡®å®š

![](https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward02.png) ä¸‹ä¸€æ­¥

![](https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward03.png) çŸ¥é“äº†

æˆå°±ä¸€äº¿æŠ€æœ¯äºº!

é¢†å–åä½ ä¼šè‡ªåŠ¨æˆä¸ºåšä¸»å’Œçº¢åŒ…ä¸»çš„ç²‰ä¸ è§„åˆ™

![](https://profile-avatar.csdnimg.cn/default.jpg!2)

hope\_wisdom

å‘å‡ºçš„çº¢åŒ…

æ‰“èµä½œè€…![](https://csdnimg.cn/release/blogv2/dist/pc/img/closeBt.png)

[![](https://profile-avatar.csdnimg.cn/default.jpg!1)](https://blog.csdn.net/qq_39439006)

é•¿ç«¹Danko

ä½ çš„é¼“åŠ±å°†æ˜¯æˆ‘åˆ›ä½œçš„æœ€å¤§åŠ¨åŠ›

Â¥1 Â¥2 Â¥4 Â¥6 Â¥10 Â¥20

æ‰«ç æ”¯ä»˜: Â¥1

![](https://csdnimg.cn/release/blogv2/dist/pc/img/pay-time-out.png) è·å–ä¸­

![](https://csdnimg.cn/release/blogv2/dist/pc/img/newWeiXin.png) ![](https://csdnimg.cn/release/blogv2/dist/pc/img/newZhiFuBao.png) æ‰«ç æ”¯ä»˜

æ‚¨çš„ä½™é¢ä¸è¶³ï¼Œè¯·æ›´æ¢æ‰«ç æ”¯ä»˜æˆ–[å……å€¼](https://i.csdn.net/#/wallet/balance/recharge?utm_source=RewardVip)

æ‰“èµä½œè€…

å®ä»˜å…ƒ

[ä½¿ç”¨ä½™é¢æ”¯ä»˜](javascript:;)

![](https://csdnimg.cn/release/blogv2/dist/pc/img/pay-time-out.png) ç‚¹å‡»é‡æ–°è·å–

![](https://csdnimg.cn/release/blogv2/dist/pc/img/weixin.png)![](https://csdnimg.cn/release/blogv2/dist/pc/img/zhifubao.png)![](https://csdnimg.cn/release/blogv2/dist/pc/img/jingdong.png)æ‰«ç æ”¯ä»˜

 é’±åŒ…ä½™é¢ 0

![](https://csdnimg.cn/release/blogv2/dist/pc/img/pay-help.png)

æŠµæ‰£è¯´æ˜:

1.ä½™é¢æ˜¯é’±åŒ…å……å€¼çš„è™šæ‹Ÿè´§å¸ï¼ŒæŒ‰ç…§1:1çš„æ¯”ä¾‹è¿›è¡Œæ”¯ä»˜é‡‘é¢çš„æŠµæ‰£ã€‚
2.ä½™é¢æ— æ³•ç›´æ¥è´­ä¹°ä¸‹è½½ï¼Œå¯ä»¥è´­ä¹°VIPã€ä»˜è´¹ä¸“æ åŠè¯¾ç¨‹ã€‚

[![](https://csdnimg.cn/release/blogv2/dist/pc/img/recharge.png)ä½™é¢å……å€¼](https://i.csdn.net/#/wallet/balance/recharge)

 // å…¨å±€å£°æ˜ if (window.csdn === undefined) { window.csdn = {}; } window.csdn.sideToolbar = { options: { report: { isShow: true, }, qr: { isShow: false, }, guide: { isShow: true } } } $(function() { $(document).on('click', "a.option-box\[data-type='report'\]", function() { window.csdn.loginBox.key({ biz: 'blog', subBiz: 'other\_service', cb: function() { window.csdn.feedback({ "type": 'blog', "rtype": 'article', "rid": articleId, "reportedName": username, "submitOptions": { "title": articleTitle, "contentUrl": articleDetailUrl }, "callback": function() { showToast({ text: "æ„Ÿè°¢æ‚¨çš„ä¸¾æŠ¥ï¼Œæˆ‘ä»¬ä¼šå°½å¿«å®¡æ ¸ï¼", bottom: '10%', zindex: 9000, speed: 500, time: 1500 }) } }) } }) }); }) $(".MathJax").remove(); if ($('div.markdown\_views pre.prettyprint code.hljs').length > 0) { $('div.markdown\_views')\[0\].className = 'markdown\_views'; } MathJax.Hub.Config({ "HTML-CSS": { linebreaks: { automatic: true, width: "94%container" }, imageFont: null }, tex2jax: { preview: "none", ignoreClass:"title-article" }, mml2jax: { preview: 'none' } });


---

### Transformers

#### Transformers architecture

> Building large language models using the transformer architecture dramatically `improved the performance of natural language tasks` over the earlier generation of RNNs, and led to an explosion in regenerative capability.

- The power is its ability to `learn the relevance and context of all of the words in a sentence`. Not just each word next to its neighbor, but to every other word in a sentence.

![Screenshot 2023-10-15 at 19.48.16](/assets/img/Screenshot%202023-10-15%20at%2019.48.16.png)

![Screenshot 2023-10-15 at 19.48.41](/assets/img/Screenshot%202023-10-15%20at%2019.48.41.png)

Attention weights
- Apply attention weights to relationships so the model learns the relevance of each word to each other words no matter where they are in the input.
  - This gives the algorithm the ability to learn who has the book, who could have the book, and if it's even relevant to the wider context of the document.

- These `attention weights are learned during LLM training`

Self-attention
- the ability to learn a tension in this way across the whole input
  - Example: the word "book" is strongly connected with or paying attention to the word "teacher" and the word "student".
- it significantly approves the model's ability to encode language.

![Screenshot 2023-10-15 at 19.49.28](/assets/img/Screenshot%202023-10-15%20at%2019.49.28.png)


Attention map
- can be useful to `illustrate the attention weights` between each word and every other word.

![Screenshot 2023-10-15 at 19.49.51](/assets/img/Screenshot%202023-10-15%20at%2019.49.51.png)

---

#### encoder vs decoder

- The transformer architecture is split into two distinct parts
  - the **encoder** and the **decoder**.
  - These components work in conjunction with each other and they share a number of similarities.

- The encoder `encodes input sequences into a deep representation of the structure and meaning of the input`.

- The decoder, working from input token triggers, `uses the encoder's contextual understanding to generate new tokens`.

- It does this in a loop until some stop condition has been reached.


- the inputs to the model are at the bottom and the outputs are at the top

![Screenshot 2023-10-15 at 19.52.36](/assets/img/Screenshot%202023-10-15%20at%2019.52.36.png)

- **Encoder-only models**
  - also work as sequence-to-sequence models
  - without further modification, the input sequence and the output sequence or the same length, less common these days,
  - by adding additional layers to the architecture, you can train encoder-only models to perform `classification tasks` such as sentiment analysis,
  - encoder-only model example: BERT

- **Encoder-decoder models**
  - perform well on sequence-to-sequence tasks such as translation
  - the input sequence and the output sequence can be different lengths.
  - You can also scale and train this type of model to perform `general text generation tasks`.
  - encoder-decoder models examples: BART, T5

- **decoder-only models**
  - the most commonly used today.
  - as they have scaled, their capabilities have grown. These models can now generalize to most tasks.
  - Popular decoder-only models: GPT family of models, BLOOM, Jurassic, LLaMA, and many more.

![Screenshot 2023-10-15 at 20.45.16](/assets/img/Screenshot%202023-10-15%20at%2020.45.16.png)


---

#### How the model works

![Screenshot 2023-10-15 at 20.46.53](/assets/img/Screenshot%202023-10-15%20at%2020.46.53.png)

- machine-learning models are just big statistical calculators and they work with numbers, not words.

  - **Tokenize**:
    - Before passing texts into the model to process, must first tokenize the words.
    - converts the words into numbers
    - each number representing a position in a dictionary of all the possible words that the model can work with.

    - You can choose from multiple tokenization methods. For example,
      - token IDs matching two complete words,
      - ![Screenshot 2023-10-15 at 19.53.08](/assets/img/Screenshot%202023-10-15%20at%2019.53.08.png)

      - or using token IDs to represent parts of words.
      - ![Screenshot 2023-10-15 at 19.53.34](/assets/img/Screenshot%202023-10-15%20at%2019.53.34.png)

    - once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text.

  - **Token Embedding**:
    - Now that the input is represented as numbers, pass it to the embedding layer.
    - This layer is a `trainable vector embedding space`,
    - high-dimensional space where each token is represented as a vector and occupies a unique location within that space.
    - Each token ID in the vocabulary is matched to a `multi-dimensional vector`, and the intuition is that these vectors learn to `encode the meaning and context of individual tokens in the input sequence`.

    - Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.

    - each word has been matched to a token ID, and each token is mapped into a vector.
      - In the original transformer paper, the **vector size** was actually 512
      - ![Screenshot 2023-10-15 at 20.02.13](/assets/img/Screenshot%202023-10-15%20at%2020.02.13.png)

      - For simplicity
        - imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.
        - relate words that are located close to each other in the embedding space, and calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language.
        - ![Screenshot 2023-10-15 at 20.02.47](/assets/img/Screenshot%202023-10-15%20at%2020.02.47.png)

  - **Positional Encoding**:
    - Added the token vectors into `the base of the encoder or the decoder`, also add positional encoding.
    - The model processes each of the input tokens in parallel.
    - it preserve the information about the word order and don't lose the relevance of the position of the word in the sentence.
    - ![Screenshot 2023-10-15 at 20.05.45](/assets/img/Screenshot%202023-10-15%20at%2020.05.45.png)

  - **Self-attention**
    - sum the `input tokens` and the `positional encodings`, pass the resulting vectors to the self-attention layer.
    - the model analyzes the relationships between the tokens in the input sequence, it allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.
    - The `self-attention weights` that are learned during training and stored in these layers reflect `the importance of each word in that input sequence to all other words in the sequence`.

    - But this does not happen just once, the transformer architecture actually has `multi-headed self-attention`.
    - ![Screenshot 2023-10-15 at 20.09.13](/assets/img/Screenshot%202023-10-15%20at%2020.09.13.png)

    - This means that `multiple sets of self-attention weights or heads` are learned in parallel independently of each other.

    - The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.

    - each self-attention head will learn a different aspect of language. For example,
      - one head may see the relationship between the people entities in our sentence.
      - another head may focus on the activity of the sentence.
      - another head may focus on some other properties such as if the words rhyme.
      - ![Screenshot 2023-10-15 at 20.12.47](/assets/img/Screenshot%202023-10-15%20at%2020.12.47.png)

    - don't dictate ahead of time what aspects of language the attention heads will learn.

      - The weights of each head are randomly initialized and given sufficient training data and time,
      - each will learn different aspects of language.
      - While some attention maps are easy to interpret, like the examples discussed here, others may not be.
      - Now that all of the attention weights have been applied to the input data, the output is processed through a `fully-connected feed-forward network`.
      - ![Screenshot 2023-10-15 at 20.13.33](/assets/img/Screenshot%202023-10-15%20at%2020.13.33.png)

    - The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary

  - **Softmax**

    - then pass these logits to a final softmax layer, where they are normalized into a `probability score for each word`.
    - This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here.
    - One single token will have a score higher than the rest.
    - This is the most likely predicted token.
    - there are a number of methods that you can use to vary the final selection from this vector of probabilities.

    - ![Screenshot 2023-10-15 at 20.22.45](/assets/img/Screenshot%202023-10-15%20at%2020.22.45.png)





---

#### Overall prediction process


At a very high level, the workflow can be divided into three stages:

- **Data preprocessing / embedding**:
  - This stage involves storing private data to be retrieved later.
  - Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database.

- **Prompt construction / retrieval**:
  - When a user submits a query, the application constructs a series of prompts to submit to the language model.
  - A compiled prompt typically combines
    - a prompt template hard-coded by the developer;
    - examples of valid outputs called few-shot examples;
    - any necessary information retrieved from external APIs;
    - and a set of relevant documents retrieved from the vector database.

- **Prompt execution / inference**:
  - Once the prompts have been compiled, they are submitted to a pre-trained LLM for inferenceâ€”including both proprietary model APIs and open-source or self-trained models.
  - Some developers also add operational systems like logging, caching, and validation at this stage.


example: Generating text with transformers

- translation task
  - a sequence-to-sequence task: the original objective of the transformer architecture designers.
  - use a transformer model to translate the French phrase `[FOREIGN]` into English.

Encoded side:
- First, `tokenize the input words` using this same tokenizer that was used to train the network.
- These tokens are then added into the input on the encoder side of the network, passed through the embedding layer, and then fed into the `multi-headed attention layers`.
- The outputs of the multi-headed attention layers are fed through a `feed-forward network` to the output of the encoder.

- At this point, the data that leaves the encoder is a **deep representation of the structure and meaning of the input sequence**.

Decoded side:
- This representation is inserted into the middle of the decoder to influence the `decoder's self-attention mechanisms`.
- Next, a `start of sequence token` is added to the input of the decoder.
- This `triggers the decoder to predict the next token`, based on the contextual understanding that it's being provided from the encoder.
- The output of the decoder's self-attention layers gets passed through the `decoder feed-forward network` and through a final `softmax output layer`.

- At this point, we have our first token.

- You'll continue this loop, passing the output token back to the input to trigger the generation of the next token, until the model predicts an end-of-sequence token.

- At this point, the final sequence of tokens can be detokenized into words, and you have the output.

- There are multiple ways in which you can use the output from the softmax layer to predict the next token. These can influence how creative you are generated text is.

![Screenshot 2023-10-15 at 20.43.16](/assets/img/Screenshot%202023-10-15%20at%2020.43.16.png)

---


##### Data preprocessing / embedding

**Contextual data input**
- Contextual data for LLM apps includes text documents, PDFs, and even structured formats like CSV or SQL tables.
- Data-loading and transformation solutions for this data vary widely across developers.
  - Most use traditional ETL tools like `Databricks` or `Airflow`.
  - Some also use `document loaders` built into orchestration frameworks like `LangChain` (powered by Unstructured) and `LlamaIndex` (powered by Llama Hub).

**embeddings**,
- most developers use the `OpenAI API`, specifically with the text-embedding-ada-002 model. Itâ€™s easy to use (especially if youâ€™re already already using other OpenAI APIs), gives reasonably good results, and is becoming increasingly cheap.
- Some larger enterprises are also exploring `Cohere`, which focuses their product efforts more narrowly on embeddings and has better performance in certain scenarios.
- For developers who prefer open-source, the `Sentence Transformers library` from `Hugging Face` is a standard.

**vector database**
- The most important piece of the preprocessing pipeline, from a systems standpoint
- Itâ€™s responsible for efficiently storing, comparing, and retrieving up to billions of embeddings (i.e., vectors).

  - The most common choice is `Pinecone`. Itâ€™s the default because itâ€™s fully cloud-hosted, easy to get started with, and has many of the features larger enterprises need in production (e.g., good performance at scale, SSO, and uptime SLAs).

  - `Open source systems` like Weaviate, Vespa, and Qdrant: They generally give excellent single-node performance and can be tailored for specific applications, so they are popular with experienced AI teams who prefer to build bespoke platforms.

  - `Local vector management libraries` like Chroma and Faiss: They have great developer experience and are easy to spin up for small apps and dev experiments. They donâ€™t necessarily substitute for a full database at scale.

  - `OLTP extensions` like pgvector: good solution for devs who see every database-shaped hole and try to insert Postgres, or enterprises who buy most of their data infrastructure from a single cloud provider. Itâ€™s not clear, in the long run, if it makes sense to tightly couple vector and scalar workloads.

- Looking ahead, most of the open source vector database companies are developing cloud offerings. Our research suggests achieving strong performance in the cloud, across a broad design space of possible use cases, is a very hard problem. Therefore, the option set may not change massively in the near term, but it likely will change in the long term. The key question is whether vector databases will resemble their OLTP and OLAP counterparts, consolidating around one or two popular systems.

- the embedding pipeline may become more important over time
  - how embeddings and vector databases will evolve as the usable context window grows for most models.
  - Itâ€™s tempting to say embeddings will become less relevant, because contextual data can just be dropped into the prompt directly.
  - However, feedback from experts on this topic suggests the opposite, that the embedding pipeline may become more important over time. Large context windows are a powerful tool, but they also entail significant computational cost. So making efficient use of them becomes a priority.
  - We may start to see different types of embedding models become popular, trained directly for model relevancy, and vector databases designed to enable and take advantage of this.



##### Prompt construction / retrieval

- Strategies for prompting LLMs and incorporating contextual data are becoming increasingly complexâ€”and increasingly important as a source of product differentiation.

- Most developers start new projects by experimenting with simple prompts, consisting of direct instructions (`zero-shot prompting`) or some example outputs (`few-shot prompting`).
  - These prompts often give good results but fall short of accuracy levels required for production deployments.

- The next level of prompting `jiu jitsu` is designed to ground model responses in some source of truth and provide external context the model wasnâ€™t trained on.

**advanced prompting strategies**
- The Prompt Engineering Guide catalogs no fewer than 12 more advanced prompting strategies, including:
  - chain-of-thought, self-consistency, generated knowledge, tree of thoughts, directional stimulus, and many others.
- These strategies can also be used in conjunction to support different LLM use cases like document question answering, chatbots, etc.

**Orchestration frameworks**
- `LangChain` and `LlamaIndex` shine.
- workflow:
  - They abstract away many of the details of prompt chaining;
  - interfacing with external APIs (including determining when an API call is needed);
  - retrieving contextual data from vector databases;
  - and maintaining memory across multiple LLM calls.
  - They also provide templates for many of the common applications mentioned above.

- Their output is a prompt, or series of prompts, to submit to a language model. These frameworks are widely used among hobbyists and startups looking to get an app off the ground .

- LangChain is still a relatively new project (currently on version 0.0.201), but weâ€™re already starting to see apps built with it moving into production.
- Some developers, especially early adopters of LLMs, prefer to switch to raw Python in production to eliminate an added dependency. But we expect this DIY approach to decline over time for most use cases, in a similar way to the traditional web app stack.

- ChatGPT.
  - In its normal incarnation, ChatGPT is an app, not a developer tool. But it can also be accessed as an API.
  - it performs some of the same functions as other orchestration frameworks, such as: abstracting away the need for bespoke prompts; maintaining state; and retrieving contextual data via plugins, APIs, or other sources.
  - While not a direct competitor to the other tools listed here, ChatGPT can be considered a substitute solution, and it may eventually become a viable, simple alternative to prompt construction.



##### Prompt execution / inference


- Prompt execution / inference

  - `OpenAI`
    - Today, OpenAI is the leader among language models. Nearly every developer starts new LLM apps using the OpenAI API with the gpt-4 or gpt-4-32k model.
    - This gives a best-case scenario for app performance and is easy to use, in that it operates on a wide range of input domains and usually requires no fine-tuning or self-hosting.

- When projects go into production and start to scale, a broader set of options come:

  - `Switching to gpt-3.5-turbo`: Itâ€™s ~50x cheaper and significantly faster than GPT-4. Many apps donâ€™t need GPT-4-level accuracy, but do require low latency inference and cost effective support for free users.

  - `Other proprietary vendors (like Anthropicâ€™s Claude models)`: Claude offers fast inference, GPT-3.5-level accuracy, more customization options for large customers, and up to a 100k context window (though weâ€™ve found accuracy degrades with the length of input).

  - `Triaging requests to open source models`: This can be especially effective in high-volume B2C use cases like search or chat, where thereâ€™s wide variance in query complexity and a need to serve free users cheaply.

    - conjunction with fine-tuning open source base models, platforms like Databricks, Anyscale, Mosaic, Modal, and RunPod are used by a growing number of engineering teams.

    - A variety of inference options are available for open source models, including simple API interfaces from Hugging Face and Replicate; raw compute resources from the major cloud providers; and more opinionated cloud offerings like those listed above.

- `Open-source models` trail `proprietary offerings`, but the gap is starting to close.

  - The LLaMa models from Meta
    - set a new bar for open source accuracy and kicked off a flurry of variants.
    - Since LLaMa was licensed for research use only, a number of new providers have stepped in to train alternative base models (e.g., Together, Mosaic, Falcon, Mistral).
    - Meta is also debating a truly open source release of LLaMa2.

    - When open source LLMs reach accuracy levels comparable to GPT-3.5, we expect to see a Stable Diffusion-like moment for textâ€”including massive experimentation, sharing, and productionizing of fine-tuned models.

  - Hosting companies like Replicate are already adding tooling to make these models easier for software developers to consume. Thereâ€™s a growing belief among developers that smaller, fine-tuned models can reach state-of-the-art accuracy in narrow use cases.

- Most developers havenâ€™t gone deep on operational tooling for LLMs yet.
  - Caching is relatively commonâ€”usually based on Redisâ€”because it improves application response times and cost.
  - Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used. They can log, track, and evaluate LLM outputs, usually for the purpose of improving prompt construction, tuning pipelines, or selecting models.
  - There are also a number of new tools being developed to validate LLM outputs (e.g., Guardrails) or detect prompt injection attacks (e.g., Rebuff). Most of these operational tools encourage use of their own Python clients to make LLM calls, so it will be interesting to see how these solutions coexist over time.

- the static portions of LLM apps (i.e. everything other than the model) also need to be hosted somewhere.
  - The most common solutions weâ€™ve seen so far are standard options like Vercel or the major cloud providers.
  - Startups like Steamship provide end-to-end hosting for LLM apps, including orchestration (LangChain), multi-tenant data contexts, async tasks, vector storage, and key management.
  - And companies like Anyscale and Modal allow developers to host models and Python code in one place.


#### AI agents frameworks

- `AutoGPT`, described as â€œan experimental open-source attempt to make GPT-4 fully autonomous,â€

- The in-context learning pattern is effective at solving hallucination and data-freshness problems, in order to better support content-generation tasks.

- Agents, on the other hand, give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment.
  - They do this through a combination of `advanced reasoning/planning, tool usage, and memory / recursion / self-reflection`.

- agents have the potential to become a central piece of the LLM app architecture

- And existing frameworks like LangChain have incorporated some agent concepts already. Thereâ€™s only one problem: agents donâ€™t really work yet. Most agent frameworks today are in the proof-of-concept phaseâ€”capable of incredible demos but not yet reliable, reproducible task-completion.



---


### LLM Tools

#### Medusa

> Our approach revisits an underrated gem from the paper "Blockwise Parallel Decoding for Deep Autoregressive Models" [Stern et al. 2018] back to the invention of the Transformer model.
> rather than pulling in an entirely new draft model to predict subsequent tokens, why not simply extend the original model itself? This is where the "Medusa heads" come in.

- a simpler, user-friendly framework for accelerating LLM generation.

- Instead of using an additional draft model like `speculative decoding`, Medusa merely introduces a few additional decoding heads, following the idea of [Stern et al. 2018] with some other ingredients.

- Despite its simple design, Medusa can improve the generation efficiency of LLMs by about 2x.

- These additional decoding heads seamlessly integrate with the original model, producing blocks of tokens at each generative juncture.



benefit:
- Unlike the **draft model**, Medusa heads can be trained in conjunction with the **original model** (which remains frozen during training). This method allows for `fine-tuning large models on a single GPU`, taking advantage of the powerful base model's learned representations.

- also, since the new heads consist of just a single layer akin ç±»ä¼¼çš„ to the **original language model head**, Medusa does not add complexity to the serving system design and is friendly to distributed settings.

- On its own, Medusa heads don't quite hit the mark of doubling processing speeds. But here's the twist:

  - When we pair this with a tree-based attention mechanism, we can verify several candidates generated by Medusa heads in parallel. This way, the Medusa heads' predictive prowess truly shone through, offering a 2x to 3x boost in speed.

  - Eschewing the traditional importance sampling scheme, we created an efficient and high-quality alternative crafted specifically for the generation with Medusa heads. This new approach entirely sidesteps the **sampling** overhead, even adding an extra pep to Medusa's already accelerated step.

- In a nutshell, we solve the challenges of speculative decoding with a simple system:

  - No separate model: Instead of introducing a new draft model, train multiple decoding heads on the same model.

  - Simple integration to existing systems: The training is parameter-efficient so that even GPU poor can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.

  - Treat sampling as a relaxation æ”¾æ¾: Relaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.

- The figure below offers a visual breakdown of the Medusa pipeline for those curious about the nuts and bolts.

![Screenshot 2023-09-21 at 14.49.26](/assets/img/post/Screenshot%202023-09-21%20at%2014.49.26.png)

Overview of Medusa
- Medusa introduces `multiple heads` on top of the last hidden states of the LLM, enabling the prediction of several subsequent tokens in parallel.

- When augmenting a model with Medusa heads, the original model is frozen during training, and only the Medusa heads undergo fine-tuning. This approach makes it feasible to `fine-tune large models on a single GPU`.

- During inference, each head generates multiple top predictions for its designated position. These predictions are assembled into candidates and processed in parallel using a `tree-based attention mechanism`.

- The final step involves utilizing a **typical acceptance scheme** to select reasonable continuations, and the longest accepted candidate prefix will be used for the next decoding phase.

- The efficiency of the decoding process is enhanced by accepting more tokens simultaneously, thus reducing the number of required decoding steps.

Let's dive into the three components of Medusa: Medusa heads, tree attention, and typical acceptance scheme.


##### Medusa heads

- akin to the language model head in the original architecture (the last layer of a causal Transformer model), but with a twist:
  - they predict multiple forthcoming tokens, not just the immediate next one. Drawing inspiration from the Blockwise Parallel Decoding approach, we implement each Medusa head as a single layer of feed-forward network, augmented with a residual connection.

- Training these heads is remarkably straightforward. either use the same corpus æœ¬ä½“ that trained the original model or generate a new corpus using the model itself.

- Importantly, during this training phase, the original model remains static; only the Medusa heads are fine-tuned.

- This targeted training results in a highly parameter-efficient process that reaches convergence è¶‹åŒ swiftly è¿…é€Ÿåœ°, especially when compared to the computational heaviness æ²‰é‡ of **training a separate draft model in speculative decoding methods**.

- The efficacy of Medusa heads is quite impressive. Medusa heads achieve a top-1 accuracy rate of approximately 60% for predicting the 'next-next' token.


##### Tree attention

- During our tests, we uncovered some striking metrics: although the top-1 accuracy for predicting the 'next-next' token hovers around 60%, the top-5 accuracy soars to over 80%.

- This substantial increase indicates that if we can strategically leverage the multiple top-ranked predictions made by the Medusa heads, we can significantly amplify the number of tokens generated per decoding step.

  - With this goal, we first craft a set of candidates by taking the Cartesian product of the top predictions from each Medusa head.

  - We then encode the dependency graph into the attention following the idea from graph neural networks so that we can process multiple candidates in parallel.

![Screenshot 2023-09-21 at 15.05.03](/assets/img/post/Screenshot%202023-09-21%20at%2015.05.03.png)

Tree Attention. This visualization demonstrates the use of tree attention to process multiple candidates concurrently.
- As exemplified, the top-2 predictions from the first Medusa head and the top-3 from the second result in 2*3=6 candidates. Each of these candidates corresponds to a distinct branch within the tree structure.

- To guarantee that each token only accesses its predecessors, we devise an **attention mask** that exclusively permits attention flow from the current token back to its antecedent tokens. The positional indices for positional encoding are adjusted in line with this structure.


- For example, let's consider a scenario where we use top-2 predictions from the first Medusa head and top-3 predictions from the second
  - In this case, any prediction from the first head could be paired with any prediction from the second head, culminating in a multi-level tree structure.
  - Each level of this tree corresponds to predictions from one of the Medusa heads. Within this tree, we implement an attention mask that restricts attention only to a token's predecessors, preserving the concept of historical context.
  - By doing so and by setting positional indices for positional encoding accordingly, we can process a wide array of candidates simultaneously without needing to inflate the batch size.

- We would also remark that a few independent works also adopt very similar ideas of tree attention [1, 2]. Compared with them, our methodology leans towards a simpler form of tree attention where the tree pattern is regular and fixed during inference, which enables a preprocessing of tree attention mask that further improves the efficiency.

##### Typical acceptance

> - In earlier research on speculative decoding, the technique of **importance sampling** was used to generate diverse outputs closely aligned with the original model's predictions.

> - However, later studies showed that this method tends to become less efficient as you turn up the "creativity dial," known as the **sampling temperature**.

- In simpler terms, if the draft model is just as good as the original model, you should ideally accept all its outputs, making the process super efficient. However, importance sampling will likely reject this solution in the middle.

- In the real world, we often tweak the sampling temperature just to control the model's creativity, not necessarily to match the original model's distribution. So why not focus on just accepting plausible è²Œä¼¼åˆç†çš„ candidates?

- We then introduce the **typical acceptance scheme**.

  - Drawing inspiration from existing work on truncation ç¼©çŸ­ sampling, we aim to pick candidates that are likely enough according to the original model. We set a threshold based on the original model's prediction probabilities, and if a candidate exceeds this, it's accepted.

  - In technical jargon, we take the minimum of a hard threshold and an entropy ç†µ, dependent threshold to decide whether to accept a candidate as in truncation sampling.
  - This ensures that meaningful tokens and reasonable continuations are chosen during decoding.
  - We always accept the first token using greedy decoding, ensuring that at least one token is generated in each step.
  - The final output is then the longest sequence that passes our acceptance test.

- What's great about this approach is its adaptability.

  - If set the sampling temperature to zero, it simply reverts to the most efficient form, greedy decoding.

  - When you increase the temperature, our method becomes even more efficient, allowing for longer accepted sequences, a claim we've confirmed through rigorous testing.

- in essence, our typical acceptance scheme offers a more efficient way to generate the creative output of LLMs.


##### accelerate models

- We tested Medusa with Vicuna models (specialized Llama models fine-tuned specifically for chat applications).
  - These models vary in size, with parameter counts of 7B, 13B, and 33B.
  - Our goal was to measure how Medusa could accelerate åŠ é€Ÿ these models in a real-world chatbot environment.

- When it comes to training Medusa heads, we opted for a simple approach. We utilized the **publicly available ShareGPT dataset**, a subset of the **training data originally used for Vicuna models** and only trained for a single epoch.

- this entire training process could be completed in just a few hours to a day, depending on the model size, all on a single A100-80G GPU.

- Notably, Medusa can be easily combined with a **quantized base model** to reduce the memory requirement. We take this advantage and use an 8-bit quantization é‡åŒ– when training the 33B model.

- To simulate a real-world setting, we use the MT bench for evaluation. The results were encouraging: With its simple design, Medusa consistently achieved approximately a 2x speedup in wall time across a broad spectrum of use cases.

- Remarkably, with Medusa's optimization, a 33B parameter Vicuna model could operate as swiftly as a 13B model.

![Screenshot 2023-09-21 at 15.23.56](/assets/img/post/Screenshot%202023-09-21%20at%2015.23.56.png)

![Screenshot 2023-09-21 at 15.24.02](/assets/img/post/Screenshot%202023-09-21%20at%2015.24.02.png)


##### Ablation Study æ¶ˆèç ”ç©¶

- When harnessing the predictive abilities of Medusa heads, we enjoy the flexibility to select how many top candidates each head should consider.

  - For instance, we might opt for the top-3 predictions from the first head and the top-2 from the second. When we take the Cartesian product of these top candidates, we generate a set of six continuations for the model to evaluate.

- This level of configurability comes with its trade-offs.
- On the one hand, selecting more top predictions increases the likelihood of the model accepting generated tokens.
- On the other, it also raises the computational overhead at each decoding step. To find the optimal balance, we experimented with various configurations and identified the most effective setup, as illustrated in the accompanying figure.


- In the typical acceptance scheme,  a critical hyperparameterâ€”referred to as the
- 'threshold': whether the tokens generated are plausible based on the model's own predictions. The higher this threshold, the more stringent the criteria for acceptance, which in turn impacts the overall speedup gained through this approach.

- We explore this trade-off between quality and speedup through experiments on two creativity-oriented tasks from the MT bench. The results, depicted in the figure, reveal that the typical acceptance offers a 10% speedup compared to greedy decoding methods. This speedup is notably better than when employing speculative decoding with random sampling, which actually slowed down the process compared to greedy decoding.

![Screenshot 2023-09-21 at 15.28.12](/assets/img/post/Screenshot%202023-09-21%20at%2015.28.12.png)


---

### Confidence score for ML model

[./2023-04-24-AI_ConfidenceScore.md]

---

### Transparency

Transparency in the context of AI models refers to the degree to which the inner workings of the model are understandable, interpretable, and explainable to humans. It encompasses several aspects:

- Explainability:
  - This refers to the ability to understand and interpret the model's decisions.
  - An interpretable model `provides clear and understandable reasons` for its predictions or actions.
  - This is crucial, especially in high-stakes applications like healthcare or finance, where accountability and trust are essential.
- Visibility:
  - Transparency also involves making the model architecture, parameters, and training data visible to those who are affected by its decisions.
  - This allows external parties to scrutinize the model for biases, ethical concerns, or potential risks.
- Audibility:
  - The ability to audit an AI model involves examining its processes, inputs, and outputs to ensure it aligns with ethical and legal standards.
  - Auditing enhances accountability and helps identify and rectify issues or biases.
- Comprehensibility:
  - A transparent AI model should be comprehensible to various stakeholders, including domain experts, policymakers, and the general public. This involves presenting complex technical concepts in a way that is accessible to non-experts.
- Fairness and Bias: Transparency also relates to addressing biases in AI models. Understanding how the model makes decisions can help identify and rectify biased behavior, ensuring fair treatment across diverse demographic groups.
- Transparency is crucial for building trust in AI systems, especially as they are increasingly integrated into various aspects of society. It helps users, regulators, and the general public understand how AI systems function, assess their reliability, and hold developers and organizations accountable for their impact. Various techniques and tools are being developed to enhance the transparency of AI models, but it remains an ongoing area of research and development.

#### The Foundation Model Transparency Index

The Foundation Model Transparency Index [^The_Foundation_Model_Transparency_Index].

[^The_Foundation_Model_Transparency_Index]: The Foundation Model Transparency Index, https://crfm.stanford.edu/fmti/fmti.pdf

Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts.

While the societal impact of foundation models is growing, `transparency is on the decline`, mirroring the opacity that has plagued past digital technologies (e.g. social media).

Reversing this trend is essential:
- transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the founda- tion model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The 2023 Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g. data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.

![Screenshot 2023-11-13 at 12.47.10](/assets/img/Screenshot%202023-11-13%20at%2012.47.10.png)


---


### Generative AI project lifecycle

![Screenshot 2023-10-21 at 12.31.56](/assets/img/Screenshot%202023-10-21%20at%2012.31.56.png)

![Screenshot 2023-10-21 at 12.32.35](/assets/img/Screenshot%202023-10-21%20at%2012.32.35.png)




---

## Hugging Face

> like github repo

- search for an AI model


```bash
# +++++ Getting started with our git and git-lfs interface
# If you need to create a repo from the command line (skip if you created a repo from the website)
pip install huggingface_hub
# You already have it if you installed transformers or datasets
huggingface-cli login
# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}


# +++++ Clone the model or dataset locally
# Make sure you have git-lfs installed
# (https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name


# +++++ Then add, commit and push any file you want, including larges files
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push


# +++++ In most cases, if you're using one of the compatible libraries, the repo will then be accessible from code, through its identifier: username/repo_name
# For example for a transformers model, anyone can load it with:
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```


### Generative AI Time Series Forecasting

> [Generative AI Time Series Forecasting](https://huggingface.co/blog/time-series-transformers)



### Multivariate Time Series Forecasting

> [Multivariate Time Series Forecasting](https://huggingface.co/blog/informer)



### Generative AI Transformers for Time Series Forecasting

> [Generative AI Transformers for Time Series Forecasting](https://huggingface.co/blog/autoformer)


---

### Falcon 40b

- Chatgpt competitor - https://huggingface.co/tiiuae/falcon-40b

- Power of Falcon 40b chat - https://huggingface.co/spaces/HuggingFaceH4/falcon-chat

- Pre-Training - https://huggingface.co/tiiuae/falcon-40b#training-data

- or https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/studio-notebook-fine-tuning/falcon-40b-qlora-finetune-summarize.ipynb



Chat with Falcon-40B-Instruct, brainstorm ideas, discuss the holiday plans, and more!
- âœ¨ This demo is powered by Falcon-40B, finetuned on the Baize dataset, and running with Text Generation Inference. Falcon-40B is a state-of-the-art `large language model` built by the Technology Innovation Institute in Abu Dhabi. It is trained on 1 trillion tokens (including RefinedWeb) and available under the Apache 2.0 license. It currently holds the ğŸ¥‡ 1st place on the ğŸ¤— Open LLM leaderboard. This demo is made available by the HuggingFace H4 team.
- ğŸ§ª This is only a first experimental preview: the H4 team intends to provide increasingly capable versions of Falcon Chat in the future, based on improved datasets and RLHF/RLAIF.
- ğŸ‘€ Learn more about Falcon LLM: `falconllm.tii.ae`
- â¡ï¸ï¸ Intended Use: this demo is intended to showcase an early finetuning of Falcon-40B, to illustrate the impact (and limitations) of finetuning on a dataset of conversations and instructions. We encourage the community to further build upon the base model, and to create even better instruct/chat versions!
- âš ï¸ Limitations: the model can and will produce factually incorrect information, hallucinating facts and actions. As it has not undergone any advanced tuning/alignment, it can produce problematic outputs, especially if prompted to do so. Finally, this demo is limited to a session length of about 1,000 words.

---

### CodeParrot



---

### TAPEX

> [Table Pre-training via Execution](https://huggingface.co/microsoft/tapex-large)

Give a table of data and then query

- 0 shot question (answer right away)
- fine tune: https://github.com/SibilTaram/tapax_transformers/tree/add_tapex_bis/examples
- demo: https://huggingface.co/microsoft/tapex-base




---

## Juptyper

> To run a shell command from within a notebook cell, you must put a ! in front of the command:
> !pip install hyperopt

```py
!nvidia-smi --list-gpus


!pip install --upgrade pip

!pip uninstall -y git+https://github.com/openai/CLIP.git \
  urllib3==1.25.10 \
  sentence_transformers \
  torch torchvision pytorch-lightning lightning-bolts

# install supporting puthon packages for Data Frame processing
# and for Progress Bar
!pip install numpy pandas matplotlib tqdm scikit-learn

# install only the older version of Torch
!pip install --ignore-installed \
    urllib3==1.25.10 \
    torch torchvision pytorch-lightning lightning-bolts

# install latest (Upgrade) sentence transformers for fine-tuning
!pip install --ignore-installed \
  urllib3==1.25.10 \
  pyyaml \
  sentence_transformers

# Use CLIP model from OpenAI
!pip install git+https://github.com/openai/CLIP.git

# load the python package to run Pandas in parallel for better speed
!pip install pandarallel

!pip install torchaudio

!pip uninstall -y nvidia_cublas_cu11
```








.
