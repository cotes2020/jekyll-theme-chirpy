---
title: AIML - AI
date: 2023-04-24 11:11:11 -0400
description:
categories: [51AIML]
# img: /assets/img/sample/rabbit.png
tags: [AIML]
---

# AIML - AI

- [AIML - AI](#aiml---ai)
  - [Overall](#overall)
  - [AI](#ai)
    - [Divisions of AI](#divisions-of-ai)
      - [Artificial Narrow Intelligence (ANI)](#artificial-narrow-intelligence-ani)
      - [Artificial General Intelligence (AGI)](#artificial-general-intelligence-agi)
  - [LLM - Large Language Model](#llm---large-language-model)
    - [Features of LLMs](#features-of-llms)
      - [Translation](#translation)
      - [Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡](#automating-mundane-tasks-è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡)
      - [Emergent Abilities æ–°å…´èƒ½åŠ›](#emergent-abilities-æ–°å…´èƒ½åŠ›)
    - [Drawbacks of LLMs](#drawbacks-of-llms)
    - [Hallucination](#hallucination)
    - [Bias](#bias)
    - [Glitch tokens](#glitch-tokens)
  - [GenAI](#genai)
    - [Traditional AIML vs GenAI](#traditional-aiml-vs-genai)
    - [Attention is all you need](#attention-is-all-you-need)
    - [subject thoery](#subject-thoery)
  - [Hugging Face](#hugging-face)
    - [Generative AI Time Series Forecasting](#generative-ai-time-series-forecasting)
    - [Multivariate Time Series Forecasting](#multivariate-time-series-forecasting)
    - [Generative AI Transformers for Time Series Forecasting](#generative-ai-transformers-for-time-series-forecasting)
    - [Falcon 40b](#falcon-40b)
    - [CodeParrot](#codeparrot)
    - [TAPEX](#tapex)
  - [Juptyper](#juptyper)

---

## Overall

> Research in artificial intelligence is increasing at an exponential rate. Itâ€™s difficult for AI experts to keep up with everything new being published, and even harder for beginners to know where to start.

â€œAI Canonâ€
- a curated list of resources weâ€™ve relied on to get smarter about modern AI
- because these papers, blog posts, courses, and guides have had an outsized impact on the field over the past several years.



**OpenSource**
- hugging face
- OpenAI
- Generative AI (answers for everything)

**Program**
- python
- panda

**AI modeling**
- pyTorch
- Tensor flow (Google)

**ML platforms**
- Jupyter Notebooks

**Time series**
- Forecasting and predictive Analytics

**Use case**
- Supply Chain Management with GenAI

OpenSource -> fine tuning -> custom result


---


## AI

- Artificial Intelligence refers to `the ability of computers to perform tasks that typically require human-level intellect`.
- AI is useful in many contexts, from automation to problem solving and merely trying to understand how humans think.

- But it is important to note that AI is only concerned with human intelligence for now â€“ it could possibly go beyond that.

  - Many people correlate the word â€˜Intelligenceâ€™ with only â€˜Human Intelligenceâ€™. Just because a chicken may not be able to solve a mathematical equation doesnâ€™t mean it wonâ€™t run when you chase it. It is â€˜Intelligentâ€™ enough to know it doesnâ€™t want you to catch it ðŸ”ðŸ—.

  - Intelligence spans a much wider spectrum, and practically expands to any living thing that can make decisions or carry out actions autonomously, even plants.


### Divisions of AI

- Artificial Intelligence is `centered around computers and their ability to mimic human actions and thought processes`.

- Programming and experiments have allowed humans to produce ANI systems. These can do things like classifying items, sorting large amounts of data, looking for trends in charts and graphs, code debugging, and knowledge representation and expression. But computers donâ€™t think like humans, they merely mimic humans.

- This is evident in voice assistants such as `Googleâ€™s Assistant, Appleâ€™s Siri, Amazonâ€™s Alexa, and Microsoftâ€™s Cortana`. They are basic ANI programs that add â€˜the human touchâ€™. In fact, people are known to be polite to these systems simply because they combine computerized abilities with a human feel.

- These assistants have gotten better over the years but fail to reach high levels of sophistication when compared to their AGI counterparts.


There are two major divisions of AI:


#### Artificial Narrow Intelligence (ANI)

- focused on a small array of similar tasks or a small task that is programmed only for one thing.
- ANI is not great in dynamic and complex environments and is used in only areas specific to it.
- Examples include self-driving cars, as well as facial and speech recognition systems.


#### Artificial General Intelligence (AGI)

- focused on a wide array of tasks and human activities.
- AGI is currently theoretical and is proposed to adapt and carry out most tasks in many dynamic and complex environments.
- Examples include J.A.R.V.I.S from Marvelâ€™s _Iron Man_ and Ava from _Ex-Machina_.








---





## LLM - Large Language Model


- a deep learning model which consists of a `neural network` with billions of parameters, trained on distinctively large amounts of unlabelled data using self-supervised learning.

- At the core of all AI are algorithms. Algorithms are procedures or steps to carry out a specific task. The more complex the algorithm, the more tasks can be carried out and the more widely it can be applied. The aim of AI developers is to find the most complex algorithms that can solve and perform a wide array of tasks.

- The procedure to create a basic fruit recognition model using an simple analogy:

  1. There are two people: A teacher and a bot creator
  2. The bot creator creates random bots, and the teacher teaches and tests them on identifying some fruits
  3. The bot with the highest test score is then sent back to the creator as a base to make new upgraded bots
  4. These new upgraded bots are sent back to the teacher for teaching and testing, and the one with the highest test score is sent back to the bot creator to make new better bots.

  - This is an oversimplification of the process, but nevertheless it relays the concept. The Model/Algorithm/Bot is continuously trained, tested, and modified until it is found to be satisfactory. More data and higher complexity means more training time required and more possible modifications.

- the developer of the model can tweak a few things about the model but may not know how those tweaks might affect the results.
- A common example of this are neural networks, which have hidden layers whose deepest layers and workings even the creator may not fully understand.

- Self-supervised learning means that rather than the teacher and the bot creator being two separate people, it is one highly skilled person that can both create bots and teach them.
  - This makes the process much faster and practically autonomous.
  - The result is a bot or set of bots that are both sophisticated and complex enough to recognise fruit in dynamic and different environments.

- In the case of LLMs, the data here are human text, and possibly in various languages. The reason why the data are large is because the LLMs take in huge amounts of text data with the aim of finding connections and patterns between words to derive context, meaning, probable replies, and actions to these text.

- The results are models that seem to understand language and carry out tasks based on prompts they're given.

  - **ChatGPT**: the greatest achievement in this field as it amassed 100 million active users in 2 months from the day of its release.
  - GPT-4 by OpenAI ðŸ”¥
  - LLaMA by Meta ðŸ¦™
  - AlexaTM by Amazon ðŸ«
  - Minerva by Google âœ–ï¸âž•

---

### Features of LLMs


#### Translation

- LLMs that are trained on an array of languages rather than just one can be used for translation from one language to another.
- It's even theorised that large enough LLMs can find patterns and connections in other languages to derive meaning from unknown and lost languages, despite not knowing what each individual word may mean.


#### Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡

- Task automation has always been a major aim of AI development. Language models have always been able to carry out syntax analysis, finding patterns in text and responding appropriately.

- Large language models have an advantage with semantic analysis è¯­ä¹‰åˆ†æž, enabling the model to understand the underlying meaning and context, giving it a higher level of accuracy.

- This can be applied to a number of basic tasks like `text summarising, text rephrasing, and text generation`.


#### Emergent Abilities æ–°å…´èƒ½åŠ›

- Emergent Abilities are `unexpected but impressive` abilities LLMs have due to the high amount of data they are trained on.

- These behaviours are usually discovered when the model is used rather than when it is programmed.

- Examples include multi-step arithmetic, taking college-level exams, and chain-of-thought prompting. æ€ç»´é“¾æç¤º



---



### Drawbacks of LLMs


### Hallucination

- An infamous outcome of Microsoftâ€™s Sydney were instances when the AI gave responses that were either bizarre å¼‚ä¹Žå¯»å¸¸, untrue, or seemed sentient æœ‰æ„Ÿæƒ….
- These instances are termed Hallucination, where the model gives answers or makes claims that are not based on its training data.


### Bias

- Sometimes, the data could be the source of the problem. If a model is trained on data that is discriminatory to a person, group, race, or class, the results would also tend to be discriminatory.

- Sometimes, as the model is being used, the bias could change to fit what users tend to input. Microsoftâ€™s Tay in 2016 was a great example of how bias could go wrong.

### Glitch tokens

- Also known as adversarial examples å¯¹æŠ—æ€§ç¤ºä¾‹, glitch tokens are inputs given to a model to intentionally make it malfunction and be inaccurate when delivering answers.



---

## GenAI


### Traditional AIML vs GenAI

Traditional AIML
- good at **identify pattern**
- learning from the pattern
- limit success with close supervised learning of very large amount of data
- must have human involved


GenAI
- produces 'content' (text, image, music, art, forecasts, etc...)
- use 'transformers' (Encoders/Decoders) based on pre-trained data using small amount of fine tuning data
  - encode and decode at the same time
    - less data and faster
    - GenAI use small data and uses encoders and decoders and Transformers to take that smaller data and be able to use it for other types of models. (Pre training)
    - then add on top of it small amounts of fine tuning data
    - and then get a a training model.
  - As perceptions, not neurons.

---

### Attention is all you need

- high alignment

![Screenshot 2023-09-06 at 22.55.58](/assets/img/post/Screenshot%202023-09-06%20at%2022.55.58.png)

![Screenshot 2023-09-06 at 22.56.59](/assets/img/post/Screenshot%202023-09-06%20at%2022.56.59.png)

- Multi-Headed Attention

![Screenshot 2023-09-06 at 23.10.28](/assets/img/post/Screenshot%202023-09-06%20at%2023.10.28.png)

---

### subject thoery

GPU
- cloud class instance (from NVIDIA)
  - google colab
  - kaggle
  - amazon sagemaker
  - gradient
  - microsoft azure

> Tesla is graphics cards from NVIDIA for AI



pyTorch
- it does these heavy mathematical computation very easily with libraries
- it got a whole set of APIs and utilities that let you manipulate all of these different tensors



tensors
- a tensor is a computer data object (data structure) that represents numeric data,
- it could be floating point data values or data objects within data objects.
- 1d tensors (column)
- 2d tensors (xy)
- 3d tensors (xyz)
- 4d tensors (**cube**)
- 5d tensors
- 6d tensors


---


## Hugging Face

> like github repo

- search for an AI model


```bash
# +++++ Getting started with our git and git-lfs interface
# If you need to create a repo from the command line (skip if you created a repo from the website)
pip install huggingface_hub
# You already have it if you installed transformers or datasets
huggingface-cli login
# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}


# +++++ Clone your model or dataset locally
# Make sure you have git-lfs installed
# (https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name


# +++++ Then add, commit and push any file you want, including larges files
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push


# +++++ In most cases, if you're using one of the compatible libraries, your repo will then be accessible from code, through its identifier: username/repo_name
# For example for a transformers model, anyone can load it with:
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```


### Generative AI Time Series Forecasting

> [Generative AI Time Series Forecasting](https://huggingface.co/blog/time-series-transformers)



### Multivariate Time Series Forecasting

> [Multivariate Time Series Forecasting](https://huggingface.co/blog/informer)



### Generative AI Transformers for Time Series Forecasting

> [Generative AI Transformers for Time Series Forecasting](https://huggingface.co/blog/autoformer)


---

### Falcon 40b

- Chatgpt competitor - https://huggingface.co/tiiuae/falcon-40b

- Power of Falcon 40b chat - https://huggingface.co/spaces/HuggingFaceH4/falcon-chat

- Pre-Training - https://huggingface.co/tiiuae/falcon-40b#training-data

- or https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/studio-notebook-fine-tuning/falcon-40b-qlora-finetune-summarize.ipynb



Chat with Falcon-40B-Instruct, brainstorm ideas, discuss your holiday plans, and more!
- âœ¨ This demo is powered by Falcon-40B, finetuned on the Baize dataset, and running with Text Generation Inference. Falcon-40B is a state-of-the-art `large language model` built by the Technology Innovation Institute in Abu Dhabi. It is trained on 1 trillion tokens (including RefinedWeb) and available under the Apache 2.0 license. It currently holds the ðŸ¥‡ 1st place on the ðŸ¤— Open LLM leaderboard. This demo is made available by the HuggingFace H4 team.
- ðŸ§ª This is only a first experimental preview: the H4 team intends to provide increasingly capable versions of Falcon Chat in the future, based on improved datasets and RLHF/RLAIF.
- ðŸ‘€ Learn more about Falcon LLM: `falconllm.tii.ae`
- âž¡ï¸ï¸ Intended Use: this demo is intended to showcase an early finetuning of Falcon-40B, to illustrate the impact (and limitations) of finetuning on a dataset of conversations and instructions. We encourage the community to further build upon the base model, and to create even better instruct/chat versions!
- âš ï¸ Limitations: the model can and will produce factually incorrect information, hallucinating facts and actions. As it has not undergone any advanced tuning/alignment, it can produce problematic outputs, especially if prompted to do so. Finally, this demo is limited to a session length of about 1,000 words.

---

### CodeParrot



---

### TAPEX

> [Table Pre-training via Execution](https://huggingface.co/microsoft/tapex-large)

Give a table of data and then query

- 0 shot question (answer right away)
- fine tune: https://github.com/SibilTaram/tapax_transformers/tree/add_tapex_bis/examples
- demo: https://huggingface.co/microsoft/tapex-base




---

## Juptyper

> To run a shell command from within a notebook cell, you must put a ! in front of the command:
> !pip install hyperopt

```py
!nvidia-smi --list-gpus


!pip install --upgrade pip

!pip uninstall -y git+https://github.com/openai/CLIP.git \
  urllib3==1.25.10 \
  sentence_transformers \
  torch torchvision pytorch-lightning lightning-bolts

# install supporting puthon packages for Data Frame processing
# and for Progress Bar
!pip install numpy pandas matplotlib tqdm scikit-learn

# install only the older version of Torch
!pip install --ignore-installed \
    urllib3==1.25.10 \
    torch torchvision pytorch-lightning lightning-bolts

# install latest (Upgrade) sentence transformers for fine-tuning
!pip install --ignore-installed \
  urllib3==1.25.10 \
  pyyaml \
  sentence_transformers

# Use CLIP model from OpenAI
!pip install git+https://github.com/openai/CLIP.git

# load the python package to run Pandas in parallel for better speed
!pip install pandarallel

!pip install torchaudio

!pip uninstall -y nvidia_cublas_cu11
```








.
