---
title: AIML - AI
date: 2023-04-24 11:11:11 -0400
description:
categories: [51AIML]
# img: /assets/img/sample/rabbit.png
tags: [AIML]
---

# AIML - AI

- [AIML - AI](#aiml---ai)
  - [Overall](#overall)
  - [AI](#ai)
    - [Divisions of AI](#divisions-of-ai)
      - [Artificial Narrow Intelligence (ANI)](#artificial-narrow-intelligence-ani)
      - [Artificial General Intelligence (AGI)](#artificial-general-intelligence-agi)
  - [Traditional AIML vs GenAI](#traditional-aiml-vs-genai)
    - [RNN - Recurrent neural networks](#rnn---recurrent-neural-networks)
    - [Attention is all you need](#attention-is-all-you-need)
      - [High alignment](#high-alignment)
      - [Multi-Headed Attention](#multi-headed-attention)
  - [GenAI](#genai)
    - [Large Language Model](#large-language-model)
    - [Features of LLMs](#features-of-llms)
      - [Translation](#translation)
      - [Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡](#automating-mundane-tasks-è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡)
      - [Emergent Abilities æ–°å…´èƒ½åŠ›](#emergent-abilities-æ–°å…´èƒ½åŠ›)
    - [Drawbacks of LLMs](#drawbacks-of-llms)
      - [Hallucination](#hallucination)
      - [Bias](#bias)
      - [Glitch tokens](#glitch-tokens)
      - [LLM Generation Inefficient](#llm-generation-inefficient)
        - [Speculative æ¨æµ‹çš„ Decoding](#speculative-æ¨æµ‹çš„-decoding)
    - [LLM Subject](#llm-subject)
    - [Generative configuration](#generative-configuration)
  - [LLMç®€ä»‹](#llmç®€ä»‹)
    - [1ã€GPTç³»åˆ—ï¼ˆOpenAIï¼‰](#1gptç³»åˆ—openai)
      - [1.1 GPT-1ã€GPT-2ã€GPT-3](#11-gpt-1gpt-2gpt-3)
      - [1.2 InstructGPT](#12-instructgpt)
      - [1.3 ChatGPT](#13-chatgpt)
      - [1.4 GPT-4](#14-gpt-4)
    - [2ã€å…¶ä»–å¤§æ¨¡å‹](#2å…¶ä»–å¤§æ¨¡å‹)
      - [2.1 ChatGLM](#21-chatglm)
      - [2.2 LLaMA](#22-llama)
  - [ä¸‰ã€è¡¥å……çŸ¥è¯†](#ä¸‰è¡¥å……çŸ¥è¯†)
    - [1ã€LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoder onlyæ¶æ„ï¼Ÿ](#1llmä¸ºä»€ä¹ˆéƒ½ç”¨decoder-onlyæ¶æ„)
    - [2ã€NLPå°çŸ¥è¯†ç‚¹](#2nlpå°çŸ¥è¯†ç‚¹)
    - [3ã€åè¯è§£é‡Š](#3åè¯è§£é‡Š)
    - [4ã€å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„ä¸€äº›ä¸ªäººæ„Ÿæƒ³](#4å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„ä¸€äº›ä¸ªäººæ„Ÿæƒ³)
      - [encoder vs decoder](#encoder-vs-decoder)
      - [How the model works](#how-the-model-works)
      - [Overall prediction process](#overall-prediction-process)
        - [Data preprocessing / embedding](#data-preprocessing--embedding)
        - [Prompt construction / retrieval](#prompt-construction--retrieval)
        - [Prompt execution / inference](#prompt-execution--inference)
      - [AI agents frameworks](#ai-agents-frameworks)
    - [LLM Tools](#llm-tools)
      - [Medusa](#medusa)
        - [Medusa heads](#medusa-heads)
        - [Tree attention](#tree-attention)
        - [Typical acceptance](#typical-acceptance)
        - [accelerate models](#accelerate-models)
        - [Ablation Study æ¶ˆèç ”ç©¶](#ablation-study-æ¶ˆèç ”ç©¶)
    - [Confidence score for ML model](#confidence-score-for-ml-model)
    - [Transparency](#transparency)
      - [The Foundation Model Transparency Index](#the-foundation-model-transparency-index)
    - [Generative AI project lifecycle](#generative-ai-project-lifecycle)
  - [Hugging Face](#hugging-face)
    - [Generative AI Time Series Forecasting](#generative-ai-time-series-forecasting)
    - [Multivariate Time Series Forecasting](#multivariate-time-series-forecasting)
    - [Generative AI Transformers for Time Series Forecasting](#generative-ai-transformers-for-time-series-forecasting)
    - [Falcon 40b](#falcon-40b)
    - [CodeParrot](#codeparrot)
    - [TAPEX](#tapex)
  - [Juptyper](#juptyper)


ref:
- [OWAPS Top10 for LLM v1](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0.pdf)
- https://www.freecodecamp.org/news/large-language-models-and-cybersecurity/
- https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html
- https://docs.whylabs.ai/docs/integrations-llm-whylogs-container
- https://hackernoon.com/security-threats-to-high-impact-open-source-large-language-models
- https://a16z.com/emerging-architectures-for-llm-applications/
- [Examining Zero-Shot Vulnerability Repair with Large Language Models](https://www.connectedpapers.com/main/a5731122200fbb8b37f048010a1e1ca4474aa606/Examining-Zero%20Shot-Vulnerability-Repair-with-Large-Language-Models/graph)
- [medusa](https://together.ai/blog/medusa)
- [awesome-generative-ai](https://github.com/steven2358/awesome-generative-ai)
- [Googleâ€™s Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)
- [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/fmti.pdf)


Link:
- [Browse State-of-the-Art](https://paperswithcode.com/sota)

---

## Overall

> Research in artificial intelligence is increasing at an exponential rate. Itâ€™s difficult for AI experts to keep up with everything new being published, and even harder for beginners to know where to start.

â€œAI Canonâ€
- a curated list of resources weâ€™ve relied on to get smarter about modern AI
- because these papers, blog posts, courses, and guides have had an outsized impact on the field over the past several years.


**Data pipelines**
- Databricks
- Airflow
- Unstructured

**Embedding model**
- OpenAI
- Cohere
- Hugging Face

**Vector database**
- Pinecone
- Weaviate
- ChromaDB
- pgvector

**Playground**
- OpenAI
- nat.dev
- Humanloop

**Orchestration**
- Langchain
- LlamaIndex
- ChatGPT


**APIs/plugins**
- Serp
- Wolfram
- Zapier

**LLM cache**
- Redis
- SQLite
- GPTCache

**Logging / LLMops**
- Weights & Biases
- MLflow
- PromptLayer
- Helicone

**Validation**
- Guardrails
- Rebuff
- Microsoft Guidance
- LMQL

**App hosting**
- Vercel
- Steamship
- Streamlit
- Modal

**LLM APIs (proprietary)**
- OpenAI
- Anthropic

**LLM APIs (open)**
- Hugging Face
- Replicate

**Cloud providers**
- AWS
- GCP
- Azure
- CoreWeave

**Opinionated clouds**
- Databricks
- Anyscale
- Mosaic
- Modal
- RunPod



**OpenSource**
- hugging face
- OpenAI
- Generative AI (answers for everything)

**Programming**
- python
- panda

**AI modeling**
- pyTorch
- Tensor flow (Google)

**ML platforms**
- Jupyter Notebooks

**Time series**
- Forecasting and predictive Analytics

**Use case**
- Supply Chain Management with GenAI

OpenSource -> fine tuning -> custom result


---


## AI

- Artificial Intelligence refers to `the ability of computers to perform tasks that typically require human-level intellect`.
- AI is useful in many contexts, from automation to problem solving and merely trying to understand how humans think.

- But it is important to note that AI is only concerned with human intelligence for now â€“ it could possibly go beyond that.

  - Many people correlate the word â€˜Intelligenceâ€™ with only â€˜Human Intelligenceâ€™. Just because a chicken may not be able to solve a mathematical equation doesnâ€™t mean it wonâ€™t run when you chase it. It is â€˜Intelligentâ€™ enough to know it doesnâ€™t want you to catch it ğŸ”ğŸ—.

  - Intelligence spans a much wider spectrum, and practically expands to any living thing that can make decisions or carry out actions autonomously, even plants.


### Divisions of AI

- Artificial Intelligence is `centered around computers and their ability to mimic human actions and thought processes`.

- Programming and experiments have allowed humans to produce ANI systems. These can do things like classifying items, sorting large amounts of data, looking for trends in charts and graphs, code debugging, and knowledge representation and expression. But computers donâ€™t think like humans, they merely mimic humans.

- This is evident in voice assistants such as `Googleâ€™s Assistant, Appleâ€™s Siri, Amazonâ€™s Alexa, and Microsoftâ€™s Cortana`. They are basic ANI programs that add â€˜the human touchâ€™. In fact, people are known to be polite to these systems simply because they combine computerized abilities with a human feel.

- These assistants have gotten better over the years but fail to reach high levels of sophistication when compared to their AGI counterparts.


There are two major divisions of AI:


#### Artificial Narrow Intelligence (ANI)

- focused on a small array of similar tasks or a small task that is programmed only for one thing.
- ANI is not great in dynamic and complex environments and is used in only areas specific to it.
- Examples include self-driving cars, as well as facial and speech recognition systems.


#### Artificial General Intelligence (AGI)

- focused on a wide array of tasks and human activities.
- AGI is currently theoretical and is proposed to adapt and carry out most tasks in many dynamic and complex environments.
- Examples include J.A.R.V.I.S from Marvelâ€™s _Iron Man_ and Ava from _Ex-Machina_.




---

## Traditional AIML vs GenAI

**Traditional AIML**
- good at **identify pattern**
- learning from the pattern
- limit success with close supervised learning of very large amount of data
- must have human involved


**GenAI**
- produces 'content' (text, image, music, art, forecasts, etc...)
- use 'transformers' (Encoders/Decoders) based on pre-trained data using small amount of fine tuning data
  - encode and decode at the same time
    - less data and faster
    - GenAI use small data and uses encoders and decoders and Transformers to take that smaller data and be able to use it for other types of models. (Pre training)
    - then add on top of it small amounts of fine tuning data
    - and then get a a training model.
  - As perceptions, not neurons.

- Generative AI is a subset of traditional machine learning.
  - And the generative AI machine learning models have learned these abilities by `finding statistical patterns in massive datasets of content` that was originally generated by humans.


choosing between LLMs or layout-based Traditional AIML[^Choosing_an_extraction_approach]
- recommends using LLM prompts for free-form, highly variable documents
- layout-based or "rule-based" queries for structured, less-variable documents.

![Screenshot 2023-11-13 at 16.21.05](/assets/img/Screenshot%202023-11-13%20at%2016.21.05.png)

[^Choosing_an_extraction_approach]: Choosing an extraction approach, https://docs.sensible.so/docs/author


---

### RNN - Recurrent neural networks

> generative algorithms are not new.

recurrent neural networks - RNNs
- Previous generations of language models made use of an architecture called RNNs.
- RNNs were limited by the amount of `compute and memory` needed to perform well at generative tasks.


- With just one previous words seen by the model, the prediction can't be very good.
  - scale the RNN implementation to be able to see more of the preceding words in the text,
  - significantly scale the resources that the model uses.
  - As for the prediction, Even though scale the model, it still hasn't seen enough of the input to make a good prediction.

![Screenshot 2023-10-10 at 00.31.57](/assets/img/post/Screenshot%202023-10-10%20at%2000.31.57.png)


- To successfully predict the next word,
  - models need to see more than just the previous few words.
  - Models needs to have an understanding of the whole sentence or even the whole document.


> How can an algorithm make sense of human language if sometimes we can't?
> in 2017, after the publication of this paper, `Attention is All You Need`, from Google and the University of Toronto, everything changed.
> The transformer architecture had arrived.
> - It can be scaled efficiently to use multi-core GPUs,
> - parallel process input data, making use of much larger training datasets, and crucially,
> - it's able to learn to pay attention to the meaning of the words it's processing.

---

### Attention is all you need

![Screenshot 2023-10-10 at 00.33.00](/assets/img/post/Screenshot%202023-10-10%20at%2000.33.00.png)

---

#### High alignment

![Screenshot 2023-09-06 at 22.55.58](/assets/img/post/Screenshot%202023-09-06%20at%2022.55.58.png)

![Screenshot 2023-09-06 at 22.56.59](/assets/img/post/Screenshot%202023-09-06%20at%2022.56.59.png)

---

#### Multi-Headed Attention

![Screenshot 2023-09-06 at 23.10.28](/assets/img/post/Screenshot%202023-09-06%20at%2023.10.28.png)




---

## GenAI

With the rise in popularity of Foundation Models, new models and tools are released almost every week and yet

![Screenshot 2023-11-13 at 22.11.36](/assets/img/Screenshot%202023-11-13%20at%2022.11.36.png)

---

### Large Language Model

> "... a language model is a Turing-complete weird machine running programs written in natural language; when you do retrieval, you are not 'plugging updated facts into the AI', you are actually downloading random new unsigned blobs of code from the Internet (many written by adversaries) and casually executing them on the LM with full privileges. This does not end well." - [Gwern Branwen on LessWrong](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K)

- a deep learning model which consists of a `neural network` with billions of parameters, trained on distinctively large amounts of unlabelled data using self-supervised learning.

- At the core of all AI are algorithms. Algorithms are procedures or steps to carry out a specific task. The more complex the algorithm, the more tasks can be carried out and the more widely it can be applied. The aim of AI developers is to find the most complex algorithms that can solve and perform a wide array of tasks.

- The procedure to create a basic fruit recognition model using an simple analogy:

  1. There are two people: A teacher and a bot creator
  2. The bot creator creates random bots, and the teacher teaches and tests them on identifying some fruits
  3. The bot with the highest test score is then sent back to the creator as a base to make new upgraded bots
  4. These new upgraded bots are sent back to the teacher for teaching and testing, and the one with the highest test score is sent back to the bot creator to make new better bots.

  - This is an oversimplification of the process, but nevertheless it relays the concept. The Model/Algorithm/Bot is continuously trained, tested, and modified until it is found to be satisfactory. More data and higher complexity means more training time required and more possible modifications.

- the developer of the model can tweak a few things about the model but may not know how those tweaks might affect the results.
- A common example of this are neural networks, which have hidden layers whose deepest layers and workings even the creator may not fully understand.

- Self-supervised learning means that rather than the teacher and the bot creator being two separate people, it is one highly skilled person that can both create bots and teach them.
  - This makes the process much faster and practically autonomous.
  - The result is a bot or set of bots that are both sophisticated and complex enough to recognise fruit in dynamic and different environments.

- In the case of LLMs, the data here are human text, and possibly in various languages. The reason why the data are large is because the LLMs take in huge amounts of text data with the aim of finding connections and patterns between words to derive context, meaning, probable replies, and actions to these text.

- The results are models that seem to understand language and carry out tasks based on prompts they're given.

  - **ChatGPT**: the greatest achievement in this field as it amassed 100 million active users in 2 months from the day of its release.
  - GPT-4 by OpenAI ğŸ”¥
  - LLaMA by Meta ğŸ¦™
  - AlexaTM by Amazon ğŸ«
  - Minerva by Google âœ–ï¸â•

---


### Features of LLMs

#### Translation

- LLMs that are trained on an array of languages rather than just one can be used for translation from one language to another.
- It's even theorised that large enough LLMs can find patterns and connections in other languages to derive meaning from unknown and lost languages, despite not knowing what each individual word may mean.


#### Automating Mundane Tasks è‡ªåŠ¨åŒ–æ—¥å¸¸ä»»åŠ¡

- Task automation has always been a major aim of AI development. Language models have always been able to carry out syntax analysis, finding patterns in text and responding appropriately.

- Large language models have an advantage with semantic analysis è¯­ä¹‰åˆ†æ, enabling the model to understand the underlying meaning and context, giving it a higher level of accuracy.

- This can be applied to a number of basic tasks like `text summarising, text rephrasing, and text generation`.


#### Emergent Abilities æ–°å…´èƒ½åŠ›

- Emergent Abilities are `unexpected but impressive` abilities LLMs have due to the high amount of data they are trained on.

- These behaviours are usually discovered when the model is used rather than when it is programmed.

- Examples include multi-step arithmetic, taking college-level exams, and chain-of-thought prompting. æ€ç»´é“¾æç¤º



---

### Drawbacks of LLMs


#### Hallucination

- An infamous outcome of Microsoftâ€™s Sydney were instances when the AI gave responses that were either bizarre å¼‚ä¹å¯»å¸¸, untrue, or seemed sentient æœ‰æ„Ÿæƒ….
- These instances are termed Hallucination, where the model gives answers or makes claims that are not based on its training data.


#### Bias

- Sometimes, the data could be the source of the problem. If a model is trained on data that is discriminatory to a person, group, race, or class, the results would also tend to be discriminatory.

- Sometimes, as the model is being used, the bias could change to fit what users tend to input. Microsoftâ€™s Tay in 2016 was a great example of how bias could go wrong.

#### Glitch tokens

- Also known as adversarial examples å¯¹æŠ—æ€§ç¤ºä¾‹, glitch tokens are inputs given to a model to intentionally make it malfunction and be inaccurate when delivering answers.


#### LLM Generation Inefficient

> From a systems perspective, LLM generation follows a memory-bound computational pattern with the main latency bottleneck arising from memory reads/writes rather than arithmetic computations. This issue is rooted in the inherently sequential nature of the auto-regressive decoding process. Each forward pass necessitates the transfer of the entire model's parameters from High-Bandwidth Memory (HBM) to the accelerator's compute units. This operation, while only producing a single token for each sample, fails to fully utilize the arithmetic computation capabilities of modern accelerators, resulting in inefficiency.


Before the rise of LLMs, a common mitigation for this inefficiency was to `simply increase the batch size, enabling the parallel production of more tokens`.


But the **situation becomes far more complicated with LLMs**.
- Increasing the batch size in this context not only introduces higher latency but also substantially `inflates è†¨èƒ€ the memory requirements` for the Transformer model's key-value cache.
  - This trade-off makes the use of large batches impractical for many applications where low latency is a critical requirement.

- also, for cost structures, as of September 2023, generation costs approximately 2x higher for GPT-4 and roughly 3x for Claude 2, compared to merely processing prompts.

![Screenshot 2023-09-20 at 17.51.23](/assets/img/post/Screenshot%202023-09-20%20at%2017.51.23.png)


##### Speculative æ¨æµ‹çš„ Decoding

> Given the challenges outlined, one appealing strategy to accelerate **text generation** is `more efficient computational utilizationâ€”specifically`, by `processing more tokens in parallel`.

speculative decoding

- The methodology employs a streamlined "draft" model to generate a batch of token candidates at each step quickly. These candidates are then validated by the original, full-scale language model to identify the most reasonable text continuations.

- The underlying logic hinges on an intriguing å¼•èµ·å…´è¶£çš„ assumption:

  - the draft model, although smaller, should be proficient enough to churn out sequences that the original model will find acceptable.

  - the draft model can rapidly produce token sequences while the original model efficiently vets å®¡æŸ¥ multiple tokens in parallel, which maximizing computational throughput.

  - Recent research indicates that with a well-tuned draft model, speculative decoding can cut latency by an impressive factor of up to 2.5x.

- However, the approach is not without its challenges:

  - Finding the Ideal Draft Model: Identifying a "small yet mighty" draft model that aligns well with the original model is easier said than done.

  - System Complexity: Hosting two distinct models in one system introduces layers of complexity, both computational and operational, especially in distributed settings.

  - Sampling Inefficiency: When doing sampling with speculative decoding, an importance sampling scheme needs to be used. This introduces additional overhead on generation, especially at higher sampling temperatures.


- These complexities and trade-offs have limited the broader adoption of speculative decoding techniques. So speculative decoding isn't widely adopted.

- Remark: We use speculative decoding to refer to those methods that require an independent draft model here. In a broader sense, our method can also be viewed as speculative decoding, while the draft model is entangled with the original model.




---


### LLM Subject


**Large language models**
- Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power.
- These foundation models with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve.

**foundation models (base models)**
- their relative size in terms of their parameters.
- ![Screenshot 2023-10-09 at 15.57.07](/assets/img/post/Screenshot%202023-10-09%20at%2015.57.07.png)
- **parameters**:
  - the model's `memory`.
  - the more parameters a model has, the more memory, the more `sophisticated` the tasks it can perform.
- By either using these models as they are or by applying fine tuning techniques to adapt them to the specific use case, you can rapidly `build customized solutions without the need to train a new model` from scratch.


**Augmenting LLMs**
- connecting LLM to external data sources or using them to invoke external APIs.
- use this ability to provide the model with information it doesn't know from its pre-training and to enable the model to power interactions with the real-world.


**Interact**
- `other machine learning and programming paradigms`: write computer code with formalized syntax to interact with `libraries and APIs`.
- `large language models`: able to take natural language or human written instructions and perform tasks much as a human would.

**prompt**
- The text that you pass to an LLM
- The space or memory that is available to the prompt is called the `context window`, and this is typically large enough for a few thousand words, but differs from model to model.

- example
  - ask the model to determine where Ganymede is located in the solar system.
  - The prompt is passed to the model, the model then predicts the next words, and because the prompt contained a question, this model generates an answer.
- The output of the model is called a `completion`, and the act of using the model to generate text is known as `inference`.
- The completion is comprised of the text contained in the original prompt, followed by the generated text.

![Screenshot 2023-10-10 at 00.25.13](/assets/img/post/Screenshot%202023-10-10%20at%2000.25.13.png)



**GPU**
- cloud class instance (from NVIDIA)
  - google colab
  - kaggle
  - amazon sagemaker
  - gradient
  - microsoft azure

> Tesla is graphics cards from NVIDIA for AI



**pyTorch**
- it does these heavy mathematical computation very easily with libraries
- it got a whole set of APIs and utilities that let you manipulate all of these different tensors



**tensors**
- a tensor is a computer data object (data structure) that represents numeric data,
- it could be floating point data values or data objects within data objects.
- 1d tensors (column)
- 2d tensors (xy)
- 3d tensors (xyz)
- 4d tensors (**cube**)
- 5d tensors
- 6d tensors


---

---

### Generative configuration

- Each model exposes a set of configuration parameters that can influence the model's output during inference.
  - training parameters: learned during training time.
  - configuration parameters: invoked at inference time and give control over things like the maximum number of tokens in the completion, and how creative the output is.


![Screenshot 2023-10-21 at 11.40.09](/assets/img/Screenshot%202023-10-21%20at%2011.40.09.png)


- **Max new tokens**: limit the number of tokens that the model will generate.
  - putting a cap on the number of times the model will go through the selection process.
  - the length of the completion is shorter
    - because another stop condition was reached, such as the model predicting and end of sequence token.
    - it's max new tokens, not a hard number of new tokens generated.
    - ![Screenshot 2023-10-21 at 11.46.18](/assets/img/Screenshot%202023-10-21%20at%2011.46.18.png)

  - The output from the `transformer's softmax layer` is a `probability distribution` across the entire dictionary of words that the model uses.
    - Here you can see a selection of words and their probability score next to them.
    - this is a list that carries on to the complete dictionary.

- **controls**

  - to generate text that's more natural, more creative and avoids repeating words, you need to use some other controls.
    - in some implementations, you may need to disable greedy and enable random sampling explicitly.
    - For example, the Hugging Face transformers implementation that we use in the lab requires that we set do sample to equal true.

  - `greedy decoding`
    - Most large language models by default will operate with `greedy decoding`.
    - the simplest form of next-word prediction
    - the model will always choose the word with the highest probability.
    - This method can work very well for short generation but is susceptible to repeated words or repeated sequences of words.

  - `Random sampling`
    - the easiest way to introduce some variability.
    - Instead of selecting the most probable word every time with random sampling, the model `chooses an output word at random using the probability distribution to weight the selection`.
    - depending on the setting, there is a possibility that the output may be too creative, producing words that cause the generation to wander off into topics or words that just don't make sense.

    - For example, in the illustration, the word banana has a probability score of 0.02. With random sampling, this equates to a 2% chance that this word will be selected. By using this sampling technique, we reduce the likelihood that words will be repeated.


    - ![Screenshot 2023-10-21 at 11.52.25](/assets/img/Screenshot%202023-10-21%20at%2011.52.25.png)


- **top k, top p sampling techniques**
  - help `limit the random sampling` and `increase the chance that the output will be sensible`.
    - With `top k`, you specify the number of tokens to randomly choose from
    - with `top p`, you specify the total probability that you want the model to choose from.

  - `top k` value:
    - limit the options while still allowing some variability
    - instructs the model to choose from only the k tokens with the highest probability.
    - In this example here, k is set to three, so you're restricting the model to choose from these three options. The model then selects from these options using the probability weighting and in this case, it chooses donut as the next word.
    - This method can help the model have some randomness while preventing the selection of highly improbable completion words.
    - This in turn makes the text generation more likely to sound reasonable and to make sense.
    - ![Screenshot 2023-10-21 at 12.04.16](/assets/img/Screenshot%202023-10-21%20at%2012.04.16.png)

  - `top p` setting:
    - limit the random sampling to the predictions whose combined probabilities do not exceed p.
    - For example, if you set p to equal 0.3, the options are cake and donut since their probabilities of 0.2 and 0.1 add up to 0.3. The model then uses the random probability weighting method to choose from these tokens.
    - ![Screenshot 2023-10-21 at 12.04.53](/assets/img/Screenshot%202023-10-21%20at%2012.04.53.png)


- **temperature**
  - control the randomness of the model output
  - can be adjusted to either increase or decrease randomness within the model output layer (softmax layer)

  - influences `the shape of the probability distribution` that the model calculates for the next token.
    - The temperature value is a `scaling factor that's applied within the final softmax layer of the model` that impacts `the shape of the probability distribution of the next token`.
    - In contrast to the `top k & p` parameters, changing the temperature actually alters the predictions that the model will make.

  - Broadly speaking, the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness.
  - ![Screenshot 2023-10-21 at 12.07.00](/assets/img/Screenshot%202023-10-21%20at%2012.07.00.png)

  - low value of temperature, say less than one,
    - the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words.
    - blue bars in the table show a probability bar chart turned on its side.
    - Most of the probability here is concentrated on the word cake. The model will select from this distribution using random sampling and the resulting text will be less random and will more closely follow the most likely word sequences that the model learned during training.


  - if set the temperature to a higher value, say, greater than one, then the model will calculate a broader flatter probability distribution for the next token.
    - Notice that in contrast to the blue bars, the probability is more evenly spread across the tokens.
    - This leads the model to generate text with a `higher degree of randomness and more variability` in the output compared to a `cool temperature setting`.
    - This can help you generate text that sounds more creative.

  - If leave the temperature value equal to one, this will leave the softmax function as default and the unaltered probability distribution will be used.


```py
generation_config = GenerationConfig(max_new_tokens=50)
# generation_config = GenerationConfig(max_new_tokens=10)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)
# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)

inputs = tokenizer(few_shot_prompt, return_tensors='pt')
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        generation_config=generation_config,
    )[0],
    skip_special_tokens=True
)

print(dash_line)
print(f'MODEL GENERATION - FEW SHOT:\n{output}')
print(dash_line)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
```

---

## LLMç®€ä»‹

â€ƒâ€ƒé€šè¿‡å‰æ–‡å¯¹TuningæŠ€æœ¯çš„ä»‹ç»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿäº†è§£åˆ°ï¼ŒTuningæŠ€æœ¯ä¾èµ–äºLLMçš„å‘å±•ï¼ŒåŒæ—¶ä¹Ÿåœ¨æ¨åŠ¨ç€LLMçš„å‘å±•ã€‚é€šå¸¸ï¼ŒLLMæŒ‡çš„æ˜¯åŒ…å«æ•°ç™¾äº¿ï¼ˆæˆ–æ›´å¤šï¼‰å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ä»‹ç»å‡ ä¸ªè€³ç†Ÿèƒ½è¯¦çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ä»–LLMçš„ç›¸å…³å†…å®¹å¯å‚è€ƒ[LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)ã€[Open LLM Leaderboard
](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ã€[å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLM)æ±‡æ€»ï¼ˆæŒç»­æ›´æ–°ä¸­ï¼‰](https://blog.csdn.net/jarodyv/article/details/129992142)

### 1ã€GPTç³»åˆ—ï¼ˆOpenAIï¼‰

#### 1.1 GPT-1ã€GPT-2ã€GPT-3

â€ƒâ€ƒ2017å¹´ï¼ŒGoogleæ¨å‡ºTransformerï¼Œåˆ©ç”¨attentionå®Œå…¨æ›¿ä»£è¿‡å¾€æ·±åº¦å­¦ä¹ ä¸­çš„Recurrenceå’ŒConvolutionç»“æ„ï¼Œç›´ç™½åœ°å±•ç°å‡ºäº†â€œå¤§ä¸€ç»Ÿæ¨¡å‹â€çš„é‡å¿ƒï¼Œ"xxx is all you need"ä¹Ÿæˆäº†ä¸€ä¸ªç©ä¸çƒ‚çš„æ¢—ã€‚
â€ƒâ€ƒ2018å¹´6æœˆï¼ŒOpenAIæ¨å‡ºåŸºäºTransformer Decoderæ”¹é€ çš„ç¬¬ä¸€ä»£GPTï¼ˆGenerative Pre-Trainingï¼‰ï¼Œæœ‰æ•ˆè¯æ˜äº†åœ¨NLPé¢†åŸŸä¸Šä½¿ç”¨é¢„è®­ç»ƒ+å¾®è°ƒæ–¹å¼çš„æœ‰æ•ˆæ€§ã€‚ç´§éšå…¶åï¼ŒåŒå¹´10æœˆGoogleæ¨å‡ºåŸºäºTransformer Encoderéƒ¨åˆ†çš„Bertï¼Œåœ¨åŒæ ·å‚æ•°å¤§å°çš„å‰æä¸‹ï¼Œå…¶æ•ˆæœé¢†è·‘äºGPT-1ï¼Œä¸€æ—¶æˆä¸ºNLPé¢†åŸŸçš„é¢†å¤´ç¾Šã€‚
â€ƒâ€ƒä¸ç”˜ç¤ºå¼±çš„OpenAIåœ¨4ä¸ªæœˆåï¼Œæ¨å‡ºæ›´å¤§çš„æ¨¡å‹GPT-2ï¼ˆGPT-1: 110Mï¼ŒBert: 340Mï¼ŒGPT-2: 1.5Bï¼‰ï¼ŒåŒæ—¶ï¼ŒOpenAIä¹ŸçŸ¥é“ï¼Œå…‰é å¢åŠ æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†æ¥è·å¾—ä¸€ä¸ªå’ŒBertå·®ä¸å¤šæ•ˆæœçš„æ¨¡å‹ï¼Œå…¶å®æ˜¯æ²¡æœ‰æŠ€æœ¯å«é‡çš„ã€‚äºæ˜¯ï¼Œåœ¨GPT-2é‡Œï¼ŒOpenAIå¼•å…¥zero-shotå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚
â€ƒâ€ƒæ­¤åï¼ŒOpenAIåœ¨LLMä¸Šä¹‰æ— åé¡¾åœ°èµ°äº†ä¸‹å»ï¼Œåœ¨2020å¹´6æœˆæ¨å‡ºå·¨äººGPT-3ï¼Œå‚æ•°é‡é«˜è¾¾175Bï¼Œå„ç±»å®éªŒæ•ˆæœè¾¾åˆ°é¡¶å³°ï¼Œæ®è¯´ä¸€æ¬¡è®­ç»ƒè´¹ç”¨ä¸º1200wç¾å…ƒï¼Œâ€œè´µâ€ä¹Ÿæˆäº†æ™®é€šå·¥ä¸šç•Œè¸è¶³GPTç³»åˆ—çš„å£å’ä¹‹ä¸€ã€‚
â€ƒâ€ƒåœ¨æ­£å¼ä»‹ç»GPTç³»åˆ—æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆä»‹ç»ä¸‹è¯­è¨€æ¨¡å‹çš„æ¦‚å¿µï¼Œè¯­è¨€æ¨¡å‹æ˜¯GPTç³»åˆ—æ¨¡å‹çš„åŸºåº§ã€‚ä»€ä¹ˆæ˜¯è¯­è¨€æ¨¡å‹ï¼Ÿç®€å•æ¥è¯´ï¼Œå°±æ˜¯çœ‹ä¸€ä¸ªå¥å­æ˜¯äººè¯çš„å¯èƒ½æ€§ã€‚ä¸“ä¸šä¸€ç‚¹æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œå…¶å­—ç¬¦æ˜¯ W = ( w 1 , w 2 , â‹¯ â€‰ , w L ) W=(w\_{1},w\_{2},\\cdots,w\_{L}) W\=(w1â€‹,w2â€‹,â‹¯,wLâ€‹)ï¼Œé‚£ä¹ˆï¼Œä»è¯­è¨€æ¨¡å‹æ¥çœ‹ï¼Œè¿™ä¸ªå¥å­æ˜¯äººè¯çš„å¯èƒ½æ€§å°±æ˜¯:
P ( W ) = P ( w 1 , w 2 , â‹¯ â€‰ , w L ) = P ( w 1 ) P ( w 2 âˆ£ w 1 ) P ( w 3 âˆ£ w 1 , w 2 ) â‹¯ P ( w L âˆ£ w 1 , w 2 , â‹¯ â€‰ , w L âˆ’ 1 ) \\begin{aligned} P(W)&=P(w\_{1},w\_{2},\\cdots,w\_{L})\\\\ &=P(w\_{1})P(w\_{2}|w\_{1})P(w\_{3}|w\_{1},w\_{2})\\cdots P(w\_{L}|w\_{1},w\_{2},\\cdots,w\_{L-1})\\\\ \\end{aligned} P(W)â€‹\=P(w1â€‹,w2â€‹,â‹¯,wLâ€‹)\=P(w1â€‹)P(w2â€‹âˆ£w1â€‹)P(w3â€‹âˆ£w1â€‹,w2â€‹)â‹¯P(wLâ€‹âˆ£w1â€‹,w2â€‹,â‹¯,wLâˆ’1â€‹)â€‹
ä½†æ˜¯ï¼Œ L L Lå¤ªé•¿å°±ä¼šå¾ˆç¨€ç–ï¼Œç›´æ¥ç®—è¿™ä¸ªæ¦‚ç‡ä¸å¥½è®¡ç®—ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¿‘ä¼¼è®¡ç®—:
P ( W ) = P ( w 1 , w 2 , â‹¯ â€‰ , w L ) = P ( w 1 ) P ( w 2 âˆ£ w 1 ) P ( w 3 âˆ£ w 1 , w 2 ) â‹¯ P ( w L âˆ£ w 1 , w 2 , â‹¯ â€‰ , w L âˆ’ 1 ) = P ( w 1 ) P ( w 2 âˆ£ w 1 ) â‹¯ P ( w L âˆ£ w L âˆ’ N , â‹¯ â€‰ , w L âˆ’ 1 ) \\begin{aligned} P(W)&=P(w\_{1},w\_{2},\\cdots,w\_{L})\\\\ &=P(w\_{1})P(w\_{2}|w\_{1})P(w\_{3}|w\_{1},w\_{2})\\cdots P(w\_{L}|w\_{1},w\_{2},\\cdots,w\_{L-1})\\\\ &=P(w\_{1})P(w\_{2}|w\_{1})\\cdots P(w\_{L}|w\_{L-N},\\cdots,w\_{L-1}) \\end{aligned} P(W)â€‹\=P(w1â€‹,w2â€‹,â‹¯,wLâ€‹)\=P(w1â€‹)P(w2â€‹âˆ£w1â€‹)P(w3â€‹âˆ£w1â€‹,w2â€‹)â‹¯P(wLâ€‹âˆ£w1â€‹,w2â€‹,â‹¯,wLâˆ’1â€‹)\=P(w1â€‹)P(w2â€‹âˆ£w1â€‹)â‹¯P(wLâ€‹âˆ£wLâˆ’Nâ€‹,â‹¯,wLâˆ’1â€‹)â€‹
è¿™å°±æ˜¯å¸¸è¯´çš„N-gramç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼ŒNé€šå¸¸æ˜¯2ï¼Œ3ï¼Œ4ã€‚ç‰¹åˆ«çš„ï¼Œå½“N=1æ—¶ï¼Œè¯­è¨€æ¨¡å‹å°±é€€åŒ–ä¸ºå„ä¸ªå­—ç¬¦å‡ºç°çš„æ¦‚ç‡ä¹‹ç§¯ã€‚å½“N=4æ—¶è¯­è¨€æ¨¡å‹å°±æ¯”è¾ƒå¤§äº†ï¼Œå®é™…åº”ç”¨ä¸­ä¸€èˆ¬æœ€å¤§ä¹Ÿå°±æ˜¯4äº†ã€‚æ ¹æ®æ¡ä»¶æ¦‚ç‡ P ( w L âˆ£ w L âˆ’ N , â‹¯ â€‰ , w L âˆ’ 1 ) P(w\_{L}|w\_{L-N},\\cdots,w\_{L-1}) P(wLâ€‹âˆ£wLâˆ’Nâ€‹,â‹¯,wLâˆ’1â€‹)ï¼Œæˆ‘ä»¬å°±èƒ½çŸ¥é“ç»™å®šå‰Nä¸ªå­—ï¼Œä¸‹ä¸€ä¸ªå­—æ˜¯ä»€ä¹ˆå­—çš„æ¦‚ç‡äº†ã€‚è¯­è¨€æ¨¡å‹çš„è¯„ä»·æŒ‡æ ‡å¯ä»¥é‡‡ç”¨PPLï¼ˆå›°æƒ‘åº¦ï¼ŒPerplexityï¼Œ[è¯­è¨€æ¨¡å‹](https://zhuanlan.zhihu.com/p/90741508)ï¼‰ã€‚
â€ƒâ€ƒæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ†åˆ«ä»‹ç»GPT-1ã€GPT-2ã€GPT-3çš„æ¨¡å‹åŸç†åŠé¢„è®­ç»ƒæ–¹æ³•ç­‰ç›¸å…³çŸ¥è¯†ã€‚

- **GPT-1**: GPT-1æ˜¯OpenAIåœ¨è®ºæ–‡[ã€ŠImproving Language Understanding by Generative Pre-Trainingã€‹](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)ä¸­æå‡ºçš„ç”Ÿæˆå¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³: é€šè¿‡äºŒæ®µå¼çš„è®­ç»ƒï¼Œç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼ˆæ— ç›‘ç£å½¢å¼ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡Fine-Tuningçš„æ¨¡å¼è§£å†³ä¸‹æ¸¸ä»»åŠ¡ï¼ˆç›‘ç£æ¨¡å¼ä¸‹ï¼‰ã€‚GPT-1å¯ä»¥å¾ˆå¥½åœ°å®Œæˆè‹¥å¹²ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€è‡ªç„¶è¯­è¨€æ¨ç†ã€é—®ç­”ã€è¯­ä¹‰ç›¸ä¼¼åº¦ç­‰ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¾®è°ƒåçš„GPT-1æ€§èƒ½å‡è¶…è¿‡äº†å½“æ—¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„SOTAæ¨¡å‹ã€‚

    - **è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNatural Language Inference æˆ–è€… Textual Entailmentï¼‰**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯åŒ…å«å…³ç³»ï¼ˆentailmentï¼‰ï¼ŒçŸ›ç›¾å…³ç³»ï¼ˆcontradictionï¼‰ï¼Œæˆ–è€…ä¸­ç«‹å…³ç³»ï¼ˆneutralï¼‰ï¼›

    - **é—®ç­”å’Œå¸¸è¯†æ¨ç†ï¼ˆQuestion answering and commonsense reasoningï¼‰**: ç±»ä¼¼äºå¤šé€‰é¢˜ï¼Œè¾“å…¥ä¸€ä¸ªæ–‡ç« ï¼Œä¸€ä¸ªé—®é¢˜ä»¥åŠè‹¥å¹²ä¸ªå€™é€‰ç­”æ¡ˆï¼Œè¾“å‡ºä¸ºæ¯ä¸ªç­”æ¡ˆçš„é¢„æµ‹æ¦‚ç‡ï¼›

    - **è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆSemantic Similarityï¼‰**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¯­ä¹‰ä¸Šæ˜¯ç›¸å…³çš„ï¼›

    - **åˆ†ç±»ï¼ˆClassificationï¼‰**: åˆ¤æ–­è¾“å…¥æ–‡æœ¬æ˜¯æŒ‡å®šçš„å“ªä¸ªç±»åˆ«ã€‚
        Â 
        ä¸‹é¢å…·ä½“ä»‹ç»ä¸‹GPT-1çš„æ¨¡å‹ç»“æ„åŠè®­ç»ƒæµç¨‹ã€‚

    - **æ¨¡å‹ç»“æ„**: GPT-1åŸºç¡€æ¶æ„æ˜¯åŸºäºTransformerçš„Decoderéƒ¨åˆ†ï¼ŒåŒæ—¶åˆ é™¤äº†Encoder-Decoder Attentionå±‚ï¼Œåªä¿ç•™äº†Masked Multi-Head Attentionå±‚å’ŒFeed Forwardå±‚ã€‚Transformerç»“æ„æå‡ºä¹‹å§‹ä¾¿ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œæœºå™¨ç¿»è¯‘æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œå› æ­¤Transformerè®¾è®¡äº†Encoderç”¨äºæå–æºç«¯è¯­è¨€çš„è¯­ä¹‰ç‰¹å¾ï¼Œè€Œç”¨Decoderæå–ç›®æ ‡ç«¯è¯­è¨€çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆç›¸å¯¹åº”çš„è¯‘æ–‡ã€‚GPT-1ç›®æ ‡æ˜¯æœåŠ¡äºå•åºåˆ—æ–‡æœ¬çš„ç”Ÿæˆå¼ä»»åŠ¡ï¼Œæ‰€ä»¥å«å¼ƒäº†å…³äºEncoderéƒ¨åˆ†ï¼ŒåŒ…æ‹¬Decoderçš„Encoder-Decoder Attentionå±‚ã€‚æ•´ä½“æ˜¯12å±‚çš„Transformer-Decoderå˜ä½“ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/7f79e1bd37914445a22c151d4d91bd12.jpeg#pic_center)
        é™¤æ­¤ä¹‹å¤–ï¼ŒGPT-1è¿˜å°†attentionçš„ç»´æ•°æ‰©å¤§åˆ°768ï¼ˆåŸæ¥ä¸º512ï¼‰ï¼Œå°†attentionçš„å¤´æ•°å¢åŠ åˆ°12ä¸ªï¼ˆåŸæ¥ä¸º8ä¸ªï¼‰ï¼Œå°†Feed Forwardå±‚çš„éšå±‚ç»´æ•°å¢åŠ åˆ°3072ï¼ˆåŸæ¥ä¸º2048ï¼‰ï¼Œæ€»å‚æ•°è¾¾åˆ°110Mã€‚GPT-1è¿˜ä¼˜åŒ–äº†å­¦ä¹ ç‡é¢„çƒ­ç®—æ³•ï¼Œä½¿ç”¨æ›´å¤§çš„BPEç è¡¨ï¼ˆè¯è¡¨å¤§å°ä¸º40478ï¼Œ478ä¸ªbase characters + 40000ä¸ªç»“åˆçš„å­—ç¬¦ï¼‰ï¼Œæ¿€æ´»å‡½æ•°ReLUæ”¹ä¸ºå¯¹æ¢¯åº¦æ›´æ–°æ›´å‹å¥½çš„é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒGeLUï¼Œå°†æ­£ä½™å¼¦æ„é€ çš„ä½ç½®ç¼–ç æ”¹ä¸ºäº†å¸¦å­¦ä¹ çš„ä½ç½®ç¼–ç ã€‚

    - **æ¨¡å‹è®­ç»ƒ**: ä¸Šæ–‡å·²ç»æåˆ°ï¼ŒGPT-1æ¨¡å‹è®­ç»ƒæ•´ä½“ä¸Šåˆ†ä¸ºä¸¤æ­¥: 1ï¼‰åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ åˆ°ä¸€ä¸ªé«˜å®¹é‡çš„è¯­è¨€æ¨¡å‹ï¼›2ï¼‰åœ¨æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚å…¶ä¸­ç¬¬äºŒæ­¥æ˜¯é’ˆå¯¹å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚

        - **æ— ç›‘ç£é¢„è®­ç»ƒ**: æ€»ä½“è®­ç»ƒä»»åŠ¡ç›®æ ‡æ˜¯æ ¹æ®å·²çŸ¥çš„è¯é¢„æµ‹æœªçŸ¥çš„è¯ã€‚åœ¨è¿™é‡Œè®¾å®šä¸€å®šçš„çª—å£å¤§å°ï¼Œå³æ ¹æ®æœ‰é™çš„è¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯: ç»™å®šä¸€ä¸ªè¯­æ–™çš„å¥å­åºåˆ— U = { u 1 , â‹¯ â€‰ , u n } \\mathcal{U}=\\{u\_{1},\\cdots,u\_{n}\\} U\={u1â€‹,â‹¯,unâ€‹}ï¼Œå·²çŸ¥å‰ k k kä¸ªè¯é¢„æµ‹å½“å‰è¯ u i u\_{i} uiâ€‹ï¼Œç”¨ä¸€ä¸ªæ ‡å‡†çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å»æå¤§åŒ–è¿™ä¸ªä¼¼ç„¶å‡½æ•°:
            L 1 ( U ) = âˆ‘ i l o g P ( u i âˆ£ u i âˆ’ k , â‹¯ â€‰ , u i âˆ’ 1 ; Î˜ ) Â  L\_{1}(\\mathcal{U})=\\sum\_{i} logP(u\_{i}|u\_{i-k},\\cdots, u\_{i-1};\\Theta)\\ L1â€‹(U)\=iâˆ‘â€‹logP(uiâ€‹âˆ£uiâˆ’kâ€‹,â‹¯,uiâˆ’1â€‹;Î˜)Â 
            å…¶ä¸­:  k k kæ˜¯æ»‘åŠ¨çª—å£å¤§å°ï¼Œ Î˜ \\Theta Î˜æ˜¯è¦ä¼˜åŒ–çš„å‚æ•°ã€‚ P ( u ) P(u) P(u)çš„è®¡ç®—æ–¹æ³•æ˜¯:
            h 0 = U W e + W p h i = t r a n s f o r m e r \_ b l o c k ( h i âˆ’ 1 ) ï¼Œ âˆ€ i âˆˆ \[ 1 , n \] P ( u ) = s o f t m a x ( h n W e T ) \\begin{aligned} h\_{0}&=UW\_{e}+W\_{p}\\\\ h\_{i}&=transformer\\\_block(h\_{i-1})ï¼Œ\\forall i\\in\[1,n\]\\\\ P(u)&=softmax(h\_{n}W\_{e}^{T}) \\end{aligned} h0â€‹hiâ€‹P(u)â€‹\=UWeâ€‹+Wpâ€‹\=transformer\_block(hiâˆ’1â€‹)ï¼Œâˆ€iâˆˆ\[1,n\]\=softmax(hnâ€‹WeTâ€‹)â€‹
            å…¶ä¸­:  W e W\_{e} Weâ€‹æ˜¯è¯å‘é‡çŸ©é˜µï¼ˆtoken embedding matrixï¼‰ï¼Œ W p W\_{p} Wpâ€‹æ˜¯ä½ç½®å‘é‡çŸ©é˜µï¼ˆposition embedding matrixï¼‰ï¼Œ U = u âˆ’ k , â‹¯ â€‰ , u âˆ’ 1 ) U=u\_{-k},\\cdots,u\_{-1}) U\=uâˆ’kâ€‹,â‹¯,uâˆ’1â€‹)æ˜¯tokensçš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆæºä»£ç ä¸­ï¼Œ u i u\_{i} uiâ€‹éƒ½æ˜¯one-hotç¼–ç å‘é‡ï¼Œç›¸å½“äºåšä¸€ä¸ªæŸ¥è¯¢æ“ä½œï¼Œ U U Uå­˜å‚¨ç´¢å¼•ï¼Œ W e W\_{e} Weâ€‹å­˜å‚¨ç€è¯å‘é‡å€¼ï¼‰ï¼Œ n n næ˜¯Decoderå±‚çš„æ•°é‡ã€‚
            â€ƒâ€ƒä¸Šé¢æ˜¯è®ºæ–‡ä¸­çš„æè¿°ï¼Œæˆ‘ä»¬ä¸¾ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œæ¥è¯´æ˜GPT-1å®é™…ä¸Šæ˜¯å¦‚ä½•è¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒçš„ã€‚ä¾‹å¦‚è¾“å…¥æ–‡æœ¬æ˜¯: ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¿™æ®µæ–‡æœ¬ç»è¿‡åˆ‡è¯è½¬æ¢ä¸ºä¸€ä¸ªä¸ªtokenåï¼Œè¾“å…¥GPT-1çš„transformer-decoderç»“æ„ï¼Œåœ¨æœ€åä¸€å±‚ï¼Œä¼šè¾“å‡ºæ¯ä¸ªtokenå¯¹åº”çš„è¡¨å¾å‘é‡ï¼Œå³ä¸Šæ–‡çš„ h n âˆˆ R m Ã— d h\_{n}\\in R^{m\\times d} hnâ€‹âˆˆRmÃ—dï¼Œå…¶ä¸­ m m mæ˜¯tokenæ•°é‡ï¼Œè¿™ä¸ªä¾‹å­ä¸­å°±æ˜¯5ï¼Œ d d dæ˜¯æ¨¡å‹ç»´åº¦ï¼ŒGPT-1ä¸­å°±æ˜¯768ï¼›æ¥ä¸‹æ¥ï¼Œ h n h\_{n} hnâ€‹å†ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œç”Ÿæˆ z n âˆˆ R m Ã— v z\_{n}\\in R^{m\\times v} znâ€‹âˆˆRmÃ—vï¼Œå…¶ä¸­ v v væ˜¯è¯è¡¨çš„å¤§å°ï¼›æœ€åï¼Œ z n z\_{n} znâ€‹ä¼šç»è¿‡softmaxæ“ä½œï¼Œç„¶åé€‰å–å®ƒæ¯ä¸€è¡Œä¸­æ•°å€¼æœ€å¤§çš„ç´¢å¼•åˆ°è¯è¡¨ä¸­æœç´¢å¯¹åº”çš„tokenï¼Œæœç´¢åˆ°çš„tokenæ€ä¹ˆç”¨å‘¢ï¼Ÿæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹ï¼Œè¾“å…¥æ˜¯ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¾“å‡ºä¹Ÿæ˜¯5ä¸ªtokenï¼Œå› ä¸ºè¾“å…¥çš„ç¬¬ä¸€ä¸ªtokenæ˜¯ã€ä»Šã€‘ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„ç¬¬ä¸€ä¸ªtokenæ˜¯ã€å¤©ã€‘ï¼Œè¾“å…¥çš„ç¬¬äºŒä¸ªtokenæ˜¯ã€å¤©ã€‘ï¼Œåˆ™å¸Œæœ›è¾“å‡ºçš„ç¬¬äºŒä¸ªtokenæ˜¯ã€å¾ˆã€‘ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°æœ€åä¸€ä¸ªè¾“å…¥tokenã€å¿ƒã€‘ï¼Œä¸è¿‡å› ä¸ºå®ƒæ²¡æœ‰ä¸‹ä¸€ä¸ªè¯ï¼Œæ‰€ä»¥åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸åœ¨æˆ‘ä»¬çš„æŸå¤±è®¡ç®—èŒƒå›´å†…ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬ä¼šæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œå°½å¯èƒ½çš„è®©æœ€ç»ˆçš„è¾“å‡ºtokençš„å‰å››ä¸ªå­—æ˜¯ã€å¤©å¾ˆå¼€å¿ƒã€‘ï¼Œè¿™å°±æ˜¯é¢„è®­ç»ƒä»»åŠ¡çš„æ•´ä½“æµç¨‹ã€‚å›è¿‡å¤´æ¥ï¼Œæˆ‘ä»¬ä¹Ÿç†è§£äº†ä¸ºä»€ä¹ˆé¢„è®­ç»ƒå«åšæ— ç›‘ç£è®­ç»ƒï¼Œå°±æ˜¯å› ä¸ºæˆ‘ä»¬å…¶å®æ²¡æœ‰æ ‡æ³¨æ ·æœ¬ï¼Œè€Œæ˜¯æ‹¿ä¸‹ä¸€ä¸ªè¯å½“åšæ ‡ç­¾è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¿™ç§æ–¹å¼ä¹Ÿè¢«ç§°ä½œè‡ªç›‘ç£è®­ç»ƒã€‚

        - **ç›‘ç£è®­ç»ƒ**: å½“å¾—åˆ°æ— ç›‘ç£çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬å°†å®ƒç›´æ¥åº”ç”¨åˆ°æœ‰ç›‘ç£ä»»åŠ¡ä¸­ç»§ç»­è®­ç»ƒã€‚å¯¹äºä¸€ä¸ªæœ‰æ ‡ç­¾çš„æ•°æ®é›† C \\mathcal{C} Cï¼Œæ¯ä¸ªå®ä¾‹æœ‰ m m mä¸ªè¾“å…¥token:  { x 1 , â‹¯ â€‰ , x m } \\{x^{1},\\cdots,x^{m}\\} {x1,â‹¯,xm}ï¼Œå®ƒå¯¹åº”çš„æ ‡ç­¾æ˜¯ y y yã€‚é¦–å…ˆå°†è¿™äº›tokenè¾“å…¥åˆ°è®­ç»ƒå¥½çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œè·å–æœ€åä¸€ä¸ªtransformer decoderçš„è¾“å‡ºï¼Œå¾—åˆ°æœ€ç»ˆçš„ç‰¹å¾å‘é‡ h l m h\_{l}^{m} hlmâ€‹ã€‚ç„¶åå†é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å¾—åˆ°é¢„æµ‹ç»“æœ y y y:
            P ( y âˆ£ x 1 , â‹¯ â€‰ , x m ) = s o f t m a x ( h l m W y ) Â  P(y|x^{1},\\cdots,x^{m})=softmax(h\_{l}^{m}W\_{y})\\ P(yâˆ£x1,â‹¯,xm)\=softmax(hlmâ€‹Wyâ€‹)Â 
            å…¶ä¸­ W y W\_{y} Wyâ€‹ä¸ºå…¨è¿æ¥å±‚çš„å‚æ•°ã€‚æœ‰ç›‘ç£çš„ç›®æ ‡åˆ™æ˜¯æœ€å¤§åŒ–ä¸‹å¼çš„å€¼:
            L 2 ( C ) = âˆ‘ x , y P ( y âˆ£ x 1 , â‹¯ â€‰ , x m ) Â  L\_{2}(\\mathcal{C})=\\sum\_{x,y}P(y|x^{1},\\cdots,x^{m})\\ L2â€‹(C)\=x,yâˆ‘â€‹P(yâˆ£x1,â‹¯,xm)Â 
            æ³¨æ„: è¿™é‡Œçš„ h l m h^m\_l hlmâ€‹æ˜¯æ¯ä¸€ä¸ªè¯å¯¹åº”çš„Decoderè¾“å‡ºæ‹¼æ¥èµ·æ¥çš„ï¼Œ h l m = { h l < 1 > , â‹¯ â€‰ , h l < m > } h^m\_l=\\{h^{<1>}\_l,\\cdots,h^{<m>}\_l\\} hlmâ€‹\={hl<1\>â€‹,â‹¯,hl<m\>â€‹}ï¼Œ , h l < i > ,h^{<i>}\_l ,hl<i\>â€‹å¯¹åº” x i x^{i} xiçš„åµŒå…¥è¡¨ç¤ºã€‚
            Â 
            GPT-1çš„å®éªŒä¸­å‘ç°ï¼ŒåŠ å…¥è¯­è¨€æ¨¡å‹å­¦ä¹ ç›®æ ‡ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æŸå¤±å‡½æ•°ä¸­åŠ å…¥ L 1 L\_{1} L1â€‹èƒ½å¸¦æ¥ä¸¤ç‚¹å¥½å¤„: 1ï¼‰æå‡ç›‘ç£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›2ï¼‰åŠ å¿«æ”¶æ•›ï¼›å› æ­¤ï¼Œæœ€ç»ˆçš„ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼ˆ Î» \\lambda Î»ä¸€èˆ¬å–0.5ï¼‰:
            L 3 ( C ) = L 2 ( C ) + Î» L 1 ( C ) Â  L\_{3}(\\mathcal{C})=L\_{2}(\\mathcal{C})+\\lambda L\_{1}(\\mathcal{C})\\ L3â€‹(C)\=L2â€‹(C)+Î»L1â€‹(C)Â 

        - **ä¸‹æ¸¸ä»»åŠ¡**: GPT-1è®ºæ–‡ä¸­ç»™å‡ºäº†å››ä¸ªä¸‹æ¸¸é€‚é…ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯æ–‡æœ¬åˆ†ç±»ã€è‡ªç„¶è¯­è¨€æ¨ç†ã€é—®ç­”ã€è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ŒåŒæ—¶ç»™å‡ºäº†é’ˆå¯¹è¿™å››ä¸ªä»»åŠ¡ï¼Œå¦‚ä½•è¿›è¡Œé’ˆå¯¹æ€§çš„å¾®è°ƒã€‚è¿™å››ä¸ªä»»åŠ¡è™½ç„¶æœ¬è´¨ä¸Šéƒ½æ˜¯å±äºè‡ªç„¶è¯­è¨€ç†è§£çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä½†æ˜¯GPT-1çš„ç»“æ„æ˜¯å¾ˆé€‚é…åšè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡çš„ã€‚ä¸‹é¢æˆ‘ä»¬ä»‹ç»ä¸‹GPT-1å¦‚ä½•åœ¨ä¸Šè¿°å››ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

            - **åˆ†ç±»ä»»åŠ¡**: å°†èµ·å§‹å’Œç»ˆæ­¢tokenåŠ å…¥åˆ°åŸå§‹åºåˆ—ä¸¤ç«¯ï¼Œè¾“å…¥transformerä¸­å¾—åˆ°ç‰¹å¾å‘é‡ï¼Œæœ€åç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å¾—åˆ°é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼›
            - **è‡ªç„¶è¯­è¨€æ¨ç†**: å°†å‰æï¼ˆpremiseï¼‰å’Œå‡è®¾ï¼ˆhypothesisï¼‰é€šè¿‡åˆ†éš”ç¬¦ï¼ˆDelimiterï¼‰éš”å¼€ï¼Œä¸¤ç«¯åŠ ä¸Šèµ·å§‹å’Œç»ˆæ­¢tokenã€‚å†ä¾æ¬¡é€šè¿‡transformerå’Œå…¨è¿æ¥å¾—åˆ°é¢„æµ‹ç»“æœï¼›
            - **è¯­ä¹‰ç›¸ä¼¼åº¦**: è¾“å…¥çš„ä¸¤ä¸ªå¥å­ï¼Œæ­£å‘å’Œåå‘å„æ‹¼æ¥ä¸€æ¬¡ï¼ˆç”±äºç›¸ä¼¼æ€§è´¨æ˜¯å¯¹ç§°çš„ï¼Œä¸ºäº†æ¶ˆé™¤é¡ºåºçš„å½±å“ï¼‰ï¼Œç„¶ååˆ†åˆ«è¾“å…¥ç»™transformerï¼Œå¾—åˆ°çš„ç‰¹å¾å‘é‡æ‹¼æ¥åå†é€ç»™å…¨è¿æ¥å¾—åˆ°é¢„æµ‹ç»“æœï¼›
            - **é—®ç­”å’Œå¸¸è¯†æ¨ç†**: å°†ä¸ªé€‰é¡¹çš„é—®é¢˜æŠ½è±¡åŒ–ä¸ºä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œå³æ¯ä¸ªé€‰é¡¹åˆ†åˆ«å’Œå†…å®¹è¿›è¡Œæ‹¼æ¥ï¼Œç„¶åå„é€å…¥transformerå’Œå…¨è¿æ¥ä¸­ï¼Œæœ€åé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ä½œä¸ºé¢„æµ‹ç»“æœã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/0c96d17a280347a6b80e71a0a4da483a.png#pic_center)

            â€ƒâ€ƒè¿™é‡Œæˆ‘ä»¬åŒæ ·é€šè¿‡ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»çš„ä¾‹å­ï¼Œæ¥ä»‹ç»ä¸‹GPT-1åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¯å¦‚ä½•å¾®è°ƒçš„ã€‚ä¾‹å¦‚ä¸‹æ¸¸ä»»åŠ¡æ˜¯æƒ…æ„Ÿæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…æ‹¬å–œã€æ€’ã€å“€ã€æƒ§ã€å…¶ä»–äº”ä¸ªç±»åˆ«ï¼Œå…¶ä¸­ä¸€ä¸ªæ ·æœ¬æ˜¯ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘ï¼ŒçœŸå®æ ‡ç­¾æ˜¯ã€å–œã€‘ã€‚é€šè¿‡å‰é¢çš„ä»‹ç»ï¼Œæˆ‘ä»¬çŸ¥é“GPT-1åœ¨ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒæŸå¤±å‡½æ•°åŒ…å«ä¸¤éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ˜¯ä¸é¢„è®­ç»ƒä¿æŒä¸€è‡´çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æŸå¤±ï¼Œè¿™éƒ¨åˆ†å°±ä¸ä»‹ç»äº†ã€‚å¦ä¸€éƒ¨åˆ†æ˜¯åˆ†ç±»æŸå¤±ï¼Œå¯¹äºåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œæˆ‘ä»¬æœ€ç»ˆä¹Ÿä¼šè·å–åˆ°GPT-1æœ€åä¸€å±‚çš„å‘é‡è¡¨å¾ h l âˆˆ R m Ã— d h\_{l}\\in R^{m\\times d} hlâ€‹âˆˆRmÃ—dï¼Œå…¶ä¸­ m m mæ˜¯tokenæ•°é‡ï¼Œè¿™ä¸ªä¾‹å­ä¸­å°±æ˜¯5ï¼Œ d d dæ˜¯æ¨¡å‹ç»´åº¦ï¼ŒGPT-1ä¸­å°±æ˜¯768ï¼Œ l l læ˜¯æ¨¡å‹å±‚æ•°ï¼›æ¥ä¸‹æ¥ï¼Œ h l h\_{l} hlâ€‹çš„æœ€åä¸€è¡Œå†ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆæ³¨æ„ï¼Œé¢„è®­ç»ƒä»»åŠ¡æ˜¯ h l h\_{l} hlâ€‹æ•´ä½“éƒ½è¦ç»è¿‡å…¨è¿æ¥å±‚ï¼Œæˆ‘ä»¬è¿™é‡Œåªéœ€ç”¨åˆ°æœ€åä¸€ä¸ªtokenï¼Œå³å›¾ç‰‡ä¸­çš„Extractå¯¹åº”çš„å‘é‡è¡¨å¾ï¼‰ï¼Œç”Ÿæˆ z l âˆˆ R c z\_{l}\\in R^{c} zlâ€‹âˆˆRcï¼Œå…¶ä¸­ c c cæ˜¯ç±»åˆ«æ•°ç›®ï¼›æœ€åï¼Œ z l z\_{l} zlâ€‹ä¼šç»è¿‡softmaxæ“ä½œï¼Œè·å–ã€ä»Šå¤©å¾ˆå¼€å¿ƒã€‘è¿™æ®µæ–‡æœ¬å¯¹åº”çš„æ¯ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼ï¼Œæˆ‘ä»¬çš„æœŸæœ›æ˜¯ã€å–œã€‘çš„æ¦‚ç‡å€¼è¦å°½å¯èƒ½çš„å¤§ï¼Œä¹Ÿå°±æ˜¯ z l z\_{l} zlâ€‹çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„å€¼è¦å°½å¯èƒ½å¤§ï¼Œè¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡ã€‚

    - **GPT-1ç‰¹ç‚¹**:

        - **ä¼˜ç‚¹**: ç‰¹å¾æŠ½å–å™¨ä½¿ç”¨äº†å¼ºå¤§çš„Transformerï¼Œèƒ½å¤Ÿæ•æ‰åˆ°æ›´é•¿çš„è®°å¿†ä¿¡æ¯ï¼Œä¸”è¾ƒä¼ ç»Ÿçš„RNNæ›´æ˜“äºå¹¶è¡ŒåŒ–ï¼›transformerçš„å¹¶è¡ŒåŒ–å¯ä»¥å‚è€ƒ[æµ…æTransformerè®­ç»ƒæ—¶å¹¶è¡Œé—®é¢˜](https://zhuanlan.zhihu.com/p/368592551)ï¼›
        - **ç¼ºç‚¹**: GPT-1æœ€å¤§çš„é—®é¢˜å°±æ˜¯ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹æ˜¯å•å‘çš„ã€‚
    - **GPT-1ä¸ELMoï¼ŒBertçš„åŒºåˆ«**:

        - **GPT-1ä¸ELMoçš„åŒºåˆ«**:
            - **æ¨¡å‹æ¶æ„ä¸åŒ**: ELMoæ˜¯æµ…å±‚çš„åŒå‘RNNï¼›GPT-1æ˜¯å¤šå±‚çš„Transformer decoderï¼›
            - **é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å¤„ç†ä¸åŒ**: ELMoå°†è¯åµŒå…¥æ·»åŠ åˆ°ç‰¹å®šä»»åŠ¡ä¸­ï¼Œä½œä¸ºé™„åŠ åŠŸèƒ½ï¼›GPTåˆ™é’ˆå¯¹æ‰€æœ‰ä»»åŠ¡å¾®è°ƒç›¸åŒçš„åŸºæœ¬æ¨¡å‹ã€‚
        - **GPT-1ä¸Bertçš„åŒºåˆ«**:
            - **é¢„è®­ç»ƒ**: GPT-1é¢„è®­ç»ƒçš„æ–¹å¼å’Œä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ä¸€æ ·ï¼Œé€šè¿‡ä¸Šæ–‡ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼›Bertä¼šåŒæ—¶åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼›
            - **æ¨¡å‹æ•ˆæœ**: GPT-1å› ä¸ºé‡‡ç”¨äº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹æ‰€ä»¥æ›´åŠ é€‚åˆç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆç±»çš„ä»»åŠ¡ (NLG)ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡é€šå¸¸æ˜¯æ ¹æ®å½“å‰ä¿¡æ¯ç”Ÿæˆä¸‹ä¸€åˆ»çš„ä¿¡æ¯ã€‚è€ŒBertæ›´é€‚åˆç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ (NLU)ã€‚å½“ç„¶è¿™æ˜¯ä¹‹å‰çš„è¯´æ³•ï¼Œç°åœ¨chatgptå‡ºæ¥ä»¥åå“ªä¸ªæ›´é€‚åˆNLUä»»åŠ¡è¿˜çœŸä¸ä¸€å®šã€‚GPT-1çš„æ¨¡å‹å‚æ•°ä¸ºL=12ï¼ŒH=768ï¼ŒA=12ï¼Œè¿™ä¸ªè®¾ç½®å’Œåæ¥Bert-baseä¸€æ¨¡ä¸€æ ·ï¼Œä½†åè€…çš„æ•ˆæœè¦å¥½ä¸Šå¾ˆå¤šã€‚åŸå› ä¹‹ä¸€æ˜¯ï¼ŒGPT-1é‡‡ç”¨Mask-Attentionç»“æ„ï¼Œå¯¹æ¨¡å‹å’Œè®­ç»ƒæ•°æ®çš„è¦æ±‚ä¼šæ›´é«˜ï¼Œå› ä¸ºæ¨¡å‹èƒ½è¯»åˆ°çš„ä¿¡æ¯åªæœ‰ä¸Šæ–‡ã€‚è€Œé‡‡ç”¨æ™®é€šattentionçš„Bertåœ¨è®­ç»ƒé˜¶æ®µå°±èƒ½åŒæ—¶è¯»åˆ°ä¸Šä¸‹æ–‡ã€‚è¿™ä¸ªæ€§è´¨å†³å®šäº†GPTæ¨¡å‹è¶Šæ¥è¶Šå¤§çš„è¶‹åŠ¿ã€‚ä½†æ˜¯ï¼Œé•¿è¿œæ¥çœ‹ï¼ŒMasked-Attentionæ˜¯æ¨åŠ¨æ¨¡å‹æ›´å¥½ç†è§£æ–‡å­—çš„é‡è¦æ‰‹æ®µï¼Œæ¯•ç«Ÿåœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬æ›´å¸Œæœ›åŸ¹å…»æ¨¡å‹çŸ¥ä¸Šæ–‡è¡¥ä¸‹æ–‡ï¼Œè€Œä¸æ˜¯å•çº¯åœ°åšå®Œå½¢å¡«ç©ºã€‚
            - **æ¨¡å‹ç»“æ„**:  GPT-1é‡‡ç”¨äº†Transformerçš„Decoderï¼Œè€ŒBerté‡‡ç”¨äº†Transformerçš„Encoderã€‚
    - **GPT-1çš„æ•°æ®é›†**: GPT-1ä½¿ç”¨äº†BooksCorpusæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†åŒ…å«7000æœ¬æ²¡æœ‰å‘å¸ƒçš„ä¹¦ç±ã€‚ä½œè€…é€‰è¿™ä¸ªæ•°æ®é›†çš„åŸå› æœ‰äºŒ: 1ï¼‰æ•°æ®é›†æ‹¥æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä½¿å¾—æ¨¡å‹èƒ½å­¦å¾—æ›´é•¿æœŸçš„ä¾èµ–å…³ç³»ï¼›2ï¼‰è¿™äº›ä¹¦ç±å› ä¸ºæ²¡æœ‰å‘å¸ƒï¼Œæ‰€ä»¥å¾ˆéš¾åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šè§åˆ°ï¼Œæ›´èƒ½éªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

- **GPT-2**: æˆ‘ä»¬çŸ¥é“ï¼ŒGPT-1å’ŒBertçš„è®­ç»ƒéƒ½æ˜¯åˆ†ä¸¤æ­¥èµ°: pre-training + supervised fine-tuningã€‚è¿™å¥—æ–¹æ³•çš„ç¼ºç‚¹:

    - è™½ç„¶å€ŸåŠ©é¢„è®­ç»ƒè¿™ä¸€æ­¥æå‡æ€§èƒ½ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šè¿˜æ˜¯éœ€è¦æœ‰ç›‘ç£çš„Fine-Tuningæ‰èƒ½ä½¿å¾—æ¨¡å‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼›
    - éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šé¢æœ‰æ ‡æ³¨çš„æ•°æ®ã€‚å½“æˆ‘ä»¬åªæœ‰å¾ˆå°‘é‡çš„å¯ç”¨æ•°æ® (å³zero-shotçš„æƒ…å†µä¸‹) æ—¶å°±å¾ˆéš¾æœ‰å¾ˆå¥½çš„æ•ˆæœã€‚
        Â 

    â€ƒâ€ƒå¦å¤–ï¼Œåœ¨Bertæ¨¡å‹æå‡ºä¹‹åï¼ŒEncoder vs Decoderï¼ŒBert vs GPT-1ï¼Œä¸¤ä¸¤ä¹‹é—´çš„æ¯”è¾ƒå°±å¼€å§‹äº†ï¼Œä½†æ˜¯æ­¤æ—¶GPT-1ä»å¤„åœ¨åŠ£åŠ¿ã€‚Bertæå‡ºä¹‹åï¼Œé™¤äº†ç”Ÿæˆä»»åŠ¡å¤–ï¼ŒNLPä»»åŠ¡çš„èŒƒå¼åŸºæœ¬å°±æ˜¯Bertçš„é¢„è®­ç»ƒ+Fine-Tuningäº†ã€‚OpenAIæ”¾å¼ƒäº†å—ï¼Ÿå¹¶æ²¡æœ‰ï¼æˆ‘ä»¬çŸ¥é“ï¼ŒåŸºäºDecoderçš„æ¨¡å‹ï¼Œæ¨¡å‹å’Œæ•°æ®é‡è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚ä½†OpenAIå¦‚æœåªåšåˆ°è¿™ä¸€ç‚¹ï¼Œä»æŠ€æœ¯ä¸Šæ¥è¯´åˆå¤ªé€Šè‰²äº†ï¼Œæ€§ä»·æ¯”ä¹Ÿä¸é«˜ã€‚å› æ­¤ï¼ŒOpenAIä»è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œå¼•å…¥äº†zero-shotè¿™ä¸€åˆ›æ–°ç‚¹ï¼ŒGPT-2å°±è¯ç”Ÿäº†[ã€ŠLanguage Models are Unsupervised Multitask Learnersã€‹](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)ã€‚
    â€ƒâ€ƒè®ºæ–‡ä¸­è®¤ä¸ºç°åœ¨çš„è®­ç»ƒæ–¹å¼è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹åªèƒ½ç®—æ˜¯ä¸€ä¸ªå°ä»»åŠ¡ä¸Šçš„ä¸“å®¶ç³»ç»Ÿï¼Œè€Œä¸”è¿˜éƒ½ä¸å¤Ÿé²æ£’ã€‚é€ æˆè¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯æ¨¡å‹éƒ½æ˜¯åœ¨å•ä¸€é¢†åŸŸå†…çš„å•ä¸€ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œç¼ºä¹æ³›åŒ–æ€§ã€‚è·Ÿäººä¸€æ ·ï¼Œè§è¯†å’ŒçŸ¥è¯†å¤ªå°‘æ—¶ï¼Œå°±å¾ˆéš¾å¯¹äº‹æƒ…æœ‰å…¨é¢çš„äº†è§£ã€‚è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªå¯è¡Œçš„æ€è·¯æ˜¯å¤šä»»åŠ¡å­¦ä¹ ï¼Œè€Œä¸”æ˜¯å¤§é‡ä¸åŒé¢†åŸŸçš„ä¸åŒä»»åŠ¡ã€‚ä½†æ˜¯ï¼Œè¿™æ ·çš„å¤šä»»åŠ¡å­¦ä¹ æ˜¯æœ‰ç›‘ç£çš„è®­ç»ƒï¼Œéœ€è¦å¤§é‡çš„æ•°æ®ï¼Œè¿™ä¸ªå°±æ¯”è¾ƒéš¾å®ç°äº†ã€‚
    â€ƒâ€ƒGPT-2åœ¨GPT-1çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†æ–°çš„å‘å±•æ€è·¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼ŒGPT-2çš„æ€è·¯å°±æ˜¯å……åˆ†ç›¸ä¿¡è¯­è¨€æ¨¡å‹ï¼Œä¸å†å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡ŒFine-Tuningæˆ–è€…å¢åŠ ä»»åŠ¡å¤´äº†ï¼Œå°±ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥è§£å†³æ‰€æœ‰ä»»åŠ¡ï¼Œç›´æ¥åšzero-shotçš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ä¸Šé«˜è´¨é‡çš„å¤§æ•°æ®ï¼Œå †å æ›´å¤šçš„å‚æ•°ï¼Œä¸åŒä»»åŠ¡æ”¹é€ æˆç”Ÿæˆä»»åŠ¡ã€‚
    â€ƒâ€ƒGPT-2æœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä½†æ˜¯ä¸ä¸€æ ·çš„æ˜¯ï¼Œå®ƒè¯æ˜äº†è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ zero-shot çš„æƒ…å†µä¸‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒGPT-2åœ¨åšä¸‹æ¸¸ä»»åŠ¡çš„æ—¶å€™å¯ä»¥æ— éœ€ä»»ä½•æ ‡æ³¨çš„ä¿¡æ¯ï¼Œä¹Ÿæ— éœ€ä»»ä½•å‚æ•°æˆ–æ¶æ„çš„ä¿®æ”¹ã€‚åæ¥çš„GPT-3ä¹Ÿæ˜¯æ²¿ç”¨äº†è¿™ä¸ªæ€è·¯ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œå·²ç»å¯ä»¥çœ‹å‡ºä¸€äº›ChatGPTçš„å½±å­äº†ã€‚

    - **æ¨¡å‹ç»“æ„**: GPT-2çš„æ¨¡å‹åœ¨GPT-1çš„åŸºç¡€ä¸Šåšäº†ä¸€äº›æ”¹è¿›ï¼Œå¦‚ä¸‹:

        - **ç»“æ„å˜åŒ–**: å¯¹äºæ¯ä¸ªsub-block: ç¬¬ä¸€ä¸ªlayer normå±‚ç§»åˆ°sub-blockçš„è¾“å…¥éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯attentionä¹‹å‰ï¼Œç¬¬äºŒä¸ªlayer normå±‚ç§»åˆ°feed forwardä¹‹å‰ï¼›å¯¹äºæ•´ä½“æ¨¡å‹æ¶æ„ï¼Œåœ¨æœ€åä¸€ä¸ªsub-blockåå†åŠ ä¸€ä¸ªlayer normå±‚ï¼›
        - **æƒé‡å˜åŒ–**: é‡‡ç”¨ä¸€ç§æ”¹è¿›çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†æ®‹å·®è·¯å¾„ä¸æ¨¡å‹æ·±åº¦çš„ç´¯ç§¯ã€‚åœ¨åˆå§‹åŒ–æ—¶å°†residual layersçš„æƒé‡æŒ‰ N \\sqrt{N} N â€‹çš„å› å­è¿›è¡Œç¼©æ”¾ï¼Œå…¶ä¸­ N N Næ˜¯residual layersçš„æ•°é‡ï¼›
            - å¤‡æ³¨: è¿™ä¸ªæ”¹åŠ¨å…¶å®æ²¡å¤ªçœ‹æ‡‚ï¼Œresidual layerså°±æ˜¯ä¸€ä¸ªç›¸åŠ æ“ä½œï¼Œæ€ä¹ˆä¼šæœ‰å‚æ•°å‘¢ï¼ŸæŸ¥é˜…äº†å¾ˆå¤šèµ„æ–™ï¼Œæºç ä¹Ÿçœ‹äº†[GPT-2](https://github.com/openai/gpt-2)ï¼Œæ²¡çœ‹åˆ°æƒé‡ç¼©æ”¾çš„æµç¨‹ã€‚åœ¨æ­¤ç»™ä¸€ä¸ªæœ¬äººçš„è§è§£: æ ¹æ®è¿™ä¸ªæ“ä½œçš„ç›®çš„å¯çŸ¥ï¼Œæ˜¯ä¸ºäº†é˜²æ­¢éšç€æ¨¡å‹æ·±åº¦çš„ç´¯ç§¯ï¼Œæ®‹å·®è¶ŠåŠ è¶Šå¤§ï¼Œå› æ­¤è®¤ä¸ºè¿™é‡Œçš„ç¼©æ”¾æŒ‡çš„æ˜¯æ¯æ¬¡è¿›è¡Œæ®‹å·®æ“ä½œä¹‹å‰ï¼ˆå³å°†è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œç›¸åŠ ä¹‹å‰ï¼‰ï¼Œå…ˆå°†è¾“å…¥è¿›è¡Œç¼©æ”¾ï¼Œç¼©æ”¾å› å­è·Ÿå½“å‰æ˜¯æ•´ä½“ç»“æ„çš„ç¬¬å‡ å±‚æœ‰å…³ï¼Œå±‚æ•°è¶Šå¤§ï¼Œç´¯ç§¯çš„è¶Šå¤§ï¼Œæ‰€ä»¥åº”è¯¥ç¼©æ”¾çš„è¶Šå¤šã€‚æ¯”å¦‚ç°åœ¨æ˜¯æ•´ä½“ç»“æ„çš„ç¬¬äº”å±‚ï¼Œé‚£ä¹ˆç¼©æ”¾å› å­ N N Nå°±æ˜¯5ã€‚ä¸Šè¿°å†…å®¹åªæ˜¯æœ¬äººçš„ä¸€ä¸ªæƒ³æ³•ï¼Œå¦‚æœ‰å¤§ç¥æ›´äº†è§£å…¶ä¸­çš„åŸç†ï¼Œæ¬¢è¿äº¤æµæŒ‡æ­£ã€‚ï¼‰ï¼›
        - **è¯è¡¨å˜åŒ–**: è¯è¡¨å¤§å°è®¾ç½®ä¸º50257ï¼›
        - **è¾“å…¥å˜åŒ–**: æ— ç›‘ç£é¢„è®­ç»ƒå¯çœ‹åˆ°çš„ä¸Šä¸‹æ–‡çš„contextç”±512æ‰©å±•ä¸º1024ï¼›
        - **æ‰¹æ¬¡å˜åŒ–**: è®­ç»ƒæ—¶ï¼Œbatchsizeå¤§å°ä»64è°ƒæ•´ä¸º512ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/707f0f02c6aa4dd1854bf28312013182.png#pic_center)

        â€ƒâ€ƒè®ºæ–‡ç»™äº†ä¸åŒå±‚æ•°çš„æ¨¡å‹ï¼Œæœ€å¤§çš„æ¨¡å‹ç§°ä¸ºGPT-2æ¨¡å‹ï¼Œå‚æ•°æœ‰1.5Bï¼›æœ€å°çš„å³æ˜¯GPT-1ï¼Œå¯¹æ ‡Bert-baseï¼›å€’æ•°ç¬¬äºŒå°çš„å¯¹æ ‡Bert-largeã€‚ä¸åŒæ¨¡å‹å¤§å°å¦‚ä¸‹: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/888b7f350a62461d84897fcd39e8b2a5.png#pic_center)

    - **æ¨¡å‹è®­ç»ƒ**: GPT-2åªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ã€‚

        - **æ— ç›‘ç£è®­ç»ƒ**: GPT-2çš„è®­ç»ƒæ–¹å¼å’ŒGPT-1çš„è®­ç»ƒæ–¹å¼ç›¸æ¯”ï¼Œä¸¤è€…éƒ½æ˜¯æœ‰é¢„è®­ç»ƒè¿‡ç¨‹çš„ï¼Œä¸è¿‡GPT-2åªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¸é‡‡ç”¨Fine-Tuningæ–¹æ³•ï¼Œè€Œæ˜¯é‡‡ç”¨è®ºæ–‡ä¸­æåˆ°çš„zero-shotæ–¹æ³•ã€‚GPT-2é‡‡ç”¨è¿™ç§æ¨¡å¼ï¼Œå½’å› äºGPT-2æå‡ºçš„æ ¸å¿ƒæ€æƒ³: å½“ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å®¹é‡è¶³å¤Ÿå¤§æ•°æ®é‡è¶³å¤Ÿä¸°å¯Œæ—¶ï¼Œå®ƒå°±è¶³ä»¥è¦†ç›–æ‰€æœ‰çš„æœ‰ç›‘ç£ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰€æœ‰çš„æœ‰ç›‘ç£å­¦ä¹ éƒ½æ˜¯æ— ç›‘ç£è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªå­é›†ï¼Œä»…ä»…é è®­ç»ƒè¯­è¨€æ¨¡å‹ä¾¿å¯ä»¥å®Œæˆå…¶ä»–æœ‰ç›‘ç£å­¦ä¹ çš„ä»»åŠ¡ã€‚
        - **ä¸‹æ¸¸ä»»åŠ¡**: GPT-2å¦‚ä½•è®©æ¨¡å‹åšä¸‹æ¸¸ä»»åŠ¡å‘¢ï¼Ÿé¦–å…ˆå‰æ–‡æåˆ°ï¼ŒGPT-2åœ¨å¤§è§„æ¨¡æ— ç›‘ç£è®­ç»ƒè¿‡ç¨‹å­¦ä¹ åˆ°äº†ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚ä½œè€…æ˜¯è¿™ä¹ˆè®¤ä¸ºçš„: æ¯”å¦‚ä¸‹æ¸¸ä»»åŠ¡æ˜¯è‹±æ–‡ç¿»è¯‘æ³•æ–‡ï¼Œé‚£ä¹ˆå¦‚æœæ¨¡å‹åœ¨æ— ç›‘ç£é¢„è®­ç»ƒçš„è¿‡ç¨‹ä¸­çœ‹è¿‡ç›¸å…³çš„æ–‡å­— (ä¾‹å¦‚"Mentez mentez, il en restera toujours quelque chose," which translates as, "Lie lie and something will always remain."è¿™å¥è¯æ˜¯è®­ç»ƒçš„è¯­æ–™)ï¼Œé‚£ä¹ˆæ¨¡å‹å°±èƒ½å¤Ÿå­¦ä¼š (translate to french, english text, french text) è¿™æ ·çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸåˆ™ä¸Šï¼Œé€šè¿‡å¤§é‡çš„è¯­æ–™è®­ç»ƒï¼Œè¯­è¨€å»ºæ¨¡èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„ç›‘ç£ä¿¡æ¯ã€‚ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆè®²å‘¢ï¼Ÿå› ä¸ºä½œè€…è®¤ä¸º: ä¸‹æ¸¸ä»»åŠ¡ (æœ‰ç›‘ç£è®­ç»ƒ) å¯ä»¥è§†ä¸ºé¢„è®­ç»ƒè¿‡ç¨‹ (æ— ç›‘ç£è®­ç»ƒ) çš„ä¸€ä¸ªå­é›†ã€‚æ— ç›‘ç£ç›®æ ‡çš„å…¨å±€æœ€ä¼˜è§£ä¹Ÿæ˜¯æœ‰ç›‘ç£è®­ç»ƒçš„å…¨å±€æœ€ä¼˜è§£ã€‚å½“é¢„è®­ç»ƒè§„æ¨¡è¶³å¤Ÿå¤§æ—¶ï¼ŒæŠŠæ— ç›‘ç£çš„ä»»åŠ¡è®­ç»ƒå¥½äº†ï¼Œæœ‰ç›‘ç£çš„ä¸‹æ¸¸ä»»åŠ¡å³ä¸å†éœ€è¦é¢å¤–è®­ç»ƒï¼Œå°±æ˜¯æ‰€è°“çš„zero-shotã€‚æ‰€ä»¥ä¸‹é¢çš„é—®é¢˜å°±å˜æˆäº†: åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¦‚ä½•èƒ½å¤Ÿä¼˜åŒ–æ— ç›‘ç£é¢„è®­ç»ƒè¿‡ç¨‹ä»¥è¾¾åˆ°æ”¶æ•›ã€‚åˆæ­¥å®éªŒè¯å®ï¼Œè¶³å¤Ÿå¤§çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ— ç›‘ç£çš„é¢„è®­ç»ƒè¿‡ç¨‹ä¹‹ååšä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†å­¦ä¹ é€Ÿåº¦æ¯”æ˜¾å¼ç›‘ç£æ–¹æ³•æ…¢å¾—å¤šã€‚é‚£ä¹ˆæœ€åä¸€ä¸ªé—®é¢˜å°±æ˜¯å…·ä½“æ€ä¹ˆå»åšä¸‹æ¸¸ä»»åŠ¡å‘¢ï¼Ÿä»¥è‹±æ–‡ç¿»è¯‘æ³•æ–‡ä¸ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡æ—¶é¢„å…ˆå‘Šè¯‰æ¨¡å‹ â€œtranslate English to Frenchâ€ï¼Œå³ç»™æ¨¡å‹ä¸€ä¸ªæç¤º (Prompt)ã€‚
    - **GPT-2çš„æ•°æ®é›†**: è®¸å¤šä¹‹å‰çš„å·¥ä½œæ˜¯åœ¨å•ä¸ªæ–‡æœ¬åŸŸä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚æ–°é—»æ–‡ç« ï¼Œç»´åŸºç™¾ç§‘æˆ–å°è¯´ç­‰ç­‰ã€‚GPT-2åˆ™æ˜¯å¸Œæœ›ä½¿å¾—è®­ç»ƒæ•°æ®é›†çš„é¢†åŸŸå’Œä¸Šä¸‹æ–‡æ›´å¤šä¸€ç‚¹ã€‚åœ¨ç½‘ç«™ä¸Šçˆ¬å–æ–‡æœ¬æ˜¯ä¸€ä¸ªæ–¹æ¡ˆï¼Œæ¯”å¦‚è¯´Common Crawlç½‘ç«™ã€‚è™½ç„¶è¿™äº›ç½‘ç«™æ‰‹æœºçš„æ•°æ®é›†åœ¨é‡çº§ä¸Šå¾ˆå¤§ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¥é‡çš„æ•°æ®è´¨é‡é—®é¢˜ï¼Œè¿™ä¸Šé¢çš„å†…å®¹æœ‰å¾ˆå¤šæ˜¯ä¿¡å™ªæ¯”å¾ˆä½çš„ï¼Œéš¾ä»¥ç†è§£çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³æ•°æ®é›†è´¨é‡çš„é—®é¢˜ï¼ŒGPT-2åªçˆ¬å–äººç±»è¿‡æ»¤ä¹‹åçš„ç½‘é¡µã€‚ä½†æ˜¯ï¼Œæ‰‹åŠ¨è¿‡æ»¤çš„ç½‘ç»œçˆ¬å–å¾ˆæ˜‚è´µï¼Œæ‰€ä»¥GPT-2ä»ç¤¾äº¤åª’ä½“å¹³å°Reddit ä¸ŠæŠ“å–äº†è‡³å°‘æ”¶åˆ°äº†3ä¸ªkarmaçš„é“¾æ¥ã€‚karmaå¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§å¯å‘å¼æŒ‡æ ‡ï¼Œç”¨äºåˆ¤æ–­å…¶ä»–ç”¨æˆ·æ˜¯å¦è®¤ä¸ºè¯¥é“¾æ¥æœ‰è¶£ã€æœ‰æ•™è‚²æ„ä¹‰æˆ–åªæ˜¯æœ‰è¶£ã€‚å¾—åˆ°çš„è¿™ä¸ªæ•°æ®é›†ç§°ä¹‹ä¸ºWebTextï¼Œæ˜¯ä¸€ä¸ªåŒ…å«äº†4500ä¸‡ä¸ªé“¾æ¥çš„æ–‡æœ¬æ•°æ®é›†ã€‚ç»è¿‡é‡å¤æ•°æ®åˆ é™¤å’Œä¸€äº›åŸºäºå¯å‘å¼çš„æ¸…ç†åï¼Œå®ƒåŒ…å«ç•¥å¤šäº800ä¸‡ä¸ªæ–‡æ¡£ï¼Œæ€»æ–‡æœ¬å®¹é‡ä¸º40GBã€‚ä½œè€…ä»WebTextä¸­åˆ é™¤äº†æ‰€æœ‰ç»´åŸºç™¾ç§‘æ–‡æ¡£ï¼Œå› ä¸ºå®ƒå¯èƒ½æ¶‰åŠåˆ° test evaluation tasksã€‚ç›®å‰å…¨é‡çš„æ•°æ®æ˜¯æ²¡æœ‰å¼€æ”¾ä¸‹è½½çš„ï¼Œå¯é€šè¿‡[GPT-2è®­ç»ƒæ•°æ®é›†](https://link.zhihu.com/?target=https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/webtext.train.jsonl)ä¸‹è½½éƒ¨åˆ†è®­ç»ƒæ•°æ®ã€‚

    - **GPT-2ç‰¹ç‚¹**:

        - **ä¼˜ç‚¹**: GPT-2ç›¸å¯¹GPT-1æ¨¡å‹çš„äº®ç‚¹æ˜¯æ”¯æŒzero-shotçš„è®¾ç½®ï¼ŒåŒæ—¶åœ¨zero-shotçš„å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­å±•ç¤ºå‡ºä¸é”™çš„æ€§èƒ½ã€‚GPT-2é¦–å…ˆæ„é€ äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†: WebTextï¼Œå®ƒæ˜¯ä¸€ä¸ªæœ‰ç™¾ä¸‡çº§åˆ«æ–‡æœ¬çš„æ•°æ®é›†ã€‚GPT-2è‡ªå·±æ˜¯ä¸€ä¸ªæœ‰ç€1.5Bå‚æ•°é‡çš„æ¨¡å‹ï¼›GPT-2æå‡ºäº†æ–°çš„NLPèŒƒå¼ï¼Œå¼ºè°ƒé€šè¿‡æ›´å¤šçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®è®­ç»ƒé«˜å®¹é‡è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ— ç›‘ç£å®Œæˆä¸‹æ¸¸å¤šä»»åŠ¡ã€‚å°è¯•ä»¥ä¸€ç§é€šç”¨çš„è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå»è§£å†³ç°æœ‰çš„å¤§éƒ¨åˆ†NLPä»»åŠ¡ï¼›
        - **ç¼ºç‚¹**: GPT-2åœ¨æ¨¡å‹æœ¬èº«ä¸Šæ²¡å•¥å¤§çš„å˜åŒ–å’Œåˆ›æ–°ã€‚
- **GPT-3**: GPT-2çš„æœ€å¤§è´¡çŒ®æ˜¯éªŒè¯äº†é€šè¿‡æµ·é‡æ•°æ®å’Œå¤§é‡å‚æ•°è®­ç»ƒå‡ºæ¥çš„è¯å‘é‡æ¨¡å‹æœ‰è¿ç§»åˆ°å…¶å®ƒç±»åˆ«ä»»åŠ¡ä¸­è€Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚ä½†æ˜¯å¾ˆå¤šå®éªŒä¹Ÿè¡¨æ˜ï¼ŒGPT-2çš„æ— ç›‘ç£å­¦ä¹ çš„èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œç”šè‡³åœ¨æœ‰äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ¯”éšæœºçš„å¥½ã€‚å°½ç®¡åœ¨æœ‰äº›zero-shotçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸é”™ï¼Œä½†æ˜¯æˆ‘ä»¬ä»ä¸æ¸…æ¥šGPT-2çš„è¿™ç§ç­–ç•¥ç©¶ç«Ÿèƒ½åšæˆä»€ä¹ˆæ ·å­ã€‚GPT-2è¡¨æ˜éšç€æ¨¡å‹å®¹é‡å’Œæ•°æ®é‡çš„å¢å¤§ï¼Œå…¶æ½œèƒ½è¿˜æœ‰è¿›ä¸€æ­¥å¼€å‘çš„ç©ºé—´ï¼ŒåŸºäºè¿™ä¸ªæ€æƒ³ï¼Œè¯ç”Ÿäº†æˆ‘ä»¬ä¸‹é¢è¦ä»‹ç»çš„GPT-3[ã€ŠLanguage Models are Few-Shot Learnersã€‹](https://arxiv.org/pdf/2005.14165.pdf)ã€‚
    â€ƒâ€ƒGPT-2åœ¨GPT-1çš„åŸºç¡€ä¸Šå¾€å‰èµ°äº†ä¸€å¤§æ­¥: å®Œå…¨æŠ›å¼ƒäº†å¾®è°ƒï¼Œå¹¶é‡‡ç”¨äº†zero-shotçš„æ–¹å¼ã€‚Zero-shotçš„æ–¹å¼è¢«GPT-2è®¤è¯å¯è¡Œåï¼ŒOpenAIå°±ä¸å¾—ä¸å¼€å§‹è€ƒè™‘æ¨¡å‹æ˜¯å¦èƒ½çœŸæ­£åšåˆ°å¼ºå¤§äº†ï¼Œæ¯•ç«Ÿç°åœ¨åªæ˜¯å’ŒBertæŒå¹³è€Œå·²ã€‚è¿™ä¸€åˆ»OpenAIå¼€å§‹æ‚Ÿè¿‡æ¥ï¼Œæ—¢ç„¶LLMè¦ä¸€è·¯èµ°åˆ°åº•ï¼Œæ—¢ç„¶æ¨¡å‹å˜å¤§é¿å…ä¸äº†ï¼Œé‚£ä¸å¦‚æ¥å¾—æ›´å½»åº•ä¸€äº›ã€‚GPT-3æ²¿ç”¨äº†å»é™¤Fine-Tuningï¼Œåªåšé€šç”¨è¯­è¨€æ¨¡å‹çš„æ€è·¯ï¼ŒåŒæ—¶æŠ€æœ¯ä¸Šå°åšæ›¿æ¢ï¼ˆsparse Transformerï¼‰ï¼›å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨ä¸åšå¾®è°ƒçš„å‰æä¸‹é‡‡ç”¨äº†few-shotçš„æ–¹å¼ï¼ˆæ¯•ç«Ÿå®Œå…¨ä¸ç»™æ¨¡å‹ä»»ä½•æ˜¾æ€§æç¤ºï¼Œæ•ˆæœç¡®å®æ²¡è¾¾åˆ°é¢„æœŸï¼‰ã€‚æœ€ç»ˆç”Ÿæˆäº†ä¸€ä¸ªå¤§å°é«˜è¾¾175Bçš„å¤§æ¨¡å‹ï¼Œå½“ç„¶æ•ˆæœä¹Ÿæ˜¯ä¸€éª‘ç»å°˜çš„ã€‚

    - **æ¨¡å‹ç»“æ„**: GPT-3çš„æ¨¡å‹ä¸GPT-2çš„æ¨¡å‹åŸºæœ¬ä¸€è‡´ï¼Œä¸»è¦æ”¹è¿›åªæœ‰ä¸€ç‚¹:

        - **Sparse Attention**: åœ¨æ¨¡å‹ç»“æ„ä¸­çš„æ³¨æ„åŠ›å±‚ï¼ŒGPT-3é‡‡ç”¨Sparse Transformerä¸­çš„Sparse Attentionæ–¹æ¡ˆï¼Œsparse attentionä¸ä¼ ç»Ÿself-attentionï¼ˆç§°ä¸ºdense attentionï¼‰çš„åŒºåˆ«åœ¨äº:

            - **dense attention**: æ¯ä¸ª token ä¹‹é—´ä¸¤ä¸¤è®¡ç®—attentionï¼Œå¤æ‚åº¦ O ( n 2 ) O(n^{2}) O(n2)ï¼›
            - **sparse attention**: æ¯ä¸ªtokenåªä¸å…¶ä»–tokençš„ä¸€ä¸ªå­é›†è®¡ç®—attentionï¼Œå¤æ‚åº¦ O ( n âˆ— l o g n ) O(n\*logn) O(nâˆ—logn)ã€‚
                Â 

            å…·ä½“æ¥è¯´ï¼Œsparse attentioné™¤äº†ç›¸å¯¹è·ç¦»ä¸è¶…è¿‡ k k kä»¥åŠç›¸å¯¹è·ç¦»ä¸º k ï¼Œ 2 k ï¼Œ 3 k ï¼Œ â‹¯ kï¼Œ2kï¼Œ3kï¼Œ\\cdots kï¼Œ2kï¼Œ3kï¼Œâ‹¯çš„tokenï¼Œå…¶ä»–æ‰€æœ‰tokençš„æ³¨æ„åŠ›éƒ½è®¾ä¸º0ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/2190cc14dc354960b919117f658590c9.jpeg#pic_center)
            æˆ‘ä»¬æ¥å…·ä½“è§‚å¯Ÿä¸€ä¸‹ï¼Œå®é™…ä¸Šå›¾ä¸­çš„ç¬¬äºŒè¡Œå°±æ˜¯æ¶‰åŠåˆ°çš„attentionçš„tokenå†…å®¹ï¼Œå¯ä»¥çœ‹å‡ºé¦–å…ˆå…³æ³¨äº†é™„è¿‘å››ä¸ªtokenï¼Œå…¶æ¬¡æ˜¯ 2 k , 3 k 2k,3k 2k,3kè·ç¦»çš„tokenï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿™ä¹ˆåšå‘¢ï¼Ÿä½¿ç”¨ sparse attentionçš„å¥½å¤„ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç‚¹:

            - å‡å°‘æ³¨æ„åŠ›å±‚çš„è®¡ç®—å¤æ‚åº¦ï¼ŒèŠ‚çº¦æ˜¾å­˜å’Œè€—æ—¶ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ï¼›
            - å…·æœ‰â€œå±€éƒ¨ç´§å¯†ç›¸å…³å’Œè¿œç¨‹ç¨€ç–ç›¸å…³â€çš„ç‰¹æ€§ï¼Œå¯¹äºè·ç¦»è¾ƒè¿‘çš„ä¸Šä¸‹æ–‡å…³æ³¨æ›´å¤šï¼Œå¯¹äºè·ç¦»è¾ƒè¿œçš„ä¸Šä¸‹æ–‡å…³æ³¨è¾ƒå°‘ã€‚
                Â 
                å…³äºSparse Transformerçš„è¯¦ç»†ä»‹ç»å¯ä»¥å‚è§OpenAIäº2019å¹´å‘è¡¨çš„è®ºæ–‡[ã€ŠGenerating Long Sequences with Sparse Transformersã€‹](https://arxiv.org/pdf/1904.10509.pdf)ã€‚
                Â 

            è®ºæ–‡ä¸­ä¾›è®­ç»ƒäº†8ä¸ªä¸é€šè§„æ¨¡çš„æ¨¡å‹ï¼Œæœ€å¤§çš„ä¸€ä¸ªç§°ä½œä¸ºGPT-3: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/49511955866740cab0d31d50b68cd8a0.png#pic_center)

    - **æ¨¡å‹è®­ç»ƒ**: GPT-3ä¹Ÿåªæœ‰é¢„è®­ç»ƒè¿‡ç¨‹ã€‚

        - **æ— ç›‘ç£è®­ç»ƒ**: GPT-3ä»é‡‡ç”¨GPT-2æå‡ºçš„ä»…åšé¢„è®­ç»ƒã€ä¸åšå¾®è°ƒçš„æ€è·¯ã€‚GPT-3é‡‡ç”¨äº†In-context learningã€‚å€Ÿç”¨meta-learningï¼ˆå…ƒå­¦ä¹ ï¼‰çš„æ€æƒ³ï¼Œåœ¨pre-trainingæœŸé—´è®©æ¨¡å‹å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½å’Œæ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œè€Œåœ¨æ¨ç†æœŸé—´åˆ©ç”¨è¿™äº›æŠ€èƒ½å’Œèƒ½åŠ›è¿…é€Ÿé€‚é…åˆ°æœŸæœ›çš„ä»»åŠ¡ä¸Šã€‚åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä»‹ç»è¿‡In-context learningï¼Œä¸‹é¢ç®€å•ä»‹ç»ä¸‹GPT-3ä¸­çš„In-context learningã€‚
            â€ƒâ€ƒIn-context learningæ˜¯è¿™ç¯‡è®ºæ–‡ä¸­ä»‹ç»çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œè¦ç†è§£In-context learningï¼Œæˆ‘ä»¬éœ€è¦å…ˆç†è§£meta-learningï¼ˆå…ƒå­¦ä¹ ï¼‰ã€‚å¯¹äºä¸€ä¸ªå°‘æ ·æœ¬çš„ä»»åŠ¡æ¥è¯´ï¼Œæ¨¡å‹çš„åˆå§‹åŒ–å€¼éå¸¸é‡è¦ï¼Œä»ä¸€ä¸ªå¥½çš„åˆå§‹åŒ–å€¼ä½œä¸ºèµ·ç‚¹ï¼Œæ¨¡å‹èƒ½å¤Ÿå°½å¿«æ”¶æ•›ï¼Œä½¿å¾—åˆ°çš„ç»“æœéå¸¸å¿«çš„é€¼è¿‘å…¨å±€æœ€ä¼˜è§£ã€‚å…ƒå­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³åœ¨äºé€šè¿‡å°‘é‡çš„æ•°æ®å¯»æ‰¾ä¸€ä¸ªåˆé€‚çš„åˆå§‹åŒ–èŒƒå›´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šå¿«é€Ÿæ‹Ÿåˆï¼Œå¹¶è·å¾—ä¸é”™çš„æ•ˆæœã€‚
            â€ƒâ€ƒè¿™é‡Œçš„ä»‹ç»ä½¿ç”¨çš„æ˜¯MAMLï¼ˆModel-Agnostic Meta-Learningï¼‰ç®—æ³•ï¼Œæ­£å¸¸çš„ç›‘ç£å­¦ä¹ æ˜¯å°†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ‰“åŒ…æˆä¸€ä¸ªbatchè¿›è¡Œå­¦ä¹ ã€‚ä½†æ˜¯å…ƒå­¦ä¹ æ˜¯å°†ä¸€ä¸ªä¸ªä»»åŠ¡æ‰“åŒ…æˆbatchï¼Œæ¯ä¸ªbatchåˆ†ä¸ºæ”¯æŒé›†ï¼ˆsupport setï¼‰å’Œè´¨è¯¢é›†ï¼ˆquery setï¼‰ï¼Œç±»ä¼¼äºå­¦ä¹ ä»»åŠ¡ä¸­çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
            â€ƒâ€ƒå¯¹ä¸€ä¸ªç½‘ç»œæ¨¡å‹ f f fï¼Œå…¶å‚æ•°è¡¨ç¤ºä¸º Î¸ \\theta Î¸ï¼Œå®ƒçš„åˆå§‹åŒ–å€¼è¢«å«åšmeta-initializationã€‚MAMLçš„ç›®æ ‡åˆ™æ˜¯å­¦ä¹ ä¸€ç»„meta-initializationï¼Œèƒ½å¤Ÿå¿«é€Ÿåº”ç”¨åˆ°å…¶å®ƒä»»åŠ¡ä¸­ã€‚MAMLçš„è¿­ä»£æ¶‰åŠä¸¤æ¬¡å‚æ•°æ›´æ–°ï¼Œåˆ†åˆ«æ˜¯å†…å¾ªç¯ï¼ˆinner loopï¼‰å’Œå¤–å¾ªç¯ï¼ˆouter loopï¼‰ã€‚å†…å¾ªç¯æ˜¯æ ¹æ®ä»»åŠ¡æ ‡ç­¾å¿«é€Ÿçš„å¯¹å…·ä½“çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ å’Œé€‚åº”ï¼Œè€Œå¤–å­¦ä¹ åˆ™æ˜¯å¯¹meta-initializationè¿›è¡Œæ›´æ–°ã€‚ç›´è§‚çš„ç†è§£ï¼Œæˆ‘ç”¨ä¸€ç»„meta-initializationå»å­¦ä¹ å¤šä¸ªä»»åŠ¡ï¼Œå¦‚æœæ¯ä¸ªä»»åŠ¡éƒ½å­¦å¾—æ¯”è¾ƒå¥½ï¼Œåˆ™è¯´æ˜è¿™ç»„meta-initializationæ˜¯ä¸€ä¸ªä¸é”™çš„åˆå§‹åŒ–å€¼ï¼Œå¦åˆ™æˆ‘ä»¬å°±å»å¯¹è¿™ç»„å€¼è¿›è¡Œæ›´æ–°ã€‚
            â€ƒâ€ƒGPT-3ä¸­ä»‹ç»çš„In-context learningåˆ™æ˜¯å…ƒå­¦ä¹ çš„å†…å¾ªç¯ï¼ŒåŸºäºè¯­è¨€æ¨¡å‹çš„SGDåˆ™æ˜¯å¤–å¾ªç¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/49030ab15e4b4045bc65c9b8b66ada22.png#pic_center)

        - **ä¸‹æ¸¸ä»»åŠ¡**: åœ¨è®­ç»ƒé˜¶æ®µï¼Œé¢„è®­ç»ƒé€šç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…·å¤‡è¯†åˆ«ä¸åŒNLPä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ­¤æ—¶æ¨¡å‹å…·å¤‡äº†ä¸€å®šçš„ICLèƒ½åŠ›ã€‚è€Œåœ¨æ¨ç†é˜¶æ®µï¼Œä¾èµ–äºæ¨¡å‹çš„ICLèƒ½åŠ›ï¼Œé’ˆå¯¹å„NLPä»»åŠ¡ï¼Œå‘æ¨¡å‹ä¸­è¾“å…¥ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œä¸Šä¸‹æ–‡åŒ…æ‹¬ä»»åŠ¡æè¿°ã€è‹¥å¹²ä¸ªä»»åŠ¡æ ·æœ¬å’Œä»»åŠ¡æç¤ºï¼Œæ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ç»™å‡ºä»»åŠ¡è¾“å‡ºã€‚æ ¹æ®ä¸Šä¸‹æ–‡åŒ…å«çš„ä»»åŠ¡æ ·æœ¬æ•°é‡å¯è¿›ä¸€æ­¥å°†ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†ä¸ºZero-Shotï¼ˆæ— ä»»åŠ¡æ ·æœ¬ï¼‰ã€One-Shotï¼ˆä»…ä¸€ä¸ªä»»åŠ¡æ ·æœ¬ï¼‰å’ŒFew-Shotï¼ˆå¤šä¸ªä»»åŠ¡æ ·æœ¬ï¼‰ä¸‰ç±»ã€‚

            - **Fine-Tunningï¼ˆFTï¼‰**: FTåˆ©ç”¨æˆåƒä¸Šä¸‡çš„ä¸‹æ¸¸ä»»åŠ¡æ ‡æ³¨æ•°æ®æ¥æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æƒé‡ä»¥è·å¾—å¼ºå¤§çš„æ€§èƒ½ã€‚ä½†æ˜¯ï¼Œè¯¥æ–¹æ³•ä¸ä»…å¯¼è‡´æ¯ä¸ªæ–°çš„ä¸‹æ¸¸ä»»åŠ¡éƒ½éœ€è¦å¤§é‡çš„æ ‡æ³¨è¯­æ–™ï¼Œè¿˜å¯¼è‡´æ¨¡å‹åœ¨æ ·æœ¬å¤–é¢„æµ‹çš„èƒ½åŠ›å¾ˆå¼±ã€‚è™½ç„¶GPT-3ä»ç†è®ºä¸Šæ”¯æŒFTï¼Œä½†è®ºæ–‡ä¸­æ²¡è¿™ä¹ˆåšï¼›
            - **Few-Shotï¼ˆFSï¼‰**: æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µå¯ä»¥å¾—åˆ°å°‘é‡çš„ä¸‹æ¸¸ä»»åŠ¡ç¤ºä¾‹ä½œä¸ºé™åˆ¶æ¡ä»¶ï¼Œä½†æ˜¯ä¸å…è®¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æƒé‡ã€‚FSè¿‡ç¨‹çš„ç¤ºä¾‹å¯ä»¥çœ‹ä¸‹å›¾ä¸­æ•´ç†çš„æ¡ˆä¾‹ã€‚FSçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å¹¶ä¸éœ€è¦å¤§é‡çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ï¼ŒåŒæ—¶ä¹Ÿé˜²æ­¢äº†æ¨¡å‹åœ¨Fine-Tuningé˜¶æ®µçš„è¿‡æ‹Ÿåˆã€‚FSçš„ä¸»è¦ç¼ºç‚¹æ˜¯ä¸ä»…ä¸Fine-Tuningçš„SOTAæ¨¡å‹æ€§èƒ½å·®è·è¾ƒå¤§ä¸”ä»éœ€è¦å°‘é‡çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ï¼›
            - **One-Shotï¼ˆ1Sï¼‰**: æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä»…å¾—åˆ°1ä¸ªä¸‹æ¸¸ä»»åŠ¡ç¤ºä¾‹ã€‚æŠŠ1Sç‹¬ç«‹äºFew-Shotå’ŒZero-Shotè®¨è®ºæ˜¯å› ä¸ºè¿™ç§æ–¹å¼ä¸äººç±»æ²Ÿé€šçš„æ–¹å¼æœ€ç›¸ä¼¼ï¼›
            - **Zero-Shotï¼ˆ0Sï¼‰**: æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä»…å¾—åˆ°ä¸€æ®µä»¥è‡ªç„¶è¯­è¨€æè¿°çš„ä¸‹æ¸¸ä»»åŠ¡è¯´æ˜ã€‚0Sçš„ä¼˜ç‚¹æ˜¯æä¾›äº†æœ€å¤§ç¨‹åº¦çš„æ–¹ä¾¿æ€§ã€å°½å¯èƒ½å¤§çš„é²æ£’æ€§å¹¶å°½å¯èƒ½é¿å…äº†ä¼ªç›¸å…³æ€§ã€‚0Sçš„æ–¹å¼æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜çš„ï¼Œå³ä½¿æ˜¯äººç±»æœ‰æ—¶å€™ä¹Ÿéš¾ä»¥ä»…ä¾èµ–ä»»åŠ¡æè¿°è€Œæ²¡æœ‰ç¤ºä¾‹çš„æƒ…å†µä¸‹ç†è§£ä¸€ä¸ªä»»åŠ¡ã€‚ä½†æ¯«æ— ç–‘é—®ï¼Œ0Sè®¾ç½®ä¸‹çš„æ€§èƒ½æ˜¯æœ€ä¸äººç±»çš„æ°´å¹³å…·æœ‰å¯æ¯”æ€§çš„ã€‚
                ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3fc26c6dcbfb4fff8d7fa67ad2a5c9ff.png#pic_center)
    - **GPT-3çš„æ•°æ®é›†**: GPT-3çš„è®­ç»ƒæ•°æ®åŒ…æ‹¬ä½è´¨é‡çš„Common Crawlï¼Œé«˜è´¨é‡çš„WebText2ã€Books1ã€Books2å’ŒWikipediaã€‚GPT-3æ ¹æ®æ•°æ®é›†çš„ä¸åŒè´¨é‡èµ‹äºˆäº†ä¸åŒçš„æƒå€¼ï¼Œæƒå€¼è¶Šé«˜çš„åœ¨è®­ç»ƒçš„æ—¶å€™è¶Šå®¹æ˜“æŠ½æ ·åˆ°ï¼ˆè§ä¸‹å›¾ï¼‰ã€‚ä¸ºäº†æ¸…ç†è„æ•°æ®ï¼ŒOpenAIåšäº†ä»¥ä¸‹çš„æ•°æ®å¤„ç†:

        - ä½¿ç”¨é«˜è´¨é‡æ•°æ®ä½œä¸ºæ­£ä¾‹ï¼Œè®­ç»ƒLRåˆ†ç±»ç®—æ³•ï¼Œå¯¹ CommonCrawl çš„æ‰€æœ‰æ–‡æ¡£åšåˆæ­¥è¿‡æ»¤ï¼›
        - åˆ©ç”¨å…¬å¼€çš„ç®—æ³•åšæ–‡æ¡£å»é‡ï¼Œå‡å°‘å†—ä½™æ•°æ®ï¼›
        - åŠ å…¥å·²çŸ¥çš„é«˜è´¨é‡æ•°æ®é›†ï¼›

        æœ€ç»ˆå¤„ç†å®Œæˆåä½¿ç”¨çš„æ•°æ®è§„æ¨¡çº¦570Gã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b28940a5d66449cf9e9e415560b2bfe8.png#pic_center)
        å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåœ¨å®é™…å®éªŒè¿‡ç¨‹ä¸­ï¼Œå¯¹ä¸åŒæ•°æ®é›†æŒ‰ç…§ä¸€å®šçš„æ¯”ä¾‹è¿›è¡Œé‡‡æ ·ï¼Œè¿™ä¸ªæ¯”ä¾‹ä¸æ˜¯æŒ‰ç…§åŸå§‹æ•°æ®é‡å¤šå°‘æ¥åˆ’åˆ†çš„ï¼Œä¸ç„¶è¿™é‡ŒåŸºæœ¬é‡‡æ ·åˆ°çš„å°±éƒ½æ˜¯Common Crawlçš„æ•°æ®äº†ï¼Œå¯ä»¥çœ‹åˆ°è¿™é‡ŒCommon Crawlçš„æ•°æ®é‡æ¯”å…¶ä»–å‡ ä¸ªå¤šå¾ˆå¤šã€‚è¿›è¡Œé‡‡æ ·çš„åŸå› ä¸»è¦è€ƒè™‘åˆ°ï¼Œå°±ç®—åšäº†ä¸€äº›æ•°æ®æ¸…æ´—è¿˜æ˜¯è§‰å¾—Common Crawlçš„æ•°æ®è´¨é‡ä¸å¦‚å…¶ä»–å‡ ä¸ªã€‚æœ€ç»ˆé‡‡æ ·çš„æ—¶å€™ï¼Œè™½ç„¶Common Crawlçš„æ•°æ®é‡æ˜¯å…¶ä»–å‡ ä¸ªæ•°æ®é›†çš„ä¸Šç™¾å€ï¼Œä½†æ˜¯å®é™…å æ¯”æ˜¯60%ï¼Œæœ‰40%çš„æ•°æ®æ˜¯èƒ½å¤Ÿä¿è¯è´¨é‡çš„ã€‚

    - **GPT-3çš„ç‰¹ç‚¹**:

        - **ä¼˜ç‚¹**: GPT-3çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸éœ€è¦å¾®è°ƒï¼Œåªéœ€è¦åœ¨è¾“å…¥åºåˆ—é‡Œç”¨è‡ªç„¶è¯­è¨€è¡¨è¿°ä»»åŠ¡è¦æ±‚ï¼Œå°±å¯ä»¥è®©æ¨¡å‹æ‰§è¡Œä¸åŒçš„å­ä»»åŠ¡ã€‚GPT-3åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†SOTAï¼Œå¹¶ä¸”éªŒè¯äº†æ¨¡å‹è§„æ¨¡è¶Šå¤§ã€ä»»åŠ¡æ•ˆæœè¶Šå¥½ï¼Œä¸”å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼ŒGPT-3çš„Few-Shotä¼˜äºOne-Shotå’ŒZero-Shotã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/da96b6d40b2c4523b21c63d0b87d1fdd.jpeg#pic_center)

        - **ç¼ºç‚¹**:

            - ç”Ÿæˆçš„å†…å®¹å­˜åœ¨é‡å¤æˆ–ä¸åˆç†çš„å¥å­ã€æ®µè½ï¼Œç¼ºä¹å¸¸è¯†ï¼Œåœ¨ä¸€äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸€èˆ¬ï¼Œç”šè‡³å’Œéšæœºåˆ¤æ–­å·®ä¸å¤šï¼›
            - æ¨¡å‹ç»“æ„ä½¿ç”¨çš„Transformerè§£ç å™¨æ˜¯ä¸€ä¸ªå•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥åœ¨ä¸€äº›éœ€è¦åŒå‘ç†è§£çš„NLPä»»åŠ¡ï¼ˆæ¯”å¦‚æ–‡æœ¬è•´å«ï¼‰ä¸Šè¡¨ç°ä¸ä½³ï¼›
            - è¯­è¨€æ¨¡å‹åº•å±‚åŸç†è¿˜æ˜¯æ ¹æ®å‰åºè¯å…ƒé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒï¼Œæ²¡æœ‰è€ƒè™‘ä¸åŒè¯å…ƒçš„æƒé‡ï¼›
            - æ¨¡å‹è§„æ¨¡å¤ªå¤§ï¼Œè®¡ç®—èµ„æºæˆæœ¬è¾ƒé«˜ï¼Œåç»­çš„ä¸€ä¸ªæ–¹å‘æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼›
            - å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¼ºï¼›
            - æ­¤å¤–ï¼Œä½œè€…è¿˜å±•æœ›äº†ä¸€ä¸‹GPT-3å¯èƒ½å¸¦æ¥çš„ç¤¾ä¼šå½±å“ã€‚æ¯”å¦‚å®ƒå¯èƒ½è¢«æ‹¿æ¥ç”Ÿæˆå‡æ–°é—»ã€åƒåœ¾é‚®ä»¶ï¼Œä»¥åŠè®ºæ–‡é€ å‡ã€‚ç”±äºGPT-3 çš„è®­ç»ƒæ•°æ®æ¥è‡ªç½‘ç»œï¼Œå…¶ä¸­åŒ…å«äº†ä¸€äº›æ€§åˆ«ã€å®—æ•™ã€ç§æ—æ­§è§†çš„ä¿¡æ¯ï¼Œå¯¼è‡´GPT-3ç”Ÿæˆçš„æ–‡æœ¬ä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ã€‚

#### 1.2 InstructGPT

â€ƒâ€ƒGPT-3è™½ç„¶åœ¨å„å¤§NLPä»»åŠ¡ä»¥åŠæ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ä¸Šä»¤äººæƒŠè‰³ï¼Œä½†æ˜¯ä»–ä»ç„¶è¿˜æ˜¯ä¼šç”Ÿæˆä¸€äº›å¸¦æœ‰åè§çš„ï¼Œä¸çœŸå®çš„ï¼Œæœ‰å®³çš„é€ æˆè´Ÿé¢ç¤¾ä¼šå½±å“çš„ä¿¡æ¯ï¼Œè€Œä¸”å¾ˆå¤šæ—¶å€™ï¼Œä»–å¹¶ä¸æŒ‰äººç±»å–œæ¬¢çš„è¡¨è¾¾æ–¹å¼å»è¯´è¯ã€‚åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼ŒOpenAIæå‡ºäº†ä¸€ä¸ªæ¦‚å¿µâ€œAlignmentâ€ï¼Œæ„æ€æ˜¯æ¨¡å‹è¾“å‡ºä¸äººç±»çœŸå®æ„å›¾å¯¹é½ï¼Œç¬¦åˆäººç±»åå¥½ã€‚å› æ­¤ï¼Œä¸ºäº†è®©æ¨¡å‹è¾“å‡ºä¸ç”¨æˆ·æ„å›¾æ›´åŠ å¯¹é½ï¼Œå°±æœ‰äº†InstructGPTè¿™ä¸ªå·¥ä½œ[ã€ŠTraining language models to follow instructionswith human feedbackã€‹](https://arxiv.org/pdf/2203.02155.pdf)ã€‚InstructGPTæå‡ºäº†ä¸€ä¸ªç†æƒ³åŒ–è¯­è¨€æ¨¡å‹çš„ä¸‰å¤§ç›®æ ‡: **helpful**ï¼ˆèƒ½å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ï¼‰ã€**honest**ï¼ˆä¸èƒ½æé€ äº‹å®ï¼Œä¸èƒ½è¯¯å¯¼ç”¨æˆ·ï¼‰ã€**harmless**ï¼ˆä¸èƒ½å¯¹ç”¨æˆ·æˆ–ç¯å¢ƒé€ æˆç‰©ç†ã€ç²¾ç¥ã€ç¤¾ä¼šå±‚é¢çš„ä¼¤å®³ï¼‰ã€‚
â€ƒâ€ƒä¸ºäº†å®ç°ä¸Šè¿°çš„ç›®æ ‡ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºäººç±»åé¦ˆæ¥å¾®è°ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªç”¨æˆ·çš„æŒ‡ç¤ºï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„è´¨é‡å’Œå¯ä¿¡åº¦ã€‚åŸºæœ¬æµç¨‹åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤:

- **æ­¥éª¤ä¸€**: ä»OpenAI APIä¸­è·å–ç”¨æˆ·æäº¤çš„æŒ‡ä»¤promptï¼ˆåé¢æåˆ°çš„æŒ‡ä»¤promptéƒ½å¯ç†è§£ä¸ºé—®é¢˜ï¼‰å’Œæ ‡æ³¨äººå‘˜ç¼–å†™çš„æŒ‡ä»¤promptä¸­æ”¶é›†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä»æ”¶é›†åˆ°çš„æŒ‡ä»¤promptæ•°æ®é›†ä¸­å–å‡ºä¸€äº›æŒ‡ä»¤promptï¼Œç„¶åè®©æ ‡æ³¨äººå‘˜æ ‡æ³¨å¯¹åº”çš„ç­”æ¡ˆï¼Œå†ç”¨è¿™äº›æ•°æ®å¾®è°ƒGPT-3å¾—åˆ°SFTæ¨¡å‹ï¼›
- **æ­¥éª¤äºŒ**: è¾“å…¥æŒ‡ä»¤promptï¼Œè®©æ¨¡å‹è¾“å‡ºå‡ ä¸ªç­”æ¡ˆï¼Œç„¶åè®©æ ‡æ³¨äººå‘˜å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œç”¨è¿™äº›æ’åºæ•°æ®è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹RMï¼Œèƒ½å¤Ÿå¯¹ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ï¼Œæ‰“åˆ†çš„å¤§å°é¡ºåºæ»¡è¶³è®­ç»ƒä½¿ç”¨çš„è¿™äº›ç­”æ¡ˆçš„é¡ºåºï¼›
- **æ­¥éª¤ä¸‰**: å†è¾“å…¥ä¸€äº›æŒ‡ä»¤promptè®©STFå»ç”Ÿæˆä¸€äº›ç­”æ¡ˆï¼ŒæŠŠç­”æ¡ˆæ”¾åˆ°RMé‡Œé¢å»æ‰“åˆ†ï¼Œç„¶åç”¨PPOç®—æ³•å»ä¼˜åŒ–SFTçš„å‚æ•°ä½¿å¾—å®ƒç”Ÿæˆæ›´é«˜çš„åˆ†æ•°ï¼Œæœ€åå¾—åˆ°InstrctGPTã€‚

â€ƒâ€ƒæœ€ç»ˆå¾—åˆ°çš„InstrctGPTç›¸è¾ƒäºGPT-3:

- å¯ä»¥æ›´å¥½åœ°ç†è§£ç”¨æˆ·æŒ‡ç¤ºä¸­éšå«æˆ–æ˜¾å¼åœ°è¡¨è¾¾å‡ºæ¥çš„ç›®æ ‡ã€çº¦æŸå’Œåå¥½ï¼Œå¹¶ç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æœŸæœ›å’Œéœ€æ±‚çš„è¾“å‡ºï¼›
- å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºä¸­æä¾›çš„ä¿¡æ¯æˆ–ç»“æ„ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œåˆç†æ¨æ–­æˆ–åˆ›é€ ã€‚
- å¯ä»¥æ›´ç¨³å®šåœ°ä¿æŒè¾“å‡ºè´¨é‡ï¼Œå¹¶å‡å°‘é”™è¯¯æˆ–å¤±è´¥ç‡ï¼›

â€ƒâ€ƒä¸‹é¢è¯¦ç»†ä»‹ç»ä¸‹InstructGPTæ•°æ®é›†æ„å»ºä»¥åŠè®­ç»ƒæµç¨‹ã€‚

- **InstructGPTæ•°æ®é›†æ„å»º**: InstructGPTæ•°æ®é›†æ„å»ºå¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯ä¸ºäº†æ„å»ºåˆå§‹çš„æŒ‡ä»¤promptæ•°æ®é›†ï¼Œå…·ä½“åšæ³•æ˜¯è®©æ ‡æ³¨äººå‘˜æ„å»ºä¸‹é¢ä¸‰ç§prompt:

    - **Plain**: åªè¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸€ä¸ªä»»æ„çš„ä»»åŠ¡ï¼ˆä¹Ÿå°±æ˜¯æŒ‡ä»¤promotï¼‰ï¼Œå¹¶ä¿è¯ä»»åŠ¡æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼›
    - **Few-shot**: è¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸€ä¸ªæŒ‡ä»¤promptï¼Œå¹¶ç»™å‡ºå¤šä¸ªç¬¦åˆè¯¥æŒ‡ä»¤çš„query/responseç»„åˆï¼›
    - **User-based**: åŸºäºç”¨æˆ·æœŸæœ›OpenAI APIä¿±å¤‡çš„èƒ½åŠ›æ‰€æå‡ºçš„ä¸€äº›ç”¨ä¾‹ï¼Œè¦æ±‚æ ‡æ³¨äººå‘˜æ„å»ºå‡ºä¸è¿™äº›ç”¨ä¾‹ç›¸å…³çš„æŒ‡ä»¤promptã€‚
        Â 

    â€ƒâ€ƒåŸºäºä¸Šé¢ä¸‰ç§æŒ‡ä»¤promptï¼ŒOpenAIå›¢é˜Ÿè®­ç»ƒäº†åˆå§‹ç‰ˆæœ¬çš„InstructGPTæ¨¡å‹ï¼Œç„¶åå°†è¿™ä¸ªInstructGPTæ¨¡å‹æ”¾åˆ°Playgroundï¼ˆPlaygroundå¯ç†è§£ä¸ºæµ‹è¯•APIï¼Œéç”Ÿäº§APIï¼‰é‡Œä¾›ç”¨æˆ·ä½¿ç”¨ï¼Œè¿™å°±å¼•ç”³è‡³InstructGPTæ•°æ®é›†æ„å»ºçš„ç¬¬äºŒä¸ªé˜¶æ®µ: ç”¨æˆ·åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œä¼šç»§ç»­é—®ä¸€äº›é—®é¢˜ï¼ŒOpenAIå›¢é˜Ÿå°†è¿™äº›é—®é¢˜æ”¶é›†å›æ¥ï¼Œå¹¶è¿›è¡Œè¿‡æ»¤ç­‰æ“ä½œï¼Œå…·ä½“æ¥è¯´ï¼Œå°†æ¯ä¸ªç”¨æˆ·IDçš„å¯¹åº”çš„æŒ‡ä»¤promptæ•°é‡é™åˆ¶ä¸º200ä¸ªï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸ªäººä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ·IDæ‹†åˆ†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ï¼ˆåŒä¸€ä¸ªç”¨æˆ·é—®é¢˜ä¼šæ¯”è¾ƒç±»ä¼¼ï¼Œä¸é€‚åˆåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ï¼‰ã€‚å¯ä»¥çœ‹å‡ºï¼Œç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µæ˜¯ä¸€ä¸ªå¾ªç¯è¿‡ç¨‹: å…ˆæ‹¿éƒ¨åˆ†æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œç„¶åé€šè¿‡æ¨¡å‹è·å–æ–°æ•°æ®ï¼Œå†ç”¨æ–°æ•°æ®ç»§ç»­ä¼˜åŒ–æ¨¡å‹ï¼Œè¿™ç§æ€è·¯ä¹Ÿå¾ˆé€‚åˆæˆ‘ä»¬ä»¥åçš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚
    â€ƒâ€ƒè‡³æ­¤ï¼Œé€šè¿‡ä¸Šè¿°ä¸¤é˜¶æ®µçš„å¤„ç†ï¼ŒOpenAIå›¢é˜Ÿå·²ç»è·å–äº†ä¸€å®šé‡çš„æŒ‡ä»¤promptï¼ˆåŒ…æ‹¬æ ‡æ³¨äººå‘˜æ„å»ºçš„promptä»¥åŠä»ç”¨æˆ·ä¾§æ”¶é›†çš„promptï¼‰ï¼Œæ¥ä¸‹æ¥å³æ˜¯é’ˆå¯¹ä¸åŒè®­ç»ƒä»»åŠ¡æ„å»ºä¸åŒçš„æ•°æ®é›†ï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸‰é˜¶æ®µã€‚åŸºäºç¬¬äºŒé˜¶æ®µè·å–çš„æŒ‡ä»¤promptï¼Œæ„å»ºä¸‰ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºåç»­çš„ä¸‰ä¸ªè®­ç»ƒä»»åŠ¡SFTã€RMã€RLã€‚

    - **SFT Dataset**: æ ‡æ³¨äººå‘˜æ ¹æ®æŒ‡ä»¤promptæ„é€ ç­”æ¡ˆï¼Œå°†promptå’Œç­”æ¡ˆæ‹¼åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€æ®µå¯¹è¯ï¼ˆpromptï¼Œanswerï¼‰ï¼Œç”¨äºè®­ç»ƒSFTæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦13kï¼ŒåŒ…æ‹¬äººå·¥æ ‡è®°prompt+ç”¨æˆ·æ”¶é›†promptã€‚
    - **RM Dataset**: å…ˆå°†promptè¾“å…¥SFTæ¨¡å‹ï¼Œæ ‡æ³¨äººå‘˜å†å¯¹SFTæ¨¡å‹è¾“å‡ºçš„ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œç„¶åç”¨è¿™äº›æ’åºæ•°æ®ï¼ˆpromptï¼ŒRankï¼‰è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹RMï¼Œèƒ½å¤Ÿå¯¹ç­”æ¡ˆè¿›è¡Œæ‰“åˆ†ã€‚æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦33kï¼ŒåŒ…æ‹¬äººå·¥æ ‡è®°prompt+ç”¨æˆ·æ”¶é›†promptã€‚
    - **RL Dataset**: æ­¤éƒ¨åˆ†æ•°æ®é›†ä¸éœ€è¦æ ‡æ³¨ï¼Œåªéœ€è¦ä»æŒ‡ä»¤promptæ•°æ®é›†é‡Œé¢è·å–éƒ¨åˆ†æŒ‡ä»¤promptï¼Œç„¶åä½¿ç”¨SFTå’ŒRMæ¨¡å‹åˆ†åˆ«å¾—åˆ°answerå’ŒRMç»™å‡ºçš„åˆ†æ•°ï¼Œæ„æˆä¸‰å…ƒç»„ï¼ˆpromptï¼Œanswerï¼ŒRMç»™å‡ºçš„åˆ†æ•°ï¼‰ï¼Œç”¨äºè¿›ä¸€æ­¥é‡‡ç”¨PPOç®—æ³•å¾®è°ƒSFTæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†æ•°æ®é‡å¤§çº¦31kï¼ŒåªåŒ…æ‹¬ç”¨æˆ·æ”¶é›†promptã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/dd5b7ba2c2714e91a38f4ef6f9ccc705.png#pic_center)

    â€ƒâ€ƒSFT Datasetå’ŒRM Datasetéƒ½éœ€è¦äººå·¥æ ‡æ³¨ï¼ŒåŒºåˆ«åœ¨äºå‰è€…çš„ç”Ÿæˆå¼çš„æ ‡æ³¨è¦æ¯”åè€…çš„åˆ¤åˆ«å¼çš„æ ‡æ³¨è´µå¾ˆå¤šï¼ŒåŒæ ·çš„æ ‡æ³¨æ—¶é—´å’Œæˆæœ¬ï¼Œè”åˆå‰è€…å’Œåè€…å¾—åˆ°çš„æ•°æ®è¦æ¯”åªç”¨å‰è€…å¾—åˆ°çš„æ•°æ®å¤šå¾ˆå¤šï¼Œåœ¨è¿™ä¸Šé¢è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ€§èƒ½å¯èƒ½ä¼šå¥½ä¸€äº›ã€‚

- **InstructGPTè®­ç»ƒæµç¨‹**: ä¸Šæ–‡å·²ç»ä»‹ç»ï¼Œå…³äºInstructGPTçš„è®­ç»ƒæµç¨‹ï¼Œè®ºæ–‡ä¸­åˆ†ä¸ºäº†ä¸‰ä¸ªæ­¥éª¤: æœ‰ç›‘ç£å¾®è°ƒï¼Œå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å®é™…ä¸Šå¯ä»¥æŠŠå®ƒæ‹†åˆ†æˆä¸¤ç§æŠ€æœ¯æ–¹æ¡ˆï¼Œä¸€ä¸ªæ˜¯æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä¸€ä¸ªæ˜¯åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œä¸‹é¢æˆ‘ä»¬ç®€å•ä»‹ç»è¿™ä¸¤ç§æŠ€æœ¯æ–¹æ¡ˆã€‚
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/8d822267b9b04a08bb20b999216c284f.png#pic_center)

    - **æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**:  ä»¥GPT-3æ¨¡å‹ä¸ºåº•åº§ï¼Œåœ¨æ ‡æ³¨å¥½çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼ˆé—®é¢˜+ç­”æ¡ˆï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œè¿­ä»£è½®æ•°ä½¿ç”¨16ä¸ªepochï¼Œå­¦ä¹ ç‡ä½¿ç”¨ä½™å¼¦è¡°å‡ï¼Œæ¨¡å‹æ®‹å·®è¿æ¥dropoutç‡ä¸º0.2ã€‚ç”±äºåªæœ‰13000ä¸ªæ•°æ®ï¼Œ1ä¸ªepochå°±è¿‡æ‹Ÿåˆï¼Œä¸è¿‡è®ºæ–‡ä¸­è¯æ˜äº†è¿™ä¸ªæ¨¡å‹è¿‡æ‹Ÿåˆä¹Ÿæ²¡ä»€ä¹ˆå…³ç³»ï¼Œç”šè‡³è®­ç»ƒæ›´å¤šçš„epochå¯¹åç»­æ˜¯æœ‰å¸®åŠ©çš„ï¼Œæœ€ç»ˆè®­ç»ƒäº†16ä¸ªepochã€‚
    - **åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**: æ­¤éƒ¨åˆ†åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¯RMæ¨¡å‹è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åˆ©ç”¨PPOç®—æ³•ç»§ç»­å¾®è°ƒSFTæ¨¡å‹ï¼Œä¸¤é˜¶æ®µç›¸ç»“åˆï¼Œå³æ˜¯RLHFè¿‡ç¨‹ã€‚
        - **å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰**: æ¨¡å‹ç»“æ„æ˜¯æŠŠSFTæ¨¡å‹æœ€åçš„unembeddingå±‚ï¼ˆå°±æ˜¯å°†æ¨¡å‹è¾“å‡ºçš„token embeddingè½¬æ¢ä¸ºlogitsçš„é‚£ä¸€å±‚ï¼‰å»æ‰ï¼Œå³æœ€åä¸€å±‚ä¸ç”¨softmaxï¼Œæ”¹æˆä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¿™æ ·è®­ç»ƒå¥½çš„RMæ¨¡å‹å°±å¯ä»¥åšåˆ°è¾“å…¥é—®é¢˜+ç­”æ¡ˆï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡çš„åˆ†æ•°ã€‚RMæ¨¡å‹ä½¿ç”¨6Bï¼Œè€Œä¸æ˜¯175Bï¼Œä¸»è¦åŸå› æ˜¯:

            - èŠ‚çœè®¡ç®—ï¼Œæ›´ä¾¿å®œï¼›
            - å¤§æ¨¡å‹175B-RMä¸ç¨³å®šï¼ˆå¤§æ¨¡å‹çš„é€šç—…ï¼Œæ¨¡å‹å‚æ•°å¾ˆå¤šï¼Œå¾ˆéš¾æ”¶æ•›ï¼‰ï¼Œå› æ­¤ä¸å¤ªé€‚åˆåœ¨RLæœŸé—´ç”¨ä½œå€¼å‡½æ•°ã€‚
                Â 

            å‰æ–‡å·²ç»ä»‹ç»ï¼ŒRMæ•°æ®é›†åœ¨æ ‡æ³¨é˜¶æ®µï¼Œæ ‡æ³¨äººå‘˜è¢«è¦æ±‚å¯¹æ¯ä¸€ä¸ªpromptä¸‹çš„ä¸åŒå›ç­”è¿›è¡Œæ’åºã€‚å¦‚ä¸‹å›¾ï¼ŒæŸä¸ªpromptä¸‹æœ‰Aã€Bã€Cä¸‰ä¸ªå›ç­”ï¼Œæ ‡æ³¨äººå‘˜è®¤ä¸ºA>B>Cã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œå‡è®¾ä¸€ä¸ªpromptä¸‹æœ‰Kä¸ªå›ç­”ï¼Œåˆ™ä¸¤ä¸¤å›ç­”ä¸€ç»„ï¼Œç»„æˆä¸€æ¡è®­ç»ƒæ•°æ®ï¼Œä¾‹å¦‚ï¼ˆprompt, A, Bï¼‰ï¼Œåˆ™ä¸€å…±æœ‰ C k 2 C\_{k}^{2} Ck2â€‹æ¡è®­ç»ƒæ•°æ®ã€‚è¿™äº›è®­ç»ƒæ•°æ®å°†ç»„æˆä¸€ä¸ªbatchï¼Œé€šè¿‡æ„é€ å¹¶æœ€å°åŒ–Pairwise Ranking Lossçš„æ–¹æ³•ï¼Œæ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæ•´ä½“è¿‡ç¨‹å¦‚ä¸‹: å…ˆä»¥RM Datasetä¸­çš„æŒ‡ä»¤promptä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ç¬¬ä¸€é˜¶æ®µå¾®è°ƒå¥½çš„SFTæ¨¡å‹ï¼Œç”ŸæˆKä¸ªä¸åŒçš„å›ç­”ï¼Œå½¢æˆ<prompt,answer1>,<prompt,answer2>â€¦.<prompt,answerK>æ•°æ®ã€‚ç„¶åï¼Œæ ‡æ³¨äººå‘˜æ ¹æ®ç›¸å…³æ€§ã€ä¿¡æ¯æ€§å’Œæœ‰å®³ä¿¡æ¯ç­‰æ ‡å‡†ï¼Œå¯¹Kä¸ªç»“æœè¿›è¡Œæ’åºï¼Œç”Ÿæˆæ’åºç»“æœæ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨è¿™ä¸ªæ’åºç»“æœæ•°æ®è¿›è¡Œpair-wise learning to rankæ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒRMæ¨¡å‹ã€‚RMæ¨¡å‹æ¥å—ä¸€ä¸ªè¾“å…¥<prompt,answer>ï¼Œç»™å‡ºè¯„ä»·å›ç­”è´¨é‡é«˜ä½çš„å¥–åŠ±åˆ†æ•°scoreã€‚å¯¹äºä¸€å¯¹è®­ç»ƒæ•°æ®<answer1,answer2>ï¼Œå‡è®¾äººå·¥æ’åºä¸­answer1æ’åœ¨answer2å‰é¢ï¼Œé‚£ä¹ˆlosså‡½æ•°åˆ™é¼“åŠ±RMæ¨¡å‹å¯¹<prompt,answer1>çš„æ‰“åˆ†è¦æ¯”<prompt,answer2>çš„æ‰“åˆ†è¦é«˜ã€‚![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/4bf95ac06149479b8bb5189c7f38cc48.jpeg#pic_center)
            æ¥ä¸‹æ¥æˆ‘ä»¬æ ¹æ®æ¨¡å‹çš„æŸå¤±å‡½æ•°Pairwise Ranking Lossï¼Œè¯¦ç»†äº†è§£ä¸‹RMæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚Pairwise Ranking Lossè¡¨è¾¾å¼å¦‚ä¸‹æ‰€ç¤º:
            l o s s ( Î¸ ) = âˆ’ 1 C k 2 E ( x , y w , y l ) âˆ¼ D \[ l o g ( Ïƒ ( r Î¸ ( x , y w ) âˆ’ r Î¸ ( x , y l ) ) ) \] Â  loss(\\theta)=-\\frac{1}{C\_{k}^{2}}E\_{(x,y\_{w},y\_{l})\\sim D}\[log(\\sigma(r\_{\\theta}(x,y\_{w})-r\_{\\theta}(x,y\_{l})))\]\\ loss(Î¸)\=âˆ’Ck2â€‹1â€‹E(x,ywâ€‹,ylâ€‹)âˆ¼Dâ€‹\[log(Ïƒ(rÎ¸â€‹(x,ywâ€‹)âˆ’rÎ¸â€‹(x,ylâ€‹)))\]Â 
            å…¶ä¸­ï¼Œ

            - x x xè¡¨ç¤ºæŸä¸ªpromptï¼›
            - y w y\_{w} ywâ€‹å’Œ y l y\_{l} ylâ€‹åˆ†åˆ«è¡¨ç¤ºè¯¥promptä¸‹çš„ä»»æ„ä¸€å¯¹å›ç­”ï¼Œå¹¶ä¸”å‡è®¾æ ‡æ³¨ä¸­ y w y\_{w} ywâ€‹çš„æ’åºæ˜¯é«˜äº y l y\_{l} ylâ€‹çš„ï¼›
            - D D Dè¡¨ç¤ºè¯¥promptä¸‹äººç±»æ ‡æ³¨æ’åºçš„æ‰€æœ‰ä¸¤ä¸¤å›ç­”ç»„åˆï¼›
            - r Î¸ r\_{\\theta} rÎ¸â€‹è¡¨ç¤ºå¥–åŠ±æ¨¡å‹ï¼›
            - Ïƒ \\sigma Ïƒè¡¨ç¤º s i g m o i d sigmoid sigmoidå‡½æ•°ã€‚
                Â 

            ä¹Ÿå¯ä»¥å‚è€ƒä¸‹å›¾ï¼ˆæœ‰ä¸ªå°é”™è¯¯ï¼Œå°±æ˜¯ s i g m o i d sigmoid sigmoidå‡½æ•°æ˜¯å°†å€¼æ˜ å°„è‡³ ( 0 ï¼Œ 1 ) (0ï¼Œ1) (0ï¼Œ1)ï¼Œè€Œä¸æ˜¯ ( âˆ’ 1 ï¼Œ 1 ) (-1ï¼Œ1) (âˆ’1ï¼Œ1)ï¼Œä¸è¿‡æ— ä¼¤å¤§é›…ï¼‰ã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b35eac803dbf4336b3a40eadd7881bee.png#pic_center)
            Â 
            è®ºæ–‡ä¸­æœŸæœ›å½“å›ç­” y y yçš„æ’åºç›¸å¯¹è¾ƒé«˜æ—¶ï¼Œ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)çš„å¾—åˆ†ä¹Ÿèƒ½è¶Šé«˜ã€‚ä¸ºäº†ä¸è®©Kçš„ä¸ªæ•°å½±å“è®­ç»ƒæ¨¡å‹ï¼Œè®ºæ–‡ä¸­åœ¨å‰é¢ä¹˜ä¸Š 1 C k 2 \\frac{1}{C\_{k}^{2}} Ck2â€‹1â€‹ï¼Œå°†losså¹³å‡åˆ°æ¯ä¸€ä¸ªç­”æ¡ˆç»„åˆä¸Šã€‚é™¤æ­¤ä¹‹å‰ï¼Œè¿˜æœ‰å‡ ç‚¹éœ€è¦æˆ‘ä»¬æ³¨æ„:

            - **Kå€¼çš„é€‰æ‹©**: è®ºæ–‡ä¸­é‡‡ç”¨äº†K=9ï¼Œè€Œä¸æ˜¯æ›´å°çš„å€¼ï¼Œæ¯”å¦‚4ã€‚åŸå› åœ¨äº:
                - è¿›è¡Œæ ‡æ³¨çš„æ—¶å€™ï¼Œéœ€è¦èŠ±å¾ˆå¤šæ—¶é—´å»ç†è§£é—®é¢˜ï¼Œä½†ç­”æ¡ˆå’Œç­”æ¡ˆæ¯”è¾ƒç›¸è¿‘ï¼Œå¯¹9ä¸ªç­”æ¡ˆåšæ’åºç›¸è¾ƒäºå¯¹4ä¸ªç­”æ¡ˆåšæ’åºå¤šèŠ±çš„æ—¶é—´ä¸åˆ°ä¸€å€ã€‚åŒæ—¶K=9ç”Ÿæˆçš„é—®ç­”å¯¹æ˜¯K=4çš„6å€ï¼ˆ C 9 2 {C\_{9}^{2}} C92â€‹\=36ï¼Œ C 4 2 {C\_{4}^{2}} C42â€‹\=6ï¼‰ï¼Œéå¸¸åˆ’ç®—ï¼›
                - K=9æ—¶ï¼Œæ¯æ¬¡è®¡ç®—RMæ¨¡å‹çš„lossæ—¶éœ€è¦éƒ½æœ‰36é¡¹ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)è¦è®¡ç®—ï¼Œè¿™ä¸ªè®¡ç®—æ¯”è¾ƒè´µï¼Œä½†å¯ä»¥é€šè¿‡é‡å¤åˆ©ç”¨ä¹‹å‰ç®—è¿‡çš„å€¼ï¼Œä½¿å¾—åªè¦è®¡ç®—9æ¬¡å°±è¡Œï¼Œä¹Ÿå°±æ˜¯è¯´å°†9ä¸ªç­”æ¡ˆå¯¹åº”çš„ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)è®¡ç®—å‡ºæ¥ä¹‹åï¼Œåé¢è®¡ç®—æŸå¤±æ—¶ï¼Œä¸¤ä¸¤ç»„åˆå°±å¯ä»¥äº†ï¼Œè¿™æ ·å°±å¯ä»¥çœä¸‹å¾ˆå¤šæ—¶é—´ã€‚
            - **è®­ç»ƒæ•°æ®è¾“å…¥æ¨¡å¼é€‰æ‹©**: è®ºæ–‡ä¸­å°† ( x , y w , y l ) âˆ¼ D (x,y\_{w},y\_{l})\\sim D (x,ywâ€‹,ylâ€‹)âˆ¼Då½“æˆä¸€ä¸ªbatchåŒæ—¶é€å…¥æ¨¡å‹ï¼Œè€Œä¸æ˜¯å°†å•æ¡ ( x , y w , y l ) (x,y\_{w},y\_{l}) (x,ywâ€‹,ylâ€‹)æ•°æ®åˆ†åˆ«é€å…¥æ¨¡å‹ï¼ŒåŸå› åœ¨äº:
                - ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆã€‚å¯¹äºæŸä¸€å¯¹ ( x , y w , y l ) (x,y\_{w},y\_{l}) (x,ywâ€‹,ylâ€‹)ä¸­çš„ä¸€ä¸ªæ ·æœ¬ ( x , y ) (x,y) (x,y)ï¼Œç”¨batchæ–¹å¼æ—¶ï¼Œå®ƒåªå‚ä¸ä¸€æ¬¡æ¢¯åº¦è®¡ç®—ï¼›ç”¨å•æ¡æ–¹å¼æ—¶ï¼Œå®ƒéœ€è¦å‚ä¸K-1æ¬¡æ¢¯åº¦è®¡ç®—ã€‚æ¨¡å‹è¶…è¿‡ä¸€ä¸ªepochåä¼šè¿‡æ‹Ÿåˆï¼Œåœ¨ä¸€ä¸ªepochä¸­åå¤ä½¿ç”¨æ•°æ®æ›´ä¼šè¿‡æ‹Ÿåˆäº†ï¼›
                - ä¸ºäº†æå‡è®¡ç®—æ•ˆç‡ã€‚åœ¨æ¨¡å‹forwardçš„è¿‡ç¨‹ä¸­ï¼Œæœ€è€—æ—¶çš„æ­¥éª¤æ˜¯è®¡ç®— r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)ã€‚ç”¨batchæ–¹å¼æ—¶ï¼Œè¯¥è®¡ç®—åªéœ€æ‰§è¡ŒKæ¬¡ï¼ˆå› ä¸ºæ¨¡å‹å‚æ•°æ²¡æœ‰æ›´æ–°ï¼Œç›¸åŒçš„(x, y)å¯ä»¥é‡å¤ä½¿ç”¨ï¼‰ï¼›é‡‡ç”¨å•æ¡æ–¹å¼æ—¶ï¼Œéœ€è¦è®¡ç®—K(K-1)æ¬¡ï¼ˆå› ä¸ºä¸€æ¡è®¡ç®—æ›´æ–°ä¸€æ¬¡æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°ï¼Œç›¸åŒçš„(x,y)éœ€è¦é‡æ–°è®¡ç®— r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)ï¼‰ã€‚å› æ­¤ï¼ŒKè¶Šå¤§æ—¶ï¼Œé‡‡ç”¨batchçš„æ–¹å¼è¶Šåˆ’ç®—ï¼Œå®ƒåœ¨ä¿è¯ç›¸å¯¹æ’åºä¿¡æ¯ä¸°å¯Œçš„åŒæ—¶ï¼ŒåˆèŠ‚çœäº†è®¡ç®—æ•ˆç‡ã€‚
            - **è®­ç»ƒepoché€‰æ‹©**: æ¨¡å‹è®­ç»ƒè¶…è¿‡ä¸€ä¸ªepochåä¼šè¿‡æ‹Ÿåˆï¼Œæ•…åªè®­ç»ƒä¸€ä¸ªepochã€‚
        - **å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼ˆRLï¼‰**: è¿™ä¸ªé˜¶æ®µå…ˆå°†RLæ¨¡å‹çš„æƒé‡åˆå§‹åŒ–ä¸ºSFTæ¨¡å‹çš„æƒé‡ï¼Œç„¶åé€šè¿‡æ”¹è‰¯åçš„PPOç®—æ³•ï¼ˆPPO-ptxç®—æ³•ï¼‰ç»§ç»­å¯¹RLæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå¾—åˆ°InstructGPTã€‚å¼ºåŒ–å­¦ä¹ çš„å¤§è‡´æµç¨‹å¯ä»¥æ€»ç»“ä¸º: æ¨¡å‹åœ¨åšå‡ºè¡ŒåŠ¨åï¼Œéœ€è¦äººæ¥å¯¹æ¨¡å‹è¿›è¡Œåé¦ˆï¼Œç„¶åæ¨¡å‹åšå‡ºå¯¹åº”çš„æ›´æ–°ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­è®­ç»ƒRMå°±æ˜¯ä¸ºäº†å­¦ä¹ äººæ¥å¯¹æ¨¡å‹è¿›è¡Œåé¦ˆï¼ŒSFTæ¨¡å‹åœ¨æ‹¿åˆ°promptå¹¶ç”Ÿæˆå¯¹åº”çš„ç­”æ¡ˆåï¼Œç”±RMè¿›è¡Œæ‰“åˆ†ï¼Œå†æ ¹æ®è¿™ä¸ªæ‰“åˆ†å»æ›´æ–°æ¨¡å‹ï¼Œç„¶åç”¨æ›´æ–°çš„æ¨¡å‹ç”Ÿæˆæ–°çš„ç­”æ¡ˆï¼Œå¹¶è¿›è¡Œä¸‹ä¸€æ­¥å­¦ä¹ ï¼Œè¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å‡½æ•° o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)å¦‚ä¸‹æ‰€ç¤ºï¼ŒRLæ¨¡å‹æœ€ç»ˆçš„è®­ç»ƒç›®æ ‡æ˜¯è®© o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)è¶Šå¤§è¶Šå¥½ã€‚
            o b j e c t i v e ( Ï• ) = E ( x , y ) âˆ¼ D Ï€ Ï• R L \[ r Î¸ ( x , y ) âˆ’ Î² l o g ( Ï€ Ï• R L ( y âˆ£ x ) / Ï€ S F T ( y âˆ£ x ) ) \] + Î³ E x âˆ¼ D p r e t r a i n \[ l o g ( Ï€ Ï• R L ( x ) ) \] \\begin{aligned} objective(\\phi)=&E\_{(x,y)\\sim D\_{\\pi\_{\\phi}^{RL}}}\[r\_{\\theta}(x,y)-\\beta log(\\pi\_{\\phi}^{RL}(y|x)/\\pi^{SFT}(y|x))\]+\\\\ &\\gamma E\_{x\\sim D\_{pretrain}}\[log(\\pi\_{\\phi}^{RL}(x))\] \\end{aligned} objective(Ï•)\=â€‹E(x,y)âˆ¼DÏ€Ï•RLâ€‹â€‹â€‹\[rÎ¸â€‹(x,y)âˆ’Î²log(Ï€Ï•RLâ€‹(yâˆ£x)/Ï€SFT(yâˆ£x))\]+Î³Exâˆ¼Dpretrainâ€‹â€‹\[log(Ï€Ï•RLâ€‹(x))\]â€‹
            å…¶ä¸­:

            - Ï€ S F T \\pi^{SFT} Ï€SFT: å³ç¬¬ä¸€é˜¶æ®µï¼Œç»è¿‡supervised fine-tuningçš„GPT-3æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯SFTæ¨¡å‹ï¼›
            - Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹: å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¨¡å‹ç§°åšpolicyï¼Œ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å°±æ˜¯éœ€è¦å­¦ä¹ çš„æ¨¡å‹ï¼Œå³æœ€ç»ˆçš„æ¨¡å‹ã€‚åˆå§‹æ—¶:  Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹\= Ï€ S F T \\pi^{SFT} Ï€SFTï¼›
            - r Î¸ r\_{\\theta} rÎ¸â€‹: å³ç¬¬äºŒé˜¶æ®µè®­ç»ƒçš„RMæ¨¡å‹ã€‚
                Â 

            æ•´ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸Šè¿°çš„ç›®æ ‡å‡½æ•°ï¼Œç°åœ¨åˆ†åˆ«ä»‹ç»ä¸‹ç›®æ ‡å‡½æ•°çš„æ¯ä¸€é¡¹ï¼Œä¹Ÿå¯ä»¥å‚è€ƒä¸‹é¢çš„å›¾ç‰‡:

            - ( x , y ) âˆ¼ D Ï€ Ï• R L (x,y)\\sim D\_{\\pi\_{\\phi}^{RL}} (x,y)âˆ¼DÏ€Ï•RLâ€‹â€‹:  x x xæ˜¯ç¬¬RL Datasetæ•°æ®é›†ä¸­çš„é—®é¢˜ï¼ˆæŒ‡ä»¤promptï¼‰ï¼Œ y y yæ˜¯ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹æ¨¡å‹å¾—åˆ°çš„ç­”æ¡ˆ;
            - r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y): å¯¹é—®é¢˜ x x x+ç­”æ¡ˆ y y yï¼Œè¾“å…¥RMæ¨¡å‹è¿›è¡Œæ‰“åˆ†ï¼Œç›®æ ‡æ˜¯å¸Œæœ›è¿™ä¸ªåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼›
            - Ï€ Ï• R L ( y âˆ£ x ) \\pi\_{\\phi}^{RL}(y|x) Ï€Ï•RLâ€‹(yâˆ£x): é—®é¢˜ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å¾—åˆ°ç­”æ¡ˆ y y yçš„æ¦‚ç‡ï¼Œå…·ä½“æ¥è¯´ Ï€ ( y âˆ£ x ) \\pi(y|x) Ï€(yâˆ£x)æ˜¯æŠŠæ¨¡å‹è¾“å‡º y y yçš„æ¯ä¸€ä¸ªtokenå¯¹åº”çš„softmaxæ¦‚ç‡ç›¸ä¹˜å¾—åˆ°çš„ç»“æœï¼Œä¸‹åŒï¼›
            - Ï€ S F T ( y âˆ£ x ) \\pi^{SFT}(y|x) Ï€SFT(yâˆ£x): é—®é¢˜ x x xé€šè¿‡ Ï€ S F T \\pi^{SFT} Ï€SFTå¾—åˆ°ç­”æ¡ˆ y y yçš„æ¦‚ç‡ï¼›
            - l o g ( Ï€ Ï• R L ( y âˆ£ x ) / Ï€ S F T ( y âˆ£ x ) ) log(\\pi\_{\\phi}^{RL}(y|x)/\\pi^{SFT}(y|x)) log(Ï€Ï•RLâ€‹(yâˆ£x)/Ï€SFT(yâˆ£x)): KLæ•£åº¦ï¼Œå–å€¼èŒƒå›´>=0ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ˜¯å¦ç›¸ä¼¼ï¼ŒKLå€¼è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šä¸ç›¸ä¼¼ï¼Œåˆ†å¸ƒç›¸åŒæ—¶KL=0ã€‚åœ¨æœ¬é˜¶æ®µï¼Œè®ºæ–‡ä¸­å¸Œæœ›å¼ºåŒ–å­¦ä¹ åå¾—åˆ°çš„æ¨¡å‹ï¼Œåœ¨èƒ½å¤Ÿç†è§£äººç±»æ„å›¾çš„åŸºç¡€ä¸Šï¼Œåˆä¸è¦å’Œæœ€åŸå§‹çš„æ¨¡å‹è¾“å‡ºç›¸å·®å¤ªè¿œï¼ˆåœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åï¼Œ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ä¼šå‘ç”Ÿå˜åŒ–ï¼Œ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ y y yä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œè€Œ r Î¸ r\_{\\theta} rÎ¸â€‹æ‰“åˆ†æ¨¡å‹æ˜¯æ ¹æ® Ï€ S F T \\pi^{SFT} Ï€SFTæ¨¡å‹çš„æ•°æ®è®­ç»ƒè€Œæ¥ï¼Œå¦‚æœ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å’Œ Ï€ S F T \\pi^{SFT} Ï€SFTå·®çš„å¤ªå¤šï¼Œåˆ™ä¼šå¯¼è‡´ r Î¸ r\_{\\theta} rÎ¸â€‹çš„åˆ†æ•°è¾“å‡ºä¸å‡†ç¡®ã€‚å› æ­¤éœ€è¦é€šè¿‡KLæ•£åº¦æ¥è®¡ç®— Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒå’Œ Ï€ S F T \\pi^{SFT} Ï€SFTç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¸è¦å·®çš„å¤ªè¿œã€‚ï¼‰ã€‚å‚æ•° Î² \\beta Î²åˆ™è¡¨ç¤ºå¯¹è¿™ç§åå·®çš„å®¹å¿ç¨‹åº¦ã€‚åç¦»è¶Šè¿œï¼Œå°±è¦ä»å¥–åŠ±æ¨¡å‹çš„åŸºç¡€ä¸Šå¾—åˆ°è¶Šå¤šçš„æƒ©ç½šï¼›
            - x âˆ¼ D p r e t r a i n x\\sim D\_{pretrain} xâˆ¼Dpretrainâ€‹:  x x xæ˜¯æ¥è‡ªGPT-3é¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®ï¼›
            - l o g ( Ï€ Ï• R L ( x ) ) log(\\pi\_{\\phi}^{RL}(x)) log(Ï€Ï•RLâ€‹(x)): è¡¨ç¤ºå°†æ¥è‡ªåˆå§‹GPT-3ä¸­çš„æ•°æ®é€å…¥å½“å‰å¼ºåŒ–æ¨¡å‹ä¸‹ï¼ŒåŒæ ·ï¼Œè®ºæ–‡ä¸­å¸Œæœ›åœ¨è®­ç»ƒå¾—åˆ°æ–°æ¨¡å‹ä¹‹åï¼Œä¸èƒ½é™ä½åœ¨åŸå§‹ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå³ä¸èƒ½å¤ªåç¦»åŸå§‹ä»»åŠ¡ï¼Œä¿è¯æ–°æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚ Î³ \\gamma Î³åˆ™æ˜¯å¯¹è¿™ç§åç¦»çš„æƒ©ç½šç¨‹åº¦ã€‚
                ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/ec1c753c065148ecbe75efbbf240120f.png#pic_center)
                Â 

            â€ƒâ€ƒæœ€åå†ç»™å‡ºå¯¹ç›®æ ‡å‡½æ•°çš„ç†è§£ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯ä½¿å¾—ä¸Šè¿°ç›®æ ‡å‡½æ•°è¶Šå¤§è¶Šå¥½ï¼Œé€šè¿‡ä¸Šè¿°ä»‹ç»ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œ o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)å¯åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼ŒRMæ‰“åˆ†éƒ¨åˆ†+KLæ•£åº¦éƒ¨åˆ†+GPT-3é¢„è®­ç»ƒéƒ¨åˆ†:

            - å°†RL Datasetæ•°æ®é›†ä¸­çš„é—®é¢˜ x x xï¼Œé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹æ¨¡å‹å¾—åˆ°ç­”æ¡ˆ y y yï¼›
            - æŠŠä¸€å¯¹ ( x , y ) (x,y) (x,y)é€è¿›RMæ¨¡å‹è¿›è¡Œæ‰“åˆ†ï¼Œå¾—åˆ° r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)ï¼Œå³ç¬¬ä¸€éƒ¨åˆ†æ‰“åˆ†éƒ¨åˆ†ï¼Œè¿™ä¸ªåˆ†æ•°è¶Šé«˜å°±ä»£è¡¨æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆè¶Šå¥½ï¼›
            - åœ¨æ¯æ¬¡æ›´æ–°å‚æ•°åï¼Œ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ä¼šå‘ç”Ÿå˜åŒ–ï¼Œ x x xé€šè¿‡ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ y y yä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œè€Œ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)æ‰“åˆ†æ¨¡å‹æ˜¯æ ¹æ® Ï€ S F T \\pi^{SFT} Ï€SFTæ¨¡å‹çš„æ•°æ®è®­ç»ƒè€Œæ¥ï¼Œå¦‚æœ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹å’Œ Ï€ S F T \\pi^{SFT} Ï€SFTå·®çš„å¤ªå¤šï¼Œåˆ™ä¼šå¯¼è‡´ r Î¸ ( x , y ) r\_{\\theta}(x,y) rÎ¸â€‹(x,y)çš„åˆ†æ•°ä¼°ç®—ä¸å‡†ç¡®ã€‚å› æ­¤éœ€è¦é€šè¿‡KLæ•£åº¦æ¥è®¡ç®— Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹ç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒå’Œ Ï€ S F T \\pi^{SFT} Ï€SFTç”Ÿæˆçš„ç­”æ¡ˆåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¸è¦å·®çš„å¤ªè¿œã€‚æˆ‘ä»¬å¸Œæœ›ä¸¤ä¸ªæ¨¡å‹çš„å·®è·è¶Šå°è¶Šå¥½ï¼Œå³KLæ•£åº¦è¶Šå°è¶Šå¥½ï¼Œå‰é¢éœ€è¦åŠ ä¸€ä¸ªè´Ÿå·ï¼Œä½¿å¾— o b j e c t i v e ( Ï• ) objective(\\phi) objective(Ï•)è¶Šå¤§è¶Šå¥½ã€‚è¿™ä¸ªå°±æ˜¯KLæ•£åº¦éƒ¨åˆ†ï¼›
            - å¦‚æœæ²¡æœ‰ç¬¬ä¸‰éƒ¨åˆ†ï¼Œé‚£ä¹ˆæ¨¡å‹æœ€ç»ˆå¯èƒ½åªå¯¹è¿™ä¸€ä¸ªä»»åŠ¡èƒ½å¤Ÿåšå¥½ï¼Œåœ¨åˆ«çš„ä»»åŠ¡ä¸Šä¼šå‘ç”Ÿæ€§èƒ½ä¸‹é™ã€‚æ‰€ä»¥ç¬¬ä¸‰éƒ¨åˆ†å°±æŠŠåŸå§‹çš„GPT-3ç›®æ ‡å‡½æ•°åŠ äº†ä¸Šå»ï¼Œä½¿å¾—å‰é¢ä¸¤ä¸ªéƒ¨åˆ†åœ¨æ–°çš„æ•°æ®é›†ä¸Šåšæ‹Ÿåˆï¼ŒåŒæ—¶ä¿è¯åŸå§‹çš„æ•°æ®ä¹Ÿä¸è¦ä¸¢ï¼Œè¿™ä¸ªå°±æ˜¯ç¬¬ä¸‰éƒ¨åˆ†GPT-3é¢„è®­ç»ƒéƒ¨åˆ†ï¼›
            - å½“ Î³ \\gamma Î³\=0æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åšPPOï¼Œå½“ Î³ \\gamma Î³ä¸ä¸º0æ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å«åšPPO-ptxã€‚InstructGPTæ›´åå‘äºä½¿ç”¨PPO-ptxï¼›
            - æœ€ç»ˆä¼˜åŒ–åçš„ Ï€ Ï• R L \\pi\_{\\phi}^{RL} Ï€Ï•RLâ€‹æ¨¡å‹å°±æ˜¯InstructGPTçš„æ¨¡å‹ã€‚
                Â 

    â€ƒâ€ƒå›é¡¾ä¸‹InstructGPTçš„è®­ç»ƒæµç¨‹ï¼Œå…±åŒ…å«ä¸¤æ¬¡å¯¹æ¨¡å‹çš„å¾®è°ƒ: GPT-3æ¨¡å‹ â‡’ \\Rightarrow â‡’SFTæ¨¡å‹ â‡’ \\Rightarrow â‡’RLæ¨¡å‹ï¼Œå…¶å®è¿™é‡Œå§‹ç»ˆéƒ½æ˜¯åŒä¸€ä¸ªæ¨¡å‹ï¼Œåªæ˜¯ä¸åŒè¿‡ç¨‹ä¸­åç§°ä¸ä¸€æ ·ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨SFTæ¨¡å‹ â‡’ \\Rightarrow â‡’RLæ¨¡å‹é˜¶æ®µï¼Œè¿˜ä¼šä¾èµ–äºå¦ä¸€ä¸ªåœ¨SFTæ¨¡å‹åŸºç¡€ä¸Šè®­ç»ƒçš„RMæ¨¡å‹ã€‚InstructGPTè®­ç»ƒSFTã€RMã€RLä¸‰ä¸ªæ¨¡å‹çš„åŸå› ä¸º:

    - **éœ€è¦SFTæ¨¡å‹çš„åŸå› **: GPT-3æ¨¡å‹ä¸ä¸€å®šèƒ½å¤Ÿä¿è¯æ ¹æ®äººçš„æŒ‡ç¤ºã€æœ‰å¸®åŠ©çš„ã€å®‰å…¨çš„ç”Ÿæˆç­”æ¡ˆï¼Œéœ€è¦äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼›
    - **éœ€è¦RMæ¨¡å‹çš„åŸå› **: æ ‡æ³¨æ’åºçš„åˆ¤åˆ«å¼æ ‡æ³¨ï¼Œæˆæœ¬è¿œè¿œä½äºç”Ÿæˆç­”æ¡ˆçš„ç”Ÿæˆå¼æ ‡æ³¨ï¼›
    - **éœ€è¦RLæ¨¡å‹çš„åŸå› **: è®©æ¨¡å‹å€ŸåŠ©å¼ºåŒ–å­¦ä¹ çš„èƒ½åŠ›ï¼Œæ›´å¥½çš„ç†è§£äººç±»çš„æ„å›¾ã€‚
        Â 

    â€ƒâ€ƒæœ€åï¼Œæˆ‘ä»¬å±•ç¤ºä¸‹è®ºæ–‡ä¸­æåˆ°çš„InstructGPTæ€§èƒ½å¯¹æ¯”ç»“æœï¼Œå¯ä»¥å‘ç°ï¼Œå‚æ•°é‡ä¸º13Bçš„InstructGPTæ¨¡å‹ï¼Œæ€§èƒ½éƒ½è¦è¿œè¿œå¥½äºå‚æ•°é‡ä¸º175Bçš„GPT-3æ¨¡å‹: ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b1e168ac492f4c478d978d06548a1018.png#pic_center)


#### 1.3 ChatGPT

â€ƒâ€ƒInstructGPTæ˜¯åœ¨GPT-3çš„åŸºç¡€ä¸Šé€šè¿‡SFT+RLHFä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå®Œæˆï¼›ChatGPTåˆ™æ˜¯åœ¨GPT-3.5çš„åŸºç¡€ä¸Šé€šè¿‡SFT+RLHFä¸¤ä¸ªé˜¶æ®µè®­ç»ƒå®Œæˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ã€‚SFå’ŒRLHFä¸¤ä¸ªé˜¶æ®µï¼Œåœ¨InstructGPTç« èŠ‚ä¸­æˆ‘ä»¬å·²ç»åšäº†è¯¦ç»†ä»‹ç»ï¼Œè¿™é‡Œä¸åšè¿‡å¤šèµ˜è¿°ã€‚å…³äºGPT-3å’ŒGPT-3.5ï¼Œå®ƒä»¬å…¶å®æ˜¯ä¸¤ä¸ªæ¨¡å‹ç³»åˆ—ï¼Œåˆ†åˆ«ç§°ä¸ºGPT-3ç³»åˆ—å’ŒGPT-3.5ç³»åˆ—ï¼Œä¸‹é¢æˆ‘ä»¬å‚è€ƒç»¼è¿°[æ‹†è§£è¿½æº¯ GPT-3.5 å„é¡¹èƒ½åŠ›çš„èµ·æº](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)ï¼Œç®€å•å±•ç¤ºä¸‹OpenAIå›¢é˜Ÿæ‰€æ„å»ºçš„GPT-3ç³»åˆ—å’ŒGPT-3.5ç³»åˆ—æ˜¯å¦‚ä½•è¿›åŒ–çš„ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/44f3c3bb1edb409682f8b2103ebd4ec6.jpeg#pic_center)

#### 1.4 GPT-4

2022å¹´3æœˆï¼ŒOpenAIå›¢é˜Ÿåˆæ”¾å¤§æ‹›ï¼Œå‘å¸ƒäº†æ›´å¼ºçš„LLM: GPT-4ã€‚è™½ç„¶æ— ä»å¾—çŸ¥GPT-4çš„è®­ç»ƒç»†èŠ‚ï¼Œä½†æ˜¯å¯ä»¥è‚¯å®šçš„æ˜¯ï¼ŒGPT-4é‡‡ç”¨äº†æ›´å¤§çš„æ¨¡å‹ç»“æ„ï¼Œå¢åŠ äº†æ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å®˜æ–¹åšå®¢[GPT-4](https://openai.com/research/gpt-4)äº†è§£ä¸‹GPT-4çš„å¼ºå¤§èƒ½åŠ›ã€‚ç›®å‰GPT-4çš„ä¸»è¦èƒ½åŠ›ç‚¹å¦‚ä¸‹:

- GPT-4æ˜¯å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå¯æ”¯æŒå›¾ç‰‡æˆ–æ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºæ˜¯æ–‡æœ¬ï¼›
- GPT-4çš„è¾“å…¥å¯æ¥å—8192ä¸ªtokenã€‚å¦å­˜åœ¨å˜ä½“æ¨¡å‹ï¼Œå¯æ¥å—è¾“å…¥32768ä¸ªtokenï¼›
- GPT-4ç›¸è¾ƒäºChatGPTï¼Œå…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨ï¼Œé™¤äº†èŠå¤©æœºå™¨äººä¹‹å¤–ï¼Œè¿˜åŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰å¤šä¸ªé¢†åŸŸã€‚è€ŒChatGPTä¸»è¦é’ˆå¯¹èŠå¤©æœºå™¨äººé¢†åŸŸï¼Œä¸ºç”¨æˆ·æä¾›æ—¥å¸¸å¯¹è¯ã€é—®é¢˜è§£ç­”ã€ä¿¡æ¯æŸ¥è¯¢ç­‰æœåŠ¡ã€‚

### 2ã€å…¶ä»–å¤§æ¨¡å‹

â€ƒâ€ƒæ¯«ä¸å¤¸å¼ çš„è¯´ï¼Œå°½ç®¡éœ€è¦è€—è´¹å·¨å¤§çš„èµ„æºï¼Œä½†æ˜¯ç›®å‰å›½å†…å¤–å„å¤§å…¬å¸éƒ½åœ¨æˆ–å¤šæˆ–å°‘çš„å‚ä¸ç€LLMçš„å†›å¤‡ç«èµ›ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿ƒè¿›ç€NLPæŠ€æœ¯çš„å‘å±•ã€‚å½’å› äºæ­¤ï¼Œç›®å‰å·²ç»æœ‰ä¸€ç³»åˆ—LLMé™†é™†ç»­ç»­é—®ä¸–äº†ã€‚æˆ‘ä»¬æ— æ³•å¯¹è¿™äº›LLMè¿›è¡Œä¸€ä¸€ä»‹ç»ï¼Œè¿™é‡ŒæŒ‘é€‰ä¸€äº›æˆ‘ä»¬åœ¨å…¶åŸºç¡€ä¸Šåšè¿‡å¾®è°ƒæˆ–æœ‰ä½¿ç”¨ç»éªŒçš„æ¨¡å‹ï¼Œå¯¹å®ƒä»¬è¿›è¡Œç®€å•ä»‹ç»ã€‚

å¤§æ¨¡å‹

å›¢é˜Ÿ

å‘å¸ƒæ—¶é—´

æ¨¡å‹è§„æ¨¡

æ˜¯å¦å¼€æº

Hugging Face

Github

ChatGLM-6B

æ¸…åå¤§å­¦

2023

6B

å·²å¼€æºï¼Œä¸å¯å•†ç”¨ï¼ˆè·å–è®¸å¯è¯ï¼‰

[ChatGLM-6B](https://www.huggingface.co/THUDM/chatglm-6b)

[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)

ChatGLM2-6B

æ¸…åå¤§å­¦

2023

6B

å·²å¼€æºï¼Œä¸å¯å•†ç”¨ï¼ˆè·å–è®¸å¯è¯ï¼‰

[ChatGLM2-6B](https://www.huggingface.co/THUDM/chatglm2-6b)

[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)

LLaMA2-7B

Meta

2023

7B

å·²å¼€æºï¼Œå¯å•†ç”¨

[LLaMA2-7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)

[LLaMA](https://github.com/facebookresearch/llama)ã€[Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)

baichuan-7B

ç™¾å·æ™ºèƒ½

2023

7B

å·²å¼€æºï¼Œå¯å•†ç”¨

[baichuan-7B](https://huggingface.co/baichuan-inc/baichuan-7B)

[baichuan-7B](https://github.com/baichuan-inc/baichuan-7B)

æ–‡å¿ƒä¸€è¨€

ç™¾åº¦

2023

åƒäº¿

æœªå¼€æºï¼Œä¸å¯å•†ç”¨

æš‚æ— 

æš‚æ— 

#### 2.1 ChatGLM

â€ƒâ€ƒChatGLMæ˜¯ä¸€ä¸ªåŸºäºåƒäº¿åŸºåº§æ¨¡å‹GLM-130Bå¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰é—®ç­”ã€å¤šè½®å¯¹è¯å’Œä»£ç ç”ŸæˆåŠŸèƒ½ã€‚ç›®å‰ï¼ŒChatGLMæœ‰ä¸¤ä¸ªç‰ˆæœ¬: åƒäº¿å‚æ•°çš„ChatGLM-130Bï¼ˆå†…æµ‹ç‰ˆï¼‰å’Œ62äº¿å‚æ•°çš„ChatGLM-6Bï¼ˆå¼€æºç‰ˆï¼Œå®˜æ–¹Githubæ˜¯[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ï¼‰ã€‚ChatGLM-6Bæ˜¯åœ¨2023å¹´3æœˆ14æ—¥æ­£å¼å¼€æºçš„ï¼Œç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€6GBæ˜¾å­˜ï¼‰ã€‚ChatGLMçš„æŠ€æœ¯åŸºç¡€æ˜¯GLM-130Bï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç›®æ ‡å‡½æ•°çš„è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–åƒäº¿è§„æ¨¡çš„æ¨¡å‹ã€‚
â€ƒâ€ƒChatGLMçš„æ€§èƒ½è¡¨ç°ä¹Ÿååˆ†å‡ºè‰²ã€‚ç»è¿‡çº¦1Tæ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åé¦ˆè‡ªåŠ©ï¼ˆRWï¼‰ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯ï¼Œ62 äº¿å‚æ•°çš„ChatGLM-6Bå·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚è€Œåƒäº¿å‚æ•°çš„ChatGLMåˆ™æ›´è¿›ä¸€æ­¥ï¼Œåœ¨é—®ç­”å’Œå¯¹è¯æ–¹é¢å…·æœ‰æ›´å¼ºå¤§çš„èƒ½åŠ›ã€‚
â€ƒâ€ƒ2023å¹´6æœˆ25æ—¥ï¼Œæ¸…åå¤§å­¦å‘å¸ƒäº†ChatGLM2-6Bï¼ŒChatGLM-6Bçš„å‡çº§ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§:

- **æ›´å¼ºå¤§çš„æ€§èƒ½**: åŸºäºChatGLMåˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œå…¨é¢å‡çº§äº†ChatGLM2-6Bçš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6Bä½¿ç”¨äº†GLMçš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº†1.4Tä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6Båœ¨MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ï¼›
- **æ›´é•¿çš„ä¸Šä¸‹æ–‡**: åŸºäºFlashAttentionæŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”±ChatGLM-6Bçš„2Kæ‰©å±•åˆ°äº†32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨8Kçš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒï¼Œå…è®¸æ›´å¤šè½®æ¬¡çš„å¯¹è¯ã€‚ä½†å½“å‰ç‰ˆæœ¬çš„ ChatGLM2-6Bå¯¹å•è½®è¶…é•¿æ–‡æ¡£çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­è¿­ä»£å‡çº§ä¸­ç€é‡è¿›è¡Œä¼˜åŒ–ï¼›
- **æ›´é«˜æ•ˆçš„æ¨ç†**: åŸºäºMulti Query AttentionæŠ€æœ¯ï¼ŒChatGLM2-6Bæœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨: åœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº†42%ï¼ŒINT4é‡åŒ–ä¸‹ï¼Œ6Gæ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”±1Kæå‡åˆ°äº†8Kã€‚

â€ƒâ€ƒæœ‰å…³ChatGLM2-6Bæ›´å¤šçš„ç»†èŠ‚ï¼Œå¤§å®¶å¯å‚è€ƒå®˜æ–¹Githubï¼Œ[ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å…ˆä»‹ç»ä¸‹åŸå§‹GLMçš„æ¨¡å‹ç»“æ„åŠé¢„è®­ç»ƒåŸç†ï¼Œå†ä»‹ç»ä¸‹GhatGLMç³»åˆ—çš„åŸºåº§æ¨¡å‹: GLM-130Bï¼Œå¦‚ä½•åœ¨GLMåŸºç¡€ä¸Šè¿›è¡Œçš„ä¼˜åŒ–è°ƒæ•´ã€‚
â€ƒâ€ƒGLMï¼ˆGeneral Language Modelï¼‰æ˜¯æ¸…åå¤§å­¦åœ¨2022å¹´å‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡ä¸­[ã€ŠGLM: General Language Model Pretraining with Autoregressive Blank Infillingã€‹](https://aclanthology.org/2022.acl-long.26.pdf)æå‡ºçš„æ¨¡å‹ã€‚GLMæ¨¡å‹è¢«æå‡ºä¹‹å‰ï¼ŒNLPé¢†åŸŸä¸»æµçš„é¢„è®­ç»ƒæ¡†æ¶å¯ä»¥åˆ†ä¸ºä¸‰ç§:

- **autoregressiveè‡ªå›å½’æ¨¡å‹ï¼ˆARæ¨¡å‹ï¼‰**: ä»£è¡¨æ˜¯GPTï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä»å·¦åˆ°å³çš„è¯­è¨€æ¨¡å‹ï¼Œå¸¸ç”¨äºæ— æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼ˆunconditional generationï¼‰ï¼Œåœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œæ¯”å¦‚è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰é¢†åŸŸçš„ä»»åŠ¡ã€‚å½“æ‰©å±•åˆ°åäº¿çº§åˆ«å‚æ•°æ—¶ï¼Œè¡¨ç°å‡ºäº†å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç¼ºç‚¹æ˜¯å•å‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨NLUä»»åŠ¡ä¸­ï¼Œæ— æ³•å®Œå…¨æ•æ‰ä¸Šä¸‹æ–‡çš„ä¾èµ–å…³ç³»ï¼›
- **autoencodingè‡ªç¼–ç æ¨¡å‹ï¼ˆAEæ¨¡å‹ï¼‰**: ä»£è¡¨æ˜¯Bertï¼Œæ˜¯é€šè¿‡æŸä¸ªé™å™ªç›®æ ‡ï¼ˆå¦‚æ©ç è¯­è¨€æ¨¡å‹ï¼‰è®­ç»ƒçš„è¯­è¨€ç¼–ç å™¨ã€‚è‡ªç¼–ç æ¨¡å‹æ“…é•¿è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ï¼Œå¸¸è¢«ç”¨æ¥ç”Ÿæˆå¥å­çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä½†ä¸èƒ½ç›´æ¥åº”ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼›
- **encoder-decoderï¼ˆSeq2Seqæ¨¡å‹ï¼‰**: ä»£è¡¨ä½œT5ï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„Transformerç»“æ„ï¼ŒåŒ…å«ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ã€‚é‡‡ç”¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šå¸¸ç”¨äºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼ˆconditional generationï¼‰ï¼Œæ¯”å¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚2019ï¼‰ã€‚å®ƒä»¬é€šå¸¸è¢«éƒ¨ç½²åœ¨æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦å’Œå›åº”ç”Ÿæˆã€‚T5é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç»Ÿä¸€äº†NLUå’Œæœ‰æ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œä½†éœ€è¦æ›´å¤šçš„å‚æ•°æ¥åŒ¹é…åŸºäºBRETçš„æ¨¡å‹çš„æ€§èƒ½ã€‚

â€ƒâ€ƒä¸Šè¿°ä¸‰ç§é¢„è®­ç»ƒæ¶æ„çš„è®­ç»ƒç›®æ ‡ä¹Ÿç•¥æœ‰ä¸åŒ:

- GPTçš„è®­ç»ƒç›®æ ‡æ˜¯ä»å·¦åˆ°å³çš„æ–‡æœ¬ç”Ÿæˆï¼›
- Bertçš„è®­ç»ƒç›®æ ‡æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œéšæœºæ©ç ï¼Œç„¶åé¢„æµ‹è¢«æ©ç çš„è¯ï¼›
- T5åˆ™æ˜¯æ¥å—ä¸€æ®µæ–‡æœ¬ï¼Œä»å·¦åˆ°å³çš„ç”Ÿæˆå¦ä¸€æ®µæ–‡æœ¬ã€‚

â€ƒâ€ƒä¸‰ç§é¢„è®­ç»ƒæ¡†æ¶å„æœ‰åˆ©å¼Šï¼Œæ²¡æœ‰ä¸€ç§æ¡†æ¶åœ¨ä»¥ä¸‹ä¸‰ç§é¢†åŸŸçš„è¡¨ç°æœ€ä½³: è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ã€æ— æ¡ä»¶ç”Ÿæˆä»¥åŠæ¡ä»¶ç”Ÿæˆã€‚GLMåŸºäºä»¥ä¸ŠèƒŒæ™¯è¯ç”Ÿäº†ã€‚GLMæ¨¡å‹æ ¸å¿ƒæ˜¯Autoregressive Blank Infillingï¼Œç»“åˆäº†ä¸Šè¿°ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹çš„æ€æƒ³ã€‚

- **é¢„è®­ç»ƒç›®æ ‡**:
    - **Autoregressive Blank Infillingï¼ˆè‡ªå›å½’çš„ç©ºç™½å¡«å……ï¼‰**: GLMæ˜¯é€šè¿‡ä¼˜åŒ–è‡ªå›å½’ç©ºç™½å¡«å……ç›®æ ‡æ¥è®­ç»ƒçš„ã€‚ç»™å®šä¸€ä¸ªè¾“å…¥æ–‡æœ¬ x = \[ x 1 , â‹¯ â€‰ , x n \] x = \[x\_{1}, \\cdots, x\_{n}\] x\=\[x1â€‹,â‹¯,xnâ€‹\]ï¼Œå¤šä¸ªæ–‡æœ¬è·¨åº¦ï¼ˆæ–‡æœ¬ç‰‡æ®µï¼‰ { s 1 , â‹¯ â€‰ , s m } \\{s\_{1},\\cdots, s\_{m}\\} {s1â€‹,â‹¯,smâ€‹}è¢«é‡‡æ ·ï¼Œå…¶ä¸­æ¯ä¸ªè·¨åº¦ s i s\_{i} siâ€‹å¯¹åº”äº x x xä¸­ä¸€ç³»åˆ—è¿ç»­çš„token:  \[ s i , 1 , â‹¯ â€‰ , s i , l i \] \[s\_{i,1}, \\cdots, s\_{i,l\_{i}}\] \[si,1â€‹,â‹¯,si,liâ€‹â€‹\]ï¼Œå…¶ä¸­ l i l\_{i} liâ€‹ä»£è¡¨è·¨åº¦ s i s\_{i} siâ€‹çš„é•¿åº¦ã€‚ x x xä¸­çš„æ¯ä¸€ä¸ªè·¨åº¦éƒ½ä¼šè¢«ä¸€ä¸ª`[Mask]`æ›¿æ¢æ‰ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªè¢«ç ´åçš„ x c o r r u p t x\_{corrupt} xcorruptâ€‹ã€‚GLMæ¨¡å‹ä»¥è‡ªå›å½’çš„æ–¹å¼é¢„æµ‹è¢«ç ´åçš„æ–‡æœ¬ä¸­ç¼ºå°‘çš„tokenï¼Œè¿™æ„å‘³ç€å½“é¢„æµ‹ä¸€ä¸ªè·¨åº¦ä¸­ç¼ºå°‘çš„tokenæ—¶ï¼ŒGLMæ—¢å¯ä»¥è®¿é—®è¢«ç ´åçš„æ–‡æœ¬ x c o r r u p t x\_{corrupt} xcorruptâ€‹ï¼Œåˆå¯ä»¥è®¿é—®è·¨åº¦ä¸­ä¹‹å‰å·²ç»è¢«é¢„æµ‹çš„tokenã€‚ä¸ºäº†å……åˆ†æ•æ‰ä¸åŒè·¨åº¦ä¹‹é—´çš„ç›¸äº’ä¾å­˜å…³ç³»ï¼ŒGLMéšæœºæ’åˆ—è·¨åº¦çš„é¡ºåºã€‚å½¢å¼ä¸Šï¼Œè®© Z m Z\_{m} Zmâ€‹è¡¨ç¤ºé•¿åº¦ä¸º m m mçš„ç´¢å¼•åºåˆ— \[ 1 , 2 , â‹¯ â€‰ , m \] \[1, 2, \\cdots, m\] \[1,2,â‹¯,m\]æ‰€æœ‰å¯èƒ½æ’åˆ—çš„é›†åˆï¼Œ s z < i s\_{z\_{<i}} sz<iâ€‹â€‹ä»£è¡¨ \[ s z 1 , â‹¯ â€‰ , s z i âˆ’ 1 \] \[s\_{z\_{1}}, \\cdots, s\_{z\_{i-1}}\] \[sz1â€‹â€‹,â‹¯,sziâˆ’1â€‹â€‹\]ï¼Œæ­¤æ—¶ï¼Œå¯å®šä¹‰é¢„è®­ç»ƒç›®æ ‡ä¸º:
        max â¡ Î¸ E z âˆ¼ Z m \[ âˆ‘ i = 1 m l o g Â  p Î¸ ( s z i âˆ£ x c o r r u p t , s z < i ) \] \\max\_{\\theta}E\_{z\\sim Z\_{m}}\[\\sum\_{i=1}^{m}log\\ p\_{\\theta}(s\_{z\_{i}}|x\_{corrupt}, s\_{z\_{<i}})\] Î¸maxâ€‹Ezâˆ¼Zmâ€‹â€‹\[i\=1âˆ‘mâ€‹logÂ pÎ¸â€‹(sziâ€‹â€‹âˆ£xcorruptâ€‹,sz<iâ€‹â€‹)\]
        å…¶ä¸­ï¼Œ z z zä»£è¡¨ Z m Z\_{m} Zmâ€‹ä¸­ä»»æ„ä¸€ä¸ªæ’åˆ—ï¼Œä¹Ÿå°±æ˜¯ç´¢å¼•é›†åˆï¼› { z 1 , â‹¯ â€‰ , z m } \\{z\_{1},\\cdots,z\_{m}\\} {z1â€‹,â‹¯,zmâ€‹}ä»£è¡¨ z z zä¸­çš„ç´¢å¼•å…ƒç´ ï¼› s z i s\_{z\_{i}} sziâ€‹â€‹ä»£è¡¨ { s 1 , â‹¯ â€‰ , s m } \\{s\_{1},\\cdots, s\_{m}\\} {s1â€‹,â‹¯,smâ€‹}ä¸­ç¬¬ z i z\_{i} ziâ€‹ä¸ªè·¨åº¦ã€‚ä¸Šè¿°å…¬å¼çš„å«ä¹‰å°±æ˜¯: ç”¨è¢«ç ´åçš„ x c o r r u p t x\_{corrupt} xcorruptâ€‹ï¼Œä¸ s z i s\_{z\_{i}} sziâ€‹â€‹ä¹‹å‰çš„è·¨åº¦ \[ s z 1 , â‹¯ â€‰ , s z i âˆ’ 1 \] \[s\_{z\_{1}}, \\cdots, s\_{z\_{i-1}}\] \[sz1â€‹â€‹,â‹¯,sziâˆ’1â€‹â€‹\]è¿›è¡Œæ‹¼æ¥ï¼Œé¢„æµ‹ç”Ÿæˆçš„æ–‡æœ¬æ˜¯è·¨åº¦ s z i s\_{z\_{i}} sziâ€‹â€‹çš„æ¦‚ç‡è¶Šå¤§è¶Šå¥½ï¼Œè¿™ä¹Ÿæ˜¯å…¸å‹çš„è¯­è¨€æ¨¡å‹ç›®æ ‡å‡½æ•°ã€‚
        â€ƒâ€ƒå¦å¤–è®ºæ–‡ä¸­æåˆ°ï¼Œç”Ÿæˆä»»åŠ¡éƒ½æ˜¯æŒ‰ç…§ä»å·¦åˆ°å³çš„é¡ºåºç”Ÿæˆæ¯ä¸ªç©ºç™½å¤„çš„æ ‡è®°ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç”Ÿæˆè·¨åº¦ s i s\_{i} siâ€‹çš„æ¦‚ç‡è¢«åˆ†è§£ä¸º:
        p Î¸ ( s z i âˆ£ x c o r r u p t , s z < i ) = âˆ j = 1 l i p ( s i , j âˆ£ x c o r r u p t , s z < i , s i < j ) p\_{\\theta}(s\_{z\_{i}}|x\_{corrupt}, s\_{z\_{<i}})=\\prod\_{j=1}^{l\_{i}}p(s\_{i,j}|x\_{corrupt}, s\_{z\_{<i}},s\_{i<j}) pÎ¸â€‹(sziâ€‹â€‹âˆ£xcorruptâ€‹,sz<iâ€‹â€‹)\=j\=1âˆliâ€‹â€‹p(si,jâ€‹âˆ£xcorruptâ€‹,sz<iâ€‹â€‹,si<jâ€‹)
        â€ƒâ€ƒåœ¨æ„å»ºå¥½ä¼˜åŒ–ç›®æ ‡åï¼Œè®ºæ–‡ä¸­é€šè¿‡ä»¥ä¸‹æŠ€æœ¯å®ç°è¯¥ç›®æ ‡ï¼Œå³ä¸Šè¿°çš„è‡ªå›å½’ç©ºç™½å¡«è¡¥ç›®æ ‡ã€‚è¾“å…¥çš„ x x xè¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚Part Aæ˜¯è¢«ç ´åçš„æ–‡æœ¬ x c o r r u p t x\_{corrupt} xcorruptâ€‹ï¼ŒPart Bç”±è¢«maskçš„è·¨åº¦ç»„æˆã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾åŸå§‹çš„æ–‡æœ¬åºåˆ—ä¸º x = \[ x 1 , x 2 , x 3 , x 4 , x 5 , x 6 \] x = \[x\_{1}, x\_{2}, x\_{3}, x\_{4}, x\_{5}, x\_{6}\] x\=\[x1â€‹,x2â€‹,x3â€‹,x4â€‹,x5â€‹,x6â€‹\]ï¼Œé‡‡æ ·çš„ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µä¸º \[ x 3 \] \[x\_{3}\] \[x3â€‹\]å’Œ \[ x 5 , x 6 \] \[x\_{5}, x\_{6}\] \[x5â€‹,x6â€‹\]ï¼Œé‚£ä¹ˆæ©ç åçš„æ–‡æœ¬åºåˆ— x c o r r u p t x\_{corrupt} xcorruptâ€‹ä¸º \[ x 1 , x 2 , \[ M \] , x 4 , \[ M \] \] \[x\_{1}, x\_{2}, \[M\], x\_{4}, \[M\]\] \[x1â€‹,x2â€‹,\[M\],x4â€‹,\[M\]\]ï¼Œä¹Ÿå°±æ˜¯Part Aã€‚æ–‡æœ¬ç‰‡æ®µ \[ x 3 \] \[x\_{3}\] \[x3â€‹\]å’Œ \[ x 5 , x 6 \] \[x\_{5}, x\_{6}\] \[x5â€‹,x6â€‹\]ç”¨äºç»„æˆPart Bï¼ŒåŒæ—¶éœ€è¦å¯¹Part Bçš„ç‰‡æ®µè¿›è¡Œshuffleï¼Œä¹Ÿå°±æ˜¯æ‰“ä¹±æ–‡æœ¬ç‰‡æ®µçš„é¡ºåºï¼ˆæ³¨æ„ä¸æ˜¯æ–‡æœ¬ç‰‡æ®µçš„å†…éƒ¨é¡ºåºï¼Œè€Œæ˜¯æ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„é¡ºåºï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªç‰‡æ®µä½¿ç”¨ \[ S \] \[S\] \[S\]å¡«å……åœ¨å¼€å¤´ä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨ \[ E \] \[E\] \[E\]å¡«å……åœ¨æœ«å°¾ä½œä¸ºè¾“å‡ºã€‚æœ€åï¼Œä»å¼€å§‹æ ‡è®° \[ S \] \[S\] \[S\]å¼€å§‹ä¾æ¬¡è§£ç å‡ºè¢«æ©ç çš„æ–‡æœ¬ç‰‡æ®µï¼Œç›´è‡³ç»“æŸæ ‡è®° \[ E \] \[E\] \[E\]ã€‚
        â€ƒâ€ƒä»¥ä¸Šæ˜¯å®ç°è‡ªå›å½’ç©ºç™½å¡«è¡¥ç›®æ ‡çš„å¤§ä½“æµç¨‹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰æœ‰ä¸¤ç‚¹éœ€è¦æ³¨æ„ï¼Œä¸€ä¸ªæ˜¯self-attention maskçš„è®¾è®¡ï¼Œä¸€ä¸ªæ˜¯`[Mask]`æ–‡æœ¬ç‰‡æ®µçš„é‡‡æ ·è®¾è®¡ã€‚

        - **self-attention mask**:
            - Part Aä¸­çš„è¯å½¼æ­¤å¯è§ï¼Œä½†ä¸å¯è§Part Bä¸­çš„è¯ï¼ˆä¸‹å›¾(d)ä¸­è“è‰²æ¡†ä¸­çš„åŒºåŸŸï¼‰ï¼›
            - Part Bä¸­çš„è¯å•å‘å¯è§ï¼ˆä¸‹å›¾(d)é»„è‰²å’Œç»¿è‰²çš„åŒºåŸŸã€‚é»„è‰²å’Œç»¿è‰²åˆ†åˆ«å¯¹åº” \[ x 3 \] \[x\_{3}\] \[x3â€‹\]å’Œ \[ x 5 , x 6 \] \[x\_{5}, x\_{6}\] \[x5â€‹,x6â€‹\]ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œä¸‹åŒï¼‰ï¼›
            - Part Bå¯è§Part Aï¼ˆä¸‹å›¾(d)ä¸­é»„è‰²å’Œç»¿è‰²çš„åŒºåŸŸï¼‰ï¼›
            - å…¶ä½™ä¸å¯è§ï¼ˆä¸‹å›¾(d)ä¸­ç°è‰²çš„åŒºåŸŸï¼‰
        - **`[Mask]`æ–‡æœ¬ç‰‡æ®µé‡‡æ ·**: è®ºæ–‡ä¸­éšæœºå¯¹è·¨åº¦çš„é•¿åº¦é‡‡æ ·ï¼Œé‡‡æ ·åˆ†å¸ƒå±äºæ³Šæ¾åˆ†å¸ƒï¼Œå…¶ä¸­ Î» = 3 \\lambda=3 Î»\=3ï¼Œç›´åˆ°è‡³å°‘15%çš„åŸå§‹tokenè¢«maskã€‚æ ¹æ®ç»éªŒï¼Œè®ºæ–‡ä¸­å‘ç°ï¼Œ15%çš„æ¯”ä¾‹å¯¹äºä¸‹æ¸¸NLUä»»åŠ¡çš„è‰¯å¥½è¡¨ç°è‡³å…³é‡è¦ã€‚
            Â 

        â€ƒâ€ƒæœ€ç»ˆé€šè¿‡ä»¥ä¸Šæ–¹å¼ï¼ŒGLMè‡ªåŠ¨å­¦ä¹ ä¸€ä¸ªåŒå‘ç¼–ç å™¨ï¼ˆPart Aï¼‰å’Œä¸€ä¸ªå•å‘è§£ç å™¨ï¼ˆPart Bï¼‰ç»Ÿä¸€çš„æ¨¡å‹ã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/71aa1a29c9804f36b06c7b4a82a96235.png#pic_center)

    - **Multi-Task Pretrainingï¼ˆå¤šä»»åŠ¡é¢„è®­ç»ƒï¼‰**: ä¸Šè¿°ä¾‹å­ä¸­ï¼ŒGLMæ©ç›–äº†çŸ­è·¨åº¦ï¼Œé€‚ç”¨äºNLUä»»åŠ¡ã€‚è€Œè®ºæ–‡çš„å…³æ³¨ç‚¹æ˜¯é¢„è®­ç»ƒä¸€ä¸ªèƒ½åŒæ—¶å¤„ç†NLUå’Œæ–‡æœ¬ç”Ÿæˆçš„æ¨¡å‹ã€‚å› æ­¤ï¼Œè®ºæ–‡ç ”ç©¶äº†ä¸€ä¸ªå¤šä»»åŠ¡é¢„è®­ç»ƒçš„è®¾ç½®ã€‚åœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œå¢åŠ ä¸€ä¸ªç”Ÿæˆè¾ƒé•¿æ–‡æœ¬çš„ç›®æ ‡ï¼Œä¸ç©ºç™½å¡«å……ç›®æ ‡å…±åŒä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œè®ºæ–‡ä¸­è€ƒè™‘ä»¥ä¸‹ä¸¤ä¸ªç›®æ ‡ã€‚

        - **æ–‡æ¡£çº§**: ä»æ–‡æ¡£ä¸­é‡‡æ ·ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µè¿›è¡Œmaskï¼Œä¸”ç‰‡æ®µé•¿åº¦ä¸ºæ–‡æ¡£é•¿åº¦çš„50%ï½100%ã€‚è¯¥ç›®æ ‡æ—¨åœ¨ç”Ÿæˆé•¿æ–‡æœ¬ï¼›
        - **å¥å­çº§**: é™åˆ¶è¢«maskçš„æ–‡æœ¬ç‰‡æ®µå¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ã€‚å¤šä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆå¥å­ï¼‰è¢«å–æ ·ï¼Œä»¥è¦†ç›–15%çš„åŸå§‹tokenã€‚è¿™ä¸€ç›®æ ‡æ˜¯é’ˆå¯¹seq2seqä»»åŠ¡ï¼Œå…¶é¢„æµ‹å¾€å¾€æ˜¯å®Œæ•´çš„å¥å­æˆ–æ®µè½ã€‚
            Â 

        â€ƒâ€ƒè¿™ä¸¤ä¸ªæ–°ç›®æ ‡çš„å®šä¹‰ä¸åŸç›®æ ‡ç›¸åŒã€‚å”¯ä¸€ä¸åŒçš„æ˜¯çš„è·¨åº¦æ•°é‡å’Œè·¨åº¦é•¿åº¦ã€‚

- **æ¨¡å‹ç»“æ„**: GLMä½¿ç”¨Transformeræ¶æ„ï¼Œå¹¶å¯¹æ¶æ„è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ã€‚å…¶ä¸­ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ï¼Œæ˜¯äºŒç»´ä½ç½®ç¼–ç ã€‚
    - **Layer Normalization**: é‡æ–°è°ƒæ•´äº†LayerNormå’Œæ®‹å·®è¿æ¥çš„é¡ºåºï¼ˆå…ˆè¿›è¡ŒLayerNormï¼Œå†è¿›è¡Œæ®‹å·®è¿æ¥ï¼Œç±»ä¼¼äºPre-LNï¼Œä¸è¿‡GLM-130Bè®­ç»ƒæ—¶åˆè°ƒæ•´ä¸ºDeepNormäº†ï¼‰ï¼›
    - **è¾“å‡ºå±‚**: ä½¿ç”¨å•ä¸ªçº¿æ€§å±‚è¿›è¡Œè¾“å‡ºtokené¢„æµ‹ï¼›
    - **æ¿€æ´»å‡½æ•°**: ä½¿ç”¨GeLUæ›¿æ¢ReLUæ¿€æ´»å‡½æ•°ï¼›
    - **äºŒç»´ä½ç½®ç¼–ç **: è‡ªå›å½’ç©ºç™½å¡«å……ä»»åŠ¡çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å¦‚ä½•å¯¹ä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚Transformerä¾é ä½ç½®ç¼–ç æ¥æ³¨å…¥tokençš„ç»å¯¹å’Œç›¸å¯¹ä½ç½®ã€‚è®ºæ–‡ä¸­æå‡ºäº†äºŒç»´ä½ç½®ç¼–ç æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å¦‚ä¸Šé¢å›¾ç‰‡æ‰€ç¤ºï¼Œå…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªtokenéƒ½æœ‰ä¸¤ä¸ªä½ç½®æ ‡è¯†ç¼–ç ã€‚ç¬¬ä¸€ä¸ªä½ç½®æ ‡è¯†ä»£è¡¨åœ¨è¢«ç ´åæ–‡æœ¬ x c o r r u p t x\_{corrupt} xcorruptâ€‹ä¸­çš„ä½ç½®ã€‚å¯¹äºè¢«maskçš„è·¨åº¦ï¼Œå®ƒæ˜¯ç›¸åº”çš„`[Mask]`çš„ä½ç½®ã€‚ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†ä»£è¡¨è·¨åº¦å†…çš„ä½ç½®ã€‚å¯¹äºAéƒ¨åˆ†çš„tokenï¼Œå®ƒä»¬çš„ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†æ˜¯0ï¼›å¯¹äºBéƒ¨åˆ†çš„tokenï¼Œå®ƒä»¬çš„ç¬¬äºŒä¸ªä½ç½®æ ‡è¯†æ˜¯ä»1åˆ°è·¨åº¦çš„é•¿åº¦ã€‚è¿™ä¸¤ä¸ªä½ç½®æ ‡è¯†é€šè¿‡å¯å­¦ä¹ çš„åµŒå…¥è¡¨æ˜ å°„ä¸ºä¸¤ä¸ªå‘é‡ï¼Œè¿™ä¸¤ä¸ªå‘é‡éƒ½è¢«æ·»åŠ åˆ°è¾“å…¥tokençš„embeddingè¡¨è¾¾ä¸­ã€‚

â€ƒâ€ƒè‡³æ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸå§‹GLMçš„é¢„è®­ç»ƒåŸç†åŠæ¨¡å‹ç»“æ„ã€‚æ›´å¤šç»†èŠ‚å¤§å®¶å¯å‚è€ƒ[GLM: è‡ªå›å½’ç©ºç™½å¡«å……çš„é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ](https://zhuanlan.zhihu.com/p/560559133)ã€[ChatGLMå®˜æ–¹åšå®¢](https://chatglm.cn/blog)ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç®€å•ä»‹ç»ä¸‹ChatGLMçš„åŸºåº§æ¨¡å‹: GLM-130Bã€‚

- **ç›¸å¯¹åŸå§‹GLMçš„ä¼˜åŒ–è°ƒæ•´**:
    - **Layer Normalization**: ä½¿ç”¨DeepNormï¼ˆPost-LNçš„å‡çº§ç‰ˆï¼‰æ¥æä¾›æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ä¸‹é¢æ˜¯ä¸‰ç§LNæ¨¡å¼ï¼Œå…¶ä¸­ f f fä»£è¡¨FFNæˆ–Attentionå±‚ã€‚
        - **Post-LN**: åŸå§‹Bertã€GPT-1é‡‡ç”¨çš„Layer Normalizationå½¢å¼ï¼Œè®­ç»ƒä¸ç¨³å®šï¼Œä½†æ•ˆæœè¾ƒå¥½ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹:
            x = L a y e r N o r m ( x + f ( x ) ) x=LayerNorm(x+f(x)) x\=LayerNorm(x+f(x))
        - **Pre-LN**: GPT-2ã€GPT-3ä»¥åŠLLaMAé‡‡ç”¨çš„Layer Normalizationå½¢å¼éƒ½è¿‘ä¼¼äºPre-LNï¼Œæ•ˆæœä¸å¦‚Post-LNï¼Œä½†ç¨³å®šæ€§è¾ƒå¥½ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹:
            x = x + L a y e r N o r m ( f ( x ) ) x=x+LayerNorm(f(x)) x\=x+LayerNorm(f(x))
        - **DeepNorm**: é›†æˆPost-LNå’ŒPre-LNçš„ä¼˜ç‚¹ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹:
            x = L a y e r N o r m ( Î± x + f ( x ) ) , ( Î± > 1 ) x=LayerNorm(\\alpha x+f(x)), (\\alpha>1) x\=LayerNorm(Î±x+f(x)),(Î±\>1)
    - **Position Embedding**: ä½¿ç”¨RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æ›¿æ¢2D Position Embeddingï¼›
    - **Feed Forward Networkï¼ˆFFNï¼‰**: ä½¿ç”¨GeGLUæ›¿æ¢GeLU ã€‚
- **GLM-130Bé¢„è®­ç»ƒé…ç½®**:
    - **è‡ªç›‘ç£ç©ºç™½å¡«å……ï¼ˆ95% tokensï¼‰**: é€šè¿‡ä¸åŒçš„Maskç­–ç•¥ï¼Œæ¥ä½¿æ¨¡å‹è·å¾—è‡ªç¼–ç å’Œè‡ªå›å½’çš„èƒ½åŠ›ï¼Œå…·ä½“æ¥è¯´:
        - **è¯Mask**: 30%çš„è®­ç»ƒtokenè¿›è¡Œè¯çº§åˆ«çš„Maskï¼ŒMaskæ–¹å¼å‚è€ƒå‰æ–‡çš„è·¨åº¦é‡‡æ ·æ–¹æ³•: è·¨åº¦é•¿åº¦éµå¾ªæ³Šæ¾åˆ†å¸ƒï¼ˆÎ»=3ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬çš„è·¨åº¦é•¿åº¦åŠ èµ·æ¥æœ€å¤šä¸ºè¯¥æ ·æœ¬é•¿åº¦çš„15%ï¼›
        - **å¥å­åŠæ–‡æ¡£Mask**: å‰©ä¸‹70%çš„tokenè¿›è¡Œå¥å­æˆ–æ–‡æ¡£çº§åˆ«çš„Maskã€‚
    - **å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆMIPï¼Œ5%tokensï¼‰**: T5å’ŒExT5ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒä¸­çš„å¤šä»»åŠ¡å­¦ä¹ æ¯”å¾®è°ƒæ›´æœ‰å¸®åŠ©ã€‚å› æ­¤ï¼ŒGLM-130Båœ¨é¢„è®­ç»ƒä¸­åŒ…å«å„é¡¹æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¯­æ„ç†è§£ã€ç”Ÿæˆå’Œä¿¡æ¯æŠ½å–ã€‚ä¸ºäº†ä¿è¯æ¨¡å‹çš„å…¶ä»–ç”Ÿæˆèƒ½åŠ›ä¸å—å½±å“ï¼Œç”¨äºMIPè®­ç»ƒçš„æ•°æ®é›†åªå äº†5%ã€‚

#### 2.2 LLaMA

â€ƒâ€ƒ2023å¹´7æœˆï¼ŒMetaæ¨å‡ºäº†å®Œå…¨å¯å•†ç”¨çš„å¼€æºå¤§æ¨¡å‹LLaMA2ã€‚è¿™é‡Œç®€å•ä»‹ç»ä¸‹ä¸¤ä»£LLaMAçš„å…±æœ‰ç»“æ„ä»¥åŠLLaMA2ç›¸è¾ƒäºåˆä»£LLaMAçš„ä¼˜åŒ–ç‚¹ã€‚

- **é€šç”¨ç»“æ„**: LLaMAçš„å…·ä½“ç»“æ„è§ä¸‹å›¾ã€‚
    - **Layer Normalization**: ä½¿ç”¨å‰ç½®çš„RMSNormï¼›åœ¨BERTã€GPTç­‰æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„LayerNormæ˜¯å¦‚ä¸‹å½¢å¼:
        y = W âˆ— x âˆ’ M e a n ( x ) V a r ( x ) + Ïµ + b y=W\*\\frac{x-Mean(x)}{\\sqrt{Var(x)+\\epsilon}}+b y\=Wâˆ—Var(x)+Ïµ â€‹xâˆ’Mean(x)â€‹+b
        RMSNorm(root mean square)å‘ç°LayerNormçš„ä¸­å¿ƒåç§»æ²¡ä»€ä¹ˆç”¨(å‡å»å‡å€¼ç­‰æ“ä½œ)ã€‚å°†å…¶å»æ‰ä¹‹åï¼Œæ•ˆæœå‡ ä¹ä¸å˜ï¼Œä½†æ˜¯é€Ÿåº¦æå‡äº†40%ã€‚RMSNormå…¬å¼ä¸º:
        y = W âˆ— x M e a n ( x 2 ) + Ïµ y=W\*\\frac{x}{\\sqrt{Mean(x^{2})+\\epsilon}} y\=Wâˆ—Mean(x2)+Ïµ â€‹xâ€‹
        æ³¨æ„é™¤äº†æ²¡æœ‰å‡å‡å€¼ï¼ŒåŠ åç½®ä»¥å¤–ï¼Œåˆ†æ¯ä¸Šæ±‚çš„æ˜¯RMSè€Œä¸æ˜¯æ–¹å·®ã€‚å¦å¤–LLaMAåœ¨Attention Layerå’ŒMLPçš„è¾“å…¥ä¸Šä½¿ç”¨äº†RMSNormï¼Œç›¸æ¯”åœ¨è¾“å‡ºä¸Šä½¿ç”¨ï¼Œè®­ç»ƒä¼šæ›´åŠ ç¨³å®šï¼Œç±»ä¼¼äºPre-LNæ–¹å¼ã€‚
    - **ä½ç½®ç¼–ç **: åœ¨Qã€Kä¸Šä½¿ç”¨RoPEæ—‹è½¬å¼ä½ç½®ç¼–ç ï¼›
    - **Causal Mask**: ä½¿ç”¨causal maskä¿è¯æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å‰é¢çš„tokensï¼›
    - **æ¿€æ´»å‡½æ•°**: ä½¿ç”¨SwiGLUæ›¿ä»£ReLUã€‚
        ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3351f0c56f4f48c29d9ca152aee90f9f.png#pic_center)
- **LLaMA2ä¼˜åŒ–ç‚¹**: å‚è€ƒ[LLaMA2 vs LLaMA](https://zhuanlan.zhihu.com/p/636784644)ã€[LLaMA2ä»‹ç»](https://zhuanlan.zhihu.com/p/647862867)ã€‚
    - **æ›´å¤šçš„è®­ç»ƒè¯­æ–™**: é¢„è®­ç»ƒè¯­æ–™ä»1ä¸‡äº¿å¢åŠ åˆ°2ä¸‡äº¿tokensï¼›
    - **æ›´é•¿çš„ä¸Šä¸‹æ–‡**: ä¸Šä¸‹æ–‡é•¿åº¦ä»2048å¢åŠ åˆ°4096ï¼›
    - **æ–°å¢SFTè¿‡ç¨‹**: æ”¶é›†äº†10ä¸‡äººç±»æ ‡æ³¨æ•°æ®è¿›è¡ŒSFTï¼›
    - **æ–°å¢RLHFè¿‡ç¨‹**: æ”¶é›†äº†100ä¸‡äººç±»åå¥½æ•°æ®è¿›è¡ŒRLHFï¼›
    - **è°ƒæ•´Attentionæœºåˆ¶**: å’ŒFalconä¸€æ ·ï¼Œä½¿ç”¨äº†Group Query Attentionï¼ŒèŠ‚çœæ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶æå‡è®¡ç®—é€Ÿåº¦ã€‚
        - **Multi Head Attentionï¼ˆMHAï¼‰**: åŸå§‹å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€æœ‰å¤´å„è‡ªä¿å­˜ç‹¬ç«‹çš„Qã€Kã€VçŸ©é˜µï¼›
        - **Multi Query Attentionï¼ˆMQAï¼‰**: æ‰€æœ‰çš„å¤´ä¹‹é—´å…±äº«åŒä¸€ä»½Kå’ŒVçŸ©é˜µï¼Œæ¯ä¸ªå¤´åªå•ç‹¬ä¿ç•™äº†ä¸€ä»½QçŸ©é˜µå‚æ•°ï¼Œä»è€Œå¤§å¤§å‡å°‘Kå’ŒVçŸ©é˜µçš„å‚æ•°é‡ï¼›
        - **Group Query Attentionï¼ˆGQAï¼‰**: æ²¡æœ‰åƒMQAä¸€æ ·æç«¯ï¼Œè€Œæ˜¯å°†Qåˆ†ç»„ï¼Œç»„å†…å…±äº«Kã€VçŸ©é˜µã€‚å½“group=1æ—¶ï¼ŒGQAç­‰ä»·äºMQAã€‚
            ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/206eaadf3abe41c4a911e0e46e6b9583.png#pic_center)

ä¸‰ã€è¡¥å……çŸ¥è¯†
------

### 1ã€LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoder onlyæ¶æ„ï¼Ÿ

â€ƒâ€ƒæ€»çš„æ¥è¯´ï¼ŒLLMä¹‹æ‰€ä»¥ä¸»è¦éƒ½ç”¨Decoder-onlyæ¶æ„ï¼Œé™¤äº†è®­ç»ƒæ•ˆç‡å’Œå·¥ç¨‹å®ç°ä¸Šçš„ä¼˜åŠ¿å¤–ï¼Œä¸€æ–¹é¢ï¼Œåœ¨ç†è®ºä¸Šæ˜¯å› ä¸ºEncoderçš„åŒå‘æ³¨æ„åŠ›ä¼šå­˜åœ¨ä½ç§©é—®é¢˜ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼›å¦ä¸€æ–¹é¢ï¼Œå°±ç”Ÿæˆä»»åŠ¡è€Œè¨€ï¼Œå¼•å…¥åŒå‘æ³¨æ„åŠ›å¹¶æ— å®è´¨å¥½å¤„ã€‚è€ŒEncoder-Decoderæ¶æ„ä¹‹æ‰€ä»¥èƒ½å¤Ÿåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ï¼Œå¤§æ¦‚åªæ˜¯å› ä¸ºå®ƒå¤šäº†ä¸€å€å‚æ•°ã€‚æ‰€ä»¥ï¼Œåœ¨åŒç­‰å‚æ•°é‡ã€åŒç­‰æ¨ç†æˆæœ¬ä¸‹ï¼ŒDecoder-onlyæ¶æ„å°±æ˜¯æœ€ä¼˜é€‰æ‹©äº†ã€‚å…·ä½“å‚è€ƒ[LLMä¸ºä»€ä¹ˆéƒ½ç”¨Decoder onlyæ¶æ„ï¼Ÿ](https://www.zhihu.com/question/588325646?utm_division=hot_list_page)ã€‚

- **è®­ç»ƒæ•ˆç‡**: Decoder-onlyæ¶æ„åªéœ€è¦è¿›è¡Œå•å‘çš„è‡ªå›å½’é¢„æµ‹ï¼Œè€ŒEncoder-Decoderæ¶æ„éœ€è¦è¿›è¡ŒåŒå‘çš„è‡ªç¼–ç é¢„æµ‹å’Œå•å‘çš„è‡ªå›å½’é¢„æµ‹ï¼Œè®¡ç®—é‡æ›´å¤§ï¼›
- **å·¥ç¨‹å®ç°**: Decoder-onlyæ¶æ„åªéœ€è¦ä¸€ä¸ªæ¨¡å—ï¼Œè€ŒEncoder-Decoderæ¶æ„éœ€è¦ä¸¤ä¸ªæ¨¡å—ï¼Œå¹¶ä¸”éœ€è¦å¤„ç†ä¸¤è€…ä¹‹é—´çš„ä¿¡æ¯ä¼ é€’å’Œå¯¹é½ï¼Œå®ç°èµ·æ¥æ›´å¤æ‚ï¼›
- **ç†è®ºåˆ†æ**: Encoderçš„åŒå‘æ³¨æ„åŠ›ä¼šå­˜åœ¨ä½ç§©é—®é¢˜ï¼Œå³æ³¨æ„åŠ›çŸ©é˜µçš„ç§©éšç€ç½‘ç»œæ·±åº¦çš„å¢åŠ è€Œé™ä½ï¼ˆæ¥è‡ªè®ºæ–‡: [Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)ï¼Œç»“è®ºæ˜¯å¦‚æœæ²¡æœ‰æ®‹å·®è¿æ¥å’ŒMLPå…œç€ï¼Œæ³¨æ„åŠ›çŸ©é˜µä¼šæœç§©ä¸º1çš„çŸ©é˜µæ”¶æ•›ï¼Œæœ€åæ¯ä¸ªtokençš„è¡¨ç¤ºéƒ½ä¸€æ ·äº†ï¼Œç½‘ç»œå°±åºŸäº†ï¼‰ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è€ŒDecoderçš„å•å‘æ³¨æ„åŠ›åˆ™ä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜ï¼›
- **ç”Ÿæˆä»»åŠ¡**: å¯¹äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒEncoderçš„åŒå‘æ³¨æ„åŠ›å¹¶æ— å®è´¨å¥½å¤„ï¼Œå› ä¸ºå®ƒä¼šå¼•å…¥å³ä¾§çš„ä¿¡æ¯ï¼Œç ´åäº†è‡ªå›å½’çš„å‡è®¾ã€‚è€ŒDecoderçš„å•å‘æ³¨æ„åŠ›åˆ™å¯ä»¥ä¿æŒè‡ªå›å½’çš„ä¸€è‡´æ€§ã€‚

### 2ã€NLPå°çŸ¥è¯†ç‚¹

- **Bert**ï¼›è‡ªç¼–ç æ¨¡å‹ï¼Œé€‚ç”¨äºNLUï¼ˆé¢„è®­ç»ƒä»»åŠ¡ä¸»è¦æŒ–æ˜å¥å­ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼‰ï¼Œæœ‰å…³Bertçš„ç†è®ºçŸ¥è¯†ï¼Œå¯å‚è€ƒ[â€”æ­¥æ­¥èµ°è¿›Bert](https://zhuanlan.zhihu.com/p/519432336)ï¼›
- **GPT**ï¼›è‡ªå›å½’æ¨¡å‹ï¼Œé€‚ç”¨äºNLGï¼ˆé¢„è®­ç»ƒä»»åŠ¡ä¸»è¦ç”¨äºç”Ÿæˆä¸‹æ–‡ï¼‰ï¼Œæœ‰å…³GPTç³»åˆ—çš„ç†è®ºçŸ¥è¯†ï¼Œå¯å‚è€ƒæœ¬æ–‡çš„GPTç³»åˆ—ç« èŠ‚ï¼›
- **OOV**: OOV é—®é¢˜æ˜¯NLPä¸­å¸¸è§çš„ä¸€ä¸ªé—®é¢˜ï¼Œå…¶å…¨ç§°æ˜¯Out Of Vocabularyï¼Œä¸‹é¢ç®€è¦çš„è¯´äº†ä¸€ä¸‹OOVã€‚
    - **å®šä¹‰**: åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸€ä¸ªå­—è¯åº“ï¼ˆvocabularyï¼‰ã€‚è¿™ä¸ªvocabularyæˆ–è€…æ˜¯æå‰åŠ è½½çš„ï¼Œæˆ–è€…æ˜¯è‡ªå·±å®šä¹‰çš„ï¼Œæˆ–è€…æ˜¯ä»å½“å‰æ•°æ®é›†æå–çš„ã€‚å‡è®¾é€šè¿‡ä¸Šè¿°æ–¹æ³•å·²ç»è·å–åˆ°ä¸€ä¸ªvocabularyï¼Œä½†åœ¨å¤„ç†å…¶ä»–æ•°æ®é›†æ—¶ï¼Œå‘ç°è¿™ä¸ªæ•°æ®é›†ä¸­æœ‰ä¸€äº›è¯å¹¶ä¸åœ¨ç°æœ‰çš„vocabularyä¸­ï¼Œè¿™æ—¶ç§°è¿™äº›è¯æ˜¯Out-Of-Vocabularyï¼Œå³OOVï¼›
    - **è§£å†³æ–¹æ³•**: Bertä¸­è§£å†³OOVé—®é¢˜ã€‚å¦‚æœä¸€ä¸ªå•è¯ä¸åœ¨è¯è¡¨ä¸­ï¼Œåˆ™æŒ‰ç…§subwordçš„æ–¹å¼é€ä¸ªæ‹†åˆ†tokenï¼Œå¦‚æœè¿é€ä¸ªtokenéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™ç›´æ¥åˆ†é…ä¸º\[unknown\]ã€‚
- **å¯¹æ¯”å­¦ä¹ **: è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)å¯ä»¥é¿å…å¯¹æ•°æ®é›†è¿›è¡Œå¤§é‡çš„æ ‡ç­¾æ ‡æ³¨ã€‚æŠŠè‡ªå·±å®šä¹‰çš„ä¼ªæ ‡ç­¾å½“ä½œè®­ç»ƒçš„ä¿¡å·ï¼Œç„¶åæŠŠå­¦ä¹ åˆ°çš„è¡¨ç¤º(representation)ç”¨ä½œä¸‹æ¸¸ä»»åŠ¡é‡Œã€‚æœ€è¿‘ï¼Œå¯¹æ¯”å­¦ä¹ è¢«å½“ä½œè‡ªç›‘ç£å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œè¢«å¹¿æ³›è¿ç”¨åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚å®ƒçš„ç›®æ ‡æ˜¯: å°†ä¸€ä¸ªæ ·æœ¬çš„ä¸åŒçš„ã€å¢å¼ºè¿‡çš„æ–°æ ·æœ¬ä»¬åœ¨åµŒå…¥ç©ºé—´ä¸­å°½å¯èƒ½åœ°è¿‘ï¼Œç„¶åè®©ä¸åŒçš„æ ·æœ¬ä¹‹é—´å°½å¯èƒ½åœ°è¿œã€‚SimCSE[ã€ŠSimCSE: Simple Contrastive Learning of Sentence Embeddingsã€‹](https://aclanthology.org/2021.emnlp-main.552.pdf)æ˜¯åŸºäºå¯¹æ¯”å­¦ä¹ çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œå³é‡‡ç”¨å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ï¼Œè·å–æ›´å¥½çš„æ–‡æœ¬è¡¨å¾ã€‚SimCSEç»†èŠ‚å¯å‚è€ƒ[è®ºæ–‡ç²¾è¯»-SimCSE](https://www.bilibili.com/video/BV12D4y1p788/?spm_id_from=333.337.search-card.all.click&vd_source=25d0b87065d3da39fe110c6e0b4906e1)ã€‚

### 3ã€åè¯è§£é‡Š

- **LLM**: Large Language Modelï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- **PLM**: Pretrain Language Modelï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
- **RL**: Reinforcement Learningï¼Œå¼ºåŒ–å­¦ä¹ ã€‚
- **SFT**: Supervised Fine-Tuningï¼Œæœ‰ç›‘ç£å¾®è°ƒã€‚
- **ICL**: In-Context Learningï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚
- **Fine-Tuning** : å¾®è°ƒã€‚
- **Prompt-Tuning**: æç¤ºå¾®è°ƒã€‚
- **Instruction-Tuning**: æŒ‡ç¤º/æŒ‡ä»¤å¾®è°ƒã€‚
- **NLU**: Natural Language Understandingï¼Œè‡ªç„¶è¯­è¨€ç†è§£ã€‚
- **NLG**: Natural Language Generationï¼Œè‡ªç„¶è¯­è¨€ç”Ÿæˆã€‚
- **CoT**: Chain-of-Thoughtï¼Œæ€ç»´é“¾ã€‚
- **OOV**: out of vocabularyï¼Œè¶…å‡ºè¯è¡¨å¤–çš„è¯ã€‚
- **shifted right**: æŒ‡çš„æ˜¯Transformer Decoderç»“æ„ä¸­ï¼Œdecoderåœ¨ä¹‹å‰æ—¶åˆ»çš„ä¸€äº›è¾“å‡ºï¼Œä½œä¸ºæ­¤æ—¶çš„è¾“å…¥ï¼Œä¸€ä¸ªä¸€ä¸ªå¾€å³ç§»ã€‚
- **é‡å‚æ•°åŒ–**: å¸¸è§„æ€æƒ³: å¯¹äºç½‘ç»œå±‚éœ€è¦çš„å‚æ•°æ˜¯ Î¦ \\Phi Î¦ï¼Œè®­ç»ƒå‡ºæ¥çš„å‚æ•°å°±æ˜¯ Î¦ \\Phi Î¦ã€‚é‡å‚æ•°åŒ–æ–¹æ³•: è®­ç»ƒæ—¶ç”¨çš„æ˜¯å¦ä¸€å¥—ä¸åŒäº Î¦ \\Phi Î¦çš„å‚æ•°ï¼Œè®­ç»ƒå®Œåç­‰ä»·è½¬æ¢ä¸º Î¦ \\Phi Î¦ç”¨äºæ¨ç†ã€‚
- **PPL**: å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼Œç”¨äºè¯„ä»·è¯­è¨€æ¨¡å‹çš„å¥½åã€‚
- **FCNN**: Fully connected neural networkï¼Œå…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚
- **FNN**: Feedforward neural networkï¼Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚
- **DNN**: Deep neural networkï¼Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚
- **MLP**: Multi-layer perceptron neural networksï¼Œå¤šå±‚æ„ŸçŸ¥æœºã€‚
- **RM**: Reward Modelï¼Œå¥–åŠ±æ¨¡å‹ã€‚
- **PPO**ï¼ŒProximal Policy Optimizationï¼Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¯¹ç›®æ ‡å‡½æ•°é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ã€‚
- **Emergent Ability**: å¾ˆå¤šèƒ½åŠ›å°æ¨¡å‹æ²¡æœ‰ï¼Œåªæœ‰å½“æ¨¡å‹å¤§åˆ°ä¸€å®šçš„é‡çº§ä¹‹åæ‰ä¼šå‡ºç°ã€‚è¿™æ ·çš„èƒ½åŠ›ç§°ä¸ºæ¶Œç°èƒ½åŠ›ã€‚
- **AutoRegression Language Model**: è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚
- **Autoencoder Language Model**: è‡ªç¼–ç è¯­è¨€æ¨¡å‹ã€‚
- **CLM**: Causal language modelingï¼Œå› æœè¯­è¨€å»ºæ¨¡ï¼Œç­‰ä»·äºAutoRegression Language Modelã€‚
- **AIGC**: Artificial Intelligence Generated Contentï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€‚
- **AGI**: Artificial General Intelligenceï¼Œé€šç”¨äººå·¥æ™ºèƒ½ã€‚

### 4ã€å¯¹å¤§æ¨¡å‹æ—¶ä»£çš„ä¸€äº›ä¸ªäººæ„Ÿæƒ³

â€ƒâ€ƒå°±ç›®å‰çš„å‘å±•è¶‹åŠ¿è€Œè¨€ï¼Œå¤§æ¨¡å‹çš„è®­ç»ƒåŸºæœ¬å°±æ˜¯æŒ‰ç…§**Pretrain**ã€**Instruction-Tuning**ã€**RLHF**ä¸‰æ­¥èµ°æ¨¡å¼è¿›è¡Œï¼Œå› æ­¤æŠ€æœ¯ä¸ŠåŸºæœ¬æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä¸»è¦ç“¶é¢ˆå­˜åœ¨äºç®—åŠ›ï¼Œå½“ç„¶å¯¹äºä¸­æ–‡å¤§æ¨¡å‹æ¥è¯´ï¼Œè·å–é«˜è´¨é‡ä¸­æ–‡æ•°æ®ä¹Ÿæ˜¯ä¸ªé—®é¢˜ã€‚å›½å†…èƒ½è€—è´¹å¤§è§„æ¨¡æˆæœ¬è¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒçš„å‚å•†å±ˆæŒ‡å¯æ•°ï¼Œå› æ­¤ä¸ªäººè®¤ä¸ºï¼Œæœªæ¥çš„å‘å±•è¶‹åŠ¿ï¼Œå¯èƒ½æœ‰ä»¥ä¸‹å››ä¸ªæ–¹å‘:

- **ç»Ÿä¸€å¤§æ¨¡å‹**: å¤´éƒ¨ä¼ä¸šé€æ­¥è¿­ä»£å‡ºå¯ç”¨æ€§å¾ˆå¼ºçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆåƒäº¿çº§åˆ«ï¼‰ï¼Œå¼€æ”¾APIæˆ–åœ¨å…¬æœ‰äº‘ä¸Šä¾›å¤§å®¶ä½¿ç”¨ï¼›
- **å‚ç›´é¢†åŸŸæ¨¡å‹**: éƒ¨åˆ†ä¼ä¸šé€æ­¥è¿­ä»£å‡ºåœ¨ç›¸å…³å‚ç›´é¢†åŸŸå¯ç”¨æ€§å¾ˆå¼ºçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆç™¾äº¿çº§åˆ«ï¼‰ï¼Œè‡ªç”¨æˆ–ç§æœ‰åŒ–æä¾›å®¢æˆ·ä½¿ç”¨ï¼›
- **å¹¶è¡Œè®­ç»ƒæŠ€æœ¯**: å…¨æ–°æˆ–æ›´å…·å¯ç”¨æ€§ã€ç”Ÿæ€æ›´å®Œæ•´çš„å¹¶è¡Œè®­ç»ƒæŠ€æœ¯/æ¡†æ¶å¼€æºï¼Œæ»¡è¶³å¤§éƒ¨åˆ†æœ‰è®­ç»ƒéœ€æ±‚çš„ä¼ä¸š/ä¸ªäººä½¿ç”¨ï¼Œé€æ­¥å®ç°äººäººéƒ½èƒ½è®­ç»ƒå¤§æ¨¡å‹ï¼›
- **é¢ è¦†**: æˆ–è®¸æŸä¸€å¤©ï¼Œæ¨ªç©ºå‡ºä¸–çš„è®ºæ–‡æ¨ç¿»äº†ç›®å‰çš„å¤§æ¨¡å‹å‘å±•è·¯çº¿ï¼Œè½¬è€Œè¯æ˜äº†äººä»¬æ›´éœ€è¦çš„æ˜¯å¦ä¸€ç§å¤§æ¨¡å‹æŠ€æœ¯ï¼Œäº‹æƒ…å°±ä¼šå˜å¾—æœ‰æ„æ€äº†ã€‚

æ€»ç»“


---

#### encoder vs decoder

- The transformer architecture is split into two distinct parts
  - the **encoder** and the **decoder**.
  - These components work in conjunction with each other and they share a number of similarities.

- The encoder `encodes input sequences into a deep representation of the structure and meaning of the input`.

- The decoder, working from input token triggers, `uses the encoder's contextual understanding to generate new tokens`.

- It does this in a loop until some stop condition has been reached.


- the inputs to the model are at the bottom and the outputs are at the top

![Screenshot 2023-10-15 at 19.52.36](/assets/img/Screenshot%202023-10-15%20at%2019.52.36.png)

- **Encoder-only models**
  - also work as sequence-to-sequence models
  - without further modification, the input sequence and the output sequence or the same length, less common these days,
  - by adding additional layers to the architecture, you can train encoder-only models to perform `classification tasks` such as sentiment analysis,
  - encoder-only model example: BERT

- **Encoder-decoder models**
  - perform well on sequence-to-sequence tasks such as translation
  - the input sequence and the output sequence can be different lengths.
  - You can also scale and train this type of model to perform `general text generation tasks`.
  - encoder-decoder models examples: BART, T5

- **decoder-only models**
  - the most commonly used today.
  - as they have scaled, their capabilities have grown. These models can now generalize to most tasks.
  - Popular decoder-only models: GPT family of models, BLOOM, Jurassic, LLaMA, and many more.

![Screenshot 2023-10-15 at 20.45.16](/assets/img/Screenshot%202023-10-15%20at%2020.45.16.png)


---

#### How the model works

![Screenshot 2023-10-15 at 20.46.53](/assets/img/Screenshot%202023-10-15%20at%2020.46.53.png)

- machine-learning models are just big statistical calculators and they work with numbers, not words.

  - **Tokenize**:
    - Before passing texts into the model to process, must first tokenize the words.
    - converts the words into numbers
    - each number representing a position in a dictionary of all the possible words that the model can work with.

    - You can choose from multiple tokenization methods. For example,
      - token IDs matching two complete words,
      - ![Screenshot 2023-10-15 at 19.53.08](/assets/img/Screenshot%202023-10-15%20at%2019.53.08.png)

      - or using token IDs to represent parts of words.
      - ![Screenshot 2023-10-15 at 19.53.34](/assets/img/Screenshot%202023-10-15%20at%2019.53.34.png)

    - once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text.

  - **Token Embedding**:
    - Now that the input is represented as numbers, pass it to the embedding layer.
    - This layer is a `trainable vector embedding space`,
    - high-dimensional space where each token is represented as a vector and occupies a unique location within that space.
    - Each token ID in the vocabulary is matched to a `multi-dimensional vector`, and the intuition is that these vectors learn to `encode the meaning and context of individual tokens in the input sequence`.

    - Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.

    - each word has been matched to a token ID, and each token is mapped into a vector.
      - In the original transformer paper, the **vector size** was actually 512
      - ![Screenshot 2023-10-15 at 20.02.13](/assets/img/Screenshot%202023-10-15%20at%2020.02.13.png)

      - For simplicity
        - imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.
        - relate words that are located close to each other in the embedding space, and calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language.
        - ![Screenshot 2023-10-15 at 20.02.47](/assets/img/Screenshot%202023-10-15%20at%2020.02.47.png)

  - **Positional Encoding**:
    - Added the token vectors into `the base of the encoder or the decoder`, also add positional encoding.
    - The model processes each of the input tokens in parallel.
    - it preserve the information about the word order and don't lose the relevance of the position of the word in the sentence.
    - ![Screenshot 2023-10-15 at 20.05.45](/assets/img/Screenshot%202023-10-15%20at%2020.05.45.png)

  - **Self-attention**
    - sum the `input tokens` and the `positional encodings`, pass the resulting vectors to the self-attention layer.
    - the model analyzes the relationships between the tokens in the input sequence, it allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.
    - The `self-attention weights` that are learned during training and stored in these layers reflect `the importance of each word in that input sequence to all other words in the sequence`.

    - But this does not happen just once, the transformer architecture actually has `multi-headed self-attention`.
    - ![Screenshot 2023-10-15 at 20.09.13](/assets/img/Screenshot%202023-10-15%20at%2020.09.13.png)

    - This means that `multiple sets of self-attention weights or heads` are learned in parallel independently of each other.

    - The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.

    - each self-attention head will learn a different aspect of language. For example,
      - one head may see the relationship between the people entities in our sentence.
      - another head may focus on the activity of the sentence.
      - another head may focus on some other properties such as if the words rhyme.
      - ![Screenshot 2023-10-15 at 20.12.47](/assets/img/Screenshot%202023-10-15%20at%2020.12.47.png)

    - don't dictate ahead of time what aspects of language the attention heads will learn.

      - The weights of each head are randomly initialized and given sufficient training data and time,
      - each will learn different aspects of language.
      - While some attention maps are easy to interpret, like the examples discussed here, others may not be.
      - Now that all of the attention weights have been applied to the input data, the output is processed through a `fully-connected feed-forward network`.
      - ![Screenshot 2023-10-15 at 20.13.33](/assets/img/Screenshot%202023-10-15%20at%2020.13.33.png)

    - The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary

  - **Softmax**

    - then pass these logits to a final softmax layer, where they are normalized into a `probability score for each word`.
    - This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here.
    - One single token will have a score higher than the rest.
    - This is the most likely predicted token.
    - there are a number of methods that you can use to vary the final selection from this vector of probabilities.

    - ![Screenshot 2023-10-15 at 20.22.45](/assets/img/Screenshot%202023-10-15%20at%2020.22.45.png)





---

#### Overall prediction process


At a very high level, the workflow can be divided into three stages:

- **Data preprocessing / embedding**:
  - This stage involves storing private data to be retrieved later.
  - Typically, the documents are broken into chunks, passed through an embedding model, then stored in a specialized database called a vector database.

- **Prompt construction / retrieval**:
  - When a user submits a query, the application constructs a series of prompts to submit to the language model.
  - A compiled prompt typically combines
    - a prompt template hard-coded by the developer;
    - examples of valid outputs called few-shot examples;
    - any necessary information retrieved from external APIs;
    - and a set of relevant documents retrieved from the vector database.

- **Prompt execution / inference**:
  - Once the prompts have been compiled, they are submitted to a pre-trained LLM for inferenceâ€”including both proprietary model APIs and open-source or self-trained models.
  - Some developers also add operational systems like logging, caching, and validation at this stage.


example: Generating text with transformers

- translation task
  - a sequence-to-sequence task: the original objective of the transformer architecture designers.
  - use a transformer model to translate the French phrase `[FOREIGN]` into English.

Encoded side:
- First, `tokenize the input words` using this same tokenizer that was used to train the network.
- These tokens are then added into the input on the encoder side of the network, passed through the embedding layer, and then fed into the `multi-headed attention layers`.
- The outputs of the multi-headed attention layers are fed through a `feed-forward network` to the output of the encoder.

- At this point, the data that leaves the encoder is a **deep representation of the structure and meaning of the input sequence**.

Decoded side:
- This representation is inserted into the middle of the decoder to influence the `decoder's self-attention mechanisms`.
- Next, a `start of sequence token` is added to the input of the decoder.
- This `triggers the decoder to predict the next token`, based on the contextual understanding that it's being provided from the encoder.
- The output of the decoder's self-attention layers gets passed through the `decoder feed-forward network` and through a final `softmax output layer`.

- At this point, we have our first token.

- You'll continue this loop, passing the output token back to the input to trigger the generation of the next token, until the model predicts an end-of-sequence token.

- At this point, the final sequence of tokens can be detokenized into words, and you have the output.

- There are multiple ways in which you can use the output from the softmax layer to predict the next token. These can influence how creative you are generated text is.

![Screenshot 2023-10-15 at 20.43.16](/assets/img/Screenshot%202023-10-15%20at%2020.43.16.png)

---


##### Data preprocessing / embedding

**Contextual data input**
- Contextual data for LLM apps includes text documents, PDFs, and even structured formats like CSV or SQL tables.
- Data-loading and transformation solutions for this data vary widely across developers.
  - Most use traditional ETL tools like `Databricks` or `Airflow`.
  - Some also use `document loaders` built into orchestration frameworks like `LangChain` (powered by Unstructured) and `LlamaIndex` (powered by Llama Hub).

**embeddings**,
- most developers use the `OpenAI API`, specifically with the text-embedding-ada-002 model. Itâ€™s easy to use (especially if youâ€™re already already using other OpenAI APIs), gives reasonably good results, and is becoming increasingly cheap.
- Some larger enterprises are also exploring `Cohere`, which focuses their product efforts more narrowly on embeddings and has better performance in certain scenarios.
- For developers who prefer open-source, the `Sentence Transformers library` from `Hugging Face` is a standard.

**vector database**
- The most important piece of the preprocessing pipeline, from a systems standpoint
- Itâ€™s responsible for efficiently storing, comparing, and retrieving up to billions of embeddings (i.e., vectors).

  - The most common choice is `Pinecone`. Itâ€™s the default because itâ€™s fully cloud-hosted, easy to get started with, and has many of the features larger enterprises need in production (e.g., good performance at scale, SSO, and uptime SLAs).

  - `Open source systems` like Weaviate, Vespa, and Qdrant: They generally give excellent single-node performance and can be tailored for specific applications, so they are popular with experienced AI teams who prefer to build bespoke platforms.

  - `Local vector management libraries` like Chroma and Faiss: They have great developer experience and are easy to spin up for small apps and dev experiments. They donâ€™t necessarily substitute for a full database at scale.

  - `OLTP extensions` like pgvector: good solution for devs who see every database-shaped hole and try to insert Postgres, or enterprises who buy most of their data infrastructure from a single cloud provider. Itâ€™s not clear, in the long run, if it makes sense to tightly couple vector and scalar workloads.

- Looking ahead, most of the open source vector database companies are developing cloud offerings. Our research suggests achieving strong performance in the cloud, across a broad design space of possible use cases, is a very hard problem. Therefore, the option set may not change massively in the near term, but it likely will change in the long term. The key question is whether vector databases will resemble their OLTP and OLAP counterparts, consolidating around one or two popular systems.

- the embedding pipeline may become more important over time
  - how embeddings and vector databases will evolve as the usable context window grows for most models.
  - Itâ€™s tempting to say embeddings will become less relevant, because contextual data can just be dropped into the prompt directly.
  - However, feedback from experts on this topic suggests the opposite, that the embedding pipeline may become more important over time. Large context windows are a powerful tool, but they also entail significant computational cost. So making efficient use of them becomes a priority.
  - We may start to see different types of embedding models become popular, trained directly for model relevancy, and vector databases designed to enable and take advantage of this.



##### Prompt construction / retrieval

- Strategies for prompting LLMs and incorporating contextual data are becoming increasingly complexâ€”and increasingly important as a source of product differentiation.

- Most developers start new projects by experimenting with simple prompts, consisting of direct instructions (`zero-shot prompting`) or some example outputs (`few-shot prompting`).
  - These prompts often give good results but fall short of accuracy levels required for production deployments.

- The next level of prompting `jiu jitsu` is designed to ground model responses in some source of truth and provide external context the model wasnâ€™t trained on.

**advanced prompting strategies**
- The Prompt Engineering Guide catalogs no fewer than 12 more advanced prompting strategies, including:
  - chain-of-thought, self-consistency, generated knowledge, tree of thoughts, directional stimulus, and many others.
- These strategies can also be used in conjunction to support different LLM use cases like document question answering, chatbots, etc.

**Orchestration frameworks**
- `LangChain` and `LlamaIndex` shine.
- workflow:
  - They abstract away many of the details of prompt chaining;
  - interfacing with external APIs (including determining when an API call is needed);
  - retrieving contextual data from vector databases;
  - and maintaining memory across multiple LLM calls.
  - They also provide templates for many of the common applications mentioned above.

- Their output is a prompt, or series of prompts, to submit to a language model. These frameworks are widely used among hobbyists and startups looking to get an app off the ground .

- LangChain is still a relatively new project (currently on version 0.0.201), but weâ€™re already starting to see apps built with it moving into production.
- Some developers, especially early adopters of LLMs, prefer to switch to raw Python in production to eliminate an added dependency. But we expect this DIY approach to decline over time for most use cases, in a similar way to the traditional web app stack.

- ChatGPT.
  - In its normal incarnation, ChatGPT is an app, not a developer tool. But it can also be accessed as an API.
  - it performs some of the same functions as other orchestration frameworks, such as: abstracting away the need for bespoke prompts; maintaining state; and retrieving contextual data via plugins, APIs, or other sources.
  - While not a direct competitor to the other tools listed here, ChatGPT can be considered a substitute solution, and it may eventually become a viable, simple alternative to prompt construction.



##### Prompt execution / inference


- Prompt execution / inference

  - `OpenAI`
    - Today, OpenAI is the leader among language models. Nearly every developer starts new LLM apps using the OpenAI API with the gpt-4 or gpt-4-32k model.
    - This gives a best-case scenario for app performance and is easy to use, in that it operates on a wide range of input domains and usually requires no fine-tuning or self-hosting.

- When projects go into production and start to scale, a broader set of options come:

  - `Switching to gpt-3.5-turbo`: Itâ€™s ~50x cheaper and significantly faster than GPT-4. Many apps donâ€™t need GPT-4-level accuracy, but do require low latency inference and cost effective support for free users.

  - `Other proprietary vendors (like Anthropicâ€™s Claude models)`: Claude offers fast inference, GPT-3.5-level accuracy, more customization options for large customers, and up to a 100k context window (though weâ€™ve found accuracy degrades with the length of input).

  - `Triaging requests to open source models`: This can be especially effective in high-volume B2C use cases like search or chat, where thereâ€™s wide variance in query complexity and a need to serve free users cheaply.

    - conjunction with fine-tuning open source base models, platforms like Databricks, Anyscale, Mosaic, Modal, and RunPod are used by a growing number of engineering teams.

    - A variety of inference options are available for open source models, including simple API interfaces from Hugging Face and Replicate; raw compute resources from the major cloud providers; and more opinionated cloud offerings like those listed above.

- `Open-source models` trail `proprietary offerings`, but the gap is starting to close.

  - The LLaMa models from Meta
    - set a new bar for open source accuracy and kicked off a flurry of variants.
    - Since LLaMa was licensed for research use only, a number of new providers have stepped in to train alternative base models (e.g., Together, Mosaic, Falcon, Mistral).
    - Meta is also debating a truly open source release of LLaMa2.

    - When open source LLMs reach accuracy levels comparable to GPT-3.5, we expect to see a Stable Diffusion-like moment for textâ€”including massive experimentation, sharing, and productionizing of fine-tuned models.

  - Hosting companies like Replicate are already adding tooling to make these models easier for software developers to consume. Thereâ€™s a growing belief among developers that smaller, fine-tuned models can reach state-of-the-art accuracy in narrow use cases.

- Most developers havenâ€™t gone deep on operational tooling for LLMs yet.
  - Caching is relatively commonâ€”usually based on Redisâ€”because it improves application response times and cost.
  - Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used. They can log, track, and evaluate LLM outputs, usually for the purpose of improving prompt construction, tuning pipelines, or selecting models.
  - There are also a number of new tools being developed to validate LLM outputs (e.g., Guardrails) or detect prompt injection attacks (e.g., Rebuff). Most of these operational tools encourage use of their own Python clients to make LLM calls, so it will be interesting to see how these solutions coexist over time.

- the static portions of LLM apps (i.e. everything other than the model) also need to be hosted somewhere.
  - The most common solutions weâ€™ve seen so far are standard options like Vercel or the major cloud providers.
  - Startups like Steamship provide end-to-end hosting for LLM apps, including orchestration (LangChain), multi-tenant data contexts, async tasks, vector storage, and key management.
  - And companies like Anyscale and Modal allow developers to host models and Python code in one place.


#### AI agents frameworks

- `AutoGPT`, described as â€œan experimental open-source attempt to make GPT-4 fully autonomous,â€

- The in-context learning pattern is effective at solving hallucination and data-freshness problems, in order to better support content-generation tasks.

- Agents, on the other hand, give AI apps a fundamentally new set of capabilities: to solve complex problems, to act on the outside world, and to learn from experience post-deployment.
  - They do this through a combination of `advanced reasoning/planning, tool usage, and memory / recursion / self-reflection`.

- agents have the potential to become a central piece of the LLM app architecture

- And existing frameworks like LangChain have incorporated some agent concepts already. Thereâ€™s only one problem: agents donâ€™t really work yet. Most agent frameworks today are in the proof-of-concept phaseâ€”capable of incredible demos but not yet reliable, reproducible task-completion.



---


### LLM Tools

#### Medusa

> Our approach revisits an underrated gem from the paper "Blockwise Parallel Decoding for Deep Autoregressive Models" [Stern et al. 2018] back to the invention of the Transformer model.
> rather than pulling in an entirely new draft model to predict subsequent tokens, why not simply extend the original model itself? This is where the "Medusa heads" come in.

- a simpler, user-friendly framework for accelerating LLM generation.

- Instead of using an additional draft model like `speculative decoding`, Medusa merely introduces a few additional decoding heads, following the idea of [Stern et al. 2018] with some other ingredients.

- Despite its simple design, Medusa can improve the generation efficiency of LLMs by about 2x.

- These additional decoding heads seamlessly integrate with the original model, producing blocks of tokens at each generative juncture.



benefit:
- Unlike the **draft model**, Medusa heads can be trained in conjunction with the **original model** (which remains frozen during training). This method allows for `fine-tuning large models on a single GPU`, taking advantage of the powerful base model's learned representations.

- also, since the new heads consist of just a single layer akin ç±»ä¼¼çš„ to the **original language model head**, Medusa does not add complexity to the serving system design and is friendly to distributed settings.

- On its own, Medusa heads don't quite hit the mark of doubling processing speeds. But here's the twist:

  - When we pair this with a tree-based attention mechanism, we can verify several candidates generated by Medusa heads in parallel. This way, the Medusa heads' predictive prowess truly shone through, offering a 2x to 3x boost in speed.

  - Eschewing the traditional importance sampling scheme, we created an efficient and high-quality alternative crafted specifically for the generation with Medusa heads. This new approach entirely sidesteps the **sampling** overhead, even adding an extra pep to Medusa's already accelerated step.

- In a nutshell, we solve the challenges of speculative decoding with a simple system:

  - No separate model: Instead of introducing a new draft model, train multiple decoding heads on the same model.

  - Simple integration to existing systems: The training is parameter-efficient so that even GPU poor can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.

  - Treat sampling as a relaxation æ”¾æ¾: Relaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.

- The figure below offers a visual breakdown of the Medusa pipeline for those curious about the nuts and bolts.

![Screenshot 2023-09-21 at 14.49.26](/assets/img/post/Screenshot%202023-09-21%20at%2014.49.26.png)

Overview of Medusa
- Medusa introduces `multiple heads` on top of the last hidden states of the LLM, enabling the prediction of several subsequent tokens in parallel.

- When augmenting a model with Medusa heads, the original model is frozen during training, and only the Medusa heads undergo fine-tuning. This approach makes it feasible to `fine-tune large models on a single GPU`.

- During inference, each head generates multiple top predictions for its designated position. These predictions are assembled into candidates and processed in parallel using a `tree-based attention mechanism`.

- The final step involves utilizing a **typical acceptance scheme** to select reasonable continuations, and the longest accepted candidate prefix will be used for the next decoding phase.

- The efficiency of the decoding process is enhanced by accepting more tokens simultaneously, thus reducing the number of required decoding steps.

Let's dive into the three components of Medusa: Medusa heads, tree attention, and typical acceptance scheme.


##### Medusa heads

- akin to the language model head in the original architecture (the last layer of a causal Transformer model), but with a twist:
  - they predict multiple forthcoming tokens, not just the immediate next one. Drawing inspiration from the Blockwise Parallel Decoding approach, we implement each Medusa head as a single layer of feed-forward network, augmented with a residual connection.

- Training these heads is remarkably straightforward. either use the same corpus æœ¬ä½“ that trained the original model or generate a new corpus using the model itself.

- Importantly, during this training phase, the original model remains static; only the Medusa heads are fine-tuned.

- This targeted training results in a highly parameter-efficient process that reaches convergence è¶‹åŒ swiftly è¿…é€Ÿåœ°, especially when compared to the computational heaviness æ²‰é‡ of **training a separate draft model in speculative decoding methods**.

- The efficacy of Medusa heads is quite impressive. Medusa heads achieve a top-1 accuracy rate of approximately 60% for predicting the 'next-next' token.


##### Tree attention

- During our tests, we uncovered some striking metrics: although the top-1 accuracy for predicting the 'next-next' token hovers around 60%, the top-5 accuracy soars to over 80%.

- This substantial increase indicates that if we can strategically leverage the multiple top-ranked predictions made by the Medusa heads, we can significantly amplify the number of tokens generated per decoding step.

  - With this goal, we first craft a set of candidates by taking the Cartesian product of the top predictions from each Medusa head.

  - We then encode the dependency graph into the attention following the idea from graph neural networks so that we can process multiple candidates in parallel.

![Screenshot 2023-09-21 at 15.05.03](/assets/img/post/Screenshot%202023-09-21%20at%2015.05.03.png)

Tree Attention. This visualization demonstrates the use of tree attention to process multiple candidates concurrently.
- As exemplified, the top-2 predictions from the first Medusa head and the top-3 from the second result in 2*3=6 candidates. Each of these candidates corresponds to a distinct branch within the tree structure.

- To guarantee that each token only accesses its predecessors, we devise an **attention mask** that exclusively permits attention flow from the current token back to its antecedent tokens. The positional indices for positional encoding are adjusted in line with this structure.


- For example, let's consider a scenario where we use top-2 predictions from the first Medusa head and top-3 predictions from the second
  - In this case, any prediction from the first head could be paired with any prediction from the second head, culminating in a multi-level tree structure.
  - Each level of this tree corresponds to predictions from one of the Medusa heads. Within this tree, we implement an attention mask that restricts attention only to a token's predecessors, preserving the concept of historical context.
  - By doing so and by setting positional indices for positional encoding accordingly, we can process a wide array of candidates simultaneously without needing to inflate the batch size.

- We would also remark that a few independent works also adopt very similar ideas of tree attention [1, 2]. Compared with them, our methodology leans towards a simpler form of tree attention where the tree pattern is regular and fixed during inference, which enables a preprocessing of tree attention mask that further improves the efficiency.

##### Typical acceptance

> - In earlier research on speculative decoding, the technique of **importance sampling** was used to generate diverse outputs closely aligned with the original model's predictions.

> - However, later studies showed that this method tends to become less efficient as you turn up the "creativity dial," known as the **sampling temperature**.

- In simpler terms, if the draft model is just as good as the original model, you should ideally accept all its outputs, making the process super efficient. However, importance sampling will likely reject this solution in the middle.

- In the real world, we often tweak the sampling temperature just to control the model's creativity, not necessarily to match the original model's distribution. So why not focus on just accepting plausible è²Œä¼¼åˆç†çš„ candidates?

- We then introduce the **typical acceptance scheme**.

  - Drawing inspiration from existing work on truncation ç¼©çŸ­ sampling, we aim to pick candidates that are likely enough according to the original model. We set a threshold based on the original model's prediction probabilities, and if a candidate exceeds this, it's accepted.

  - In technical jargon, we take the minimum of a hard threshold and an entropy ç†µ, dependent threshold to decide whether to accept a candidate as in truncation sampling.
  - This ensures that meaningful tokens and reasonable continuations are chosen during decoding.
  - We always accept the first token using greedy decoding, ensuring that at least one token is generated in each step.
  - The final output is then the longest sequence that passes our acceptance test.

- What's great about this approach is its adaptability.

  - If set the sampling temperature to zero, it simply reverts to the most efficient form, greedy decoding.

  - When you increase the temperature, our method becomes even more efficient, allowing for longer accepted sequences, a claim we've confirmed through rigorous testing.

- in essence, our typical acceptance scheme offers a more efficient way to generate the creative output of LLMs.


##### accelerate models

- We tested Medusa with Vicuna models (specialized Llama models fine-tuned specifically for chat applications).
  - These models vary in size, with parameter counts of 7B, 13B, and 33B.
  - Our goal was to measure how Medusa could accelerate åŠ é€Ÿ these models in a real-world chatbot environment.

- When it comes to training Medusa heads, we opted for a simple approach. We utilized the **publicly available ShareGPT dataset**, a subset of the **training data originally used for Vicuna models** and only trained for a single epoch.

- this entire training process could be completed in just a few hours to a day, depending on the model size, all on a single A100-80G GPU.

- Notably, Medusa can be easily combined with a **quantized base model** to reduce the memory requirement. We take this advantage and use an 8-bit quantization é‡åŒ– when training the 33B model.

- To simulate a real-world setting, we use the MT bench for evaluation. The results were encouraging: With its simple design, Medusa consistently achieved approximately a 2x speedup in wall time across a broad spectrum of use cases.

- Remarkably, with Medusa's optimization, a 33B parameter Vicuna model could operate as swiftly as a 13B model.

![Screenshot 2023-09-21 at 15.23.56](/assets/img/post/Screenshot%202023-09-21%20at%2015.23.56.png)

![Screenshot 2023-09-21 at 15.24.02](/assets/img/post/Screenshot%202023-09-21%20at%2015.24.02.png)


##### Ablation Study æ¶ˆèç ”ç©¶

- When harnessing the predictive abilities of Medusa heads, we enjoy the flexibility to select how many top candidates each head should consider.

  - For instance, we might opt for the top-3 predictions from the first head and the top-2 from the second. When we take the Cartesian product of these top candidates, we generate a set of six continuations for the model to evaluate.

- This level of configurability comes with its trade-offs.
- On the one hand, selecting more top predictions increases the likelihood of the model accepting generated tokens.
- On the other, it also raises the computational overhead at each decoding step. To find the optimal balance, we experimented with various configurations and identified the most effective setup, as illustrated in the accompanying figure.


- In the typical acceptance scheme,  a critical hyperparameterâ€”referred to as the
- 'threshold': whether the tokens generated are plausible based on the model's own predictions. The higher this threshold, the more stringent the criteria for acceptance, which in turn impacts the overall speedup gained through this approach.

- We explore this trade-off between quality and speedup through experiments on two creativity-oriented tasks from the MT bench. The results, depicted in the figure, reveal that the typical acceptance offers a 10% speedup compared to greedy decoding methods. This speedup is notably better than when employing speculative decoding with random sampling, which actually slowed down the process compared to greedy decoding.

![Screenshot 2023-09-21 at 15.28.12](/assets/img/post/Screenshot%202023-09-21%20at%2015.28.12.png)


---

### Confidence score for ML model

[./2023-04-24-AI_ConfidenceScore.md]

---

### Transparency

Transparency in the context of AI models refers to the degree to which the inner workings of the model are understandable, interpretable, and explainable to humans. It encompasses several aspects:

- Explainability:
  - This refers to the ability to understand and interpret the model's decisions.
  - An interpretable model `provides clear and understandable reasons` for its predictions or actions.
  - This is crucial, especially in high-stakes applications like healthcare or finance, where accountability and trust are essential.
- Visibility:
  - Transparency also involves making the model architecture, parameters, and training data visible to those who are affected by its decisions.
  - This allows external parties to scrutinize the model for biases, ethical concerns, or potential risks.
- Audibility:
  - The ability to audit an AI model involves examining its processes, inputs, and outputs to ensure it aligns with ethical and legal standards.
  - Auditing enhances accountability and helps identify and rectify issues or biases.
- Comprehensibility:
  - A transparent AI model should be comprehensible to various stakeholders, including domain experts, policymakers, and the general public. This involves presenting complex technical concepts in a way that is accessible to non-experts.
- Fairness and Bias: Transparency also relates to addressing biases in AI models. Understanding how the model makes decisions can help identify and rectify biased behavior, ensuring fair treatment across diverse demographic groups.
- Transparency is crucial for building trust in AI systems, especially as they are increasingly integrated into various aspects of society. It helps users, regulators, and the general public understand how AI systems function, assess their reliability, and hold developers and organizations accountable for their impact. Various techniques and tools are being developed to enhance the transparency of AI models, but it remains an ongoing area of research and development.

#### The Foundation Model Transparency Index

The Foundation Model Transparency Index [^The_Foundation_Model_Transparency_Index].

[^The_Foundation_Model_Transparency_Index]: The Foundation Model Transparency Index, https://crfm.stanford.edu/fmti/fmti.pdf

Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts.

While the societal impact of foundation models is growing, `transparency is on the decline`, mirroring the opacity that has plagued past digital technologies (e.g. social media).

Reversing this trend is essential:
- transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the founda- tion model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The 2023 Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g. data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.

![Screenshot 2023-11-13 at 12.47.10](/assets/img/Screenshot%202023-11-13%20at%2012.47.10.png)


---


### Generative AI project lifecycle

![Screenshot 2023-10-21 at 12.31.56](/assets/img/Screenshot%202023-10-21%20at%2012.31.56.png)

![Screenshot 2023-10-21 at 12.32.35](/assets/img/Screenshot%202023-10-21%20at%2012.32.35.png)




---

## Hugging Face

> like github repo

- search for an AI model


```bash
# +++++ Getting started with our git and git-lfs interface
# If you need to create a repo from the command line (skip if you created a repo from the website)
pip install huggingface_hub
# You already have it if you installed transformers or datasets
huggingface-cli login
# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}


# +++++ Clone the model or dataset locally
# Make sure you have git-lfs installed
# (https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name


# +++++ Then add, commit and push any file you want, including larges files
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push


# +++++ In most cases, if you're using one of the compatible libraries, the repo will then be accessible from code, through its identifier: username/repo_name
# For example for a transformers model, anyone can load it with:
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```


### Generative AI Time Series Forecasting

> [Generative AI Time Series Forecasting](https://huggingface.co/blog/time-series-transformers)



### Multivariate Time Series Forecasting

> [Multivariate Time Series Forecasting](https://huggingface.co/blog/informer)



### Generative AI Transformers for Time Series Forecasting

> [Generative AI Transformers for Time Series Forecasting](https://huggingface.co/blog/autoformer)


---

### Falcon 40b

- Chatgpt competitor - https://huggingface.co/tiiuae/falcon-40b

- Power of Falcon 40b chat - https://huggingface.co/spaces/HuggingFaceH4/falcon-chat

- Pre-Training - https://huggingface.co/tiiuae/falcon-40b#training-data

- or https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/studio-notebook-fine-tuning/falcon-40b-qlora-finetune-summarize.ipynb



Chat with Falcon-40B-Instruct, brainstorm ideas, discuss the holiday plans, and more!
- âœ¨ This demo is powered by Falcon-40B, finetuned on the Baize dataset, and running with Text Generation Inference. Falcon-40B is a state-of-the-art `large language model` built by the Technology Innovation Institute in Abu Dhabi. It is trained on 1 trillion tokens (including RefinedWeb) and available under the Apache 2.0 license. It currently holds the ğŸ¥‡ 1st place on the ğŸ¤— Open LLM leaderboard. This demo is made available by the HuggingFace H4 team.
- ğŸ§ª This is only a first experimental preview: the H4 team intends to provide increasingly capable versions of Falcon Chat in the future, based on improved datasets and RLHF/RLAIF.
- ğŸ‘€ Learn more about Falcon LLM: `falconllm.tii.ae`
- â¡ï¸ï¸ Intended Use: this demo is intended to showcase an early finetuning of Falcon-40B, to illustrate the impact (and limitations) of finetuning on a dataset of conversations and instructions. We encourage the community to further build upon the base model, and to create even better instruct/chat versions!
- âš ï¸ Limitations: the model can and will produce factually incorrect information, hallucinating facts and actions. As it has not undergone any advanced tuning/alignment, it can produce problematic outputs, especially if prompted to do so. Finally, this demo is limited to a session length of about 1,000 words.

---

### CodeParrot



---

### TAPEX

> [Table Pre-training via Execution](https://huggingface.co/microsoft/tapex-large)

Give a table of data and then query

- 0 shot question (answer right away)
- fine tune: https://github.com/SibilTaram/tapax_transformers/tree/add_tapex_bis/examples
- demo: https://huggingface.co/microsoft/tapex-base




---

## Juptyper

> To run a shell command from within a notebook cell, you must put a ! in front of the command:
> !pip install hyperopt

```py
!nvidia-smi --list-gpus


!pip install --upgrade pip

!pip uninstall -y git+https://github.com/openai/CLIP.git \
  urllib3==1.25.10 \
  sentence_transformers \
  torch torchvision pytorch-lightning lightning-bolts

# install supporting puthon packages for Data Frame processing
# and for Progress Bar
!pip install numpy pandas matplotlib tqdm scikit-learn

# install only the older version of Torch
!pip install --ignore-installed \
    urllib3==1.25.10 \
    torch torchvision pytorch-lightning lightning-bolts

# install latest (Upgrade) sentence transformers for fine-tuning
!pip install --ignore-installed \
  urllib3==1.25.10 \
  pyyaml \
  sentence_transformers

# Use CLIP model from OpenAI
!pip install git+https://github.com/openai/CLIP.git

# load the python package to run Pandas in parallel for better speed
!pip install pandarallel

!pip install torchaudio

!pip uninstall -y nvidia_cublas_cu11
```








.
