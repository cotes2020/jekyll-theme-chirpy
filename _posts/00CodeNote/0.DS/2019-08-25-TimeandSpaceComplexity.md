---
title: DS - pythonds3 - 8. Graphs and Graph Algorithms
# author: Grace JyL
date: 2019-08-25 11:11:11 -0400
description:
excerpt_separator:
categories: [00CodeNote, PythonNote]
tags:
math: true
# pin: true
toc: true
# image: /assets/img/sample/devices-mockup.png
---

# Time and Space Complexity

- [Time and Space Complexity](#time-and-space-complexity)
  - [Overview](#overview)
  - [â± æ—¶é—´å¤æ‚åº¦ Time Complexity](#-æ—¶é—´å¤æ‚åº¦-time-complexity)
    - [ğŸ“˜ ç¤ºä¾‹ 1ï¼šä¸¤ä¸ªæ•°ç›¸åŠ ](#-ç¤ºä¾‹-1ä¸¤ä¸ªæ•°ç›¸åŠ )
    - [ğŸ“˜ ç¤ºä¾‹ 2ï¼šå¯»æ‰¾æ•°ç»„ä¸­å’Œä¸º Z çš„ä¸€å¯¹æ•°](#-ç¤ºä¾‹-2å¯»æ‰¾æ•°ç»„ä¸­å’Œä¸º-z-çš„ä¸€å¯¹æ•°)
    - [ğŸ“˜ ç¤ºä¾‹ 3ï¼šåµŒå¥—å¾ªç¯ï¼ˆN, N/2, N/4...ï¼‰](#-ç¤ºä¾‹-3åµŒå¥—å¾ªç¯n-n2-n4)
    - [Big-O Notation](#big-o-notation)
      - [O(1) â€“ Constant Time](#o1--constant-time)
      - [O(log n) â€“ Logarithmic Time](#olog-n--logarithmic-time)
      - [O(n) â€“ Linear Time](#on--linear-time)
      - [O(n log n) â€“ Linearithmic Time](#on-log-n--linearithmic-time)
      - [O(n^2) â€“ Quadratic Time](#on2--quadratic-time)
      - [O(2^n) â€“ Exponential Time](#o2n--exponential-time)
      - [O(n!) â€“ Factorial Time](#on--factorial-time)
    - [general time complexities](#general-time-complexities)
  - [ğŸ’¾ ç©ºé—´å¤æ‚åº¦ï¼ˆSpace Complexityï¼‰](#-ç©ºé—´å¤æ‚åº¦space-complexity)
    - [Definition](#definition)
    - [ğŸ“˜ ç¤ºä¾‹ï¼šè®¡ç®—æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ å‡ºç°æ¬¡æ•°](#-ç¤ºä¾‹è®¡ç®—æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ å‡ºç°æ¬¡æ•°)

![Screen Shot 2021-09-29 at 12.02.22 AM](https://i.imgur.com/p9lkwcJ.png)

ref:
- https://www.geeksforgeeks.org/dsa/time-complexity-and-space-complexity/


---

## Overview

| æŒ‡æ ‡             | å®šä¹‰                                       | ç¤ºä¾‹              |
| ---------------- | ------------------------------------------ | ----------------- |
| **æ—¶é—´å¤æ‚åº¦**   | ç®—æ³•éšè¾“å…¥è§„æ¨¡å¢é•¿æ‰€éœ€çš„æ—¶é—´               | O(1), O(N), O(NÂ²) |
| **ç©ºé—´å¤æ‚åº¦**   | ç®—æ³•éšè¾“å…¥è§„æ¨¡å¢é•¿æ‰€éœ€çš„ç©ºé—´               | O(1), O(N)        |
| **å¸¸ç”¨åˆ†ææ–¹æ³•** | åªå…³æ³¨è¾“å…¥è§„æ¨¡çš„æœ€é«˜é˜¶é¡¹ï¼Œå¿½ç•¥å¸¸æ•°å’Œä½é˜¶é¡¹ | â€”                 |

![enter image description here](https://assets.leetcode.com/users/images/3abe4254-bf99-44af-87f3-39784bd5d4a8_1725720092.0678196.png)

- Many times there are more than one ways to solve a problem with different algorithms and we need a way to compare multiple ways.

- ç®—æ³•çš„æ€§èƒ½é€šå¸¸ä»ä¸¤ä¸ªè§’åº¦è¡¡é‡ï¼š
  - æ—¶é—´å¤æ‚åº¦ï¼ˆTime Complexityï¼‰ï¼šç®—æ³•è¿è¡Œæ‰€éœ€æ—¶é—´ä¸è¾“å…¥è§„æ¨¡ï¼ˆNï¼‰çš„å…³ç³»ã€‚
  - ç©ºé—´å¤æ‚åº¦ï¼ˆSpace Complexityï¼‰ï¼šç®—æ³•è¿è¡Œæ‰€éœ€å†…å­˜ä¸è¾“å…¥è§„æ¨¡ï¼ˆNï¼‰çš„å…³ç³»ã€‚
- è¿™äº›åˆ†ææ˜¯ç‹¬ç«‹äºæœºå™¨æ€§èƒ½çš„ï¼Œåªä¸ç®—æ³•é€»è¾‘å’Œè¾“å…¥è§„æ¨¡æœ‰å…³ã€‚

- know how much time and resources an algorithm might take when implemented.

- different method
  - Independent of the machine and its configuration, on which the algorithm is running on.
  - Shows a direct correlation with the number of inputs.
  - Can distinguish two algorithms clearly without ambiguity.

## â± æ—¶é—´å¤æ‚åº¦ Time Complexity

**Time Complexity**:

- The time complexity of an algorithm quantifies the amount of `time taken` by an algorithm to run as a function of the length of the input.

  - Note that the time to run is `a function of the length of the input` and not the actual execution time of the machine on which the algorithm is running on.

- The valid algorithm takes a finite amount of time for execution. The time required by the algorithm to solve given problem is called `time complexity` of the algorithm.
- It is the time needed for the completion of an algorithm.

- To estimate the time complexity, we need to consider the cost of each fundamental instruction and the number of times the instruction is executed.

- In order to calculate time complexity on an algorithm, it is assumed that a constant time `c` is taken to execute one operation, and then the total operations for an input length on `N` are calculated.

---

### ğŸ“˜ ç¤ºä¾‹ 1ï¼šä¸¤ä¸ªæ•°ç›¸åŠ 

```cpp
Algorithm ADD SCALAR(A, B)
//Description: Perform arithmetic addition of two numbers
//Input: Two scalar variables A and B
//Output: variable C, which holds the addition of A and B
C <- A + B
return C
```

> The addition of two scalar numbers requires one addition operation.
> the time complexity of this algorithm is `constant`, so `T(n) = O(1)` .
> åªæ‰§è¡Œä¸€æ¬¡åŠ æ³•æ“ä½œï¼Œå› æ­¤æ—¶é—´å¤æ‚åº¦ä¸º `O(1)`ã€‚

### ğŸ“˜ ç¤ºä¾‹ 2ï¼šå¯»æ‰¾æ•°ç»„ä¸­å’Œä¸º Z çš„ä¸€å¯¹æ•°

- Consider an example to understand the process of calculation: Suppose a problem is to find whether a pair (X, Y) exists in an array, A of N elements whose sum is Z.
- The simplest idea is to consider every pair and check if it satisfies the given condition or not.

- The pseudo-code is as follows:

```cpp
int a[n];
for(int i = 0;i < n;i++)
  cin >> a[i]

for(int i = 0;i < n;i++)
  for(int j = 0;j < n;j++)
    if(i!=j && a[i]+a[j] == z)
       return true
return false
```

Below is the implementation of the above approach:

```py
# Python3 program for the above approach

# Function to find a pair in the given
# array whose sum is equal to z
def findPair(a, n, z) :
    # Iterate through all the pairs
    for i in range(n) :
        for j in range(n) :
            # Check if the sum of the pair
            # (a[i], a[j]) is equal to z
            if (i != j and a[i] + a[j] == z) :
                return True
    return False

# Driver Code

# Given Input
a = [ 1, -2, 1, 0, 5 ]
z = 0
n = len(a)

# Function Call
if (findPair(a, n, z)) :
    print("True")
else :
    print("False")
    # This code is contributed by splevel62.

# Output
False
```

> Assuming that each of the operations in the computer takes approximately constant time, let it be `c`.
> The number of lines of code executed actually depends on the value of `Z`.
> During analyses of the algorithm, mostly the worst-case scenario is considered, i.e., when there is no pair of elements with sum equals `Z`.
> In the worst case,
>
> - N\*c operations are required for input.
> - The outer loop `i` loop runs `N` times.
> - For each `i`, the inner loop `j` loop runs `N` times.
>   So total execution time is `N*c + N*N*c + c`. Now ignore the lower order terms since the lower order terms are relatively insignificant for large input, therefore only the highest order term is taken (without constant) which is N\*N in this case. Different notations are used to describe the limiting behavior of a function, but since the worst case is taken so big-O notation will be used to represent the time complexity.
> - å¤–å±‚å¾ªç¯æ‰§è¡Œ `N` æ¬¡ï¼Œå†…å±‚å¾ªç¯ä¹Ÿæ‰§è¡Œ `N` æ¬¡
> - æ€»æ“ä½œæ¬¡æ•°ï¼š`N Ã— N = NÂ²`
> - ğŸ‘‰ æ—¶é—´å¤æ‚åº¦ï¼š`O(NÂ²)`

### ğŸ“˜ ç¤ºä¾‹ 3ï¼šåµŒå¥—å¾ªç¯ï¼ˆN, N/2, N/4...ï¼‰

```py
count = 0
i = N
while(i > 0):
  for j in range(i):
    count + = 1
  i /= 2
```

- When i = N, it will run N times.
- When i = N / 2, it will run N / 2 times.
- When i = N / 4, it will run N / 4 times.
- The total number of times count++ will run is `N + N/2 + N/4+...+1= 2 * N`.
- So the time complexity will be `O(N)`.

---

### Big-O Notation

#### O(1) â€“ Constant Time

The runtime does not depend on the input size.

```java
// accessing an element in an array by its index is an O(1) operation.
int getElement(int arr[], int index) {
    return arr[index];  // O(1)
}
```

#### O(log n) â€“ Logarithmic Time

This occurs when the algorithm cuts the problem size in half at each step, such as in binary search.

```java
int binarySearch(int arr[], int n, int target) {
    int left = 0, right = n - 1;
    while (left <= right) {
        int mid = left + (right - left) / 2;
        if (arr[mid] == target)
            return mid;
        else if (arr[mid] < target)
            left = mid + 1;
        else
            right = mid - 1;
    }
    return -1;  // O(log n)
}
```

#### O(n) â€“ Linear Time

The algorithm's runtime grows directly proportional to the input size

```java
// simple loop through an array.
int findMax(int arr[], int n) {
    int maxVal = arr[0];
    for (int i = 1; i < n; i++) {
        if (arr[i] > maxVal)
            maxVal = arr[i];
    }
    return maxVal;  // O(n)
}
```

#### O(n log n) â€“ Linearithmic Time

This complexity is common in efficient sorting algorithms like Merge Sort and Quick Sort.

```java
void mergeSort(int arr[], int l, int r) {
    if (l >= r) return;
    int m = l + (r - l) / 2;
    mergeSort(arr, l, m);
    mergeSort(arr, m + 1, r);
    merge(arr, l, m, r);  // O(n log n)
}
```

#### O(n^2) â€“ Quadratic Time

The runtime grows quadratically with the input size. 

This is typical for algorithms with nested loops, like bubble sort.
 
```java
void bubbleSort(int arr[], int n) {
    for (int i = 0; i < n-1; i++) {
        for (int j = 0; j < n-i-1; j++) {
            if (arr[j] > arr[j+1])
                swap(arr[j], arr[j+1]);
        }
    }  // O(n^2)
}
```

#### O(2^n) â€“ Exponential Time

This complexity appears in recursive algorithms where the problem branches into multiple subproblems, like the naive Fibonacci implementation.
 
```java
int fib(int n) {
    if (n <= 1) return n;
    return fib(n-1) + fib(n-2);  // O(2^n)
}
```

#### O(n!) â€“ Factorial Time

Algorithms with this complexity grow rapidly and are generally impractical for large inputs, like the solution to the Traveling Salesman Problem and Print Permutations using brute force.

```java
vector<int> nums = {1, 2, 3}; // Example vector
void printPermutations(vector<int>& nums) {
    do {
        for (int num : nums) 
            cout << num << " ";
        cout << endl;
    } while (next_permutation(nums.begin(), nums.end())); // O(!n)
}
```

---

### general time complexities

| Input Length | Worst Accepted Time Complexity | Usually type of solutions                                       |
| ------------ | ------------------------------ | --------------------------------------------------------------- |
| 10 -12       | `O(N!)`                        | Recursion and backtracking                                      |
| 15-18        | `O(2N \* N)`                   | Recursion, backtracking, and bit manipulation                   |
| 18-22        | `O(2N \* N)`                   | Recursion, backtracking, and bit manipulation                   |
| 30-40        | `O(2N/2 \* N)`                 | Meet in the middle, Divide and Conquer                          |
| 100          | `O(N4)`                        | Dynamic programming, Constructive                               |
| 400          | `O(N3)`                        | Dynamic programming, Constructive                               |
| 2K           | `O(N2\* log N)`                | Dynamic programming, Binary Search, Sorting, Divide and Conquer |
| 10K          | `O(N2)`                        | Dynamic programming, Graph, Trees, Constructive                 |
| 1M           | `O(N\* log N)`                 | Sorting, Binary Search, Divide and Conquer                      |
| 100M         | `O(N), O(log N), O(1)`         | Constructive, Mathematical, Greedy Algorithms                   |

---

## ğŸ’¾ ç©ºé—´å¤æ‚åº¦ï¼ˆSpace Complexityï¼‰

### Definition

- ç®—æ³•æ‰§è¡Œæ—¶æ‰€éœ€çš„å†…å­˜æ€»é‡ã€‚åŒ…æ‹¬ï¼š
- å›ºå®šéƒ¨åˆ†ï¼šä»£ç ã€å¸¸é‡ã€å˜é‡ç­‰ï¼Œä¸è¾“å…¥è§„æ¨¡æ— å…³ã€‚
- å¯å˜éƒ¨åˆ†ï¼šé€’å½’æ ˆã€åŠ¨æ€æ•°æ®ç»“æ„ï¼Œä¸è¾“å…¥è§„æ¨¡ç›¸å…³ã€‚

- the amount of memory needed for the completion of an algorithm.

- Problem-solving using computer requires memory to hold temporary data or final result while the program is in execution. The amount of memory required by the algorithm to solve given problem is called space complexity of the algorithm.

- The space complexity of an algorithm quantifies the amount of space taken by an algorithm to run as a function of the length of the input. Consider an example: Suppose a problem to find the frequency of array elements.

To estimate the memory requirement we need to focus on two parts:

(1) A fixed part: It is independent of the input size. It includes memory for instructions (code), constants, variables, etc.

(2) A variable part: It is dependent on the input size. It includes memory for recursion stack, referenced variables, etc.

### ğŸ“˜ ç¤ºä¾‹ï¼šè®¡ç®—æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ å‡ºç°æ¬¡æ•°

```cpp
Algorithm ADD SCALAR(A, B)
//Description: Perform arithmetic addition of two numbers
//Input: Two scalar variables A and B
//Output: variable C, which holds the addition of A and B
C <â€” A+B
return C
```

> The addition of two scalar numbers requires one extra memory location to hold the result. 
> Thus the space complexity of this algorithm is constant, hence `S(n) = O(1)`.

The pseudo-code is as follows:

```cpp
int freq[n];
int a[n];
for(int i = 0; i<n; i++)
{
cin>>a[i];
freq[a[i]]++;
}  
```

Below is the implementation of the above approach:

```py
# Python program for the above approach

# Function to count frequencies of array items
def countFreq(arr, n):
    freq = dict()
    
    # Traverse through array elements and
    # count frequencies
    for i in arr:
        if i not in freq:
            freq[i] = 0
        freq[i]+=1
        
    # Traverse through map and print frequencies
    for x in freq:
        print(x, freq[x])

# Driver Code

# Given array
arr =  [10, 20, 20, 10, 10, 20, 5, 20 ]
n = len(arr)

# Function Call
countFreq(arr, n)

# This code is contributed by Shubham Singh
```

> éœ€è¦ä¸¤ä¸ªé•¿åº¦ä¸º N çš„æ•°ç»„ + å¸¸æ•°ç©ºé—´
> æ€»ç©ºé—´ = `2N + 1 â‰ˆ O(N)`ã€‚
> ğŸ“Œ è¾…åŠ©ç©ºé—´ï¼ˆAuxiliary Spaceï¼‰ï¼šæŒ‡é™¤è¾“å…¥æ•°æ®å¤–çš„é¢å¤–ç©ºé—´ã€‚
> åœ¨ä¸Šä¾‹ä¸­ï¼Œfreq[] æ˜¯è¾…åŠ©ç©ºé—´ï¼Œå› æ­¤è¾…åŠ©ç©ºé—´å¤æ‚åº¦ä¸º `O(N)`ã€‚


---