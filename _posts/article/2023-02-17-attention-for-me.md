---
categories:
  - article
date: 2023-02-17 00:00:00 +0900
math: true
tags:
  - NLP
title: 自分なりの Self Attention の解釈
parse_block_html: true
---

WIP

<details markdown="1">
<summary>ポエム</summary>
せっかくルールが存在しない個人サイトなので、たまにはブログのような話から入ってみます。

卒論第一稿を書き終えて、久しぶりに何も無い日を 2 日くらい過ごせたので、結構メンタルが回復した気がします。
しかし自分にはやらなければならないことがどうやら 2 つあるようです。
それは呼吸器内科に行くことと歯科に行くことです。

さっき「あるようです」と語尾をぼかしたのは、単純にめんどくさいからです。
家でボーッとしていたい怠惰な気持ち vs 病院に行かないと後悔する予感を対決させると怠惰が大差で勝利するので、
いつも症状が深刻になってから病院に行きます。
でもこの性格のおかげで散財する機会が減って、貯金が増えるという面もあるので悪いことばっかりってわけでもないです。
体調は悪いですが。

<!-- prettier-ignore -->
```html
<details markdown="1" open>
  <summary>ただのメモ</summary>
  `markdown="1"` を指定すると、details 内でもコードブロックとか使えるのね。（数年前に使った記憶あるけど）\
</details>
```

なんか、自分に飽きたのでさっさと本題に入ります。

</details>

さて、今回は卒研で 1 年弱付き合ってきた Transformer の Self Attention について書きます。
解説というよりは、自分がどう Self Attention を解釈しているかという内容です。
なので、ちょっとだけ Self Attention のことを知っているほうが面白いかもしれません。
ちなみに、私は新米ゆえに、研究者が考えていることをどこまでオープンなところで喋って良いのか分からないので、
当たり前なことだけ触れることにします（考えすぎだと思うけど）。

# 分布仮説: 単語ベクトルの原点

言葉を分散表現や埋め込み (Embedding) と呼ばれる数ベクトルで表そうという発想の根底には **[分布仮説 (Distributional Hypothesis)](https://aclweb.org/aclwiki/Distributional_Hypothesis)** という考え方があります。
分布仮説とは、同じ文脈で出現する単語は似たような意味を持つ傾向があるという考え方で、ここから「単語は仲間によって特徴づけられる」という考え方に発展しました。

次の例で実際に体験してみましょう。

Q. [MASK] に入るのは何でしょう。

1. <details markdown=1><summary>私は毎晩 [MASK] を見るのが好きです。</summary>

   1. 映画: 0.166
   2. テレビ: 0.151
   3. 夢: 0.101
   4. 鏡: 0.052
   5. 空: 0.038

   </details>

1. <details markdown=1><summary>この [MASK] は最新型なので高価です。</summary>

   1. モデル: 0.176
   2. エンジン: 0.099
   3. 機種: 0.070
   4. 車: 0.053
   5. カメラ: 0.028

   </details>

1. <details markdown=1><summary>[MASK] のリモコンを見つけられない。</summary>

   1. リモコン: 0.181
   2. テレビ: 0.093
   3. ロボット: 0.034
   4. 自分: 0.026
   5. カメラ: 0.023

   </details>

1. <details markdown=1><summary>私たちは、試合観戦をするために [MASK] を買った。</summary>

   1. チケット: 0.200
   2. 馬: 0.071
   3. 車: 0.033
   4. ユニフォーム: 0.019
   5. テレビ: 0.017

   </details>

皆さんは何を思い浮かべましたか。
正解というわけではないですが、これらの文はいずれも `テレビ` のつもりで作成しました。
ただ、1 個目だと `星空` の方がロマンチックで、4 個目は `チケット` の方がむしろ妥当ですね。

> - 例文は ChatGPT に「`テレビ` という言葉で例文作って」と送ったときの返信です。
> - トグル ▶ を開くと表示される 5 個の単語は BERT の推論結果の上位 5 位です。
>   [Hugging Face](https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking) で試すことができます。

とりあえず、`[MASK]` に当てはまる単語の選択肢が文脈により制限される、
すなわち、「単語は仲間によって特徴づけられる」というのは共感してもらえたことにします。

# Self Attention における分布仮説

まずは数式を使わずに Self Attention がやっていることを説明します。
Self Attention は、内部で計算されるアテンション行列を可視化することで何をやっているのかを推測することができます。

<img src="/assets/img/posts/attention.png" width="400px">
*アテンション行列 ([Clark, 2019](https://aclanthology.org/W19-4828/))*

> `[CLS]` と `[SEP]` はそれぞれ文頭と文末を示すトークンです。この記事では無視します。

シンプルな左図だけ詳しく見てみましょう。このとき、Self Attention は以下の文が入力されたときに何を出力するか考えています。

```
This market has been very badly damaged.
```

そして、中間にある線は「左端の単語の意味が右端の単語に吸収される」様子と私は解釈しています。例えば、今回なら次のように線の右端の単語が情報を受け取って意味が更新されることを表しています。

| 情報  |   吸収先    | 気持ち                                     |
| :---: | :---------: | :----------------------------------------- |
| This  |   market    | 自分って名詞？                             |
|  has  |    been     | この文って現在完了？                       |
| very  |    badly    | 強調あざす                                 |
| been  | damaged (1) | 自分って受動態？                           |
| badly | damaged (2) | もしかして僕あまり良い意味の単語じゃない？ |

ここで、「吸収」とは単純に差し替えを意味しています。なので、プログラミング風に雑に書くならこんな感じになります。

```python
market = This
damaged = been + badly
been = has
badly = very
```

割と誇張しましたが、Self Attention はこんな感じに単語ベクトルを更新しています。
つまり、「単語は仲間によって特徴づけられる」というアイデアをとても直接的にモデル化しています。

# Self Attention の定義と解釈

Self Attention は、クエリ $Q$ とキー $K$ とバリュー $V$ の 3 つを入力として、次のように定義されます。

$$
\begin{aligned}
  Q &= XW^Q, ~ K = XW^K, ~ V = XW^V \\
  A &= \text{softmax}\left( \dfrac{QK^T}{\sqrt{d}} \right) \\
  \text{Attention}(Q,K,V) &= AV
\end{aligned}
$$

この $A$ がアテンション行列です。
最後の式は、単語ベクトルからなるバリュー $V$ にアテンション $A$ を掛けることで、
単語同士の重み付き和を計算しています。ただし、大体の重みをゼロと思えば、
（大きな重みに対応する）関連語に自身を差し替えているとみなせます。

$$
\begin{aligned}
  x_2 &= a x_1 + by_1 + cz_1 \\
  &= 0 \cdot x_1 + 1\cdot y_1 + 0\cdot z_1 \\
  &= y_1
\end{aligned}
$$

## 「キーなんてなかった」

よく、Self Attention は「クエリ・キー・バリュー」の 3 つで構成される辞書のようなものと言われます。去年は自分も辞書という例えで「なるほど」と思ったのですが、やはり、辞書のキーが変動するのがずっともやもやしていました。このせいで Transformer ベースの手法が数ヶ月理解できませんでした。

じゃあ、キーなくせばいいやん！「クエリ・キー・バリュー」なんてものは所詮人が考えた概念です。（ノイジー・マイノリティ感のある発言。）なので、これらの概念を一旦忘れて数式を同値変形すれば自分なりの解釈をすることができます。
私が解釈しやすいように書き換えると、こうなります。

$$
\begin{aligned}
  \widetilde{Q} &= XW^Q(W^K)^T = X\widetilde{W}^Q \\
  \widetilde{X} &= \text{softmax}\left( \dfrac{\widetilde{Q}X^T}{\sqrt{d}} \right) X\\
  \text{Attention}(Q,K,V) &= \widetilde{X}W^V
\end{aligned}
$$

最後の $\text{Attention}(Q,K,V)$ の式に上 2 つの式を代入すると明らかですが、この定義は一般的な定義と全く等価です。

<!-- attention は nested dict -->
<!-- {This: {pos: noun, color: null}} -->

<!-- 私は、研究室の輪読で機械学習の手法だけでなく背景にある言語学にも触れました。
このおかげで、モデル自体ではなくモデルが何を実現したいのかを意識して -->
