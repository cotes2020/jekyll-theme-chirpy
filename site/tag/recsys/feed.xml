<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/recsys/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2022-05-22T19:09:32+09:00</updated>
  <id>http://localhost:4000/tag/recsys/feed.xml</id>

  
  
  

  
    <title type="html">Beanie in the wind | </title>
  

  
    <subtitle>Beanie &amp; Kaze&apos;s tech blog</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">(ì‘ì„±ì¤‘) RecSys &amp;amp; RL - Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System</title>
      <link href="http://localhost:4000/RecSys-&-RL-Top-%F0%9D%90%BE-Off-Policy-Correction-for-a-REINFORCE-Recommender-System" rel="alternate" type="text/html" title="(ì‘ì„±ì¤‘) RecSys &amp; RL - Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System" />
      <published>2022-05-20T16:03:00+09:00</published>
      <updated>2022-05-20T16:03:00+09:00</updated>
      <id>http://localhost:4000/RecSys%20&amp;%20RL%20-%20Top-%F0%9D%90%BE%20Off-Policy%20Correction%20for%20a%20REINFORCE%20Recommender%20System</id>
      <content type="html" xml:base="http://localhost:4000/RecSys-&amp;-RL-Top-%F0%9D%90%BE-Off-Policy-Correction-for-a-REINFORCE-Recommender-System">&lt;p&gt;ì´ì „ì— ì½ì—ˆë˜ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A Deep Reinforcement Learning Framework for News Recommendation&lt;/code&gt; ë…¼ë¬¸ì€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì½”ë“œë¥¼ ì°¾ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ê·¸ë˜ì„œ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œê°€ ê°™ì´ ìˆìœ¼ë©´ì„œ ì ë‹¹íˆ challengingí•œ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì°¾ì•„ë³´ì•˜ê³  ì´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System&lt;/code&gt; ì´ ê°€ì¥ ê´œì°®ì•„ë³´ì—¬ ì½ì–´ë³´ì•˜ë‹¤. ì´í›„ ì—¬ëŸ¬ ìƒ˜í”Œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ public datasetìœ¼ë¡œ ì§ì ‘ êµ¬í˜„ê¹Œì§€ í•´ë³¼ ì˜ˆì •ì´ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;ì£¼ìš”-contribution&quot;&gt;ì£¼ìš” Contribution&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì˜ ì£¼ìš”í•œ ê¸°ì—¬ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;REINFORCE Recommender&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Large, non-stationary state and action spaces&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Off-Policy Candidate Generation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;RLì—ì„œ ì¶”ì²œì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ë°ì´í„°ì˜ ë¶€ì¡±ì´ íŠ¹íˆ ë¬¸ì œê°€ ëœë‹¤. ê³ ì „ì ì¸ RLì—ì„œëŠ” ì´ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë§ì€ ì–‘ì˜ training dataë¥¼ self-replayë‚˜ simulationì„ í†µí•´ ìˆ˜ì§‘í•˜ì˜€ë‹¤.
  í•˜ì§€ë§Œ ì¶”ì²œì‹œìŠ¤í…œ ë¬¸ì œì˜ ê²½ìš°, ë³µì¡í•˜ê²Œ ì–½í˜€ìˆëŠ” ì¶”ì²œì‹œìŠ¤í…œ í™˜ê²½ ë•Œë¬¸ì— simulationì„ í†µí•˜ì—¬ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ rewardë¥¼ ê´€ì°°í•˜ëŠ” ê²ƒ ìì²´ê°€ ì‹¤ì œ ìœ ì €ì—ê²Œ ì‹¤ì œ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— íƒìƒ‰ë˜ì§€ ì•Šì€ ê³µê°„ì˜ state, action spaceì— ëŒ€í•˜ì—¬ rewardê°’ì„ ì‰½ê²Œ ì•Œì•„ë‚´ê¸° ì–´ë µë‹¤.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;
  Â &lt;/p&gt;

    &lt;p&gt;ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, &lt;strong&gt;ë‹¤ë¥¸ ì¶”ì²œì‹œìŠ¤í…œì´ë‚˜ ê³¼ê±°ì˜ policyì—ì„œ ì–»ì€ ìœ ì € feedbackì˜ ë¡œê·¸ë“¤ì„ í™œìš©í•˜ì—¬ Off-policy learningì„ ìˆ˜í–‰&lt;/strong&gt; í•œë‹¤. ì´ ë•Œ, ë‹¤ë¥¸ policyì—ì„œ ìˆ˜ì§‘í•˜ì˜€ê¸° ë•Œë¬¸ì— í•„ì—°ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” biasë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Top-K Off-Policy Correction&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ì‹¤ì œ ì¶”ì²œì‹œìŠ¤í…œ í™˜ê²½ì—ì„œëŠ” 1ê°œê°€ ì•„ë‹Œ ì—¬ëŸ¬ê°œì˜ ì¶”ì²œì„ ë™ì‹œì— ì œê³µí•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” top-K recommender systemì„ ìœ„í•œ ìƒˆë¡œìš´ top-K off-policy correctionì„ ì •ì˜í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Benefits in Live Experiments&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ì•„ì´í…œì— ëŒ€í•œ ìœ ì € ì„ í˜¸ë„ëŠ” ê³„ì† ë³€í•¨ -&amp;gt; ìœ ì € state ê°’ì´ ì§€ì†ì ìœ¼ë¡œ ë°”ë€ë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;reinforce-recommender&quot;&gt;Reinforce Recommender&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&quot;mdp-modeling&quot;&gt;MDP Modeling&lt;/h3&gt;
&lt;p&gt;ì¶”ì²œì‹œìŠ¤í…œì„ ê°•í™”í•™ìŠµ ì„¸íŒ…ì— ì í•©í•˜ê²Œ ë§ì¶°ë³´ì. ê°•í™”í•™ìŠµì„ ìœ„í•˜ì—¬ í™˜ê²½ì„ Markov Decision Process(MDP)ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$S$ : ìœ ì € stateë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—°ì†ì ì¸ state space&lt;/li&gt;
  &lt;li&gt;$A$ : ì¶”ì²œ ì•„ì´í…œ í›„ë³´êµ°ì„ í¬í•¨í•˜ê³  ìˆëŠ” discreteí•œ action space&lt;/li&gt;
  &lt;li&gt;$P$ : state transition probability ( $S \times A \times S \to \mathbb{R}$ )&lt;/li&gt;
  &lt;li&gt;$R$ : ë³´ìƒ í•¨ìˆ˜ ( $S \times A \to \mathbb{R}$ ), ì´ ë•Œ, $ r(s, a) $ ëŠ” ìœ ì € state sì—ì„œ action aë¥¼ í•  ë•Œ ë°”ë¡œ ì–»ì–´ì§€ëŠ” rewardë¥¼ ì˜ë¯¸í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;$ \rho_{0} $ : ì´ˆê¸° ìƒíƒœ ë¶„í¬&lt;/li&gt;
  &lt;li&gt;$ \gamma $ future rewardì— ëŒ€í•œ discount factor&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h3&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;off-policy-correction&quot;&gt;Off-Policy Correction&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;top-k-recommendation&quot;&gt;Top-K Recommendation&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;exploration&quot;&gt;Exploration&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Beanie</name>
        
        
      </author>

      

      
        <category term="RecSys" />
      
        <category term="RL" />
      
        <category term="paper" />
      

      
        <summary type="html">ì´ì „ì— ì½ì—ˆë˜ A Deep Reinforcement Learning Framework for News Recommendation ë…¼ë¬¸ì€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì½”ë“œë¥¼ ì°¾ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ê·¸ë˜ì„œ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œê°€ ê°™ì´ ìˆìœ¼ë©´ì„œ ì ë‹¹íˆ challengingí•œ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì°¾ì•„ë³´ì•˜ê³  ì´ Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System ì´ ê°€ì¥ ê´œì°®ì•„ë³´ì—¬ ì½ì–´ë³´ì•˜ë‹¤. ì´í›„ ì—¬ëŸ¬ ìƒ˜í”Œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ public datasetìœ¼ë¡œ ì§ì ‘ êµ¬í˜„ê¹Œì§€ í•´ë³¼ ì˜ˆì •ì´ë‹¤.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RecSys &amp;amp; RL - A Deep Reinforcement Learning Framework for News Recommendation</title>
      <link href="http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1" rel="alternate" type="text/html" title="RecSys &amp; RL - A Deep Reinforcement Learning Framework for News Recommendation" />
      <published>2022-05-01T17:32:00+09:00</published>
      <updated>2022-05-01T17:32:00+09:00</updated>
      <id>http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%20%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0-1</id>
      <content type="html" xml:base="http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1">&lt;p&gt;ì´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A Deep Reinforcement Learning Framework for News Recommendation&lt;/code&gt; ë…¼ë¬¸ì€ ê°•í™”í•™ìŠµì„ í†µí•˜ì—¬ ì„±ëŠ¥ ì¢‹ì€ ë‰´ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆë‹¤. 2018ë…„ì— ì‘ì„±ë˜ì—ˆìœ¼ë©° www í•™íšŒì— accept ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Â 
ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í™œìš©í•œ ê¸°ì¡´ ë‰´ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì·¨ì•½ì &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ë‹¤ì´ë‚˜ë¯¹í•˜ê²Œ ë³€í•˜ëŠ” ë‰´ìŠ¤ì˜ íŠ¹ì„±ê³¼ ë‰´ìŠ¤ì— ëŒ€í•œ ìœ ì €ì˜ ì„ í˜¸ë„ ë³€í™”ë¥¼ ê³ ë ¤í•  ë•Œ, online learning ì´ í•„ìš”í•˜ë‹¤. ê¸°ì¡´ì—ë„ ë‰´ìŠ¤ íŠ¹ì„±ê³¼ ìœ ì € ì„ í˜¸ë„ì˜ ë‹¤ì´ë‚˜ë¯¹í•œ ë³€í™”ë¥¼ ë°˜ì˜í•˜ëŠ” online recommendation modelì´ ìˆê¸°ëŠ” í–ˆì§€ë§Œ, ì´ ëª¨ë¸ë“¤ì€ í˜„ì¬ì˜ reward(e.g. Click Trhough Rate)ë§Œ ìµœì í™”í•˜ê¸° ë•Œë¬¸ì— í˜„ì¬ì˜ ì¶”ì²œì´ ë¯¸ë˜ì— ê°€ì ¸ì˜¬ íš¨ê³¼ê°€ ë¬´ì‹œë˜ì—ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ê¸°ì¡´ ì¶”ì²œ ì‹œìŠ¤í…œì€ ìœ ì € feedbackë¡œ ì˜¤ì§ click/no click ì •ë³´ë§Œ ê³ ë ¤í–ˆë‹¤. ë”°ë¼ì„œ ë‰´ìŠ¤ë¥¼ íƒìƒ‰í•˜ë©´ì„œ ì‹¤ìˆ˜ë¡œ ì˜ëª» ëˆ„ë¥¸ ê¸°ì‚¬ì™€ ì •ë§ ì½ê³  ì‹¶ì–´ì„œ ì°¾ì•„ì„œ ë“¤ì–´ê°„ ê¸°ì‚¬ì˜ rewardëŠ” í™•ì‹¤íˆ ë‹¬ë¼ì•¼í•˜ì§€ë§Œ ì´ëŸ¬í•œ íŠ¹ì„±ì€ ì¡ì•„ë‚´ì§€ ëª»í•˜ì˜€ë‹¤.&lt;/li&gt;
  &lt;li&gt;ê¸°ì¡´ ì¶”ì²œ ì‹œìŠ¤í…œì€ ìœ ì €ì—ê²Œ ìœ ì‚¬í•œ ë‰´ìŠ¤ë¥¼ ê³„ì† ì¶”ì²œí•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆë‹¤. ê·¸ë ‡ì§€ë§Œ ì´ë ‡ê²Œ ê³„ì† ë¹„ìŠ·í•œ ë‰´ìŠ¤ë¥¼ ì¶”ì²œí•´ì£¼ë©´ ì£¼ì œì— ëŒ€í•œ ì‚¬ìš©ìì˜ í¥ë¯¸ê°€ ì‰½ê²Œ ë–¨ì–´ì§„ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì´ ë…¼ë¬¸ì´ ì“°ì—¬ì§ˆ ë‹¹ì‹œ State-of-art ê°•í™”í•™ìŠµ ë°©ì‹ì€ ë§ì€ ê²½ìš° ê°„ë‹¨í•œ $ \epsilon - greedy $ ë°©ì‹ì´ë‚˜ Upper Confidence Bound (UCB)ë¥¼ í™œìš©í•´ì™”ë‹¤. í•˜ì§€ë§Œ $ \epsilon - greedy $ ì€ ìœ ì €ê°€ ì „í˜€ ê´€ì‹¬ì—†ì–´í•˜ëŠ” ì•„ì´í…œì„ ì¶”ì²œí•  ìˆ˜ë„ ìˆê³ , UCBëŠ” cold-starter ë¬¸ì œê°€ ì¡´ì¬í•œë‹¤.(ê¸°ì¡´ ë°ì´í„°ê°€ ë§ì´ ì—†ë‹¤ë©´ reward ì¶”ì²­ì´ ë¶€ì •í™•í•¨)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed method&lt;/h2&gt;
&lt;p&gt;Â 
ì´ëŸ¬í•œ ì·¨ì•½ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•˜ì—¬ ì´ ë…¼ë¬¸ì—ì„œëŠ” DQNì„ í™œìš©í•œ ë‹¤ìŒê³¼ ê°™ì€ ì¶”ì²œì‹œìŠ¤í…œì„ ì œì•ˆí•œë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Contextual Multi-Armed Bandit models
    &lt;ul&gt;
      &lt;li&gt;ìµœê·¼ì— ëª‡ëª‡ ì‚¬ëŒë“¤ì´ ë”ìš± ë³µì¡í•œ ìœ ì € ì•„ì´í…œ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ê¸° ìœ„í•˜ì—¬ banditì„ clustering based collaborative filteringì´ë‚˜ matrix factorizationê³¼ í•©ì¹˜ê³ , reward functionì„ ê²°ì •í•˜ê¸° ìœ„í•˜ì—¬ social network ê´€ê³„ë¥¼ í™œìš©í•˜ëŠ” ì‹œë„ë¥¼ í•˜ì˜€ë‹¤.&lt;/li&gt;
      &lt;li&gt;í•˜ì§€ë§Œ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ëª¨ë¸ì€ ì´ì „ ëª¨ë¸ê³¼ëŠ” ë‹¤ë¥¸ë°, ë…¼ë¬¸ì˜ ëª¨ë¸ì€ Markov Decision Process (MDP)ë¥¼ ì ìš©í•˜ì—¬, ëª¨ë¸ì˜ future rewardë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Markov Decision Process models
    &lt;ul&gt;
      &lt;li&gt;ìœ ì €ì™€ ì•„ì´í…œ(ë‰´ìŠ¤)ê°„ì˜ ë³µì¡í•œ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ëŒ€ì‹ ì—, online news recommendationì˜ ë‹¤ì´ë‚˜ë¯¹í•œ íŠ¹ì„±ì— ì§‘ì¤‘í•˜ì—¬ future rewardë¥¼ ëª¨ë¸ë§í•œë‹¤.
(ê¸°ì¡´ì˜ Marti-Armed Bandit (MAB) ë°©ë²•ë“¤ê³¼ ë‹¤ë¦„)&lt;/li&gt;
      &lt;li&gt;ë” ë‚˜ì•„ê°€ MDP frameworkë¥¼ continuous stateì™€ action representationì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆê³ , ëª¨ë“  (state, action, reward) tupleì„ í™œìš©í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
(ê¸°ì¡´ì˜ MAP ë°©ë²•ë“¤ê³¼ ë‹¤ë¦„)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ë˜í•œ, ì¶”ì²œì— ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•˜ì—¬ Exploration strategyë¡œ Dueling Bandit Gradient Descent exploration strategyë¥¼ í™œìš©í•˜ëŠ”ë°, ì—¬ê¸°ì—ëŠ” ë‘ ê°€ì§€ ì´ìœ ê°€ ìˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;recommendation ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•¨&lt;/li&gt;
  &lt;li&gt;ê³ ì „ì ì¸ exploration strategies (ex&amp;gt; e-greedy, Upper Confidence Bound)ë¡œ ë¶€í„° ë°œìƒí•˜ëŠ” recommendation accuracy ê°ì†Œë¥¼ ë§‰ê¸° ìœ„í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-framework&quot;&gt;Model framework&lt;/h3&gt;
&lt;div style=&quot;text-align: left&quot;&gt;
   &lt;img src=&quot;/assets/img/post_images/recsys1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;ìœ„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ë…¼ë¬¸ì˜ ëª¨ë¸ì€ offline ë°©ì‹ê³¼ online ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì ¸ ìˆë‹¤.
offline stageì—ì„œëŠ” ë¯¸ë¦¬ ìˆ˜ì§‘í•´ë‘” ë°ì´í„°(user-news click logs)ì— Deep Q-Networkë¥¼ ì ìš©í•˜ì—¬ rewardë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ offline ë°©ì‹ì„ í†µí•˜ì—¬ 4ê°€ì§€ì˜ feature(News, User, User news, Context)ê°€ ì¶”ì¶œëœë‹¤.
ê·¸ëŸ° ë‹¤ìŒ, online stageì—ì„œëŠ” ì‹¤ì œ ì‘ë™í•˜ëŠ” online serviceë¥¼ í†µí•˜ì—¬ recommendation agent Gê°€ ì§ì ‘ ìœ ì €ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° networkë¥¼ ì—…ë°ì´íŠ¸ í•´ë‚˜ê°„ë‹¤.
ì˜¨ë¼ì¸ ì—…ë°ì´íŠ¸ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;(1) PUSH:
ìœ ì €ê°€ ë‰´ìŠ¤ ìš”ì²­ì„ ë³´ë‚´ë©´, recommendation agent GëŠ” í•´ë‹¹ ìœ ì €ì˜ featuresì™€ ë‰´ìŠ¤ í›„ë³´êµ°(í˜„ì¬ ì¶”ì²œ ë°›ì€ ë¦¬ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ ë¬´ì‘ìœ„ ì¶”ì¶œ)ì„ ë°›ì•„ ì¶”ì²œí•  top-k ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.
(2) FEEDBACK:
ìœ ì €ëŠ” ë‰´ìŠ¤ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ê³  ê°ê°ì˜ ì•„ì´í…œì„ í´ë¦­í•˜ê±°ë‚˜, í´ë¦­í•˜ì§€ ì•ŠìŒìœ¼ë¡œì„œ feedbackì„ ì¤€ë‹¤.
(3) MINOR UPDATE
ê° timestampë§ˆë‹¤ exploitation networkì™€ exploration networkë¥¼ ë¹„êµí•˜ì—¬, ë§Œì•½ exploration networkì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ë©´, í˜„ì¬ networkë¥¼ exploration networkìª½ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ê³ , ë°˜ëŒ€ë¡œ exploitation networkì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ë©´ ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
(4) MAJOR UPDATE
íŠ¹ì • ì •í•´ë‘” ì‹œê°„ì´ ì§€ë‚˜ê³  ë‚œ í›„, &lt;strong&gt;experience replay&lt;/strong&gt; techniqueì„ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. ì¢€ ë” ìì„¸í•˜ê²Œ, ì‚¬ìš©ì feedbackê³¼ ë©”ëª¨ë¦¬ì— ì €ì¥ ëœ user activenessë¥¼ ì¶”ê°€í•œë‹¤. (agentê°€ ìµœê·¼ì˜ click, activeness ê¸°ë¡ì„ ìœ ì§€)&lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;h3 id=&quot;offline-part&quot;&gt;Offline Part&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ìƒì—… ë‰´ìŠ¤ ì¶”ì²œ ì•±ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;online-part&quot;&gt;Online Part&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;offline dataë¡œ ëª¨ë¸ì„ pre-train&lt;/li&gt;
  &lt;li&gt;ì‹¤í—˜ìš© ì•±ì„ ë°°í¬í•˜ì—¬ í•œë‹¬ ë™ì•ˆ ìš´ì˜. ì‹¤í—˜êµ°ì„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  ê° ê·¸ë£¹ì— í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë¡œ ì¶”ì²œëœ ë‰´ìŠ¤ë¥¼ ë³´ì—¬ì¤€ë‹¤.&lt;/li&gt;
  &lt;li&gt;ìœ ì €ë¡œë¶€í„° ë‰´ìŠ¤ ìš”ì²­ì´ ë“¤ì–´ê°ˆ ë•Œë§ˆë‹¤ ë‰´ìŠ¤ë¥¼ ì¶”ì²œí•´ì£¼ê³  ì´ë“¤ ë‰´ìŠ¤ì— ëŒ€í•œ ìœ ì € í”¼ë“œë°±(click or not)ì´ ê¸°ë¡ëœë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overall-experiment-flow&quot;&gt;Overall experiment flow&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;ë¨¼ì € offline dataë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.&lt;/li&gt;
  &lt;li&gt;Online ë‰´ìŠ¤ ì•±ì— ê°™ì€ ë¹„ìœ¨ë¡œ ê°ê° ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë¸ì„ ë°°ì •í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;Onlineìœ¼ë¡œ ë°°í¬í•œ ë‰´ìŠ¤ ì•±ì— ìœ ì €ê°€ ë“¤ì–´ì™€ì„œ ë‰´ìŠ¤ë¥¼ ìš”ì²­í•˜ë©´, candidates ê¸°ì‚¬ ì„¸íŠ¸ë¥¼ ë°°ì •ëœ ëª¨ë¸ì— ë³´ë‚¸ë‹¤.&lt;/li&gt;
  &lt;li&gt;ëª¨ë¸ì„ Current networkì™€ Explore networkë¡œ ë‚˜ëˆ„ì–´ candidates ê¸°ì‚¬ inputì„ ì ìš©í•œë‹¤. (PUSH ê³¼ì •)
    &lt;ul&gt;
      &lt;li&gt;ì´ ë•Œ, Explore networkëŠ” current network ëª¨ë¸ì—ì„œì˜ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°ë¥¼ Gaussian Random Noiseê¸°ë²•ì„ í†µí•´ ì—…ë°ì´íŠ¸í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&quot;text-align: left&quot;&gt;
    &lt;img src=&quot;/assets/img/post_images/drn2.png&quot; width=&quot;70%&quot; /&gt;
  &lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;ë‘ ëª¨ë¸ì—ì„œ ë°˜í™˜í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì³ ìœ ì €ì˜ rewardê°€ ë°œìƒí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ë‘ ëª¨ë¸ì˜ rewardë¥¼ ë¹„êµí•œë‹¤. (FEEDBACK ê³¼ì •)&lt;/li&gt;
  &lt;li&gt;ë§Œì•½ Explore networkì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ë©´, Explore networkì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ìŒ stepì˜ íŒŒë¼ë¯¸í„°ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤. (MINOR UPDATE ê³¼ì •)&lt;/li&gt;
  &lt;li&gt;Major update ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì„¤ì •ëœ ì‹œê°„ì´ ë˜ê¸° ì „ê¹Œì§€ 2~6 ê³¼ì •ì„ ë°˜ë³µí•œë‹¤. ì´ ê³¼ì •ì—ì„œ user activenessì— ëŒ€í•œ ì •ë³´ë¥¼ memoryì— ì¶”ê°€í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;ë©”ëª¨ë¦¬ì— ìˆëŠ” ë°ì´í„°(recent historical click &amp;amp; user activeness records) ì¤‘ batch sizeë§Œí¼ ìƒ˜í”Œë§í•´ì„œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•œë‹¤. (MAJOR UPDATE ê³¼ì •)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ì‹¤í—˜ ê²°ê³¼, ë…¼ë¬¸ì˜ ë°©ë²•ìœ¼ë¡œ ì¶”ì²œ ì •í™•ë„ì™€ ì¶”ì²œ ë‹¤ì–‘ì„±ì„ ìƒë‹¹íˆ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ì´í›„ ì‹¤í—˜ì—ì„œ ìœ ì €ë¥¼ ì—¬ëŸ¬ ê·¸ë£¹ì„ ë‚˜ëˆ„ì–´ì„œ(heavy users and one-time users ë“±) ëª¨ë¸ì„ ë””ìì¸í•˜ë©´ ë” ì˜ë¯¸ìˆì„ ê²ƒì´ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ë§Œì•½ ê° ìœ ì € ê·¸ë£¹ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì´ ë°œê²¬ë˜ë©´ ë” ë§ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Â &lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;references-&quot;&gt;references :&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://jisoo-coding.tistory.com/27&quot;&gt;https://jisoo-coding.tistory.com/27&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Beanie</name>
        
        
      </author>

      

      
        <category term="RecSys" />
      
        <category term="RL" />
      
        <category term="paper" />
      

      
        <summary type="html">ì´ A Deep Reinforcement Learning Framework for News Recommendation ë…¼ë¬¸ì€ ê°•í™”í•™ìŠµì„ í†µí•˜ì—¬ ì„±ëŠ¥ ì¢‹ì€ ë‰´ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆë‹¤. 2018ë…„ì— ì‘ì„±ë˜ì—ˆìœ¼ë©° www í•™íšŒì— accept ë˜ì—ˆë‹¤.</summary>
      

      
      
    </entry>
  
</feed>
