<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/paper/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2022-05-22T19:09:32+09:00</updated>
  <id>http://localhost:4000/tag/paper/feed.xml</id>

  
  
  

  
    <title type="html">Beanie in the wind | </title>
  

  
    <subtitle>Beanie &amp; Kaze&apos;s tech blog</subtitle>
  

  

  
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">(ì‘ì„±ì¤‘) RecSys &amp;amp; RL - Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System</title>
      <link href="http://localhost:4000/RecSys-&-RL-Top-%F0%9D%90%BE-Off-Policy-Correction-for-a-REINFORCE-Recommender-System" rel="alternate" type="text/html" title="(ì‘ì„±ì¤‘) RecSys &amp; RL - Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System" />
      <published>2022-05-20T16:03:00+09:00</published>
      <updated>2022-05-20T16:03:00+09:00</updated>
      <id>http://localhost:4000/RecSys%20&amp;%20RL%20-%20Top-%F0%9D%90%BE%20Off-Policy%20Correction%20for%20a%20REINFORCE%20Recommender%20System</id>
      <content type="html" xml:base="http://localhost:4000/RecSys-&amp;-RL-Top-%F0%9D%90%BE-Off-Policy-Correction-for-a-REINFORCE-Recommender-System">&lt;p&gt;ì´ì „ì— ì½ì—ˆë˜ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A Deep Reinforcement Learning Framework for News Recommendation&lt;/code&gt; ë…¼ë¬¸ì€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì½”ë“œë¥¼ ì°¾ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ê·¸ë˜ì„œ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œê°€ ê°™ì´ ìˆìœ¼ë©´ì„œ ì ë‹¹íˆ challengingí•œ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì°¾ì•„ë³´ì•˜ê³  ì´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System&lt;/code&gt; ì´ ê°€ì¥ ê´œì°®ì•„ë³´ì—¬ ì½ì–´ë³´ì•˜ë‹¤. ì´í›„ ì—¬ëŸ¬ ìƒ˜í”Œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ public datasetìœ¼ë¡œ ì§ì ‘ êµ¬í˜„ê¹Œì§€ í•´ë³¼ ì˜ˆì •ì´ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;ì£¼ìš”-contribution&quot;&gt;ì£¼ìš” Contribution&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì˜ ì£¼ìš”í•œ ê¸°ì—¬ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;REINFORCE Recommender&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Large, non-stationary state and action spaces&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Off-Policy Candidate Generation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;RLì—ì„œ ì¶”ì²œì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ë°ì´í„°ì˜ ë¶€ì¡±ì´ íŠ¹íˆ ë¬¸ì œê°€ ëœë‹¤. ê³ ì „ì ì¸ RLì—ì„œëŠ” ì´ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë§ì€ ì–‘ì˜ training dataë¥¼ self-replayë‚˜ simulationì„ í†µí•´ ìˆ˜ì§‘í•˜ì˜€ë‹¤.
  í•˜ì§€ë§Œ ì¶”ì²œì‹œìŠ¤í…œ ë¬¸ì œì˜ ê²½ìš°, ë³µì¡í•˜ê²Œ ì–½í˜€ìˆëŠ” ì¶”ì²œì‹œìŠ¤í…œ í™˜ê²½ ë•Œë¬¸ì— simulationì„ í†µí•˜ì—¬ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ rewardë¥¼ ê´€ì°°í•˜ëŠ” ê²ƒ ìì²´ê°€ ì‹¤ì œ ìœ ì €ì—ê²Œ ì‹¤ì œ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— íƒìƒ‰ë˜ì§€ ì•Šì€ ê³µê°„ì˜ state, action spaceì— ëŒ€í•˜ì—¬ rewardê°’ì„ ì‰½ê²Œ ì•Œì•„ë‚´ê¸° ì–´ë µë‹¤.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;
  Â &lt;/p&gt;

    &lt;p&gt;ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, &lt;strong&gt;ë‹¤ë¥¸ ì¶”ì²œì‹œìŠ¤í…œì´ë‚˜ ê³¼ê±°ì˜ policyì—ì„œ ì–»ì€ ìœ ì € feedbackì˜ ë¡œê·¸ë“¤ì„ í™œìš©í•˜ì—¬ Off-policy learningì„ ìˆ˜í–‰&lt;/strong&gt; í•œë‹¤. ì´ ë•Œ, ë‹¤ë¥¸ policyì—ì„œ ìˆ˜ì§‘í•˜ì˜€ê¸° ë•Œë¬¸ì— í•„ì—°ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” biasë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Top-K Off-Policy Correction&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ì‹¤ì œ ì¶”ì²œì‹œìŠ¤í…œ í™˜ê²½ì—ì„œëŠ” 1ê°œê°€ ì•„ë‹Œ ì—¬ëŸ¬ê°œì˜ ì¶”ì²œì„ ë™ì‹œì— ì œê³µí•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” top-K recommender systemì„ ìœ„í•œ ìƒˆë¡œìš´ top-K off-policy correctionì„ ì •ì˜í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Benefits in Live Experiments&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ì•„ì´í…œì— ëŒ€í•œ ìœ ì € ì„ í˜¸ë„ëŠ” ê³„ì† ë³€í•¨ -&amp;gt; ìœ ì € state ê°’ì´ ì§€ì†ì ìœ¼ë¡œ ë°”ë€ë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;reinforce-recommender&quot;&gt;Reinforce Recommender&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;h3 id=&quot;mdp-modeling&quot;&gt;MDP Modeling&lt;/h3&gt;
&lt;p&gt;ì¶”ì²œì‹œìŠ¤í…œì„ ê°•í™”í•™ìŠµ ì„¸íŒ…ì— ì í•©í•˜ê²Œ ë§ì¶°ë³´ì. ê°•í™”í•™ìŠµì„ ìœ„í•˜ì—¬ í™˜ê²½ì„ Markov Decision Process(MDP)ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$S$ : ìœ ì € stateë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—°ì†ì ì¸ state space&lt;/li&gt;
  &lt;li&gt;$A$ : ì¶”ì²œ ì•„ì´í…œ í›„ë³´êµ°ì„ í¬í•¨í•˜ê³  ìˆëŠ” discreteí•œ action space&lt;/li&gt;
  &lt;li&gt;$P$ : state transition probability ( $S \times A \times S \to \mathbb{R}$ )&lt;/li&gt;
  &lt;li&gt;$R$ : ë³´ìƒ í•¨ìˆ˜ ( $S \times A \to \mathbb{R}$ ), ì´ ë•Œ, $ r(s, a) $ ëŠ” ìœ ì € state sì—ì„œ action aë¥¼ í•  ë•Œ ë°”ë¡œ ì–»ì–´ì§€ëŠ” rewardë¥¼ ì˜ë¯¸í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;$ \rho_{0} $ : ì´ˆê¸° ìƒíƒœ ë¶„í¬&lt;/li&gt;
  &lt;li&gt;$ \gamma $ future rewardì— ëŒ€í•œ discount factor&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h3&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;off-policy-correction&quot;&gt;Off-Policy Correction&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;top-k-recommendation&quot;&gt;Top-K Recommendation&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;exploration&quot;&gt;Exploration&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;experiment-results&quot;&gt;Experiment Results&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Beanie</name>
        
        
      </author>

      

      
        <category term="RecSys" />
      
        <category term="RL" />
      
        <category term="paper" />
      

      
        <summary type="html">ì´ì „ì— ì½ì—ˆë˜ A Deep Reinforcement Learning Framework for News Recommendation ë…¼ë¬¸ì€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì½”ë“œë¥¼ ì°¾ê¸°ê°€ ì–´ë ¤ì› ë‹¤. ê·¸ë˜ì„œ ì•Œê³ ë¦¬ì¦˜ ì½”ë“œê°€ ê°™ì´ ìˆìœ¼ë©´ì„œ ì ë‹¹íˆ challengingí•œ ë…¼ë¬¸ì„ ë‹¤ì‹œ ì°¾ì•„ë³´ì•˜ê³  ì´ Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System ì´ ê°€ì¥ ê´œì°®ì•„ë³´ì—¬ ì½ì–´ë³´ì•˜ë‹¤. ì´í›„ ì—¬ëŸ¬ ìƒ˜í”Œ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ public datasetìœ¼ë¡œ ì§ì ‘ êµ¬í˜„ê¹Œì§€ í•´ë³¼ ì˜ˆì •ì´ë‹¤.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RecSys &amp;amp; RL - A Deep Reinforcement Learning Framework for News Recommendation</title>
      <link href="http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1" rel="alternate" type="text/html" title="RecSys &amp; RL - A Deep Reinforcement Learning Framework for News Recommendation" />
      <published>2022-05-01T17:32:00+09:00</published>
      <updated>2022-05-01T17:32:00+09:00</updated>
      <id>http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%20%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0-1</id>
      <content type="html" xml:base="http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-1">&lt;p&gt;ì´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A Deep Reinforcement Learning Framework for News Recommendation&lt;/code&gt; ë…¼ë¬¸ì€ ê°•í™”í•™ìŠµì„ í†µí•˜ì—¬ ì„±ëŠ¥ ì¢‹ì€ ë‰´ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆë‹¤. 2018ë…„ì— ì‘ì„±ë˜ì—ˆìœ¼ë©° www í•™íšŒì— accept ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Â 
ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í™œìš©í•œ ê¸°ì¡´ ë‰´ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì·¨ì•½ì &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ë‹¤ì´ë‚˜ë¯¹í•˜ê²Œ ë³€í•˜ëŠ” ë‰´ìŠ¤ì˜ íŠ¹ì„±ê³¼ ë‰´ìŠ¤ì— ëŒ€í•œ ìœ ì €ì˜ ì„ í˜¸ë„ ë³€í™”ë¥¼ ê³ ë ¤í•  ë•Œ, online learning ì´ í•„ìš”í•˜ë‹¤. ê¸°ì¡´ì—ë„ ë‰´ìŠ¤ íŠ¹ì„±ê³¼ ìœ ì € ì„ í˜¸ë„ì˜ ë‹¤ì´ë‚˜ë¯¹í•œ ë³€í™”ë¥¼ ë°˜ì˜í•˜ëŠ” online recommendation modelì´ ìˆê¸°ëŠ” í–ˆì§€ë§Œ, ì´ ëª¨ë¸ë“¤ì€ í˜„ì¬ì˜ reward(e.g. Click Trhough Rate)ë§Œ ìµœì í™”í•˜ê¸° ë•Œë¬¸ì— í˜„ì¬ì˜ ì¶”ì²œì´ ë¯¸ë˜ì— ê°€ì ¸ì˜¬ íš¨ê³¼ê°€ ë¬´ì‹œë˜ì—ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ê¸°ì¡´ ì¶”ì²œ ì‹œìŠ¤í…œì€ ìœ ì € feedbackë¡œ ì˜¤ì§ click/no click ì •ë³´ë§Œ ê³ ë ¤í–ˆë‹¤. ë”°ë¼ì„œ ë‰´ìŠ¤ë¥¼ íƒìƒ‰í•˜ë©´ì„œ ì‹¤ìˆ˜ë¡œ ì˜ëª» ëˆ„ë¥¸ ê¸°ì‚¬ì™€ ì •ë§ ì½ê³  ì‹¶ì–´ì„œ ì°¾ì•„ì„œ ë“¤ì–´ê°„ ê¸°ì‚¬ì˜ rewardëŠ” í™•ì‹¤íˆ ë‹¬ë¼ì•¼í•˜ì§€ë§Œ ì´ëŸ¬í•œ íŠ¹ì„±ì€ ì¡ì•„ë‚´ì§€ ëª»í•˜ì˜€ë‹¤.&lt;/li&gt;
  &lt;li&gt;ê¸°ì¡´ ì¶”ì²œ ì‹œìŠ¤í…œì€ ìœ ì €ì—ê²Œ ìœ ì‚¬í•œ ë‰´ìŠ¤ë¥¼ ê³„ì† ì¶”ì²œí•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆë‹¤. ê·¸ë ‡ì§€ë§Œ ì´ë ‡ê²Œ ê³„ì† ë¹„ìŠ·í•œ ë‰´ìŠ¤ë¥¼ ì¶”ì²œí•´ì£¼ë©´ ì£¼ì œì— ëŒ€í•œ ì‚¬ìš©ìì˜ í¥ë¯¸ê°€ ì‰½ê²Œ ë–¨ì–´ì§„ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì´ ë…¼ë¬¸ì´ ì“°ì—¬ì§ˆ ë‹¹ì‹œ State-of-art ê°•í™”í•™ìŠµ ë°©ì‹ì€ ë§ì€ ê²½ìš° ê°„ë‹¨í•œ $ \epsilon - greedy $ ë°©ì‹ì´ë‚˜ Upper Confidence Bound (UCB)ë¥¼ í™œìš©í•´ì™”ë‹¤. í•˜ì§€ë§Œ $ \epsilon - greedy $ ì€ ìœ ì €ê°€ ì „í˜€ ê´€ì‹¬ì—†ì–´í•˜ëŠ” ì•„ì´í…œì„ ì¶”ì²œí•  ìˆ˜ë„ ìˆê³ , UCBëŠ” cold-starter ë¬¸ì œê°€ ì¡´ì¬í•œë‹¤.(ê¸°ì¡´ ë°ì´í„°ê°€ ë§ì´ ì—†ë‹¤ë©´ reward ì¶”ì²­ì´ ë¶€ì •í™•í•¨)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed method&lt;/h2&gt;
&lt;p&gt;Â 
ì´ëŸ¬í•œ ì·¨ì•½ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•˜ì—¬ ì´ ë…¼ë¬¸ì—ì„œëŠ” DQNì„ í™œìš©í•œ ë‹¤ìŒê³¼ ê°™ì€ ì¶”ì²œì‹œìŠ¤í…œì„ ì œì•ˆí•œë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Contextual Multi-Armed Bandit models
    &lt;ul&gt;
      &lt;li&gt;ìµœê·¼ì— ëª‡ëª‡ ì‚¬ëŒë“¤ì´ ë”ìš± ë³µì¡í•œ ìœ ì € ì•„ì´í…œ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ê¸° ìœ„í•˜ì—¬ banditì„ clustering based collaborative filteringì´ë‚˜ matrix factorizationê³¼ í•©ì¹˜ê³ , reward functionì„ ê²°ì •í•˜ê¸° ìœ„í•˜ì—¬ social network ê´€ê³„ë¥¼ í™œìš©í•˜ëŠ” ì‹œë„ë¥¼ í•˜ì˜€ë‹¤.&lt;/li&gt;
      &lt;li&gt;í•˜ì§€ë§Œ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ëª¨ë¸ì€ ì´ì „ ëª¨ë¸ê³¼ëŠ” ë‹¤ë¥¸ë°, ë…¼ë¬¸ì˜ ëª¨ë¸ì€ Markov Decision Process (MDP)ë¥¼ ì ìš©í•˜ì—¬, ëª¨ë¸ì˜ future rewardë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Markov Decision Process models
    &lt;ul&gt;
      &lt;li&gt;ìœ ì €ì™€ ì•„ì´í…œ(ë‰´ìŠ¤)ê°„ì˜ ë³µì¡í•œ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ëŒ€ì‹ ì—, online news recommendationì˜ ë‹¤ì´ë‚˜ë¯¹í•œ íŠ¹ì„±ì— ì§‘ì¤‘í•˜ì—¬ future rewardë¥¼ ëª¨ë¸ë§í•œë‹¤.
(ê¸°ì¡´ì˜ Marti-Armed Bandit (MAB) ë°©ë²•ë“¤ê³¼ ë‹¤ë¦„)&lt;/li&gt;
      &lt;li&gt;ë” ë‚˜ì•„ê°€ MDP frameworkë¥¼ continuous stateì™€ action representationì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆê³ , ëª¨ë“  (state, action, reward) tupleì„ í™œìš©í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
(ê¸°ì¡´ì˜ MAP ë°©ë²•ë“¤ê³¼ ë‹¤ë¦„)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ë˜í•œ, ì¶”ì²œì— ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•˜ì—¬ Exploration strategyë¡œ Dueling Bandit Gradient Descent exploration strategyë¥¼ í™œìš©í•˜ëŠ”ë°, ì—¬ê¸°ì—ëŠ” ë‘ ê°€ì§€ ì´ìœ ê°€ ìˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;recommendation ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•¨&lt;/li&gt;
  &lt;li&gt;ê³ ì „ì ì¸ exploration strategies (ex&amp;gt; e-greedy, Upper Confidence Bound)ë¡œ ë¶€í„° ë°œìƒí•˜ëŠ” recommendation accuracy ê°ì†Œë¥¼ ë§‰ê¸° ìœ„í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-framework&quot;&gt;Model framework&lt;/h3&gt;
&lt;div style=&quot;text-align: left&quot;&gt;
   &lt;img src=&quot;/assets/img/post_images/recsys1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;ìœ„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ë…¼ë¬¸ì˜ ëª¨ë¸ì€ offline ë°©ì‹ê³¼ online ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì ¸ ìˆë‹¤.
offline stageì—ì„œëŠ” ë¯¸ë¦¬ ìˆ˜ì§‘í•´ë‘” ë°ì´í„°(user-news click logs)ì— Deep Q-Networkë¥¼ ì ìš©í•˜ì—¬ rewardë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ offline ë°©ì‹ì„ í†µí•˜ì—¬ 4ê°€ì§€ì˜ feature(News, User, User news, Context)ê°€ ì¶”ì¶œëœë‹¤.
ê·¸ëŸ° ë‹¤ìŒ, online stageì—ì„œëŠ” ì‹¤ì œ ì‘ë™í•˜ëŠ” online serviceë¥¼ í†µí•˜ì—¬ recommendation agent Gê°€ ì§ì ‘ ìœ ì €ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° networkë¥¼ ì—…ë°ì´íŠ¸ í•´ë‚˜ê°„ë‹¤.
ì˜¨ë¼ì¸ ì—…ë°ì´íŠ¸ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;(1) PUSH:
ìœ ì €ê°€ ë‰´ìŠ¤ ìš”ì²­ì„ ë³´ë‚´ë©´, recommendation agent GëŠ” í•´ë‹¹ ìœ ì €ì˜ featuresì™€ ë‰´ìŠ¤ í›„ë³´êµ°(í˜„ì¬ ì¶”ì²œ ë°›ì€ ë¦¬ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ ë¬´ì‘ìœ„ ì¶”ì¶œ)ì„ ë°›ì•„ ì¶”ì²œí•  top-k ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.
(2) FEEDBACK:
ìœ ì €ëŠ” ë‰´ìŠ¤ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ê³  ê°ê°ì˜ ì•„ì´í…œì„ í´ë¦­í•˜ê±°ë‚˜, í´ë¦­í•˜ì§€ ì•ŠìŒìœ¼ë¡œì„œ feedbackì„ ì¤€ë‹¤.
(3) MINOR UPDATE
ê° timestampë§ˆë‹¤ exploitation networkì™€ exploration networkë¥¼ ë¹„êµí•˜ì—¬, ë§Œì•½ exploration networkì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ë©´, í˜„ì¬ networkë¥¼ exploration networkìª½ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ê³ , ë°˜ëŒ€ë¡œ exploitation networkì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ë©´ ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
(4) MAJOR UPDATE
íŠ¹ì • ì •í•´ë‘” ì‹œê°„ì´ ì§€ë‚˜ê³  ë‚œ í›„, &lt;strong&gt;experience replay&lt;/strong&gt; techniqueì„ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. ì¢€ ë” ìì„¸í•˜ê²Œ, ì‚¬ìš©ì feedbackê³¼ ë©”ëª¨ë¦¬ì— ì €ì¥ ëœ user activenessë¥¼ ì¶”ê°€í•œë‹¤. (agentê°€ ìµœê·¼ì˜ click, activeness ê¸°ë¡ì„ ìœ ì§€)&lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;h3 id=&quot;offline-part&quot;&gt;Offline Part&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ìƒì—… ë‰´ìŠ¤ ì¶”ì²œ ì•±ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;online-part&quot;&gt;Online Part&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;offline dataë¡œ ëª¨ë¸ì„ pre-train&lt;/li&gt;
  &lt;li&gt;ì‹¤í—˜ìš© ì•±ì„ ë°°í¬í•˜ì—¬ í•œë‹¬ ë™ì•ˆ ìš´ì˜. ì‹¤í—˜êµ°ì„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  ê° ê·¸ë£¹ì— í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë¡œ ì¶”ì²œëœ ë‰´ìŠ¤ë¥¼ ë³´ì—¬ì¤€ë‹¤.&lt;/li&gt;
  &lt;li&gt;ìœ ì €ë¡œë¶€í„° ë‰´ìŠ¤ ìš”ì²­ì´ ë“¤ì–´ê°ˆ ë•Œë§ˆë‹¤ ë‰´ìŠ¤ë¥¼ ì¶”ì²œí•´ì£¼ê³  ì´ë“¤ ë‰´ìŠ¤ì— ëŒ€í•œ ìœ ì € í”¼ë“œë°±(click or not)ì´ ê¸°ë¡ëœë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overall-experiment-flow&quot;&gt;Overall experiment flow&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;ë¨¼ì € offline dataë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.&lt;/li&gt;
  &lt;li&gt;Online ë‰´ìŠ¤ ì•±ì— ê°™ì€ ë¹„ìœ¨ë¡œ ê°ê° ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë¸ì„ ë°°ì •í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;Onlineìœ¼ë¡œ ë°°í¬í•œ ë‰´ìŠ¤ ì•±ì— ìœ ì €ê°€ ë“¤ì–´ì™€ì„œ ë‰´ìŠ¤ë¥¼ ìš”ì²­í•˜ë©´, candidates ê¸°ì‚¬ ì„¸íŠ¸ë¥¼ ë°°ì •ëœ ëª¨ë¸ì— ë³´ë‚¸ë‹¤.&lt;/li&gt;
  &lt;li&gt;ëª¨ë¸ì„ Current networkì™€ Explore networkë¡œ ë‚˜ëˆ„ì–´ candidates ê¸°ì‚¬ inputì„ ì ìš©í•œë‹¤. (PUSH ê³¼ì •)
    &lt;ul&gt;
      &lt;li&gt;ì´ ë•Œ, Explore networkëŠ” current network ëª¨ë¸ì—ì„œì˜ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°ë¥¼ Gaussian Random Noiseê¸°ë²•ì„ í†µí•´ ì—…ë°ì´íŠ¸í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&quot;text-align: left&quot;&gt;
    &lt;img src=&quot;/assets/img/post_images/drn2.png&quot; width=&quot;70%&quot; /&gt;
  &lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;ë‘ ëª¨ë¸ì—ì„œ ë°˜í™˜í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì³ ìœ ì €ì˜ rewardê°€ ë°œìƒí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ë‘ ëª¨ë¸ì˜ rewardë¥¼ ë¹„êµí•œë‹¤. (FEEDBACK ê³¼ì •)&lt;/li&gt;
  &lt;li&gt;ë§Œì•½ Explore networkì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤ë©´, Explore networkì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ìŒ stepì˜ íŒŒë¼ë¯¸í„°ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤. (MINOR UPDATE ê³¼ì •)&lt;/li&gt;
  &lt;li&gt;Major update ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì„¤ì •ëœ ì‹œê°„ì´ ë˜ê¸° ì „ê¹Œì§€ 2~6 ê³¼ì •ì„ ë°˜ë³µí•œë‹¤. ì´ ê³¼ì •ì—ì„œ user activenessì— ëŒ€í•œ ì •ë³´ë¥¼ memoryì— ì¶”ê°€í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;ë©”ëª¨ë¦¬ì— ìˆëŠ” ë°ì´í„°(recent historical click &amp;amp; user activeness records) ì¤‘ batch sizeë§Œí¼ ìƒ˜í”Œë§í•´ì„œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•œë‹¤. (MAJOR UPDATE ê³¼ì •)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Â &lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ì‹¤í—˜ ê²°ê³¼, ë…¼ë¬¸ì˜ ë°©ë²•ìœ¼ë¡œ ì¶”ì²œ ì •í™•ë„ì™€ ì¶”ì²œ ë‹¤ì–‘ì„±ì„ ìƒë‹¹íˆ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ì´í›„ ì‹¤í—˜ì—ì„œ ìœ ì €ë¥¼ ì—¬ëŸ¬ ê·¸ë£¹ì„ ë‚˜ëˆ„ì–´ì„œ(heavy users and one-time users ë“±) ëª¨ë¸ì„ ë””ìì¸í•˜ë©´ ë” ì˜ë¯¸ìˆì„ ê²ƒì´ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ë§Œì•½ ê° ìœ ì € ê·¸ë£¹ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì´ ë°œê²¬ë˜ë©´ ë” ë§ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Â &lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;references-&quot;&gt;references :&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://jisoo-coding.tistory.com/27&quot;&gt;https://jisoo-coding.tistory.com/27&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Beanie</name>
        
        
      </author>

      

      
        <category term="RecSys" />
      
        <category term="RL" />
      
        <category term="paper" />
      

      
        <summary type="html">ì´ A Deep Reinforcement Learning Framework for News Recommendation ë…¼ë¬¸ì€ ê°•í™”í•™ìŠµì„ í†µí•˜ì—¬ ì„±ëŠ¥ ì¢‹ì€ ë‰´ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆë‹¤. 2018ë…„ì— ì‘ì„±ë˜ì—ˆìœ¼ë©° www í•™íšŒì— accept ë˜ì—ˆë‹¤.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RL - Autonomous Driving Based on Modified SAC Algorithm through Imitation Learning Pre-training</title>
      <link href="http://localhost:4000/RL-Autonomous-Driving-Based-on-Modified-SAC-Algorithm-through-Imitation-Learning-Pre-training" rel="alternate" type="text/html" title="RL - Autonomous Driving Based on Modified SAC Algorithm through Imitation Learning Pre-training" />
      <published>2022-04-30T17:32:00+09:00</published>
      <updated>2022-04-30T17:32:00+09:00</updated>
      <id>http://localhost:4000/RL%20-%20Autonomous%20Driving%20Based%20on%20Modified%20SAC%20Algorithm%20through%20Imitation%20Learning%20Pre-training</id>
      <content type="html" xml:base="http://localhost:4000/RL-Autonomous-Driving-Based-on-Modified-SAC-Algorithm-through-Imitation-Learning-Pre-training">&lt;p&gt;This paper is a follow-up to the paper written 2 years ago. Although the core idea is the same, since the previously used DDPG algorithm did not converge well, SAC was used instead.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This paper is based on the previous study with very similar settings but difference
model.&lt;/li&gt;
  &lt;li&gt;The DDPG algorithm(previously used) seems to have not much high convergence
during training&lt;/li&gt;
  &lt;li&gt;Instead, use SAC(Soft Actor Critic) algorithm
    &lt;ul&gt;
      &lt;li&gt;Robustness, stability and well-convergence&lt;/li&gt;
      &lt;li&gt;State-of-the-art off-policy actor critic deep reinforcement learning algorithm based on the maximum entropy reinforcement learning framework&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed method&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Single-Q SAC Algorithm : use SAC with some slight differences due to the method of combining imitation learning and reinforcement learning
    &lt;ul&gt;
      &lt;li&gt;The original SAC has two target parameters for Q-function&lt;/li&gt;
      &lt;li&gt;However, with previous experiment, the average return over 5 runs of 3 million iterations of SAC algorithm with double-A and SAC algorithm with single-Q are quite similar, so they only use one target parameter for each network and do not update the temperature parameter Î± for simplicity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Although using pure SAC can result in a good performance, it still has a lower
average accumulated reward after 100 episodes than their method.
    &lt;div style=&quot;text-align: left&quot;&gt;
  &lt;img src=&quot;/assets/img/post_images/sc3.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Beanie</name>
        
        
      </author>

      

      
        <category term="Autonomous driving" />
      
        <category term="RL" />
      
        <category term="paper" />
      

      
        <summary type="html">This paper is a follow-up to the paper written 2 years ago. Although the core idea is the same, since the previously used DDPG algorithm did not converge well, SAC was used instead.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RL - Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Drivings</title>
      <link href="http://localhost:4000/RL-Improved-Reinforcement-Learning-through-Imitation-Learning-Pretraining-Towards-Image-based-Autonomous-Driving" rel="alternate" type="text/html" title="RL - Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Drivings" />
      <published>2022-04-30T10:32:00+09:00</published>
      <updated>2022-04-30T10:32:00+09:00</updated>
      <id>http://localhost:4000/RL%20-%20Improved%20Reinforcement%20Learning%20through%20Imitation%20Learning%20Pretraining%20Towards%20Image-based%20Autonomous%20Driving</id>
      <content type="html" xml:base="http://localhost:4000/RL-Improved-Reinforcement-Learning-through-Imitation-Learning-Pretraining-Towards-Image-based-Autonomous-Driving">&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Drivings&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Autonomous Driving Based on Modified SAC Algotirhm through Imitation Learning Pre-training&lt;/code&gt;, which will be covered in the next article, were written in the lab of Prof. Dong Eui Chang at KAIST and deal with how to improve autonomous driving learning using reinforcement learning.&lt;/p&gt;

&lt;p&gt;More specifically, in the image-based autonomous driving task, imitation learning and reinforcement learning are combined and used together to utilize each othersâ€™ strength. The difference between the two papers is that the first paper uses DDPG as reinforcement learning whereas the second paper uses SAC.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The problem of generating control decision given a monocular camera image and other auxiliary vehicle sensorsâ€™ outputs has aroused a lot of interest in recent years.&lt;/li&gt;
  &lt;li&gt;Conducting imitation learning towards given human policy might produce a relative well-performed policy
    &lt;ul&gt;
      &lt;li&gt;However, limitation on the possible policy performance and the potential lack of generalizability for unseen situations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Instead, using RL in a simulator and then transfer the learned policy to real world is very popular in recent research.
    &lt;ul&gt;
      &lt;li&gt;Nonetheless, when the state and action spaces are huge or high dimensional, pure RL algorithm training process usually diverges.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;They present a method which combines these two methods and thus leverage both their advantages for this image-based autonomous driving task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed method&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;
Â 
(1) pre-training(imitation learning) -&amp;gt; (2) fine-tuning(DDPG)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Network architecture
    &lt;div style=&quot;text-align: left&quot;&gt;
  &lt;img src=&quot;/assets/img/post_images/rl_driving1_1.png&quot; width=&quot;70%&quot; /&gt;
&lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;ResNet-34 architecture
        &lt;ul&gt;
          &lt;li&gt;One of CNN network for deep reinforcement learning tasks like environmental perception, model compression, and object detection&lt;/li&gt;
          &lt;li&gt;Use ResNet-34 as the backbone structure for both actor and critic networks&lt;/li&gt;
          &lt;li&gt;The simple and light ResNet-34 out performs other architectures with attention module integrated in terms of both final testing loss and the generated policyâ€™s performance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Imitation learning phase
    &lt;ul&gt;
      &lt;li&gt;Aim to generate a good initial policy&lt;/li&gt;
      &lt;li&gt;Consider current image input and current vehicle speed as our current state inputs, and throttle, brake from zero to one and steering from minus one to positive one as our control outputs&lt;/li&gt;
      &lt;li&gt;Use Huber loss : to less sensitive to outliers and can make the training process more stable compared to MSE as a result.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement learning phase
    &lt;ul&gt;
      &lt;li&gt;Reward : distance to the nearest obstacle and current velocity
        &lt;div style=&quot;text-align: left&quot;&gt;
  &lt;img src=&quot;/assets/img/post_images/rl_driving1_2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;Continuous and deterministic policy setting : to prevent possible catastrophic effect under undesired behavior.&lt;/li&gt;
      &lt;li&gt;In order to combine with imitation learning, they make several changes on the original DDPG
        &lt;ul&gt;
          &lt;li&gt;actor, critic networks
            &lt;ul&gt;
              &lt;li&gt;actor network : give all the weights of the pre-trained actor network to the actor networks used in the DDPG.&lt;/li&gt;
              &lt;li&gt;critic network : give all the weights except the final fully connected layer to critic networks used in the DDPG.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;replay pool : train actor and critic networks using the samples collected under imitation learningâ€™s generated policy. Then, use normal DDPG to collect new experience&lt;/li&gt;
          &lt;li&gt;removing OU noise&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;Â &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Imitation learning phase
    &lt;ul&gt;
      &lt;li&gt;Use an Xbox controller to control the car under various weather and lighting conditions and record the data using Airsim API&lt;/li&gt;
      &lt;li&gt;1000 for each episode and terminates when a collision happens.&lt;/li&gt;
      &lt;li&gt;Randomly choose the weather condition and the initial position of the car before each episode starts and calculate a 5 episodesâ€™ average accumulated reward as the criteria.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RL phase
    &lt;ul&gt;
      &lt;li&gt;Same setting as imitation learnin phase&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Results
    &lt;div style=&quot;text-align: left&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;/assets/img/post_images/rl_driving1_3.png&quot; /&gt;
&lt;/div&gt;
    &lt;ul&gt;
      &lt;li&gt;Eventually, their proposed method achieves a considerable performance boost from the original imitation learningâ€™s learned policy while the pure DDPG never performs well and does not show any improving trend.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Beanie</name>
        
        
      </author>

      

      
        <category term="Autonomous driving" />
      
        <category term="RL" />
      
        <category term="paper" />
      

      
        <summary type="html">Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Drivings and Autonomous Driving Based on Modified SAC Algotirhm through Imitation Learning Pre-training, which will be covered in the next article, were written in the lab of Prof. Dong Eui Chang at KAIST and deal with how to improve autonomous driving learning using reinforcement learning.</summary>
      

      
      
    </entry>
  
</feed>
